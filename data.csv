,id,cell_id,source,ancestor_id,pct_rank
1,9169c4e9c33c90,7f26da4c,"# Amazon Top 50 EDA

For a quick synopsis, check out the [Summary](#Summary) section which includes all the main insights.",725bf880,0.0
3,916ccf243827f1,61e7635a,"#### Deep Neural Network written in Python for the MNSIT handwritten dataset from scratch without using any deep learning frameworks. I have implemented Droupout technique for regularisation and Mini-batch, Adams optimisation for optimising the gradient descent and Sigmoid is used as an activation function.",5147f4d2,0.0
4,09bac0c221388e,76aca5da,# Summarizing Medical Documents using spacy,bea4aa2e,0.0
5,917957c6c4065f,c9d49c2b,# 유튜브 인기 동영상 데이터 분석,55b8ed68,0.0
6,918040fad252ec,fc1ed504,**Import** file training dari Kaggle input,966fcd8f,0.0
8,921fff7d3040db,92c64ae5,# 1. Setup environment,5f36ced9,0.0
9,923e97b05be00b,7cfb8ed3,"# **""A Deep Learning Radiology Cookbook""**: using fast.ai on the NIH CXR14 dataset

Welcome to this kernel! This kernel is adapated/inspired from the very excellent ""Hello World Deep Learning"" [kernel](https://www.kaggle.com/wfwiggins203/hello-world-deep-learning-siim) by Walter Wiggins (2018-2019). The goal of this presentation is to show radiologists \[**without** technical backgrounds\] how it's simple to create AI models that can find patterns in medical images, and hopefully empower radiologists to make their own.

We will use **fast.ai** and the **NIH CXR14 [dataset](https://nihcc.app.box.com/v/ChestXray-NIHCC)** to come up with our own world class machine learning models in 60 minutes or less.

# Overview

![image.png](https://i.imgur.com/WebTYSX.jpg)

For this presentation, we will break down the process into four steps (image credit [ACR AI-LAB](https://ailab.acr.org/)):

1. **Defining a problem:** what do we want our cxr model to look for? For this we'll need to think of pathology you might find in a chest x-ray.
2. **Data preparation:** for this we'll talk about how to manipulate our data in a way the model can understand.
3. **Configuring a model:** we'll use a technology called fast.ai to create our model.
4. **Train and test:** we'll talk about some important considerations when creating a model like this.

# How to use this kernel

To explore this model and data set, please <kbd>Copy and Edit</kbd> the notebook in the menu above (i.e. create a copy under your Kaggle profile). 

>When you get into the draft environment, please ensure that you see **""GPU on""** and **""Internet on""** under **Settings** so you can utilize the cloud GPU for faster model training.

In this Notebook editing environment, each block of text is referred to as a **cell**.  Cells containing formatted text are **Markdown** cells, as they use the *Markdown* formatting language. Similarly, cells containing code are **code** cells.

Clicking within a cell will allow you to edit the content of that cell (a.k.a. enter **edit mode**). You can also navigate between cells with the arrow keys. Note that the appearance of Markdown cells will change when you enter edit mode.

You can **run code cells** (and format Markdown cells) as you go along by clicking within the cell and then clicking the **blue button with *one* arrow** next to the cell or at the bottom of the window. You can also use the keyboard shortcut <kbd>SHIFT</kbd> + <kbd>ENTER</kbd> (press both keys at the same time).

Let's try this out by **running** the cell below. This will help us load the technologies we need to power our model.",3a4a22dd,0.0
10,9276fa5cc2fef6,2f0888dd,"## Why  Feature Engineering ?
It might be obvious that model learns the patterns and interactions while it trains. So, how does the engineering comes into the place. Also, there is a line where we should leave some interactions for models to find out and not hand label features. The need for feature engineering;

![Image](https://www.analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png)

<sup>Source:  analyticsindiamag </sup>

1.  Helps the model to catch those exceptions when you engineer a significant interaction
2. The model converges faster if you happen to find good set of features
3. When you have new source of information. A chance to make better model. You engineer features!

Let's start with basic imports and loading the dataset...
",24aa6a52,0.0
11,9289395e9c480f,83ee7b10,"# This is a notebook that address therapeutics, interventions, and clinical studies.",55d03d67,0.0
12,92e9fc3a0ff5c0,b70e8d0d,## **Importing Libraries**,d53da425,0.0
14,9395559895004f,20e949bf,## Import Library,b5a0494b,0.0
15,93f5423667b9d5,e693acab,# Data Loading,55bdf071,0.0
16,9456df44ab9308,70284c6d,# Pose Estimate,a8e94298,0.0
17,9519b558f7baec,95544b04,"A different way to do this problem is to use cuts, train a bunch of binary classifiers and then feed them into linalg or scipy optimize.  Note this is not optimized; I just slapped it together for learning! ;)",e32ab82f,0.0
22,09cabbffb7909b,a15811d7,"# TPS Feb 2021
Starter Notebook

## Deleverables
1. EDA
    - What's going on?
    - Show me the data...
2. Model
    - Baseline...
    - Simple...
    - Evaluation...
    - Improvement...
3. RAPIDS Bonus
    - Apply RAPIDS ([Starter Notebook](https://www.kaggle.com/tunguz/tps-feb-2021-rapids-starter))
    - Replace pandas with [cuDF](https://github.com/rapidsai/cudf) & sklearn with [cuML](https://github.com/rapidsai/cuml)
    - Context: [What is RAPIDS?](https://medium.com/future-vision/what-is-rapids-ai-7e552d80a1d2?source=friends_link&sk=64b79c363beeffb9923e16482f3977cc)
    
    
#### Troubleshooting
- [Data](https://www.kaggle.com/c/tabular-playground-series-feb-2021/data)
- [Overview](https://www.kaggle.com/c/tabular-playground-series-feb-2021/overview)
- [RF Starter Notebook](https://www.kaggle.com/warobson/tps-feb-2021-rf-starter)
- [ML repo on GitHub](https://github.com/gumdropsteve/intro_to_machine_learning)
- [Most simple RAPIDS Notebook submission](https://www.kaggle.com/warobson/simple-rapids-live) (Has stuff like `train_test_split()` with cuml..)
    
#### Load Data",835687fc,0.0
23,098fedfcd07456,a8d4a0cd,"## Domain Neural Network Using Convolutional neural network
1.  98% Accuracy with CNN in Classifing 60,000 Images
1. Objective : Classifying hand written Images .
1. More information on dataset can be found here . http://yann.lecun.com/exdb/mnist/
1. There are 60,000 Train and 10000 Test Images train images have labels .",052ece26,0.0
24,91473a39b85068,ef482da4,"# What is StackOverflow
### Description
##### Stack Overflow is the largest, most trusted online community for developers to learn, share their programming knowledge, and build their careers. It is something which every programmer use one way or another. Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. It features questions and answers on a wide range of topics in computer programming. The website serves as a platform for users to ask and answer questions, and, through membership and active participation, to vote questions and answers up or down and edit questions and answers in a fashion similar to a wiki or Digg. As of April 2014 Stack Overflow has over 4,000,000 registered users, and it exceeded 10,000,000 questions in late August 2015. Based on the type of tags assigned to questions, the top eight most discussed topics on the site are: Java, JavaScript, C#, PHP, Android, jQuery, Python and HTML. https://stackoverflow.com/.

### Business Problem
The problem says that we will be provided a bunch of questions. A question in Stack Overflow contains three segments Title, Description and Tags. By using the text in the title and description we should suggest the tags related to the subject of the question automatically. These tags are extremely important for the proper working of Stack Overflow.",6e3d91c2,0.0
25,90ead00a8ee283,c9fab23d,"# Introduction
Welcome to the **[Learn Pandas](https://www.kaggle.com/learn/pandas)** track. These hands-on exercises are targeted for someone who has worked with Pandas a little before. 
Each page it's own list of `relevant resources` you can use if you get stumped. The top item in each list has been custom-made to help you with the exercises on that page.

The first step in most data analytics projects is reading the data file. In this section, you'll create `Series` and `DataFrame` objects, both by hand and by reading data files.

# Relevant Resources
* ** [Creating, Reading and Writing Reference](https://www.kaggle.com/residentmario/creating-reading-and-writing-reference)**
* [General Pandas Cheat Sheet](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)

# Set Up
**First, Fork this notebook using the button towards the top of the screen.  Then you can run and edit code in the cells below.**

Run the code cell below to load libraries you will need (including coad to check your answers).",612efa48,0.0
27,8bb432d338a70b,ddaf8a8b,Carga de librerías,7aab1dfd,0.0
28,8c7e00ca3dc5a7,15f58ac7,"The following kernel does some of the simple but essential steps to perform the Regression Analysis. By doing the it, it gives nice performence which puts the kernel in top 20% with the score of 0.12023 at the time of submission.",c83346e4,0.0
29,8cd6656a65e6e7,ae26264d,"<div style=""width: 100%"">
    <img style=""width: 100%"" src=""https://storage.googleapis.com/kaggle-datasets-images/681739/1196904/5c9764c44d37ca06ae29daeaa405e3a3/dataset-cover.jpg""/>
</div>",c8e1697a,0.0
31,8d0aebab1e5914,4d415c90,"----
----

# My Other Notebook:
* [Simple Linear Regression](https://www.kaggle.com/mukeshmanral/linear-regression-basic)
* [Multiple Linear Regression](https://www.kaggle.com/mukeshmanral/multiple-linear-regression-basic)
* [Polynomial Regression](https://www.kaggle.com/mukeshmanral/polynomial-regression-basic)
* [Advanced Linear Regression](https://www.kaggle.com/mukeshmanral/advance-linear-regression-basic-gridsearchcv-hpt)

----
* [Feature Engineering 1](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-1)
* [Feature Engineering 2](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-2)
* [Feature Engineering 3](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-3)
* [Feature Engineering 4](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-4)

----

* [How KNN-Algorith(1) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-1-basic)
* [How KNN-Algorith(2) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-2-basic)

____

* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https://www.kaggle.com/mukeshmanral/bagging-ensemble-concept-rf-extree-basic) 

____
____",084e671f,0.0
34,8dbf17f707ef20,172c3fe8,"# Навчання рекурентних нейронних мереж засобами TensorFlow
**Самошин Андрій КА-83**",624ce794,0.0
35,8dd655515e7d18,3e9a607c,## Personality Traits Analysis of Celebrity Twitter Accounts,895f41cf,0.0
36,8ddaa0c6c395ec,2ee5deef,"# Mask RCNN Example

Using MatterPort with Keras: https://github.com/matterport/Mask_RCNN

Based on https://towardsdatascience.com/object-detection-using-mask-r-cnn-on-a-custom-dataset-4f79ab692f6d
",9fccabdc,0.0
37,8e9d63e1f6319e,ef6fcdd5,## Problem Description,4743b346,0.0
38,8ec771f5600a61,68d17377,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load",48364c1f,0.0
40,8f50c9c16db95f,a69b748e,"# Table of content

* [Introduction](#intro)
* [Kickoff as a special team play](#kickoff)
* [Outcomes of a kickoff](#outcome)
* [Gained yards by kicking teams](#gainedyards)
* [Kick the ball far, if you want to gain more yards](#far)
* [Initial analysis - How to kick the ball far](#initial)
* [Variable 1 - Foot speed](#speed)
* [Variable 2 - Body orientation](#orientation)
* [Conclusion and future research](#conclusion)
* [Reference](#reference)",26cc763a,0.0
41,904ba7392f2271,f8cd65f5,"* Single Neural Network version: https://www.kaggle.com/binhlc/jane-street-tensorflow-dense
* Highest Scored: 7736
* Time: 2.5 hours on GPU",780079a8,0.0
42,90691864eb68c7,fe9d9bb1,"The variable in the 19th column 'Sold' is the output variable to be predicted. All other variables are to be used as the predictor variables.
We are going to predict if the House can be sold or Not.",3555ef9b,0.0
43,0a1fcda859252c,4938b589,"Hello everyone!! Hope everything is fine and you are enjoying things on Kaggle as usual. The rage for competing on Kaggle should never end. 
Machine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn't that simple. The stake is very high. It's more than just a `classification` problem. But if applied very carefully, it can benefit the world in enormous ways. **And as a Machine learning engineer, it's our responsibility to help people as much as we can in all possible ways.**

Pneumonia is a very common disease. It can be either: 1) Bacterial pneumonia  2) Viral Pneumonia  3) Mycoplasma pneumonia   and 4) Fungal pneumonia.
This dataset consists pneumonia samples belonging to the first two classes.  The dataset consists of only very few samples and that too unbalanced. The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. We all know that deep learning models are data hungry but if you know how things work, you can build good models even with a limited amount of data. ",13a38774,0.0
44,907f08f9a2c6cf,4dae36b2,"# Captchas Classification
## By Sergei Issaev
### Introduction
This notebook is part of a pipeline which takes in CAPTCHA images and outputs the solution. This is the third notebook in the four part series. In this notebook, we will develop an MNIST-like model for digit recognition, with the goal of training a model to classify all of the segmented alphanumerics accurately. For a more complete description, please see my article published at: https://medium.com/@sergei740.
### Import Libraries 📚⬇",aa84c325,0.0
46,90964081c7faab,b9eca9ef,# Bank Marketing,b423b0c3,0.0
47,912bb73358069c,c41a702e,"I joined this competition due to [impressive kernel](https://www.kaggle.com/its7171/gru-lstm-with-feature-engineering-and-augmentation) last week. But the pain began since I transferred [Keras into Pytorch](https://www.kaggle.com/daishu/why-keras-is-better-than-pytorch).

In my experiments, CV is always worse(about 0.02) in Pytorch than Keras. Because the difference couldn't be found, I cannot sleep well, but tonight, it is a sweet dream.",0cf9db82,0.0
49,957e035ba5b9d5,e35b8fe4,"<h1>Table of Contents<span class=""tocSkip""></span></h1>
<div class=""toc""><ul class=""toc-item""><li><span><a href=""#Final-Project-Submission"" data-toc-modified-id=""Final-Project-Submission-1""><span class=""toc-item-num"">1&nbsp;&nbsp;</span>Final Project Submission</a></span></li><li><span><a href=""#Import-necessary-libraries"" data-toc-modified-id=""Import-necessary-libraries-2""><span class=""toc-item-num"">2&nbsp;&nbsp;</span>Import necessary libraries</a></span></li><li><span><a href=""#Dataset-Prep"" data-toc-modified-id=""Dataset-Prep-3""><span class=""toc-item-num"">3&nbsp;&nbsp;</span>Dataset Prep</a></span><ul class=""toc-item""><li><span><a href=""#Checking-for-invalid-images"" data-toc-modified-id=""Checking-for-invalid-images-3.1""><span class=""toc-item-num"">3.1&nbsp;&nbsp;</span>Checking for invalid images</a></span></li></ul></li><li><span><a href=""#Read-in-Data"" data-toc-modified-id=""Read-in-Data-4""><span class=""toc-item-num"">4&nbsp;&nbsp;</span>Read in Data</a></span><ul class=""toc-item""><li><span><a href=""#What-is-the-distribution-across-the-categories?"" data-toc-modified-id=""What-is-the-distribution-across-the-categories?-4.1""><span class=""toc-item-num"">4.1&nbsp;&nbsp;</span>What is the distribution across the categories?</a></span></li><li><span><a href=""#Calculate-number-of-images-in-train,-test-and-validation"" data-toc-modified-id=""Calculate-number-of-images-in-train,-test-and-validation-4.2""><span class=""toc-item-num"">4.2&nbsp;&nbsp;</span>Calculate number of images in train, test and validation</a></span></li></ul></li><li><span><a href=""#Preprocessing"" data-toc-modified-id=""Preprocessing-5""><span class=""toc-item-num"">5&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=""toc-item""><li><span><a href=""#Create-keras-model"" data-toc-modified-id=""Create-keras-model-5.1""><span class=""toc-item-num"">5.1&nbsp;&nbsp;</span>Create keras model</a></span></li><li><span><a href=""#Save-model"" data-toc-modified-id=""Save-model-5.2""><span class=""toc-item-num"">5.2&nbsp;&nbsp;</span>Save model</a></span></li><li><span><a href=""#Visualize-training-history"" data-toc-modified-id=""Visualize-training-history-5.3""><span class=""toc-item-num"">5.3&nbsp;&nbsp;</span>Visualize training history</a></span></li><li><span><a href=""#Evalute-test-data"" data-toc-modified-id=""Evalute-test-data-5.4""><span class=""toc-item-num"">5.4&nbsp;&nbsp;</span>Evalute test data</a></span></li></ul></li><li><span><a href=""#Pre-Trained-Network-Part-1"" data-toc-modified-id=""Pre-Trained-Network-Part-1-6""><span class=""toc-item-num"">6&nbsp;&nbsp;</span>Pre-Trained Network Part 1</a></span><ul class=""toc-item""><li><span><a href=""#Fine-tuning-the-network"" data-toc-modified-id=""Fine-tuning-the-network-6.1""><span class=""toc-item-num"">6.1&nbsp;&nbsp;</span>Fine-tuning the network</a></span></li><li><span><a href=""#Save-model"" data-toc-modified-id=""Save-model-6.2""><span class=""toc-item-num"">6.2&nbsp;&nbsp;</span>Save model</a></span></li><li><span><a href=""#Visualize-training-history"" data-toc-modified-id=""Visualize-training-history-6.3""><span class=""toc-item-num"">6.3&nbsp;&nbsp;</span>Visualize training history</a></span></li><li><span><a href=""#Evaluate-test-data"" data-toc-modified-id=""Evaluate-test-data-6.4""><span class=""toc-item-num"">6.4&nbsp;&nbsp;</span>Evaluate test data</a></span></li></ul></li><li><span><a href=""#Pre-Trained-Network-Part-2-(Experimental)"" data-toc-modified-id=""Pre-Trained-Network-Part-2-(Experimental)-7""><span class=""toc-item-num"">7&nbsp;&nbsp;</span>Pre-Trained Network Part 2 (Experimental)</a></span><ul class=""toc-item""><li><span><a href=""#Visualize-training-history"" data-toc-modified-id=""Visualize-training-history-7.1""><span class=""toc-item-num"">7.1&nbsp;&nbsp;</span>Visualize training history</a></span></li><li><span><a href=""#Evaluate-test-data"" data-toc-modified-id=""Evaluate-test-data-7.2""><span class=""toc-item-num"">7.2&nbsp;&nbsp;</span>Evaluate test data</a></span></li><li><span><a href=""#TODO"" data-toc-modified-id=""TODO-7.3""><span class=""toc-item-num"">7.3&nbsp;&nbsp;</span>TODO</a></span></li></ul></li><li><span><a href=""#train_test_split-approach"" data-toc-modified-id=""train_test_split-approach-8""><span class=""toc-item-num"">8&nbsp;&nbsp;</span>train_test_split approach</a></span><ul class=""toc-item""><li><span><a href=""#Creating-a-tuple-of-all-images-and-its-category"" data-toc-modified-id=""Creating-a-tuple-of-all-images-and-its-category-8.1""><span class=""toc-item-num"">8.1&nbsp;&nbsp;</span>Creating a tuple of all images and its category</a></span></li><li><span><a href=""#Process-data"" data-toc-modified-id=""Process-data-8.2""><span class=""toc-item-num"">8.2&nbsp;&nbsp;</span>Process data</a></span></li><li><span><a href=""#Build-Sequential-Model-and-fit"" data-toc-modified-id=""Build-Sequential-Model-and-fit-8.3""><span class=""toc-item-num"">8.3&nbsp;&nbsp;</span>Build Sequential Model and fit</a></span></li><li><span><a href=""#Visualize-Loss/Accuracy"" data-toc-modified-id=""Visualize-Loss/Accuracy-8.4""><span class=""toc-item-num"">8.4&nbsp;&nbsp;</span>Visualize Loss/Accuracy</a></span></li><li><span><a href=""#Display-confusion-matrix"" data-toc-modified-id=""Display-confusion-matrix-8.5""><span class=""toc-item-num"">8.5&nbsp;&nbsp;</span>Display confusion matrix</a></span></li></ul></li><li><span><a href=""#flow_from_dataframe-approach"" data-toc-modified-id=""flow_from_dataframe-approach-9""><span class=""toc-item-num"">9&nbsp;&nbsp;</span>flow_from_dataframe approach</a></span><ul class=""toc-item""><li><span><a href=""#Create-dataframes-from-directories"" data-toc-modified-id=""Create-dataframes-from-directories-9.1""><span class=""toc-item-num"">9.1&nbsp;&nbsp;</span>Create dataframes from directories</a></span></li><li><span><a href=""#Create-generators"" data-toc-modified-id=""Create-generators-9.2""><span class=""toc-item-num"">9.2&nbsp;&nbsp;</span>Create generators</a></span></li><li><span><a href=""#Build-Sequential-Model-and-fit"" data-toc-modified-id=""Build-Sequential-Model-and-fit-9.3""><span class=""toc-item-num"">9.3&nbsp;&nbsp;</span>Build Sequential Model and fit</a></span></li><li><span><a href=""#Predict-and-save-results-in-csv-file"" data-toc-modified-id=""Predict-and-save-results-in-csv-file-9.4""><span class=""toc-item-num"">9.4&nbsp;&nbsp;</span>Predict and save results in csv file</a></span></li><li><span><a href=""#Multi-label-classification-with-a-Multi-Output-Model"" data-toc-modified-id=""Multi-label-classification-with-a-Multi-Output-Model-9.5""><span class=""toc-item-num"">9.5&nbsp;&nbsp;</span>Multi-label classification with a Multi-Output Model</a></span></li></ul></li><li><span><a href=""#Conclusion"" data-toc-modified-id=""Conclusion-10""><span class=""toc-item-num"">10&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>",778ab3d3,0.0
51,9c26c5dcd46a25,1438bb98,"# <font color=""#114b98"">Conception d'une application au service de la santé publique</font>
![Sante-publique-France-logo.png](attachment:3eae8bc6-bfb7-4fb7-8b97-95939f0b030b.png)

## <font color=""#00afe6"">Analyse exploratoire des données</font>

Après avoir nettoyé les données issues de la base OpenFoodFacts dans le notebook PSanté_01_nettoyage, nous avons exporté un nouveau dataset cleané que nous allons ici pouvoir analyser. 

Nous réaliserons ici des analyses univariées, bivariées et multivariées ainsi qu'une réduction dimensionnelle.",1bbbb677,0.0
53,9ca9a30fc69d9b,26999353,# International football resluts from 1872-2017,f715c2e5,0.0
55,9ceb7278784462,1b5234c9,"# Table of contents
<a href='#0'> Dataset Introduction </a> <br>
<a href='#1'>1. Importing Libraries </a> <br>
<a href='#2'>2. Data </a> <br>
<a href='#3'>3. Exploratory Data Analysis </a> <br>
<a href='#4'>3.1 Missing Value </a> <br>
<a href='#5'>3.2 Correlation Matrix </a> <br>
<a href='#6'>3.3 Describe Function </a> <br>
<a href='#7'>4. Ratings </a> <br>
<a href='#8'>5. Type Of The Book  </a> <br>
<a href='#9'>6. Reviews </a> <br>
<a href='#10'>7. Price </a> <br>
<a href='#11'>8. Number Of Pages </a> <br>
<a href='#12'>9. Word Cloud </a> <br>
<a href='#13'>10. Data Preprocessing </a> <br>
<a href='#14'>10.1 Local Outlier Factor </a> <br>
<a href='#15'>11. Sample Data </a> <br>
<a href='#16'>12. Linear Regression  </a> <br>
<a href='#17'>13.SVR  </a> <br>
<a href='#18'>14.KNN </a> <br>
<a href='#19'>15.MLP Regressor </a> <br>
<a href='#20'>16.Decision Tree Regressor </a> <br>
<a href='#21'>17.Bagging Regressor </a> <br>
<a href='#22'>18.Random Forests </a> <br>
<a href='#23'>19.Gradient Boosting Regressor </a> <br>
<a href='#24'>20.Xgboost </a> <br>
<a href='#25'>21.Model Comparison </a> <br>
<a href='#26'>22.Conclusion </a> <br>
<a href='#27'>23.End Note </a> <br>",3768a567,0.0
56,9cec5ddf8b6f49,286365e7,# 1. Preparation,d39fc8e7,0.0
58,9d561aa4a298f3,e65656d2,"# Introduction

One of the most dangerous feature of COVID-19 and any virus is the capacity to mutate frequently. The short reproduction time, the relative large R factor, the widespread presence in almost all parts of the World facilitated the aparition of multiple mutations. Those, especially the ones with increased transmission rate, put under increased presure the medical response to COVID-19 Pandemics, even after vaccination programmes started.",f56bdd1c,0.0
60,9daf8b4a46725e,85798935,# Churn Risk Score Prediction,7d9cc411,0.0
61,0932046e1f485d,de0a95e7,**Introduction**,218cc7a3,0.0
62,9e27af2600925c,91ad8c1d,"# Logistic Regression with a Neural Network mindset

Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.

**Instructions:**
- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.

**You will learn to:**
- Build the general architecture of a learning algorithm, including:
    - Initializing parameters
    - Calculating the cost function and its gradient
    - Using an optimization algorithm (gradient descent) 
- Gather all three functions above into a main model function, in the right order.",9b556435,0.0
64,9ec2fb131cf677,0c97ef12,"# History of TED TALKS
![](https://09c449efca3bbeb52dcea716-ddjaey2ypcfdo.netdna-ssl.com/wp-content/uploads/2017/03/TED.gif)
* [TED](https://www.ted.com/about/our-organization) was **born in 1984 out of Richard Saul Wurman's observation of a powerful convergence among three fields: technology, entertainment and design.**
* The first **TED, which he co-founded with Harry Marks,** included a demo of the compact disc, the e-book and cutting-edge 3D graphics from Lucasfilm, while mathematician Benoit Mandelbrot demonstrated how to map coastlines using his developing theory of fractal geometry.
* But despite a **stellar lineup,** the event lost money, and it was six years before **Wurman and Marks tried again.** This time, in 1990, the world was ready. 
* The **TED Conference became an annual event in Monterey, California**, attracting a growing and influential audience from many different disciplines united by their curiosity and open-mindedness -- and also by their shared discovery of an exciting secret. (Back then, TED was an invitation-only event. It is not now -- you're welcome and encouraged to apply to attend.)

# Introduction
- TED is a nonprofit devoted to spreading ideas, usually in the form of short, powerful talks (18 minutes or less). TED began in 1984 as a conference where Technology, Entertainment and Design converged, and today covers almost all topics — from science to business to global issues — in more than 100 languages. Meanwhile, independently run TEDx events help share ideas in communities around the world.",211ea6bd,0.0
65,9ed43a550802d7,cf290468,"In this notebook I created a .csv file, which consists of 10 columns: labels for the type of disease,the image id, the path of the .dcm file in the file system, the coordinates and size of the bounding boxes.The entries in the table are repeated because there can be several bounding boxes for one picture. Coordinates of bounding boxes are calculated for the size of pictures 512х512. You can change the coordinates and sizes of the bounding boxes by making elementary proportions :)",1225fb02,0.0
67,9f0ccf5b9e8f03,227779c1,"The WHO said on Friday (15 may 2020) that it is studying a possible link between COVID-19 and Kawasaki disease, an inflammatory syndrome that has affected some children around the world.  Then I decide to upload a Dataset about Kawasaki disease.
“The initial hypotheses indicate that this syndrome may be linked to COVID-19 (…). We call on all doctors worldwide to work with their national authorities and WHO to be alert and better understand this syndrome in children, ”said WHO Director-General, Tedros Adhanom Ghebreyesus, at a virtual press conference. in Geneva. 
""It is crucial to characterize this clinical syndrome accurately and urgently, to understand its causality and describe treatment protocols,"" he added. 
https://translate.google.com.br/translate?hl=en&sl=pt&u=https://istoe.com.br/oms-diz-estudar-possivel-ligacao-entre-a-covid-19-e-a-doenca-de-kawasaki-em-criancas/&prev=search",66691203,0.0
69,a070fd03ae8ed2,e192e31c,"### Дипломная работа Александра Соколова

#### Анализ моделей градиентного бустинга LightGBM
Кернел 5 из 5 в разделе ML (отредактирован 21.04.2021)
---

# 1. Импорт библиотек, инициализация глобальных констант
## 1.1. Импорт библиотек",c0ec4138,0.0
70,a077820f7ab459,2e1297f0,"# RPS DINO Attention Image Transfer Learning 
",05a43104,0.0
73,95d896e75f9a50,f4c7cd2d,"# Feature-0 Exploration
As [mentioned previously](https://www.kaggle.com/c/jane-street-market-prediction/discussion/199462), feature 0 seems to be somewhat different from the other features, in that it's a binary indicator of something. At the same time, [another notebook](https://www.kaggle.com/hkailee/eda-lung-shape-umap-clusters-comparison) of the data showed a lung-shaped UMAP of all the features, indicating two major ""lungs""/clusters, with potentially some internal structure. 

## Experiment 1: TriMap of features 1-130
What I wanted to check out below is whether if we run dimensionality reduction on all features **except feature 0**, in order to find the two major clusters as from the lung UMAP, will those two clusters correspond to the values of feature 0. I'll use TriMAP instead of UMAP for my investigation, but I expect we'll see similar clusters in the TriMAP space.",2721b6f5,0.0
75,9b5de3823ad5ab,3cbea2f6,"# Recursion Cellular Image Classification

---
### Universidade de Brasília

CIC0193 - Fundamentos de Sistemas Inteligentes

Prof.: Vinicius Borges

Aluno: Pedro Lucas Silva Haga Torres

Matrícula: 16/0141575

##### Atividade IV - Redes Neurais Convolucionais

---
This is an assignment for the *Fundamentos de Sistemas Inteligentes* (Fundaments of Inteligent Systems) course at Universidade de Brasília (University of Brasília). All of the code and documentation is in english, the only exception being the header above, which identifies myself as an student undertaking the course mentioned previously.",33e48774,0.0
76,96c4c0e36b8ec0,ac2b0dbf,"**Hi there**,

**This is just a small note book of me plotting and interperating some graphs**",4dd6de8c,0.0
77,96e686f327577f,e155fa2c,"# Player tracker (WIP)

Using the pretrained yolov5 model to detect persons. With the detections trying to follow each individual player by choosing the nearest bounding box in the next frame. This approach has multiple issues e.g. sometimes two bounding boxes are merged to one, players leave the frame, etc.",8d150014,0.0
79,98a6794067932a,27e4cfd7,"# Projet de session - Analytique de la chaîne logistique

**Travail remis à :** Martin Cousineau

**Date de remise :** 8 décembre 2020

**Membres de l'équipe :** Alexis Caplette(11222588) - Kevin Hervieux(11275381) - Alexis Laplante(11220436) - Maylis Schwoerer(11203735)",08600fe2,0.0
82,98fd05fcc5c3e3,1bb70e42,![](https://miro.medium.com/max/809/0*tamvSiqDneDfw2Vr),55fe7ece,0.0
84,994900d90fdab8,d428ca71,"# Google Ventilator Feature Importance with LOFO

![](https://raw.githubusercontent.com/aerdem4/lofo-importance/master/docs/lofo_logo.png)

**LOFO** (Leave One Feature Out) Importance calculates the importances of a set of features based on **a metric of choice**, for **a model of choice**, by **iteratively removing each feature from the set**, and **evaluating the performance** of the model, with **a validation scheme of choice**, based on the chosen metric.

LOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported.

While other feature importance methods usually calculate how much a feature is used by the model, LOFO estimates how much a feature can make a difference by itself given that we have the other features. Here are some advantages of LOFO:
* It generalises well to unseen test sets since it uses a validation scheme.
* It is model agnostic.
* It gives negative importance to features that hurt performance upon inclusion.
* It can group the features. Especially useful for high dimensional features like TFIDF or OHE features. It is also good practice to group very correlated features to avoid misleading results.
* It can automatically group highly correlated features to avoid underestimating their importance.

https://github.com/aerdem4/lofo-importance",ecbc8c3e,0.0
86,999258a81ba32a,17ae6889,"# Why
I'm a bit new to using machine learning for data analysis.  I've always done analysis by hand and figured since I knew Python, why not give this a try.  Who knows what we might find.

Note: This is just for fun and is not something I'm particularly skilled in.  Best practices may or not be in use.",48cd3d21,0.0
87,99afe9f3af6dbc,c928acde,Import library dan load data yang akan dibutuhkan.,cdec9b3a,0.0
88,09751c520b0616,8037e4e3,"# <font color=blue>*HOUSE PRICING: Preprocessing,EDA and Modeling*</font>",a4d0c7e9,0.0
89,99bf357eaf61f1,380f616f,"### Hi...In this notebook i used exploratory data analysis and Linear,Random forest model for predicting the house prices.",9d92fafe,0.0
91,9a040a4f21091e,0c2d3121,Handling some imports and creating the train and test pandas dataframes:,f591b57d,0.0
92,9a96e1588410ee,988ca0cc,"# Intro: Context & Quick Overview

This is my first Kaggle notebook. I used it for my MNIST submmission and got an accuracy of 0.99296.

This code uses [FastAI ](https://www.fast.ai/) library and [Pytorch's pretrained model](https://pytorch.org/hub/pytorch_vision_resnet/) resnet34.

I am sharing this book as a quick reference guide for someuseful fastai patterns and a decent, very quick image classifier. The other fastai based notebooks of MNIST (Digit Recognizer) are saving the csv files into images to be read back again for processing. That is one area I improved by directly creating an image in memory using the csv content as that is the format resnet expects. It is not by any means intended as a guide (as evident from the documentation or the lack of). 

If you find any optimiztions or tricks I can use to make it better, please let me know.

# Code: The main part
## Imports of the standard libariries",3fa6cf92,0.0
95,9ad9a97e628bfa,89b2ab33,"본 커널은 과거에 기존 Kaggle Titanic Competition에 연습을 위해 작성해본 커널을 바탕으로 Building해서 작성했습니다. 
타이타닉 competition의 공개된 커널을 참고한 부분이 포함되어 있습니다. ",0a7e1136,0.0
97,9bcfa825c8b2e6,6549e587,# DIABETES PREDICTION,220f36e4,0.0
98,8985a124d4b657,b3ef8de1,"**I will be using the Aotizhongxin dataset (the first dataset among the ones I have uploaded) in this kernel for analysis. I will print the outputs of only the first 5 entries for all arrays and lists else you guys have to scroll through a lot to reach the next cell and it also takes a lot of time to commit. If you want to print all the entries, you can fork this kernel and just run a loop until the length of the array or list and print all entries. **",586d1846,0.0
100,892be0a523578c,5b2a3451,"<a id=""Introduction""></a>
# <font color=#4e79a7>Bellabeat case study </font>
This is a capstone project from the [Google data analysis specialization](https://www.coursera.org/professional-certificates/google-data-analytics) hosted on coursera, meaning to practice what I learned from the course. So, this project will be divided into 5 parts according to the analysis steps lectured in the course (exclude the **act** step), including: **ask**, **prepare**, **process**, **analysis**, **share**. Since I am more familiar with the python language, I will finish this project using Python.

<div>
    <h1 id=""Ask"" style=""color:#4e79a7"">
        Ask
    </h1>
 </div>

### Business task
Giving recommendations for marketing strategies by analyzing how users use their smart device

### Questions that may relate to the task
* How many groups of customer can we identify based on their activities?
* What are the features of different customer groups?
* How do these results have influence on marketing strategy?

### Stakeholders
* Urška Sršen, cofounder and Chief Creative Officer
* Sando Mur, cofounder, key member of the Bellabeat executive team
* Head of the marketing department

<div>
    <h1 id=""Prepare"" style=""color:#4e79a7"">
        Prepare
    </h1>
 </div>

### Description of the data
* The data records several kinds of health data, such as sleep, steps, heartrate, calories consumed in three time dimensions as second, minute and hour
* Some data is already merged together, for example, dailyActivity_merged.csv is merged from dailyCalories_merged.csv, dailyIntensities_merged.csv, dailySteps_merged.csv
* Some data is provided as both long and wide format, such as minuteCalories


### Problems of the data
* The data is stored as .csv files, and each of them contain so many records that it may be more practical to conduct analysis using R/Python than using spreadsheet.
* The data is not collected directly from Bellabeat but from a simliar product, so it is a second party data.
* It will be great if we have the demographics data about the partifipants.
* Lacking of metadata, which brings obstacles for interpreting the data. However, thanks for the kaggler [Laimis Andrijauskas](https://www.kaggle.com/laimisandrijauskas), you can find it [here](https://www.kaggle.com/arashnic/fitbit/discussion/281341)
#### Let's deep dive into the specific datasets
First of all, let's import the pandas package",b0e8d7c0,0.0
101,76c8afe761adc1,ce0d32cb,"Some pointers - 
1) https://www.kaggle.com/mihaskalic/lstm-is-all-you-need-well-maybe-embeddings-also 
2) https://www.kaggle.com/shujian/different-embeddings-with-attention-fork-fork

Added RL model with text input as state and prediction quality as reward
",8258944b,0.0
102,76d94f2011a1cb,3d1c1df0,"## Cactus Classifier with fastai.vision

This dataset is perfect for beginners, because the problem is literally trying to find stick-like patterns.Also fastai.vision library gives score 1 with little to no effort.
I've tried densenet and resnet here and they both give perfect score. ",9820aca8,0.0
104,77f958b3f41a70,1bf05c78,"# COVID-19 Open Research Dataset Challenge (CORD-19)
# What has been published about information sharing and inter-sectoral collaboration?

https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=583


# Results and discussion

The following results were manually selected to highlight research and news articles that are relevant to information-sharing and collaboration:
- [SOCRATES: An online tool leveraging a social contact data sharing initiative to assess mitigation strategies for COVID-19](https://doi.org/10.1101/2020.03.03.20030627)
- [Clinical AI Leader Jvion Launches COVID Community Vulnerability Map.](https://www.globalbankingandfinance.com/category/news/clinical-ai-leader-jvion-launches-covid-community-vulnerability-map/)
- [Sharing public health data and information across borders: lessons from Southeast](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6162912/)
- [U.S. county-level characteristics to inform equitable COVID-19 response](https://doi.org/10.1101/2020.04.08.20058248)
- [Age-dependent risks of Incidence and Mortality of COVID-19 in Hubei Province and Other Parts of China](https://doi.org/10.1101/2020.02.25.20027672)
- [Contributing to communicable diseases intelligence management in Canada: CACMID meeting, March 2007, Halifax, Nova Scotia](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533573/)
- [ClinMicroNet — Sharing experiences and building knowledge virtually](https://doi.org/10.1016/s0196-4399%2803%2980038-7)
- [TAXONOMY, CLASSIFICATION AND NOMENCLATURE OF VIRUSES](https://doi.org/10.1006/rwvi.1999.0277)
- [DATA, DATA, AND MORE DATA](https://doi.org/10.1006/rwvi.1999.0277)


# Highlights and suggestions for improvements

The advantages of this work includes:
- Use of news content for greater information coverage. For example, the ""[Clinical AI Leader Jvion Launches COVID Community Vulnerability Map.](https://www.globalbankingandfinance.com/category/news/clinical-ai-leader-jvion-launches-covid-community-vulnerability-map/)"" resource was obtained from the news dataset. Another example includes ""[UPDATE 1-G20 finance leaders pledge 'appropriate' fiscal, monetary actions in coronavirus response.](https://www.cnbc.com/2020/03/06/reuters-america-update-1-g20-finance-leaders-pledge-appropriate-fiscal-monetary-actions-in-coronavirus-response.html)"".
- Accurate results
- Data table fromatting with clickable URLs
- Simple data pipeline to understand and re-use: Users can easily enter a query and find relevant documents by simply calling
> q='equity considerations and problems of inequity'
<br>
> search(q, 'inequity')
- Results are summarized as a WordCloud image

Suggestions for improvements:
- To speed up the process and minimize computation, only the title and part of the abstract / news content are used in the analysis. Analyzing full-text may reveal additional information.
- Results can be fed into a document summarization algorithm so results can be more focussed even further
- Further improvements in how data is presented with a more advanced user interface can be beneficial


# Methodology
Data sources:
- [CORD-19 Research Papers](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)
- [COVID-19 Public Media Dataset](https://www.kaggle.com/jannalipenkova/covid19-public-media-dataset)

To build the corpus, the title and the content from both data soucres are combined (only the first 3,000 characters are used). The task questions were used as search queries for relevant documents in the corpus. For each document in the corpus and search query pair, the word vectors are retrieved from the GoogleNews word embeddings and the cosine distance is calculated. The top documents that match each query are shown in this notebook together with a summary of results presented as a WordCloud image. Additional results are available as CSV files.

I experimented with using the Word Mover's Distance measure as an alternative similarity measure to cosine distance however, it proved to be a lot slower and it was difficult to objectively decide if the results were better than cosine distance. I also tried using LDA (latent dirichlet allocation) to identify the major topics in the results but I decided to use WordClouds instead because it is more visually appealing and more keywords are highlighted.


I hope that the findings from this notebook can help inform researchers and curious minds about the ongoing COVID-19 research. I want to thank the researchers, competition organizers, Kaggle and the dataset providers for making this work possible.",2ad9bb69,0.0
105,786475feda0190,3c28e529,"![cover.jpg](attachment:cover.jpg)

",e4663d97,0.0
111,7a75cba9317186,adaa26b1,# HOMEWORK 1,3d7e3235,0.0
113,7b59fc0ff0d10b,83bfbace,"In this notebook, we deduce that the measured NOx values need to be scaled to account for cloud cover.",c192b94c,0.0
114,7b5ce7d4109181,1364a082,"Credit goes to @suicaokhoailang

https://www.kaggle.com/suicaokhoailang/an-embarrassingly-simple-baseline-0-960-lb",4247d63c,0.0
115,7ba63a2d9abb58,c9ed95be,"<h1>Realtime Covid-19 Tracker</h1>
",821a261f,0.0
117,7c7a7db391c517,a478b558,"# 1 Preface
Seoul is the capital of south Korea,",f53450dc,0.0
119,7cfd96218dd933,4cc8091a,"# HISTORY

#### INVESTIGATIVE WILDFIRE DATA FOR TURKEY/ NASA

* BE CAREFUL OF THE FILE NAMES.

* IT CONTAINS THE DATA NEEDED TO RESEARCH LATEST FOREST FIRES IN TURKEY.

* PAY ATTENTION TO THE DATE INTERVALS.

* THESE ARE 7-11 DAILY DATA OF LAST TIMES.

fire _ nrt _ M _ C61 _ 212465 _ all _ countries.csv

* This file is important for all countries becuase it contains fire data of last 11 days for all around the world

#### Content

* Data on recent forest fires in Turkey, published with permission from NASA Portal.
* The data was created based on the hotspots and obtained from the satellite.

#### 3 SEPARATE SATELLITE DATA:

* MODIS C6.1
* SUOMI VIIRS C2
* J1 VIIRS C1

#### GENERAL ATTRIBUTES

* Latitude

Center of nominal 375 m fire pixel

* Longitude

Center of nominal 375 m fire pixel

* Bright_ti4

(Brightness temperature I-4)

VIIRS I-4: channel brightness temperature of the fire pixel measured in Kelvin.

* Scan

(Along Scan pixel size)

The algorithm produces approximately 375 m pixels at nadir. Scan and track reflect actual pixel size.

* Track

(Along Track pixel size)

The algorithm produces approximately 375 m pixels at nadir. Scan and track reflect actual pixel size.

* Acq_Date

(Acquisition Date)

Date of VIIRS acquisition.

* Acq_Time

(Acquisition Time)

Time of acquisition/overpass of the satellite (in UTC).

* Satellite

N Suomi National Polar-orbiting Partnership (Suomi NPP)

* Confidence

This value is based on a collection of intermediate algorithm quantities used in the detection process. It is intended to help users gauge the quality of individual hotspot/fire pixels. Confidence values are set to low, nominal and high. Low confidence daytime fire pixels are typically associated with areas of sun glint and lower relative temperature anomaly (15K) temperature anomaly in either day or nighttime data. High confidence fire pixels are associated with day or nighttime saturated pixels.

#### Please note:

Low confidence nighttime pixels occur only over the geographic area extending from 11° E to 110° W and 7° N to 55° S. This area describes the region of influence of the South Atlantic Magnetic Anomaly which can cause spurious brightness temperatures in the mid-infrared channel I4 leading to potential false positive alarms. These have been removed from the NRT data distributed by FIRMS.

* Version

Version identifies the collection (e.g. VIIRS Collection 1) and source of data processing: Near Real-Time (NRT suffix added to collection) or Standard Processing (collection only).

""1.0NRT"" - Collection 1 NRT processing.

""1.0"" - Collection 1 Standard processing.

* Bright_ti5

(Brightness temperature I-5)

I-5 Channel brightness temperature of the fire pixel measured in Kelvin.

* FRP

(Fire Radiative Power)

FRP depicts the pixel-integrated fire radiative power in MW (megawatts). Given the unique spatial and spectral resolution of the data, the VIIRS 375 m fire detection algorithm was customized and tuned in order to optimize its response over small fires while balancing the occurrence of false alarms. Frequent saturation of the mid-infrared I4 channel (3.55-3.93 µm) driving the detection of active fires requires additional tests and procedures to avoid pixel classification errors. As a result, sub-pixel fire characterization (e.g., fire radiative power [FRP] retrieval) is only viable across small and/or low-intensity fires. Systematic FRP retrievals are based on a hybrid approach combining 375 and 750 m data. In fact, starting in 2015 the algorithm incorporated additional VIIRS channel M13 (3.973-4.128 µm) 750 m data in both aggregated and unaggregated format.


Satellite measurements of fire radiative power (FRP) are increasingly used to estimate the contribution of biomass burning to local and global carbon budgets. Without an associated uncertainty, however, FRP-based biomass burning estimates cannot be confidently compared across space and time, or against estimates derived from alternative methodologies. Differences in the per-pixel FRP measured near-simultaneously in consecutive MODIS scans are approximately normally distributed with a standard deviation (ση) of 26.6%. Simulations demonstrate that this uncertainty decreases to less than ~5% (at ±1 ση) for aggregations larger than ~50 MODIS active fire pixels. Although FRP uncertainties limit the confidence in flux estimates on a per-pixel basis, the sensitivity of biomass burning estimates to FRP uncertainties can be mitigated by conducting inventories at coarser spatiotemporal resolutions.

http://cedadocs.ceda.ac.uk/770/1/SEVIRI_FRP_documentdesc.pdf

* Type

(Inferred hot spot type)

0 = presumed vegetation fire

1 = active volcano

2 = other static land source

3 = offshore detection (includes all detections over water)

* DayNight

(Day or Night)

D= Daytime fire

N= Nighttime fire",7c34d96c,0.0
120,7dd46c750653eb,adffee4d,![irina-grotkjaer-dyPNRXLevJY-unsplash.jpg](attachment:irina-grotkjaer-dyPNRXLevJY-unsplash.jpg),c2644713,0.0
121,7dec6bdea6d779,b3202cc9,v1: last activation changed to softmax,18be5949,0.0
122,7686f42e1f28d2,f60df0e7,# Loading Neccessary Libraries,6c128859,0.0
123,7e1da639035ac5,2dd626af,"<img src=""http://static1.squarespace.com/static/5576f1c0e4b08f31a497b582/t/5576fcf6e4b0a6d0afa92d0f/1527185421758/"">",120b6c23,0.0
124,7650e0ac081e94,6ce208ce,This notebook is based on Slawek Biel's notebook (https://www.kaggle.com/slawekbiel/positive-score-with-detectron-2-3-training),0081cee5,0.0
125,75adb7945ef9bd,aaeecbbd,"## Basic NLP on Disaster Tweets

This is a getting started competition of predicting whether a tweet is actually referring to real disasters.",785c5095,0.0
128,72d393488311b6,bf8a7658,# import Libraries,80663df0,0.0
129,0cb9adc158b705,6f13bb34,"As the title suggests, we will be using fastai to achieve the best possible score with minimum lines of code.

This is the training notebook, you can find the [inference notebook here](https://www.kaggle.com/ankursingh12/fastai-plant2021-starter-inference).

Lets get started . . . 

First, we will to import fastai.",3abf056e,0.0
130,72d528df923403,d8bb09c5,"# Introduction

In this Notebook we are going to understand everything related to M5 Forecasting - Accuracy competition data. 
The dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated.  The products are sold across ten stores, located in three States (CA, TX, and WI).",d51c8e8e,0.0
131,72e098fe5b2a04,aeffdc06,"# Overview
Trained mainly using this kernel [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch)

LB score is the following:
- roberta1 = 465 (roberta base)
- roberta2 = 462
- roberta3 = 460 noPT 4 layer
- deberta3 = 460 noPT 4 layer
- electra =  462 noPT

* noPT : no pretraining
* 4 layer : sum the last 4 hidden layer ",5399eebd,0.0
133,7325ce9461a814,4274cdda,"# DATA PREPARATION
data preparation with pandas",c73a7825,0.0
136,0cb456a5456cf9,8874c0e6,# EDA of hotel data 酒店预订数据的综合分析<br>My work can be simple described as 3 part: <br> a.Roughly feel of the data <br> 数据的简单浏览 <br> b. Predict cancel rate using logistic regression<br>使用Logistic回归预测用户退订情况<br>c. Predict cancel rate using random forest<br>使用随机森林预测用户退订情况,5701729c,0.0
137,738bfced935b69,5f0e9a5a,# 100000 UK Used car Data Analysis,2d3c592d,0.0
138,73ca9abcc2034e,d8921f29,"# **This is a initial descriptive analysis of the Reddit wallstreetbets posts. It contains a basic statistics of words, character count, and occurence. At the bottom, you will find the analysis of the most common mentioned NYSE or other stock tickers. Enjoy!**",cec3446c,0.0
139,0caaec057f7184,ba7e85fb,"This is my first project for eda using the product sales records. The report will focus a lot more on discussing the type of the test data (the relationship between test and train), and take on some detail looks into some examples. Here's the outline: 

[Input data information](#Input-data-information)
* [Original features](#Original-features)
* [Data cleaning](#Data-cleaning)

[EDA on features](#EDA-on-training-data)
* [Category](#Category)
* [Shops](#Shops)
* [Price](#Price)
* [Sales information / item based](#Sales-information-/-item-based)

[Test data (depending on training data)](#Train-data-and-Test-data-overlapped?)
* [Items in test data](# items-in-shops)
* [Old items new launched in shop items](#Old-items-new-launched-in-shop-items)
* [New items new launched in shop items](#New-items-new-launched-in-shop-items)
* [Discussion](#Discussion)

[Special case](#Input-data-information)
* [Negative price](#Negative-price)
* [Negative sales](#Negative-sales)

[Conclusion](#Input-data-information)",b875533e,0.0
140,73d8e56bc709b1,8fe0ab40,# 1. Have a look on dataset,78ec3cce,0.0
141,743ae010f5e875,abcbae6a,# Setting,02c54445,0.0
142,7454fdc444df16,56314a17,# CHECK OUT MY [NEW & IMPROVED NOTEBOOK](https://www.kaggle.com/amerii/breast-cancer-classification-guide-pca-svms),a7818ef5,0.0
143,74a03887600114,c64d2e29,"## Recommendation Engines
Recommendation Engine filter out the poducts that a paticular customer would be interested in or would buy based on his/her previous buying history but if the customer is new then this method will fail as we have no previous data from the customer.So,to tackle this issue different methods are used,for example oftern the most popular products are recommended.This recommendations would not be must accurate as they are not customer dependent and are same for all new customers.

So there are many types of recommndation engines:
- Collabrative Recommender System
- Content based Recommender System
- Demographic based Recommender System
- Utility based Recommende System
- Knowledge based Recommende System
- Hybrid Recommende System",c0ffb2f0,0.0
144,757fa8de4edc4c,82da0db0,Importng Libraries and Data Overview,87211008,0.0
146,758e6b0aa40398,ca0e8b2e,"**Kindergarten level ML**

I just recently stumbled across [fast.ai](http://) webpage and not long after I realised how their motto makes a lot of sense - ""Making neural nets uncool again"".

Being able to learn how to write code shown below doesn't make you special. It just proves that anyone can do it. Of course we're talking of the level mentioned in the header of this notebook. Eventhough I, like many of the beginners, tried playing with cats and dogs categorising and ASL dataset, I was particularly  happy to find Four Shapes data sets.

![image.png](attachment:image.png)

Sorting shapes is exactly what kids do, don't they?",481b1b30,0.0
149,7e2644d6b415bc,2321998e,![Difference-Between-Recruitment-and-Selection.png](attachment:Difference-Between-Recruitment-and-Selection.png),52de7ef0,0.0
150,7e275c8d5ff2a0,11b854d2,"# COVID ANALYSIS AND PREDICTION :
                                                            
Corona Virus disease (COVID-19) is an infectious disease caused by a newly discovered virus, which emerged in Wuhan, China in December of 2019.Most people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people and those with underlying medical problems like cardiovascular disease, diabetes, chronic respiratory disease, and cancer are more likely to develop serious illness.
The COVID-19 virus spreads primarily through droplets of saliva or discharge from the nose when an infected person coughs or sneezes, so you might have heard caution to practice respiratory etiquette (for example, by coughing into a flexed elbow).
# Libraries

We import a few important libraries that we shall use in the model. Pandas is an extremely fast and flexible data analysis and allows you to allow you to store and manipulate tabular data. We also import visualisation libraries such as matplotlib, seaborn and plotly.

# Prediction

",b3afcc98,0.0
152,84127ade6fde87,ca5cdfea,All data from book **Deep Learning with PyTorch** https://pytorch.org/deep-learning-with-pytorch,f55d05b6,0.0
153,842547b2def18c,e71d77f2,"Fork from: https://www.kaggle.com/startupsci/titanic-data-science-solutionsm
Competition: https://www.kaggle.com/c/titanic",b8efde6d,0.0
154,8447633e1d256c,c59b0fe3,"# NLP Text Classification(Multiclass)

## Dataset

BBC News Article datasets are made available for non-commercial and research purposes only, and all data is provided in pre-processed matrix format. 

* Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.
* Class Labels: 5 (business, entertainment, politics, sport, tech)

## Approach using OneVsRestClassifier

One-vs-the-rest (OvR) multiclass/multilabel strategy, its also known as one-vs-all. This approach consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.


## Import Libraries",60d593ce,0.0
155,84d1ef55b89e17,c23b0232,"# **Assignment1: **

Design a Neural Networks to train the XOR problem as following using Python programming language.
XOR problem

Inputs     |    output

   i1 i2
   
   0  0  ----->        0
   
   0  1  ----->        1
   
   1  0  ----->        1
   
   1  1  ----->        0

Take the output of the network for the ranges  i1 =[-0.8, 1.2],  i2 =[-0.8, 1.2]
with the interval of 0.05 sequentially and draw the output in 3D.",2d0b9d51,0.0
157,8539260444e6b5,87b58758,#  Importing Important Packages,0369463f,0.0
159,858da4bb312f67,0a834a48,"## Introduction
I guess **generating additional data by GANs or other generators can be an approach to improve the performance** of CNN classifier. So I've applied one of the best-known GAN ""CycleGAN"" on the Cassava Leaf dataset.",9cca4391,0.0
162,0ac930de622582,7ad56408,"Based on this kernel https://www.kaggle.com/avirl3364/lstm-covid19, credit to @avirl3364.
I changed it to Bi-LSTM with Mish and Swish and some other minor modifications.
It's also config. to be tested on public LB, with no leak.
",de1f35dd,0.0
165,869a39a3d4dea2,fc4a3702,"I am reading a book on ""Pratical Python & Open CV"" and implementing in this notebook based on the things that I understand, even though I already have an experience in working with the images, I hope this will make me understand the concepts which I really wanted to learn and know in the computer vision, this notebook is a starter",9020daf8,0.0
169,87e94f864d74be,f6c64e06,"**<center> <span style=""color:crimson;font-family:serif; font-size:32px;""> NETFLIX MOVIES AND TV SHOWS EDA 📊</span> </center>**",294bfe9f,0.0
170,87e96e14f8f5ce,d2951e84,# Titanic Competition ,f9f7a3a2,0.0
172,83df814455f06c,af13a847,"<a class=""anchor"" id=""0""></a>
# **Decision Tree Classifier Tutorial with Python**


Hello friends,

In this kernel, I build a Decision Tree Classifier to predict the safety of the car. I build two models, one with criterion `gini index` and another one with criterion `entropy`. I implement Decision Tree Classification with Python and Scikit-Learn. ",c9cff71a,0.0
173,0ad8d416b89b78,fd405aff,"# **Predicting An Individual Incomes via Census Data**

This is my fist Kaggle Notebook, it is a further exploration on top of introductory studies to the fundamentals of analytics and machine learning models. The initial section will explain the data mining problem at hand and the process taken to provide a solution to this problem. The second section will provide an exploration of the attributes with in the dataset, identifying key attributes that show promise in being able to assist with the classification task. The final section will involve the creation, development & results of the classification methodologies used to solve the task.",0b0562f0,0.0
174,835a7b4e660d23,8fbebd83,## 1. [Matplotlib](#plt),53bc7a6e,0.0
175,835a2889752e94,4e47dec8,"데이터분석 노베이스 문과 출신 직장인의 수준 낮은 코드입니다.

시험장가서 csv 파일 제출만이라도 제대로 하고 오길 바라는 사람의 코드이니,

그냥 이렇게 간단하게 코딩하는 사람도 있구나 정도로만 봐주시면 감사하겠습니다 :)",312ecc9c,0.0
176,7e89d387feb9f5,25410a6c,## Общие действия,989e3a1b,0.0
177,7eb3041762a1b0,eec2391b,"## Strategy:
The strategy here is very simple. You pick a threshold somewhere between 0.1 and 0.9, which is determined by the parameter pp_theshold. Then you scale all predictions greater than the threshold towards 1 and all smaller than the threshold towards 0. The degree of scaling is determined by setting the parameter pp_factor.

## Results:
This postprocessing is very parameter sensitive and parameter tuning should optimally be done by averaging optimal parameters across multiple folds in order to provide a stable result, but even without doing that this provided us with a 0.0001 boost on local CV and a .00002 boost on private LB. Unfortunately, our solution was overall not good enough for this to be valuable.",92a35cee,0.0
184,0b01138ad120fc,f49c4668,### Vinícius Rodrigues Ferraz,0b4b72e6,0.0
185,806ce45c8fa303,21f02683,"# Autoencoder Architecture

## Introduction
Autoencoder is a kind of Deep Learning architectures. Autoencoder architecture encompasses two sub-systems as encoder and decoder. Both these sub-systems are made up of independent Neural Network with a defined set of layers and activation functions. The fundamental characteristic feature of Autoencoder architecture is extracting the latent(hidden) data points from the given dataset. 

This notebook is created out of inspiration from the post on [Geekforgeek](https://www.geeksforgeeks.org/ml-classifying-data-using-an-auto-encoder/)",3e5c34dc,0.0
186,a0a5baa6c7e12a,617264c9,"# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">Introduction</div>

This notebook is intended to extract useful insights for the datasets of ‘Tabular Playground Series - Dec 2021’ competition in Kaggle. 

For this competition, you will be predicting a categorical target based on a number of feature columns given in the data. 

The data is synthetically generated by a GAN that was trained on a the data from the [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction/overview). This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.

**Note:** Please refer to this [data page](https://www.kaggle.com/c/forest-cover-type-prediction/data) for a detailed explanation of the features.

We are going to perform the complete and comprehensive EDA as follows
-	Automate the generic aspects of EDA with AutoViz, one of the leading freeware Rapid EDA tools in Pythonic Data Science world
-	Deep into the problem-specific advanced analytical questions/discoveries with the custom manual EDA routines programmed on top of standard capabilities of conventional Python visualization libraries",551d41de,0.0
189,80e2d45b451e4b,6e9197a7,"YOLOv3 Inference using experiencor's yolo3 implementation. https://github.com/experiencor/keras-yolo3
Single Model with Weighted Boxes Fusion.",9200c91d,0.0
190,80ecc4c67a9f54,429be821,"# Location of files:

 /kaggle/input/football-players-and-teams-from-brazil/teams.csv
 
 /kaggle/input/football-players-and-teams-from-brazil/players.csv",4bbf546c,0.0
193,81712ee7510ac5,37fefc00,"* Data types
* Numbers
* Strings
* Printing
* Lists
* Dictionaries
* Booleans
* Tuples
* Sets
* Comparison Operators
* if, elif, else Statements
* for Loops
* while Loops
* range()
* list comprehension
* functions
* lambda expressions
* map and filter
* methods",c4685e79,0.0
194,817449886e2cbd,d5bdeb8f,"I play around with Random Forest hyper-parameter optimization here, as well as some visualizations.",ea681120,0.0
195,826ccb616bd2a8,eee17c63,"# Recommending which Drug to which Patient

<i>Answer:</i><br>
<u><b>Order of Drug Operation:</b></u>
1. If `Na_to_K` greater than `14.829` recommend **Drug Y** (if less, go to step 2)
2. If `Blood Pressure` is `HIGH` then (if not high, go to step 3)
    -  `Age` is `50 or younger` then recommend **Drug A**
    -  `Age` is `Older than 50` then recommend **Drug B**
3. If `Cholesterol` is `HIGH` then **Drug C**, if not the recommend **Drug X**

<br>

**See how below!**",4d7df2ec,0.0
197,8336d84cf3ff6b,6e20f79a,"https://towardsdatascience.com/machine-learning-multiclass-classification-with-imbalanced-data-set-29f6a177c1a

https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d

https://towardsdatascience.com/backpropagation-and-batch-normalization-in-feedforward-neural-networks-explained-901fd6e5393e

https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/

https://towardsdatascience.com/dealing-with-multiclass-data-78a1a27c5dcc

https://machinelearningmastery.com/voting-ensembles-with-python/

https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html

https://towardsdatascience.com/random-forest-in-python-24d0893d51c0

https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d

https://towardsdatascience.com/predictive-modeling-and-multiclass-classification-a4d2c428a2eb

https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/

https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/

https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc


https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63",b96b58a0,0.0
198,80ad12f326ab70,aa3f35b9,# LearnPlatform COVID-19 Impact on Digital Learning,da404a16,0.0
199,726833f92fb87a,11ee3cd5,# BANK MARKETING PROJECT,7dc5e1b6,0.0
200,a0b321057e7402,b40dbc2b,"# **Introduction**

Bitcoins are one of the largest and most well-known cryptocurrencies in the world. It first appeared in 2009 and has grown exponentially since gaining mainstream appeal. Over the years a is a wide range of opinions about the currency formed. Some consider it an investment, a fad and I simply consider it an eco-friendly alternative to burning money.

This notebook has two goals:

* Finishing an exploratory data analysis of the Bitcoin price
* Creating a SARIMAX univariate prediction of the Bitcoin price ",5f73fb91,0.0
201,a1a31459abf078,6500157d,"# RIIID : Answer Correctness Prediction

![](https://res-2.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco/zcyhpowwzhmv9zvynidc)",66fc0f54,0.0
203,be616f0785c32d,6088e305,"
<div id=""top""></div>
",b78e18aa,0.0
204,be9597c72542a2,804d6f16,**DAY 6**,6f29c6d8,0.0
206,beef5463692752,9eb368d9,"# <h2>1) Exploring the dataset</h2>
<p>Reading the image data and their corresponding captions from the flick dataset folder. Showing the image and captions to get the insighs of the data. Dowload link for the dataset used <a href=""https://www.kaggle.com/adityajn105/flickr8k"">here</a></p>",34fb20b2,0.0
207,bef2347846e476,055cdd87,"**Overview**

Could you predict which applications are being downloaded by people ?And how many of them appeals to teenagers,how many of them appeals to matures or everyone ?What are installation numbers and ratings ?If we have a good analysis of applications that are downloaded by people everyday then we can produce much better applications.Currently,application stores are already classifying and suggesting the new applications according to user's interest and before we download some applications we can see the previous experiences of people by investigating and observing the ratings values.With all of these properties that we can keep track of peoples interests can be predicted by analiysts and can be designed in a more effective way by developers.

Thus,we need to build a machine learning model to find the properties of best appplications.Or, we can say that we'll be able to know which properties should our application has while we creating a new one.


**About the Dataset**

We'll be working with two csv file in this dataset.The first dataset includes application name,category,reviews,sizes,install,types,android version etc. it's like more technical details...And the second data set includes sentiment,sentiment polarity,sentiment subjectivity etc.Emotions are closely related to sentiments. The strength of a sentiment or opinion is typically linked to the intensity of certain emotions, e.g., joy and anger.Polarity, also known as orientation is he emotion expressed in the sentence. It can be positive, neagtive or neutral. Subjectivity is when text is an explanatory article which must be analysed in context.Literally...Let's implement them into applications.

",cb93bf51,0.0
208,bfe6c7096b1ad0,08b5c6b8,# Python для Data Science: Итоговый проект,fffd95e0,0.0
210,c09fac3c943d51,05da06bc,"![](data:image/png;base64,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)",678d076d,0.0
211,c0ddb77bf32e2b,a5ea6e96,"
# What we are looking for in this dataset?(Why we analysis this dataset?) #

*(Note:  After  the first edition of  this kernel was committed, some improvement thoughts had appeared in my mind. But I am busy working on project recently. Please  feel free to fork and modify this kernel or leave a message below, thank you!)*

![credit:yahoo news,  https://tw.news.yahoo.com/pm2-5%E5%81%8F%E9%AB%98-%E5%9C%8B%E5%81%A5%E7%BD%B2%E7%B1%B2-%E6%88%B6%E5%A4%96%E6%B4%BB%E5%8B%95%E5%AE%9C%E6%B8%9B%E5%B0%91-103036186.html](https://imgur.com/2ce0M3x.jpg)
Recently, the pollution of PM2.5 in northern Taiwn has became more severe. It has decrease the life expectancy in Asia by 2 years, and people in Taiwan are warning not to exercise outside off works. Let's have a look at the news.

[研究：PM2.5空污縮短人類預期壽命 亞洲影響最劇](http://news.ltn.com.tw/news/world/breakingnews/2530451)
> 德州大學奧斯汀分校科學團隊先分析空污以外的疾病、傷害和相關喪命風險因素導致的死亡率，再檢視全球185個暴露在PM2.5當中的國家之狀況，探究每個國家的預期壽命與空污之間的關聯，發現全球人類的預期壽命平均短少2年，在英美地區人們的平均壽命少了4個月。
> 研究指出，亞洲、非州與中東地區的空污問題嚴重，孟加拉人民預期壽命減少1.87年，埃及人民則少掉1.85年，巴基斯坦人民短少1.56年，印度民眾少掉1.53年，阿拉伯人民少了1.48年，奈及利亞民眾短少1.28年，中國人民則是平均少了1.25年的壽命。

(Translated to English by me)
>Science team of University of Texas at Austin analyzes the mortality caused by diseases, injuries rather than air pollution, and then view the status of 185 countries exposed to PM2.5. They analysised the relationship of  the air pollution with life expectancy  of each country. The correlation between air pollution and life expectancy is  found that the life expectancy of human beings worldwide is 2 years shorter averagely, and the average life expectancy in the British and America is 4 months shorter.
The study also pointed out that the air pollution problem in Asia, Africa and the Middle East is extremely severe. The life expectancy of the Bangladeshi is decrease by 1.87 years, the Egypt is 1.85 years shorter, the Pakistan  is 1.56 years shorter, the India is 1.53 years shorter, and the Arab people is 1.48 shorter, and China is 1.25 years shorter.

[下班後戶外運動好嗎？心臟科名醫：不可以！](https://health.udn.com/health/story/5978/3364185)
> 根據美國心臟學會2010年的共識報告，PM2.5每增加10個單位，總死亡率約增加15％，心肺疾病死亡率約增加15％，心血管疾病死亡率增加10至15％，缺血性心臟病死亡率增加15至20％。
>為何空氣汙染會傷害心血管？洪惠風指出，心肌梗塞好比土石流，土石流通常是在颱風豪雨後較容易發生，對心血管而言，空氣汙染、壓力和緊張等都是颱風豪雨。由於PM2.5是空氣汙染的一種，它非常微小，會隨著呼吸進入肺泡再深入血液，當人暴露在PM2.5之下容易升高血壓，提高發炎反應，導致血管抽筋、破裂，產生血栓，進而容易發生心臟病。


>Is it good for outdoor sports after work?  Cardiology doctor:definitely not! (WIP: (Translated to English by me))

OK, Let's get started.",a0cb45f7,0.0
212,c0e2a467cf23ee,000dec1c,"This kernel is a modification of the kernel ""Concorde solver"" (https://www.kaggle.com/wcukierski/concorde-solver)",6d02cd29,0.0
213,06ecf7a304c309,72e4f4f4,"# How Autoencoders work - Understangin the math and implementation

> 이 커널은 [Shivam Bansal님의 오토인코더 예시](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)을 번역한 커널입니다.

original kernel : https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases

**Thanks to @ShivamBansal for  awesome kernel about Autoencoder.**

### Contents
- 1. Introduction
    - 1.1 What are Autoencoders ?
    - 1.2 How Autoencoders Work ?
- 2. Implementation and UseCases
    - 2.1 UseCase 1: Image Reconstruction
    - 2.2 UseCase 2: Noise Removal
    - 2.3 UseCase 3: Sequence to Sequence Prediction
    
    
더 많은 좋은 자료를 만들려고 노력하고 있습니다.

- **블로그** : [안수빈의 블로그](https://subinium.github.io)
- **페이스북** : [어썸너드 수비니움](https://www.facebook.com/ANsubinium)
- **유튜브** : [수비니움의 코딩일지](https://www.youtube.com/channel/UC8cvg1_oB-IDtWT2bfBC2OQ)",714de627,0.0
214,c115e287523aab,40825f76,"# [PetFinder.my - Pawpularity Contest](https://www.kaggle.com/c/petfinder-pawpularity-score)
> Predict the popularity of shelter pet photos

![](https://storage.googleapis.com/kaggle-competitions/kaggle/25383/logos/header.png)",feb1288b,0.0
215,c13f73168789c2,3d23e859,"# Pandas tutorial : Day 3

Selecting, Slicing and Filtering data in a Pandas DataFrame

* [Selecting](#1)
 1. [Select rows and columns using labels](#2)
   * [To select a single column](#3)
   * [To select multiple columns](#4)
   * [Select a row by it's label](#5)
   * [Select multiple row by it's label](#6)
   * [Accessing values by row label and column name](#7)
   * [Accessing values from multiple columns of same row](#8)
   * [Accessing values from multiple rows but same columns](#9)
   * [Accessing values from multiple rows and multiple columns](#10)
 2. [Select by index position](#11)
   * [Select a row by index location](#12)
   * [Select a column by index location](#13)
   * [Select data at specified row and column location](#14)
   * [Select multiple rows and columns](#15)
 3. [Selecting top n largest values of given column](#16)
 4. [Selecting top n samllest values of given column](#17)
 5. [Selecting random sample from the dataset](#18)
 6. [Conditional selection of columns](#19)
* [Slicing](#20)
 1. [Slicing rows and columns using labels.](#21)
   * [Slice row by label](#22)
   * [Slice columns by label](#23)
   * [Slice row and columns by label](#24)
 2. [Slicing rows and columns by position.](#25)
   * [To slice rows by index position](#26)
   * [To slice columns by index position](#27)
   * [To slice row and columns by index position](#28)
* [Subsetting by boolean conditions](#29)
 1. [Select rows based on column value](#30)
   * [To select all rows whose column contain the specified value(s)](#31)
   * [Rows that match multiple column conditions](#32)
   * [Select rows whose column DOES NOT contain specified values](#33)
 2. [Select columns based on row value](#34)
 3. [Subsetting using filter method](#35)
 
 
Let's gets started!
 
[Data for daily news for stock market prediction](https://www.kaggle.com/aaron7sun/stocknews)",16175052,0.0
216,c155cf2103db3b,58dd09d5,"
### Author : Khushkumar Patel
### Slack ID : @Khush
### ----------
### Model Used Name : Inception
I have used Inceptionv3 because it has more complex and more layes than anyother. And this will give better accuracy. Moreover, It works totally fine with Problems like Overfitting. 

A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting

Inception work with 300px where as all other are working as 224px data. So images are trained full without lossing more data. 

CrossEntropyLoss and SGD works very perfectly with Inception so here I used it. 
### ----------
### criterion = CrossEntropyLoss
### optimizer = SGD",3378dcf6,0.0
217,c18267b203f28a,41dc78ec,"# Intro

I want to see how diffrent model performs at the data, so I set up this simple baseline. 

Check history versions for each model's training log and metrics


### Model List / 添加各模型的训练基线,包含:
* InceptionV3 [Version2-TPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49382127)
* resnet50v2
* resnet101v2 [Version4-TPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49386174)
* resnet152v2
* InceptionResnetV2 [Version6-TPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49390571)
* DenseNet121 [Version8-TPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49397189)
* Xception [Version9-TPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49403249)
* VGG16 [Version14-TPU]
* NASNetLarge (调试完成,等待训练,比较慢)
* ResNet50  [Version12](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49442732)
* ResNet101  [Version13](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49445531)

GPU for EfficientNet ( Kaggle的TPU版本目前为 tf2.2, 不支持EfficienctNet, 使用GPU训练)
* EfficentNetB0 [Version11-GPU](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease?scriptVersionId=49419318)
* EfficentNetB3 TBD (low memory, low memory, low memory !!!)


**Change these 2 lines for model switch / 选择模型时修改模型名和模型backbone方法这两行即可:**

```
MODEL_NAME = 'ResNet101'
......
base_model ,preprocess_layer = resnet101_base()

```

### Here is the Submit Kernel / 提交Kernel在此:
* [TPUs + Cassava Leaf Disease[Infer]](https://www.kaggle.com/tianyu5/tpus-cassava-leaf-disease-infer)

### References
Thanks for these kernels to help me get start:

* [Getting Started: TPUs + Cassava Leaf Disease](https://www.kaggle.com/jessemostipak/getting-started-tpus-cassava-leaf-disease)
* [Tensorflow Resnet50 (train with new tfrecords)](https://www.kaggle.com/wuliaokaola/tensorflow-resnet50-train-with-new-tfrecords)",09ca8efb,0.0
220,c2a9f2fb3e1594,c9a586e3,"
>Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think/code. Not only will you be able to submit your first competition, but you’ll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. *Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop.* Thanks, and may you have ""statistically significant"" luck!


# Table of Contents
1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)
1. [Chapter 2 - A Data Science Framework](#ch2)
1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)
1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)
1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)
1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)
1. [Chapter 7 - Step 5: Model Data](#ch7)
1. [Chapter 8 - Evaluate Model Performance](#ch8)
1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)
1. [Chapter 10 - Tune Model with Feature Selection](#ch10)
1. [Chapter 11 - Step 6: Validate and Implement](#ch11)
1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)
1. [Change Log](#ch90)
1. [Credits](#ch91)

**How-to Use this Tutorial:** Read the explanations provided in this Kernel and the links to developer documentation. The goal is to not just learn the whats, but the whys. If you don't understand something in the code the print() function is your best friend. In coding, it's okay to try, fail, and try again. If you do run into problems, Google is your second best friend, because 99.99% of the time, someone else had the same question/problem and already asked the coding community. If you've exhausted all your resources, the Kaggle Community via forums and comments can help too.",53411c04,0.0
223,be357c1e2c975d,f79ea5e4,<h2 align=center> Facial Expression Recognition</h2>,2486a061,0.0
224,c3498779cda661,4c52bfb3,"# Machine Learning Aplicado

Tarea 8

César Jiménez Mena",0f531b65,0.0
225,06faae14dfe21d,e6ee6664,"### TASK
### https://www.hackerearth.com/challenges/competitive/hackerearth-machine-learning-challenge-predict-windmill-power/problems/
## Predict The Power Generated By WindMill",2be83140,0.0
227,b91c9eef23d284,589cca13,![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSJVRFaLVDi0cF4rUCeXySkZowxVPgFQzpUGQ&usqp=CAU)sk.pinterest.com,8fb353fa,0.0
228,b9328fe3b0cefc,ad4f0d24,"First of all, I want to apologize for my English, and you will see the reason.",3a35eb23,0.0
229,07d6ca51d43510,3570e198,"## NIPS Conference Papers 1987-2015

+ Clustering research paper basis on keywords of papers (K - Means Clustering)",38e74a14,0.0
233,07bfec3562f9b3,e734533d,"## Overview

This notebook is a first part of full **Offline Handwritten Text Recognition**.     
See full system on Github: https://github.com/IrinaArmstrong/HandwrittenTextRecognition

**Offline Handwritten Text Recognition (HTR)** systems transcribe text contained in scanned images into digital text.             
In this case, the system is developed to deal with cyrillic alphabet. It also involves full dictionary of the Russian language.

In general, developed HTR consists of several parts, which are responsible for processing pages with full text (scanned or photographed), dividing them into lines, splitting the resulting lines into words and following recognition of words from them.

For solve the problem of recognition, it was decided to use Neural Network (NN). It consists of convolutional NN (CNN) layers, recurrent NN (RNN) layers and a final Connectionist Temporal Classification (CTC) layer.

But in this notebook only the task of page segmentation is highlighted. It was decided to do this in several different ways shown below.",327e7d5b,0.0
234,ba4b3bd184acbb,3d852db9,"# Pandas DataFrames Tutorial
#### By: [Dylan Manchester](http://dmanchester.info)

Datasource: https://www.kaggle.com/lava18/google-play-store-apps",0f5de724,0.0
236,bb0905d33ae417,e502a471,"# Introduction
`fastai` is a free deep learning API built on [PyTorch V1](https://pytorch.org/). The [fast.ai team](https://www.fast.ai/2018/10/02/fastai-ai/) incorporates their reseach breakthroughs into the software, enabling users to achieve more accurate results faster and with fewer lines of code.

This kernel illustrates the simplicity of deploying the `fastai.vision` package for image classification tasks. I am in no way a domain expert in this topic, in fact having no domain knowledge at all before this competition! I will heavily rely on published kernels (which are all referenced under [Acknowledgements](#Acknowledgements)) in guidance for setting hyperparameters in this task.

I will be deploying standard techniques taught in the fast.ai course to see how well these techniques can perform without needing expert knowledge. The techniques are:
1. Learning rate finder
2. 1-cycle learning
3. Differential learning rates for model finetuning
4. Data augmentation
5. Test time augmentation
6. Transfer learning via low-resolution images

This kernel had the previous name of **Minimal fast.ai kit for image classification**, which is a slight misnomer now, considering the detailed techniques being deployed in this image classification task.",25fd1965,0.0
238,bb8f5d7807718b,0bcfdb2d,"I recently wrote a two part article on some of the advanced plots in Matploltib.I am sharing some of the code snippets here but incase you want a more detailed read, you can read the articles here:

* [Advanced plots in Matplotlib — Part 1](https://parulpandey.com/2020/08/04/advanced-plots-in-matplotlib%e2%80%8a-%e2%80%8apart-1/)
* [Advanced plots in Matplotlib - Part 2](https://parulpandey.com/2020/08/17/advanced-plots-in-matplotlib%e2%80%8a-%e2%80%8apart-2/)

The basic capabilities of the Matplotlib library, including the ability to create bar graphs, histograms, pie charts, etc. are well known. However, in this article, I will showcase some of the advanced plots in matplotlib, which can take our analysis a notch higher.

Also here is a handy cheatsheet provides an excellent glimpse of the various functionalities of Matplotlib.

![](https://parulpandeycom.files.wordpress.com/2020/08/0-1-1.jpeg?w=600)

Matplotlib cheat sheet. Full image: https://lnkd.in/dD5fE8V",181ec286,0.0
239,bbaa07ad21cf4e,460972ef,"## 1. Import libraries <a class=""anchor"" id=""2""></a>",3ab6b254,0.0
240,bbad077c274022,729def29,"> Do the difficult things while they are easy and do the great things while they are small. A journey of a thousand miles must begin with a single step. 
*-Lao Tzu*

I came across Kaggle recently; in the process of heading towards Data Science. This is my very first notebook which I am publishing. 

I see folks here tremendously showcasing their works and exploiting the datasets to get the gem out of them. I thought of starting with a small dataset which can be more linked with me. So, I am performing a short data analysis on my walking activity by analysing the step counts I make every day.

I have recently installed an app in my mobile which tracks my steps - the outdoor and the basic ones; on hourly basis. 
    
![](https://i.imgur.com/a8p80vDm.png) 
        ",3c2e3dea,0.0
241,bbb3f4b76a4559,78bcb96d,"# LightGBM_with_Optuna
This notebook show you how to use lightGBM and how to tuning with optuna.  
This is my first public kernel and I'm a beginner of ML (and also English...). So if you feel ""I don't understand what this note say"" please don't hesitate to comments and questions. 
  
  
The highlight of this kernel is :  
Using optuna to tuning hyperparameter : Hyperparameter tuning is always bother but optuna automatically find them.  
Using PCA to drop feature dim : The titanic dataset is not learge and doesn't have too many cols to learn but in ML this is a rare case. This kernel introduce how to drop dims with PCA.  
Using SMOTE to oversampling : Survived label is imblance and sometimes imbalance dataset makes difficult to predict. So I show the way to use SMOTE the library to preprocess imblanced data. (Maybe this not suit titanic dataset...)",75185823,0.0
244,bd0e173abb7b52,af038589,"<center>
<img src=""https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg"">
    
## [mlcourse.ai](mlcourse.ai) – Open Machine Learning Course 

<center>Author: [Yury Kashnitskiy](http://yorko.github.io) <br>
Translated and edited by [Sergey Isaev](https://www.linkedin.com/in/isvforall/), [Artem Trunov](https://www.linkedin.com/in/datamove/), [Anastasia Manokhina](https://www.linkedin.com/in/anastasiamanokhina/), and [Yuanyuan Pao](https://www.linkedin.com/in/yuanyuanpao/) <br>All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.",9bce3b0d,0.0
246,07544ba83da480,ded4b4c5,# SALES ANALYSIS,dc2f52b1,0.0
247,bddd799cdbbae8,3927c814,"# ****Predict Multi Cyberbullying Tweets with Multiclassifier****

This project aims to predict cyberbullying tweets using the Support Vector Machine classifier, multinomial Naive Bayes classifier, and Logistic Regression classifier. There are six types of bullying used: not bullying, gender, ethnicity, age, religion, and others cyberbullying. In achieving the goal, the steps that will carry out are as follows.
## Contents
<a href='#1'>1. Import Library </a> <br>
<a href='#2'>2. Dataset </a> <br>
<a href='#3'>3. Data Preprocessing </a> <br>
<a href='#4'>4. Data Visualization </a> <br>
<a href='#5'>5. Spliting Data </a> <br>
<a href='#6'>6. Features Extractions </a> <br>
<a href='#6'>7. Classification </a> <br>
<a href='#7'>8. Evaluation </a> <br>
<a href='#8'>9. Conclusion </a> <br>
",b44e3c08,0.0
248,be2f4d8a6b73ca,0cc090ab,"# <span style=""font-family:cursive;"">Predicting Heart Disease using Machine Learning ❤</span>",5d8ce40a,0.0
250,c349ee5a821411,e6d0f604,"HAPPINES IN THE WORLD

![](https://ichef.bbci.co.uk/news/660/cpsprodpb/1655A/production/_92928419_thinkstockphotos-508347326.jpg)



Hi there!
Im gonna show you some vizualitations about the Happiness Report from 2015.
In this Kernel i want to emphasize in the correlation of different variables with the Happiness Score and extract some insights.",572b269d,0.0
252,c970849d1f6da2,47ebf34c,"# Language Model on COVID Research Papers
* This notebook explores COVID related research papers through a custom skipgram language model trained on the corpus. 
* Analysing the word embeddings could help draw connections between words and the model can be used for information retrieval, to pull out certain topics, meanings from the documents

",056e3955,0.0
253,c98a0dbd5eb6d7,4c6930c9,"Number of cases in the wild will always be higher than the number of cases detected by health officials. 

ie. (Actual cases > confirmed cases)

An asymptomatic patient can lead to far wider spread of the disease and eventually lead to a condition known as *community spread*.  

The only way one can control it is by increasing the number of tests. 

Countries which were pro-active in prepaing for the COVID-19 pandemic,were able to control the spread & have a lower mortality rate. Their labs were preparing testing kits as far back as in early Jan [link to news article](https://www.nytimes.com/2020/03/20/world/europe/coronavirus-testing-world-countries-cities-states.html)

Please also read Nate Silver's critique on how the [number of case counts is meaningless](https://fivethirtyeight.com/features/coronavirus-case-counts-are-meaningless/) unless it's correlated with the testing strategy. 

In this notebook, I will illustrate the impact of testing on the total number of cases.
1. Iceland has a large outlier, having conducted more than 45k+ tests/ million
2. Note that Iceland has a population of 3.64 lacs & has conducted 16k+ tests! 
3. Within Italy, the region of Veneto has a higher incidence of tests/million while the confirmed cases/million is also lower  

Following countries have a high ``Tests/Positive (Confirmed Cases)``

|Country/Region | Tests/Positive |
| --- | --- |
|Canada-NW territories | 833 |
|UAE | 385 |
|Russia | 292 |


+ All Canada territories have high ``Tests/Positive (Confirmed Cases)`` 
+ For the following countries the ratio is 

|Country/Region| Tests/Positive |
| --- | --- |
|Australia| 53|
|Singapore| 69|
|Taiwan| 98|
|Hong-Kong| 132|  

*Assumptions & Limitations*
1. Countries have different testing strategies.
2. Testing strategy is often a function of the

   a. country's preparedness for the disease,
   
   b. availability of testing kits and labs and often
   
   c. political will!
   
3. The data has been collated from multiple sources, hence it's as good as the data collected
4. Sources of the data points are cited in the [data file](https://www.kaggle.com/skylord/covid19-tests-conducted-by-country)
5. Highly connected cities/countries will have a larger incidence of the disease",33c15d04,0.0
254,c9b4e282e4e2c1,bdff965e,"First of all, why is so important the growing application of data science in sports? I want to do a short dissertation here. As a novice in the Kaggle environment, I think this is a good oportunity to present the motivations that lead me to focus on the Data Science Field, especially in its applications to sports. I think that the main goal of Data Science is improving the quality of life of the people. At least, that is the vision that I have. So the natural course is using the data in the field of professional sport: with the data, we can study about the injuries that ends promising careers of athletes and how to avoid them; about how can we improve their performance on the field and consequently their satisfaction and self esteem (mental health has a great impact on the efficiency of these athletes too); in conclusion, we study about how can we improve their security and well-being.  ",f44d339f,0.0
255,c9dc8d00773da4,055b6ab8,"# What's parquet file

I have not use with `parquet` file and do not know how to use it.

For now, I want to retrieve image information.",d9aa2f85,0.0
258,caaa6793391520,4113079f,"# Debiasing Imputation #

This notebook is about dealing with missing data that does not increase bias (gender bias, race bias, etc.), or even potentially reduce it

",1e79f342,0.0
260,cb4ad8ed4cb300,4c0cabaa,# 1. Questions,7c0f3236,0.0
261,cb570c7b7f0501,2e3208eb,"
# Project: Investigate a Dataset (Medical Appointment No Shows)
## Table of Contents
<ul>
<li><a href=""#intro"">Introduction</a></li>
<li><a href=""#wrangling"">Data Wrangling</a></li>
<li><a href=""#eda"">Exploratory Data Analysis</a></li>
<li><a href=""#conclusions"">Conclusions</a></li>
</ul>",a200a0ec,0.0
262,cb6f349e54c2a1,79f8e3c3,"# Introduction:

This notebook is an introduction for building a simple Neural Network using TensorFlow through Sequential and Functional APIs and Estimator API. The notebooks deals with MNIST digits dataset.

**TensorFlow**

Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. TensorFlow bundles together a slew of machine learning and deep learning (aka neural networking) models and algorithms and makes them useful by way of a common metaphor. It uses Python to provide a convenient front-end API for building applications with the framework, while executing those applications in high-performance C++.

![image.png](attachment:image.png)",962bade8,0.0
264,cd10f3afd970b3,9fc74237,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",2db3c8e4,0.0
266,cdde3953a908f7,53f3c1f2,Credit Baseline  - https://www.kaggle.com/abhishek/melanoma-detection-with-pytorch,15fa5897,0.0
267,ce7abd85d777b5,27e9bdcc,"According to OMRON website(https://components.omron.com/us-en/solutions/iot ),in the home security field, a wide variety of sensors are used with wireless camera networks such as detecting doors and windows open/close status, warning you if you left a window or a door open, monitoring and measuring room temperature, humidity and light intensity, and detecting motion when a person or an animal is in the room.

![IoT-sec4_home-security-min.png](attachment:10d726e6-ffdf-47cb-a37e-1a8227255f3f.png)

(from OMRON website https://components.omron.com/us-en/solutions/iot )

And according to the abstract of article 'Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models' ( https://www.sciencedirect.com/science/article/abs/pii/S0378778815304357 ), it may be obvious that 'Light' is the most critical feature for predicting. This is the Graphical abstractof Occupancy CART model for temperature, humidity, light, CO2 and humidity ratio.

![1-s2.0-S0378778815304357-fx1_lrg.jpg](attachment:ba2b4b1a-c2b7-4eaa-bc98-c8d95378bdc2.jpg)
image from https://www.sciencedirect.com/science/article/abs/pii/S0378778815304357

But on the other hand, light intensity is sometimes tuned by people, and it may cause 'leakage'. And if we would like to use this model for home security, 'Light' is not suitable feature because thief does not turn on the light.

So I tried to find the model without 'Light' feature.

![image.png](attachment:3ed93f5e-6e7f-4fdb-a988-bc76936b2cb3.png)

By useing PyCaret, I found that Extra Trees Classifier is the best model.
Accuracy: 0.9853, AUC: 0.9962 may be not so bad. We can use it without 'Light'.",0a340dbb,0.0
268,ce9ed5e2d601d7,21ca4cc9,"# Welcome and have fun learning multiclass classification with plug and play Neural Network

#### You are dealing with a **heavly skewed** dataset when some classes(1, 2 and 3) are much more frequent than others. **Accuracy** is not the perferred performance measure for multiclass problem in this case. Softvoting and weighted average is the objective to score towards target class 1, 2 and 3.

Objective of this notebook used to be a ~simple~ and robust neural network multiclass classifier for future use.

<blockquote style=""margin-right:auto; margin-left:auto; padding: 1em; margin:24px;"">
    <strong>Fork This Notebook!</strong><br>
Create your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.
</blockquote>

**Notes:**
Run time -
4 hours and 31 minutes 4000000 samples
3318.3s 395712 samples
4375.7s 1468136 samples
12905.1s 2262087 samples
5555.3s - GPU 2262087 samples
6941.8s - TPU v3-8 2262087 samples

Version 98: 300, 256, 128, 128 0.1DROP MCDrop
Version 121: AlphaDropout dropped bad performance

## Imports and Configuration ##",f58a2f43,0.0
271,cee088a6840708,56096baf,"# Graph deep learning with keras 

I will discuss Graph deep learning in depth. I will focus in each kernel on one topic.
In this kernel, I will take screenshots from [Kipf paper](https://arxiv.org/pdf/1609.02907.pdf) and also from the [dgl library](https://github.com/dmlc/dgl/tree/master/examples/tensorflow/gcn)

## Motivation
> Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural network models to such structured datasets.
In the last couple of years, a number of papers re-visited this problem of generalizing neural networks to work on arbitrarily structured graphs (Bruna et al., ICLR 2014; Henaff et al., 2015; Duvenaud et al., NIPS 2015; Li et al., ICLR 2016; Defferrard et al., NIPS 2016; Kipf & Welling, ICLR 2017), some of them now achieving very promising results in domains that have previously been dominated by, e.g., kernel-based methods, graph-based regularization techniques and others.   WRITTEN BY [KIPF](https://tkipf.github.io/graph-convolutional-networks/) 


## GCN vs RGCN
This tutorial is for GCN. I wrote another [tutorial on RCGN.](https://www.kaggle.com/elmahy/relational-graph-convolution-networks-explained)

|  | <font style=""color:red"">Graph convolution networks</font> | <font style=""color:red""> Relational Graph convolution networks</font> |
| - | - | - |
| Type of netwok that it works on | Simple just some nodes connected with edges | A directed labeled network  |
| Example | papers citing each others | knowledge graph (google it)
| Weights | Just make a dense layer for each node and pass messages from each node to the other | make a dense layer for each relation type since we have many types of relation e.g. ahmed employes(relation type) mohamed, John visits (relation type) [Dahab](https://www.youtube.com/watch?v=CheLNATs4IA&t=33s) because it's awesome and cheap and the author can invite you for dinner there.",55463e1c,0.0
272,cf08b03b002c13,68ed96f2,"# NSFW (18+) content On Reddit
* ### How big is NSFW content?
* ### How do people react on NSFW conent?
* ### How many NSFW posts are deleted and who is deleting them?
* ### Does the score rises once the post gets attention?",104d416f,0.0
274,c3dfa835621ac4,d23117a2,"This work is greatly inspired by [Cellular Automata as a Language for Reasoning](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning), [Simple and Interactive visualization of tasks](https://www.kaggle.com/bsridatta/simple-and-interactive-visualization-of-tasks#Interactive-visualization), and [Training Cellular Automata Part I: Game of Life](https://www.kaggle.com/teddykoker/training-cellular-automata-part-i-game-of-life). Great thanks to all the great works!

# Key Points 

This notebook

1. Proposed a DSL to describe Cellular Automata rules in a simple, plain-text format like
    
        x x x
        r g b -> b
        r r r
    
2. Created a interactive tool for fast CA experiment. Simply type in your rules and see how it works.

<p><font color=""red"" size=3>If you like my work please upvote this kernel. It encorages me to produce more quality content!</font></p>



# Intro

Since Cellular Automata(CA) could be a powerful tool to solve this task, this notebook is to provide a way leading to CA-based Domain Specific Languages (DSLs) which is likely easily simulated or synthesized by machine learning models. If you don't have the knowledge of CA, [Cellular Automata as a Language for Reasoning](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning) is a greate introduction. 

# Rule Format

## Basic Format

To step further, this notebook is an exploration in developing a DSL with more regular and less number of syntax rules. Rather than write CA rules in pure Python code, a CA rule can be written in a form like

    x x x
    x x x, m -> o, n
    x x x

On the left hand of the arrow, the 3x3 array is a filter to match a 3x3 neighbor area around the central point (maybe 5x5 would be more powerful?), and `m` after the comma is a memory used in [Cellular Automata as a Language for Reasoning](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning) to expand the capability of CA. On the right side of the arrow, `o` is the output and `n` is the updated memory. This rule simply means ""when the 3x3 neighbor area matches the template and the memory equals the given value, change the value of the central point to `o` and update the memory at this place to `n`"". Of course we can omit the memory part, which implies we are not comparing the memory value and not updating it. 

Simple wildcard is useful and will be supported, like a dot(`.`) means matching any value.

## Transform of Filters

To let us write less number of rules, we introduce the transform of filters. To explain this, let's take a simple example: if the value at any of the four corners are 0, change it to 1. For the case of top-left corner, it's easy to be written as

    # # #
    # 0 . -> 1
    # . .
 
where `#` means out-of-border and `.` means ""any value"". However, we need to repeat 3 more times to write rules for another corners.

However, the filters for other 3 corners are merely rotations of that for top-left corner. If we introduce a concept of ""rotation"" we can unify the four rules into one, like

         # # #
    [R4] # 0 . -> 1
         # . .
         
where `R4` means a rotation with number 4 (rotation in 90 degrees results in 4 distint filters).

## Syntax List

All the syntax of the DSL introduced in this work are listed as follows:

A CA rule is in form of

        x x x
    [T] x x x, m -> o, n
        x x x

where `T` is the transform, 3x3 `x`s are the template, `m` is memory input, `o` is output, and `n` is memory update. `T`, `m` and `n` can be optional. When no `T` is presented, the default is no transform, i.e. exact match.

### Memory

Only integer values are supported in memory at this point, and only the value in memory at the central point is considered (not its neighborhood). It could be useful if we take into account the same neighbor region in memory as in the state, as a future work.

### Transforms

Supported `T`s:

- `I`: fixed filter, i.e. exact match, the default value

- Histogram: only the count of certain values matters (the location is irrelevant). 
  + `H` histogram includes the central point. 
  + `HN` only compares the histogram of 8 values in the neighborhood and the central value should be exact match. 
  + `HD` is like `HN` but only takes into account the 4-direct neighbor values(i.e. the values at 4 corners are not compared).

- rotations and flips(mirrors): `R8`, `R4`, `R2`, `L8`, `L4`, `L2`, `FH`, `FV`, `FD`, `FA`, `R8F`, `R4F`, `R2F`, `L8F`, `L4F`, `L2F`
  + `R`, `L` means left or right rotation, respectively
  + the number means the angle of rotations, 8 means 45 degrees (resulting in 8 distinct rotations), 4 means 90 degree, and 2 means 180 degree. So infact `R2 == L2`. However, if the filter itself has a certain symmetric property, the result will be less than the number indicated. E.g. the 90 degrees rotation of a filter equals itself, so `R8` only ends up with 2 filters.
  + `F` means flip (mirror), while `H`, `V`, `D`, `A` means horizontal, vertical, diagonal (top left to bottom right), anti-diagonal (top right to bottom left)
  + the suffix `F` in any `R` or `F` means also including the rotations of the mirror of the filter. 

### Filter

 For the values in a filter, some special values are defined:
 
 - `#`: out of border
 - `.`: any value include out-of-border
 
 Some other values might be also useful but not supported yet, like: `*` any value inside the border;
 `+` any value but black (0). Apparently we can introduce more complex syntax leading to a regex-like rules, and this is a trade-off between capability and simplicity.
 
 And the color values are defined as follows. Both index and code are supported, e.g. either `0` or `k` can be used for ""black"".
",0126bdad,0.0
275,0687cd5c8597db,8f0a7c69,# <center><b><u>Hand Written Digit Recognition using CNN</u></b> </center>,4edec76a,0.0
279,c54ea4523bd49c,fac1711f,I'm basing this kaggle on my previous [Plant Seedling - Simple CNN](https://www.kaggle.com/masonblier/plant-seedling-simple-cnn) kaggle notebook. ,097ccba2,0.0
280,06c7ba9203293f,c1168170,![_111325972_pub2.jpg](attachment:_111325972_pub2.jpg),1e1a2b48,0.0
281,c5fef7cc592736,f7ce7705,"# DIGIT RECOGNITION WITH FASTAI

In this notebook I will show how to arrange the data from this competition using fast.ai mid-level API. Many notebooks manipulate the data converting the columns to numeric vectors to images and the images to tensors, while it is a valid solution it takes extra steps not required. Here the vectors from the dataframes are arranged directly to tensors. ",d21dc2c1,0.0
282,c65a65d4041018,8dbbd92a,"## General information

Some time ago Kaggle launched a big online survey for kagglers and now this data is public. There were multiple choice questions and some forms for open answers. Survey received 23,859 usable respondents from 147 countries and territories. As a result we have a big dataset with rich information on data scientists using Kaggle.

In this kernel I'll try to analyze this data and provide various insights. Main tools for this will be Python and seaborn + plotly.

***Scroll down to find new interactive visualizations where you can choose any country and the graph will be shown for it***

![](https://www.kaggle.com/static/images/host-home/host-home-business.png)

### Main idea: comparing countries

While there are many ways to analyse the data, I have decided to perform the analysis based on the country of the responders. You'll see more about this lower.",824fb229,0.0
283,c65f7b375af4ef,5273edc3,"# TAVSİYE SİSTEMLERİ

**Bu kernelde size sosyal medya , web sitelerinde ve mobil uygulamalar gibi platformlarda reklamlardaki önerilerin nasıl karşınıza çıktığını açıklayacağım **

Tavsiye sistemi nedir ?  nasıl çalışır ? 
Günlük hayatta karşımıza çıkan  ne yiyebilceğimi , ne izleyeceğimizi, kimlerle ile arkadaş olabilceğimize kadar bir çok tavsiyeler vermektedir. 
Bu tavsiyeler, kullanıcıların dijital dünyada verdikleri kararlar sayesinde oluşan  veriler sayesinde çalışır.
Örneğin sizin neylerden hoşlandığınızdan ziyade size benzeyen kullanıcaların yaptıklarına bakarak size onu tavsiye eder

### kısa basit  bir örnek
Siz kullanıcı olarak Kolpaçino ve maskeli beşler filmlerini izliyorsunuz veya bi kaç tanesini izlediniz. burda da datasetimizde oyuncularında tutulduğunu ve kategorileri de  sayarsak size en çok 
önereceği filmler ya komedi filmidir yada Şafak Sezer' in fimlerinden bir tanesidir.  ;) 
",871d53ca,0.0
284,c6673ce23495fc,f104168e,"# CombPurgedKFoldCV 

Refer to *Advances-in-Financial-Machine-Learning* p.163

Original Code from https://github.com/sam31415/timeseriescv

Modification:

Fixed an error in the code.

embargo delta time accepts int type(number of rows) instead of pandas Timedelta object",43f1ea4a,0.0
286,c6f8ff61a5fa87,04a9a856,"# <span style=""color:red;"">Machine learning-detected signal predicts time to earthquake</span>",3eea586b,0.0
289,c7e5f658090347,5f9dda62,"## Summary 
This is a binary classification problem with a highly imbalanced dataset (ratio of approx. 1:600) that is about distinguishing fraudulent card transactions from non-fraudulent transactions. The input features are the card purchase amount and a set of 29 principal components (PCs) from principal component analysis (PCA). To tackle this challenge I attempted to use (1) both over and under sampling, (2) feature removal, (3) outlier removal (only from the not fraudulent class, because I did not want to lose any examples from the fraudulent class) and (4) various ML models and hyperparameters (with GridSearchCV).

#### The Final Scores are as Follows: 

| Model | ROC AUC for Train/Test Dataset | ROC AUC for Validation Dataset |
| --- | --- |  --- |
| Logistic Regression | 0.983 | 0.947|
| Random Forest |  0.983 | 0.919 |
| Support Vector Machine | 0.985 | 0.931 |
| Gradient Boosting | 0.984 | 0.931 |

Meaning the Logistic Regression model had the best performance on the validation set, but all models performed very similarly. Note that the ROC AUC is the [Area Under the Receiver Operating Characteristic Curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), a metric that can handle class imbalances. 

When studying the above results we can see most models have highly similar performances (according to this metric). Analysis of the confusion matrices would suggest I have made a good filter to identify ""suspicious transactions"", as whilst many non-fraudulent transactions are labelled as fraud, almost all fraudulent transactions were correctly identified. I think this is likely in part due to using outlier removal on the not fraud class, meaning more uncertain cases would be trained to predict fraud over not fraud. In a practical context this may actually be ideal. One can imagine a scenario where a fairly rapid pre-filtering model (something like this) is used to identify potentially fraudulent transactions (but casts a wide enough net to ultimately catch many non-fraudulent ones too). These transactions could then be passed onto a more advanced model (that could for instance also consider a given card user's most recent purchases, location etc..) to hopefully make a more accurate decision at that point. If this was the goal, then the model developed here could be further improved by working on the outlier removal of the non-fraudulent data (or using harsher filtering criteria) and/or altering the cost function used to make false negatives more undesirable.  


### Links to the Different Sections of the Notebook

### [Setting up the Environment and Reading in the Dataset](#Enviro_Setup)

### [Exploratory Data Analysis](#Exploratory_Data_Analysis)

### [Comparing the Distributions of each Feature to the Target Classes](#Feat_Distrib)

### [Linear and Non-Linear Correlations to the Target Class](#Correlations)

### [Some Initial ML Model Tests and Outlier Removal](#Initial_ML)

### [Final Model Building](#Final_ML)

### [Model Evaluations](#Model_Eval)

### [Comparison of the Feature Importances for the Different Models](#Feauture_Importance)",43c78e7d,0.0
290,c80939c7c626cf,98272a4d,"# 1 Collecting the data
Download from kaggle directly https://www.kaggle.com/c/3136/download-all",b9ac31e2,0.0
291,c818250dd720eb,815d397d,"# Introduction


**The Task:** Design a system to grade Whole Slide Images of Prostate Tissue Biopsies.


**The motivation:** Within Cancer grade assessment there can be significant inter-observer variability, and the time of expert pathologists is a valuable limited resource. Machine Learning has already shown promise in this domain. This system was designed as an entry to a $25,000 Kaggle competition hosted by the Karolinska Institute Medical University (Sweden) and the Radboud University Medical Center (Netherlands).


**The Data:** 10,616 Whole Slide Images (between 5,000 and 40,000 pixels in both width and height, lots of empty space) of prostate tissue (format: Multi-level tiff file), with accompanying ""Masks"" which contain pixelwise labels for each image.  


**The System:** 
1.     Access the intermediate layer of the Whole Slide Image multi-level tiff file, split it into 224x224 px tiles and select the 30 tiles with the most tissue.

2. Each of the selected tiles is fed to three distinct binary classifiers. These binary classifiers consist of a RESNET50 model pretrained on ImageNet, and trained on our Data to detect one of three possible Gleason patterns of prostate cancer.

3. The output of each of these models are collected for each of the 30 tiles. These 90 values, split according to data provider are fed to one of two small Neural Networks which were trained on our data to output a final ISUP grade 0,1,2,3,4 or 5.


**The Result:** Quadratic Weighted Kappa score of 0.53. (1 is perfect, 0 is no better than chance)


**Designing the System:**


The rest of this notebook will describe at a relatively high level each step of designing and training this system. Links to the code will be provided, and questions in the comments are welcome. Techniques which proved fruitful for other entrants will be discussed as well as the principal avenues for improvement.",68ee40de,0.0
292,c84925c8171900,4c9e1b05,"<h1 style=""text-align:center"">   
      <font color = purple >
            <span style='font-family:Georgia'>
                EDA - 🎮 Video Game Sales
            </span>   
        </font>    
</h1>
<hr style=""width:100%;height:5px;border-width:0;color:gray;background-color:gray"">
<center><img src=""https://miro.medium.com/max/700/0*73xpoUa9e8Q_vH6e.jpg""></center>",e21ff7ec,0.0
293,c85c94076e9c3a,8d20e450,"# project : Marketing Data
",3ea0c443,0.0
294,c8bf959b9608cf,72f467eb,Import the libraries,155e3672,0.0
299,b8ffad33f2b369,e33637d0,"# Credit Card Fraud Detection

Throughout the financial sector, machine learning algorithms are being developed to detect fraudulent transactions.  In this project, that is exactly what we are going to be doing as well.  Using a dataset of of nearly 28,500 credit card transactions and multiple unsupervised anomaly detection algorithms, we are going to identify transactions with a high probability of being credit card fraud.  In this project, we will build and deploy the following two machine learning algorithms:

* Local Outlier Factor (LOF)
* Isolation Forest Algorithm

Furthermore, using metrics suchs as precision, recall, and F1-scores, we will investigate why the classification accuracy for these algorithms can be misleading.

In addition, we will explore the use of data visualization techniques common in data science, such as parameter histograms and correlation matrices, to gain a better understanding of the underlying distribution of data in our data set. Let's get started!

## 1. Importing Necessary Libraries

To start, let's print out the version numbers of all the libraries we will be using in this project. This serves two purposes - it ensures we have installed the libraries correctly and ensures that this tutorial will be reproducible. ",484e5560,0.0
301,b86bda7afe3ac3,f7e781ee,"# Basic Text Regressor using Auto-Keras

Basic implementatio for toxic comment challange.",16197934,0.0
304,a7eab06345d255,6677dd40,"# **Download results**
We are using Time-series modeling to predict the WTI by determing seasonality and flutuation.

Instructions:

 

Click on the ""Copy & Edit"" blue button on top
Click on Run All 
Once the Code is executed, it will create a file in Output Folder (Under Data - right hand side panel)
Download the file ""Submission.csv""

 
You can save a copy of the code by clicking ""Save Version"" buttom on top 
Select ""Save and Run All""
You can share the notebook if you make any changes for the benefit of others
Exit the notebook - Click leave editor
 
 
Now go back to the competition main page
https://www.kaggle.com/c/ntt-data-global-ai-challenge-06-2020
 

Click - Submit Predictions
Upload the file ""Submission.csv""
 

Try out by changing some of the variables in the code and rerun it. If you need help - please post a query in the comments section.

Please make changes in the date while submitting code currently submissions for prices after 15 june 2020 is not evaluated and considered as 0",f87834a4,0.0
305,a81661cc35d8d2,dd1bc7f4,"<b><font size=""10""><center>Heart Failure Prediction</center></font></b>",3331f113,0.0
307,a871419285588a,8c6ec0a3,"# Brain Tumor

### This Notebook was collected from this [here](https://github.com/guillaumefrd/brain-tumor-mri-dataset) and then slightly modified",5e08e15f,0.0
308,a8c042af6b7245,bb2c1114,"#### This kernel used from the Porto Seguro’s Safe Driver Prediction and copied from the 'Data Preparation & Exploration' written by Bert Carremans

#### Data Preparation & Exploration : [URL](https://www.kaggle.com/bertcarremans/data-preparation-exploration) 

*Thanks for sharing kernel, Bert Carremans*",2487ac62,0.0
309,a915263bc207da,6ccb629a,"# A Movie Recommendation System using Item Based Collaborative Filtering
The recommender is created using data from [GroupLens](https://grouplens.org/) where movie and other data is made public for research and other purposes. The two files *u.item* & *u.data* containing movie rating data of the 90's movies were downloaded.


**Item Based Collaborative Filtering:**

Item-item, item based or item to item, is a kind of collaborative filtering technique used to recommend items through finding similarity scores based on ratings on those items provided by users. It was invented by Amazon.com and has many advantages over the pre-existing user based collaborative filtering as it was computationally expensive due to keeping records of all users. Again, it was easier to game the system. This notebook is a demonstration of a simple process to recommend items to users based on their ratings. As mentioned in above the dataset was downloaded from grouplens.org as file names: u.data & u.item",b17ebcda,0.0
310,aa46e9376825a5,894253ed,"# Ohio State’s leading causes of death from the year 2012.
You have been provided with a dataset that lists Ohio State’s leading causes of death from the year 2012. Using the two data points:
1. Cause of deaths and
2. Percentile


Draw a pie chart to visualize the dataset.",57792d96,0.0
312,08e3444f9eddcf,987aaf66,"# About this notebook

You might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting. The reason for that is there are many thousand different entries for each `installation_id`, each representing an `event`. This notebook simply gathers all the events into 17k groups, each group corresponds to an `installation_id`. Then, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each `installation_id`. After that, it simply fits a model on that dataset.
Credit to @Tanrei(nama).",1d9d4f73,0.0
313,aa7db7b023d0a2,d49c0cfb,"<h1 style=""background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%"">Amyotrophic Lateral Sclerosis Mortality</h1>

Amyotrophic Lateral Sclerosis Mortality in the United States, 2011-2014

Citation: Larson TC, Kaye W, Mehta P, Horton DK. Amyotrophic Lateral Sclerosis Mortality in the United States, 2011-2014. Neuroepidemiology. 2018;51(1-2):96-103. doi: 10.1159/000488891. Epub 2018 Jul 10. PMID: 29990963; PMCID: PMC6159829.

""The International Classification of Disease, 10th Revision (ICD-10) did not include a code specific for Amyotrophic lateral sclerosis (ALS) until 2017.""

""The proportion of excluded records coded G12.2 but not ALS was 0.21, resulting in 24,328 ALS deaths. The overall age-adjusted mortality rate was 1.70 (95% CI 1.68-1.72). The rate among males was 2.09 (95% CI 2.05-2.12) and females was 1.37 (95% CI 1.35-1.40). The overall rate among whites was 1.84, blacks 1.03, and other races 0.70. For both sexes and all races, the rate increased with age and peaked among 75-79 year-olds. Rates tended to be greater in states at higher latitudes.""

""Conclusions: Previous reports of ALS mortality in the United States showed similar age, sex, and race distributions but with greater age-adjusted mortality rates due to the inclusion of other diseases in the case definition. When using ICD-10 data collected prior to 2017, additional review of multiple-cause of death data is required for the accurate estimation of ALS deaths.""

https://pubmed.ncbi.nlm.nih.gov/29990963/",ec912af3,0.0
316,ab6da5994949a3,25fb57ef,"Hi, I'm going to apply 6 supervised machine learning classification models and an ANN model on the given dataset to classify mushrooms as poisonous or eatable.
1. Logistic Regression
2. Support Vector machines (SVC)
3. K-Nearest Neighbours(K-NN)
4. Naive Bayes classifier
5. Decision Tree Classifier
6. Random Forest Classifier
7. Artificial Neural Networks

I'll proceed by converting categorical variables into dummy/indicator variables, then reducing dimensions using Princple Component Analysis to reduce 23 categorical variables (which will become 95 variables after conversion)
to only 2 variables (Principle Components) and training different classification models over these two principle components. Finally, I'll visualize the outputs so that decision boundaries of different models can be seen in 2D-plane. Here the preference is not given to accuracy as the goal is to visualize the decision boundaries. For greater accuracy one can choose more than two variables.",fae6b91d,0.0
318,ac04ba639d1c93,c87c51fd,"<img src=""https://raw.githubusercontent.com/EdsonAvelar/auc/master/molecular_banner.png"" width=1900px height=400px />",748059d5,0.0
320,0885edf1a61429,6426ad36,"I reported that [discovering missing labels and retraining](https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/208830) did not worked.  
For your inference, I show missing labels example.

example: *0ab6aa734.flac*",cf8ce75e,0.0
323,a758983a68c014,7f77bfed,"# <center> Diving into Word2Vec basics. Skip-Gram implementation with Pytorch. <center>
##### <center> By Artur Kolishenko (@payonear)",ab89f181,0.0
324,ace6043570c334,00c57662,"This Kernel is base or upgrade on [Alexander Milekhin](https://www.kaggle.com/kenseitrg/simple-fastai-exercise) and [Hsin Wen Chang](https://www.kaggle.com/hsinwenchang/simple-fastai-exercise-densenet-201), here I applied transfer learning skill with [vgg19_bn](https://pytorch.org/docs/0.3.0/torchvision/models.html#id5)",9678a57a,0.0
325,a6eb631926a4d7,c09e75a5,"# Reviews Classification Using SVC, Naive Bayes & Random Forest",0d502aab,0.0
327,a1ba5ffd30dbde,3c43b895,"<h2 align=""center""> Predicting Parkinson’s Disease </h2>",48e57546,0.0
328,a1c7a94fc12ad8,c588501d,Beginners guide to Image Segmentation with fast.ai,c67e3237,0.0
329,a1dcd92986bc84,e35d7309,**Credit:** All I did here is I took the tutorial from https://keras.io/examples/nlp/nl_image_search/ and then I made a bunch of small modifications in order to get it to run inside of a GPU-enabled Kaggle Notebook.  The natural language image search is a neat concept!  It was fun to explore.,730acaaa,0.0
331,a2286e7c88bb76,8780e5e3,## Importing the libraries,be48f3fb,0.0
332,a2444ab5d5f147,f1cb6b00,# EDA Movie Review Dataset Rotten Tomatoes,10617755,0.0
333,a2573183738753,c3ee3a1c,# import all libraries,f6429599,0.0
337,0925f172b5eb74,b7728ad7,"# Introduction

This kernel is designed to work as the training environment for my final degree project, where I intend to create a Deep Learning model able to diagnose illnesses on vegetables, specifically working with the [plant pathology 2021 dataset](https://www.kaggle.com/c/plant-pathology-2021-fgvc8), and therefore can also be seen as one more attempt on the overall competition.

For this reason, the code present here will be only responsible for training and testing the models, as these tasks require heavy computations capabilities for which TPUs are needed. The models will then be saved into .h5 files.

For the rest of the tasks such as data analysis, dataset division and results analysis, they all will be performed locally. All the code related to the utilities functions that implement these parts can be found at github.

Check out the project at my [github repo](https://github.com/gfelis/TFG).",ec34cd72,0.0
339,a44368590e878a,e42294c2,# Import,77743ba8,0.0
340,a4a494c667c673,db92ab0c,"# Task for Today  

***

## Poland House Price Prediction  

Given *data about houses in Poland*, let's try to predict the **price** of a given house.

We will use a gradient boosting regression model to make our predictions.",397d12f8,0.0
341,a4aa36df07fd53,ba25ed87,"# Pelatihan Data Science Workshop PPI Hsinchu  

Kernel Asli : https://www.kaggle.com/andresionek/what-makes-a-kaggler-valuable/notebook?utm_medium=social&utm_source=twitter.com&utm_campaign=Weekly-Kernel-Awards

Selamat Datang di python jupiter notebook! Kaggle menyediakan komputasi online gratis dimana setiap sesi memiliki batasan:
Run time : 6 Hours
Memory : 16 GB
Environtment : https://github.com/Kaggle/docker-python/blob/master/Dockerfile

Dalam sesi komputasi ini kita tidak perlu melakukan instalasi apapun lagi karena hampir semua libray penting sudah di sediakan dalam environtmentnya. Untuk tahu lebih jelas apa yang sudah terinstall bisa akses link environtment diatas. Kita juga diberi batasan hanya diperbolehkan menjalankan komputasi sebanyak maksimal 25 script saja dalam waktu yang bersamaan. 
Ini bisa dimanfaatkan untuk mencoba berbagai macam ide nantinya kalau kita ingin mencoba ide mana yang berhasil dan tidak dalam iterasi model matematis. 

## Survey Komunitas Kaggle 2018  

Seperti yang sudah dijelaskan sebelumnya kita tidak akan bermain langsung dengan data finhacks dikarenakan saya tidak diperbolehkan untuk menyebarluaskan data dan model dalam bentuk apapun, oleh karena itu kita akan mencoba menerapkan dan berlatih teknik teknik dasar yang saya ketahui dengan menggunakan dataset hasil survey komunitas kaggle 2018 ini.

Survey ini di isi oleh 23,859 responden yang pengisi survey diberi pertanyaan seputar kegiatan dan posisi mereka sehari hari yang memiliki keterkaitan dengan datascience. Seperti framework apa yang mereka gunakan dan lain sebagainya. 

Sebelum memulai ada baiknya untuk mengamati sejenak semua elemen yang ada di komputer ini.

## Memuat Data  

Langkah pertama dalam setiap analisis data adalah memuat datanya sendiri, Disini kita menggunakan ""pandas"" sebagai data manipulator. Jalankan cell berikut untuk load data ke dalam environtment.
",d2f42b6d,0.0
342,a4f0a3e1316ff9,1895c3f7,"# Introduction

I do a lot of research in currency trading so this is and interesting data set for me. Thanks @ruchi798 for making this available.

I will be looking at the dataset with a particular interest in Australia, there is no particular reason for this and it should be easy to swap out for another country if a person is interested. If you feel there are things I could have done better please comment, I'm on Kaggle to learn.",53bf0160,0.0
344,a566b5b7c374e7,7b729a87,"# Personal Sleep Analysis (OURA, Sleep Cycle App)

This Notebook combines data from three separate sources:
1. Oura Ring - Data is collected in the Oura app and downloaded to a CSV file for use in this model. The following data is collected:
    - Raw sleep data, including the amount of time in each spent in each sleep stage each night. These are determined based on movement, pulse rate, and other proprietary factors, and the efficacy of Oura's sleep stage measurements have been disputed.
    - Raw heart rate, heart rate variability, and respiratory rate data. These are much more reliabile measurements
    - A variety of ""scores"" based on the raw data collected. These include Sleep Score, Deep Sleep Score, REM Sleep Score, Readiness Score, and Recovery Index Score, among others. 
2. Sleep Cycle App - Data is collected in the Sleep Cycle App and downloaded to a CSV file for use in this model. The following data is collected:
    - Bed time and waking time. This app is an alarm clock that you set each night.
    - Customizable tags/activities for each day. You can customize the list of tags. For exmaple, maybe you want to be able to analyze whether lifting weights impacts your sleep. Each day, before bed, I run through the list and tag the relevant activities for that day. 
3. Sleep Journal - I maintain a personal sleep journal in an Excel file. I collect the following information:
    - Restfulness Score. This is my own personal measure of how rested I feel.
    - Notes about how I feel throughout the day and my perception of how my sleep went. I might provide notes about staying up late, drinking alcohol, or feeling sick. Anything that might provide context when reviewing at a later date.
    - Dream notes in addition to a binary column for whether or not I had vivid dreams each night.


These are my objectives for this analysis:
1. Test the efficacy of the Oura Ring and the Sleep Cycle App and determine the most useful metrics. There is a lot of information housed in these apps, particularly the Oura app, and I'd like to know which of these metrics are most critical to how rested I feel every day.
2. Understand which of my activities correlate most strongly with these key metrics that impact my rest and recovery.
3. Identify beneficial/problematic patterns in my behavior that can be enhanced/corrected.
4. Present the information in fun, interesting ways that provide insights.
5. Build in such a way that allows others to plug in their data without too much trouble.

<br>

**Side Notes:**

- HRV (Heart Rate Variability) is a widely accepted measure of a person's readiness to perform difficult tasks. Higher HRV means that the nervous system is more balanced, as it is more responsive to the parasympathetic system and the sympathetic system. The parasympathetic system tells the heart to beat faster, while the sympathetic system tells it to beat slower. If HRV is lower, it means that the heart is responding more strongly to the inputs from one of these two systems, and that the body is working hard for some reason. Perhaps the person is sick, stressed, or recovering from a difficult workout or injury.


- I prefer to use *direct measurements* in this analysis wherever possible, rather than ""scores"" as determined by Oura. As such, I will favor metrics like ""Average HRV"" and ""Average Resting Heart Rate"" over metrics like ""Readiness Score,"" which is calculated in ways that Oura does not share.


- So far, I've only collected data for nearly 2 months (began at the beginning of February 2020). Any conclusions would be premature.

- This code was written in Jupyter Lab, and the code gets chopped up in weird ways sometimes in this notebook.",b3dc5545,0.0
345,a5a419dc7245b0,e47b7cfb,"**Your client is a large MNC and they have 9 broad verticals across the organisation. One of the problem your client is facing is around identifying theright people for promotion (only for manager position and below) and prepare them in time. Currently the process, they are following is:**
1. They first identify a set of employees based on recommendations/ past performance
2. Selected employees go through the separate training and evaluation program for each vertical. These programs are based on  the required skill of each vertical.At the end of the program, based on various factors such as training performance, KPI completion (only employees with KPIs completed greater than 60% are considered) etc., employee gets promotion
3. For above mentioned process, the final promotions are only announced after the evaluation and this leads to delay in transition to their new roles. Hence, company needs your help in identifying  the eligible candidates at a particular checkpoint so that they can expedite the entire promotion cycle. ",4279726e,0.0
347,a69d41047fdd3e,0bf71ae9,"**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**

---
",b1f28647,0.0
348,a6c34cd514e30e,8e224716,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",bf603ddd,0.0
349,ad121e0531afa4,910a0be5,"<div class=""alert alert-info"">
    <h2 align='center'>🐾 PyTorch Lightning Training Baseline for GPU & TPU + W&B Tracking 🚄</h1>
</div>

<p style='text-align: center'>
    To run the model on TPU, un-comment and run the below cell and change the <code>gpus=1</code> argument to <code>tpu_cores=1</code> or <code>tpu_cores=8</code> in the <code>Trainer</code> class.
</p>",a3492905,0.0
350,ad157351a0c298,460ce791,"### Meeting a Sayed Athar's request, I'm using the Kernel altered by Khoi Nguyen to explain how the whole code works.
### If any part is not clear, please comment.  
### Please upvote if it was helpful.",6e957e95,0.0
351,ad26c020235dfc,f3bb2b0e,"# Intro
Welcome to the [Acea Smart Water Analytics](https://www.kaggle.com/c/acea-water-prediction) competition.
![](https://storage.googleapis.com/kaggle-competitions/kaggle/24191/logos/header.png)

There are different waterbodies with different features. We will consider 
* Aquifer,
* Water Spring,
* Lake,
* River.

The goal is to predict the amount of water in each unique waterbody.

<span style=""color: royalblue;"">Please vote the notebook up if it helps you. Thank you. </span>",bf766e48,0.0
353,b4ecd6e4277e3c,f4cf9774,"### Preface

Hello . This is basically cutting and pasting from the amazing kernels of this competition. Please notify me if I don't attribute something correctly.

* https://www.kaggle.com/gmhost/gru-capsule
* How to: Preprocessing when using embeddings
https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings
* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing
* Simple attention layer taken from https://github.com/mttk/rnn-classifier/blob/master/model.py
* https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm
* https://www.kaggle.com/hengzheng/pytorch-starter

**UPDATE**: I seems that the shuffling the data doesn't add the features in the correct order. To address this issue I added a custom dataset class that can return indexes so that they can be accessed while training and properly put each feature with the corresponding sample. The training time though is increased, so you might need to make the model lighter in order to submit results.",94d79d5f,0.0
355,b547f0f38f7744,25e75659,"# TorchVision Faster R-CNN Finetuning

## Prepare Dataset",b6ba66b3,0.0
358,b57862be79c695,34023050,"# Introduction

Hello. In this data set, I will try to convert the numbers shown in sign language into computer language.

First, I will load data and after I will try to implement ANN in a simple form.",989222cb,0.0
359,0800c019d227f2,f3057ee8,"## Abstract
- In this notebook, I will introduce very simple example of 'LightGBMTunerCV' from 'optuna.integration.lightgbm'.
- Because 'it tunes the important hyperparameter variables in order', we can reduce the number of trials for tuning.
- For more details, see [official documant](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html) and [this blog](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)
- Finnaly I got CV = 0.6974 !",9cd3ffa1,0.0
361,07f5853e4db8f8,915c663c,"#  **Introduction**

Current research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities.

Education technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform’s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what’s working and enacting it to benefit students, teachers, and budgets.

The data and feature description for this challenge can be found Here in kaggle computation.

**Business Need

- What is the state of digital learning in 2020? 

- And how does the engagement of digital learning relate to factors such as district demographics, broadband access, and state/national level policies and events?

",d13c2c32,0.0
362,b61ab8f81dc03d,5a15188b,"# Table of Contents
* [History](#history)  
* [Understand the data](#understand_data)
    - [Data Dictionary](#data_dictionary)
    - [Variable Notes](#variable_notes)
    - [Graphs](#graphs)
    - [%Survived (Sex)](#survived_sex)
    - [Age/Sex (Survived/Not Survived)](#age_sex)
    - [PClass/Sex/Age](#pclass_sex_age)
    - [PClass/Survived/Age](#pclass_survived_age)     
    - [Pclass/Sex/Survived](#pclass_sex_survived)
    - [Embarked/Survived/Sex](#embarked_survived_sex)
* [Prepare the data](#prepare_the_data)
    - [Missing values](#missing_values)
    - [Checking cardinality for categorical values](#checking_cardinality)
    - [Predicting missing data using ML](#predicting_missing_data)
    - [Mapping](#mapping)
    - [Converting types](#converting_types)
    - [Checking the correlation](#checking_correlation)
    - [Heatmap](#heatmap)
* [Select a model](#select_model) 
    - [Basic predicting in different models](#basic_predicting)  
    - [Model comparison](#model_comparison)
* [Hyperparameter Tuning](#hyperparameter_tuning)
    - [Random Forest Regressor](#random_forest_regressor)
    - [Decision Tree Classifier](#decision_tree_classifier)
    - [Random Forest Classifier](#random_forest_classifier)
    - [XGBoost](#xgboost)
    - [Checking Accuracy](#checking_accuracy)
    - [Confusion Matrix](#confusion_matrix)
    - [ROC-AUC Score](#roc-auc_score)
* [Submit the results](#submit_results)
* [References and Credits](#references_credits)",64d05394,0.0
363,b660910fcc2954,30fe3bdb,"# Introduction

In this challenge, Kaggle invites the competitors to solve a regression problem.

<center><img src=""https://images.unsplash.com/photo-1579343580826-6323adb66545?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=1050&q=80"" width=600></img></center>",80b74f88,0.0
367,b74076b2f8ba1d,3ec2f053,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",9ace22d4,0.0
368,b7452d87e4abfe,242c37c4,**Let's start by importing the dataset.,9c75a86d,0.0
369,b779c3ce7b671a,d73d2266,"# Planet simulation
    
Welcome all, here is a guide on how to create a simulation of planets orbiting a star using ordinary differential equations and ``matplotlib``. Enjoy! 

*Uncomment `# os.mkdir('./images')` to create an image folder, same applies for a `movies` folder.*",ca778770,0.0
370,b7b1057764fa02,0e8eb809,"# 1. American Sign Language (ASL)¶

American Sign Language (ASL) is the primary language used by many deaf individuals in North America, and it is also used by hard-of-hearing and hearing individuals. The language is as rich as spoken languages and employs signs made with the hand, along with facial gestures and bodily postures.

<img src=""https://upload.wikimedia.org/wikipedia/commons/7/7d/American_Sign_Language_ASL.svg"" alt='""ASL"" spelled out in American Sign Language fingerspelling' style=""width: 600px;""/>

A lot of recent progress has been made towards developing computer vision systems that translate sign language to spoken language. This technology often relies on complex neural network architectures that can detect subtle patterns in streaming video. However, as a first step, towards understanding how to build a translation system, we can reduce the size of the problem by translating individual letters, instead of sentences.

In this notebook, I will train a convolutional neural network to classify images of American Sign Language (ASL) letters. After loading, examining, and preprocessing the data, I will train the network and test its performance.

In the code cell below, I load the necessary libraries, and training and test data directories.",5053a192,0.0
371,b809d07ddd17ed,1d3cd3bb,"# Ramen/Spaghetti Classify MONAI Pytorch
This notebook referred to MONAI's Image Classification Tutorial with the MedNIST Dataset<br/>
https://colab.research.google.com/drive/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe",e32bf3b1,0.0
374,b42180a6a5b42f,c63b9c9f,"# **Análise Exploratória de Dados do COVID-19 no Brasil**

### *Uma análise crítica ao tratamento de dados feito pelo Ministério da Saúde da República Federativa do Brasil*


### Índice:
* [Justificativa](#justificativa)
* [Fontes](#fonte)
* [Análise](#analise)
* [Conclusões](#conclusoes)
* [Referências](#referencias)

---

# Justificativa <a id='justificativa'></a>
Existe uma narrativa, de alguns governos estaduais de que o Brasil segue em ascensão exponencial em número de casos/óbitos.
E, argumentam, os gestores destes estados, que por este motivo é necessário a implementação de uma extensão das quarentenas.
A ALERJ, por exemplo, votou nesta quarta-feira (dia 20) um projeto de lei que impunha medidas restritivas de locomoção ainda maiores e prolongadas, na qual requeria-se uma extensão por mais 15 dias. O projeto de lei, entretanto fora rejeitado pela maioria dos Deputados Estaduais.

Todavia um estudo pela Singapore University of Technology, em meados de Abril demonstrou, mediante uma análise preditiva, de que o ciclo do Covid-19 atinge aproximadamente 80% do contágio se dá nos 2 primeiros meses (https://www.flasog.org/static/COVID-19/COVID19PredictionPaper20200426.pdf)

De acordo com o estudo, o ciclo do contágio forma um modelo gaussiano (Distribuição Normal Padrão), atingindo seu percentil 50 em aproximadamente 40 dias.

O Brasil, por sua vez, desde a constatação do primeiro caso em 16 de março até hoje, perfaz 2 meses de coleta de dados. E para nossa surpresa, percebemos um padrão dissonante com aquele apresentado pela Singapore University of Technology.

No site criado pelo governo para publicação dos dados relacionados à pandemia (https://covid.saude.gov.br/) vemos que o gráfico de número de óbitos por data de notificação segue uma forma de Distribuição assimétrica à esquerda ou assimétrica negativa (left skewed ou negative skewed), conforme modelo abaixo:",987cea5f,0.0
375,b4153d15e36294,b23ccfe5,This initial step aims to acces zip data of plankton images. The code can list all files in the input folder. The code can also select and display a single or several plankton images,f05d3393,0.0
378,adb8441ad28019,8965a2db,"<div class=""alert alert-success"">
    <h1 align='center'>Fetal Health Classification</h1>
</div>",d89de993,0.0
379,adf419444a59df,a86c310a,"### In this notebook I will try to design yolov1 from stratch in pytorch. The code will be basic and understandable anyone who know about pytorchs nn module and basic training steps. I will be trying to comment anything that you might miss.
### You can read the original paper  <a href = 'https://arxiv.org/pdf/1506.02640.pdf'>here </a>.
### Some parts of this code taken from  <a href = 'https://www.youtube.com/watch?v=n9_XyCGr-MI'>Aladdin Persson </a> video.",3a275e7f,0.0
380,ae058c3f1439c3,8dc1d5c7,## Analyzing Skills required at Google.,965da99d,0.0
381,ae0edd94cf2c96,1e922992,"## Inspired by:
* https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings
* https://www.kaggle.com/shujian/single-rnn-with-4-folds-v1-9
* http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/
* https://arxiv.org/abs/1607.06450
* https://github.com/keras-team/keras/issues/3878
* https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings
* https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout
* https://www.kaggle.com/aquatic/entity-embedding-neural-net
* https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate
* https://ai.google/research/pubs/pub46697
* https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/
* https://www.goodreads.com/book/show/33986067-deep-learning-with-python


A proper baseline for all others...

## Params:


**dim = 100
num_words = 50000
max_len = 100     
**

        {'threshold': 0.11, 'f1': 0.6141708124219246}
    RESULTS ON TEST SET:
                   precision    recall  f1-score   support

               0       0.98      0.96      0.97    306162
               1       0.55      0.69      0.61     20369

       micro avg       0.95      0.95      0.95    326531
       macro avg       0.77      0.83      0.79    326531
    weighted avg       0.95      0.95      0.95    326531



**dim = 200
num_words = 50000
max_len = 100     
**

    {'threshold': 0.21, 'f1': 0.6188755470766468}
    RESULTS ON TEST SET:
                   precision    recall  f1-score   support

               0       0.98      0.97      0.97    306162
               1       0.57      0.68      0.62     20369

       micro avg       0.95      0.95      0.95    326531
       macro avg       0.77      0.82      0.80    326531
    weighted avg       0.95      0.95      0.95    326531


**dim = 100
num_words = 50000
max_len = 200     
**

    {'threshold': 0.21, 'f1': 0.6192865861928659}
    RESULTS ON TEST SET:
                   precision    recall  f1-score   support

               0       0.98      0.97      0.97    306162
               1       0.57      0.67      0.62     20369

       micro avg       0.95      0.95      0.95    326531
       macro avg       0.78      0.82      0.80    326531
    weighted avg       0.95      0.95      0.95    326531


**dim = 100
num_words = 50000
max_len = 400     
**

    {'threshold': 0.23, 'f1': 0.6193301865587773}
    RESULTS ON TEST SET:
                   precision    recall  f1-score   support

               0       0.98      0.97      0.97    306162
               1       0.57      0.68      0.62     20369

       micro avg       0.95      0.95      0.95    326531
       macro avg       0.77      0.82      0.80    326531
    weighted avg       0.95      0.95      0.95    326531
",dbfa21b5,0.0
382,af6556ced704f6,b71b2998,# **Comic Characters Analysis**,881577c0,0.0
384,b05ee1ea1c8269,e9ef20a4,"# Bar Chart Race of World Covid19 Variant ""Omicron""
https://www.kaggle.com/stpeteishii/bar-chart-race2-of-world-covid19-variant-omicron<br/>
This notebook is a copy of the following noteboook.<br/>
https://www.kaggle.com/stpeteishii/bar-chart-race-of-world-covid19-variant-omicron
<div align=""left"">
<img src=""https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle"" alt=""upvote"">
</div>",19e4d303,0.0
386,b0c2805cd5c087,7ba6ff85," ""The annual rundown of ML data points is in its 3rd year. The AI world is booming in a range of metrics: research, education, and technical achievements. The AI Index covers a lot of ground. They have released two new tools. One for AI research papers and the other for investigating country-level data on research and investment. To save you from having to trudge through its 290 pages, here are some of the points: AI research is rocketing. AI education is equally popular. ML courses in universities and online continues to rise. The US is still the global leader. AI algorithms are becoming faster and cheaper to train. Autonomous vehicles received more private investment than any AI field."" https://www.theverge.com/2019/12/12/21010671/ai-index-report-2019-machine-learning-artificial-intelligence-data-progress
* The Report has many works. Choose one. Pick one. It's up to you. I just printed the heads of some of the files.",0446f327,0.0
388,b0e401bb7b5943,1121ac88,"# Plotting Areas/Polygons on a Map: Milanese Neighbourhoods

## What is this for?

Do you want to plot polygons on a map (usually Countries or their subdivisions)? You are on the right track. This notebook deals exactly with that with an easy to understand example.
We will deal with Milanese neighbourhoods, plot them on a Folium map using a choropleth, and we will add markers at the approximate centre of each neighbourhood to display their names. You can even save the generated map as an html file and place it wherever you want. Neat, right?

## Dataset for this project

You can find the GeoJSON file with the Milanese neighbourhoods [Here](http://dati.comune.milano.it/it/dataset/ds61_infogeo_nil_localizzazione_/resource/af78bd3f-ea45-403a-8882-91cca05087f0).

## Libraries for this project

Just a handful in this case: **folium** for the map part of this map project (it is that important for this Notebook), **shapely** to find the _representative point_ of each neighbourhood (a centre which is not really a centre but we like it because it's a good enough impostor and computationally not so difficult to calculate), and **json** to load and save the various GeoJSON files (yes, GEOjson, for they contain coordinates and other information). We are lucky, as the GeoJSON file that we are going to use no longer adopts the **EPSG:32632 WGS 84 / UTM zone 32N** as in previous versions (I had to convert its coordinates from that standard to latitude and longitude for a similar project), even though they are actually swapped: it's longitude and latitude, instead of the more common opposite. We need to swap them, or else all that we will be doing is going to end up somewhere in Somalia.

## Functions created for the project

**swap_geojson_coordinates** : it swaps the coordinates and creates a new GeoJSON file with said swapped coordinates.

**create_neighbourhoods_dictionary** : it opens a GeoJSON file and searches for \['properties'\]\['NIL'\] (it's very likely to be different in other GeoJSON files, maybe \['properties'\]\['province'\], or \['properties'\]\['country'\]) to generate a dictionary with the neighbourhood name as the key, and the coordinates as the corresponding value.

**create_neighbourhood_centres_dictionary**: we feed this function the previously generated dictionary of neighbourhoods. It will find the approximate centres of each neighbourhood to then return a dictionary where said centres are the values, and the neighbourhood names are the keys.",ed8abc16,0.0
389,b10bd75889dad9,ad28936b,# Telecom Churn Logistic Regression,ee00ceee,0.0
391,b211c8c107f56d,bc2b598b,"Hi Kagglers ! 

This is a notebook to test what you can do in a few minutes with h2o's migthy GBM. I also included some basic grid search and early stopping to make the model more competitive. There is no CV in this kernel, I simply used a train/validation split framework. Unfortunately h2o does not allow you to correct for stratification when splitting (to my knowledge), so I rather used sklearn splitting function. 

A neat feature is that you can keep an eye on the scoring history along the training with scoring_history() function on many metrics of interest, not only AUC ROC.

Feel free to comment, fork and upvote, happy kaggling, Cheers!
",805b90f3,0.0
392,b241b847319d13,b58bec6c,# **SIIM-FISABIO-RSNA COVID-19 Detection**,0fb698f0,0.0
393,b27bdd02db1bbd,42f4564a,"## 결측치 제거 및 그룹 합계에서 조건에 맞는 값 찾아 출력
- 주어진 데이터 중 basic1.csv에서 'f1'컬럼 결측 데이터를 제거하고, 'city'와 'f2'을 기준으로 묶어 합계를 구하고, 'city가 경기이면서 f2가 0'인 조건에 만족하는 f1 값을 구하시오
- 데이터셋 : basic1.csv
- 오른쪽 상단 copy&edit 클릭 -> 예상문제 풀이 시작",79340a85,0.0
394,b290039151fb39,9c91519b,"# Bengali.AI: Curriculum Dropout implementation

This kernel extends upon the work of iafoss in his [Grapheme fast.ai starter](https://www.kaggle.com/iafoss/grapheme-fast-ai-starter-lb-0-964) kernel which provides a strong baseline with the Bengali.AI Handwritten Grapheme Classification.

Here I'm aiming to demonstrate the techniques describes in the paper [Curriculum Dropout (2017)](https://arxiv.org/abs/1703.06229) by Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Rene Vidal and Vittorio Murino.

The idea is quite simple: instead of using a fixed dropout, we instead ""start easy"" and use very little dropout in the early stages of training and gradually increase the amount of dropout until we reach our desired level.

![image.png](https://snipboard.io/PUWxZj.jpg)


I've implemented it as a [fastai v1](https://github.com/fastai/fastai) callback but hopefully the code is simple enough to be ported to any framework.",1836a79c,0.0
397,b39684e6670dd7,211efdd3,# RobustScaler,83de9873,0.0
398,b3e0b7e9ff6849,4fb7bf9d,# **Buy Till You Die Models: Customer Lifetime Value**,f6e4bb0d,0.0
400,cf39cde80e66b7,b684c567,Style of the data,aed4bc9b,0.0
401,725e1f52ca5118,9304df21,Google Colab Notebook,7c108fba,0.0
403,12f4d16fc21645,91bfe818,"### Data fields

N - ratio of Nitrogen content in soil -  kg/ha<br>
P - ratio of Phosphorous content in soil - kg/ha  <br>
K - ratio of Potassium content in soil - kg/ha  <br>
temperature - temperature in degree Celsius <br>
humidity - relative humidity in % <br>
ph - ph value of the soil <br>
rainfall - rainfall in mm <br>",c7752038,0.0
404,2facf256353117,95c795da,"# Medical Imaging
* Medical imaging is the technique and process of imaging the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging.
* Amedical image data set consists typically of 
>* one or more images representing the projection of an anatomical volume onto an image plane (projection or planar imaging), 
>* a series of images representing thin slices through a volume (tomographic or multislice two-dimensional imaging), 
>* a set of data from a volume (volume or three-dimensional imaging), 
>* multiple acquisition of the same tomographic or volume image over time to produce a dynamic series of acquisitions (four- dimensional imaging). 


* A medical image is the representation of the internal structure or function of an anatomic region in the form of an array of picture elements called pixels or voxels. It is a discrete representation resulting from a sampling/reconstruction process that maps numerical values to positions of the space. The number ofpixels used to describe the field-of-view ofa certain acquisition modality is an expression of the detail with which the anatomy or function can be depicted. 
* What the numerical value ofthe pixel expresses depends on:
>* the imaging modality, 
>* the acquisition protocol, 
>* the reconstruction, 
>* the post-processing.

* As a discipline and in its widest sense, it is part of biological imaging and incorporates radiology, which uses the imaging technologies of 
>* X-ray radiography, 
>* magnetic resonance imaging (MRI) 
>* ultrasound, 
>* endoscopy, 
>* elastography, 
>* tactile imaging, 
>* thermography, 
>* medical photography, 
>* nuclear medicine functional imaging techniques as positron emission tomography (PET) and single-photon emission computed tomography (SPECT).",18f579be,0.0
405,301658c5b2bf29,ef42bb16,## Imports,5efbefd4,0.0
407,30fdc4a6e3c1db,afec5c24,"#  Introduction

## Problem Statement
The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.

In this competition, the fifth iteration, hierarchical sales data from Walmart, the world’s largest company by revenue is given,** to forecast daily sales for the next 28 days.** The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.

![Walmart](https://entrackr.com/wp-content/uploads/2020/01/Walmart-cash-and-carry.jpg)

## Data Provided
In the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.
Files
*      **calendar.csv** - Contains information about the dates on which the products are sold.
*     **sales_train_validation.csv** - Contains the historical daily unit sales data per product and store (d_1 - d_1913)
*     **sample_submission.csv** - The correct format for submissions. Reference the Evaluation tab for more info.
*     **sell_prices.csv** - Contains information about the price of the products sold per store and date.
*     **sales_train_evaluation.csv** - Available once month before competition deadline. Will include sales (d_1 - d_1941)

We will have a sneak peak into the dataset below 

## Evaluation Metric
This competition uses a Weighted Root Mean Squared Scaled Error (RMSSE). The RMSSE metric is a variant of the original MASE (mean absolute scaled error) metric. The scaling was introduced to provide a scale-free error regardless of the data. An example for this competition would be a product that sells hundreds of units per day vs. a product that only sells a few times a week or month (the intermittent demand we are seeing in there series).
Keeping that in mind our measurement is now scale free which allows us to compare many different types of time series. Another reason we are using RMSSE is because it does not suffer from actual values being zero like the MAPE metric.",6111ddee,0.0
408,312135b445bd23,035d8178,# **COVID-19 Open Research Dataset Challenge (CORD-19)**,8ced381f,0.0
410,312d2a3c7547f1,f3f02a9c,Working on Titanic Dataset from Kaggle,8fd1efcd,0.0
411,31b564f11ef638,35b5f8a2,# Bike Shring Demand Top 6.6% Solution for Everyone !!,424f9692,0.0
412,324c699253abc2,de982bce,"## Objective:

The Human Freedom Index is a measure of how good a country is ranked amongst others countries in terms of freedom across government, society and economics variables. The idea is to implement principal component analysis in order to understand if a single indicator describe all these variables.
",7e81e44e,0.0
417,33d736abb432d0,8aada596,## 导库，设置必要的cfg,d64052a2,0.0
418,347c7b0f48c53f,9b9ddc82,Text Summarization with NLTK in Python,c58305ba,0.0
419,1294fb4c86f993,5770bd39,"
# Project: FBI NICS Guns Check
<img src=""Images/guns.jpg"" align=""right"" width=""500"" height=""500"" />


## Table of Contents
<ul>
<li><a href=""#intro"">Introduction</a></li>
<li><a href=""#wrangling"">Data Wrangling</a></li>
<li><a href=""#eda"">Exploratory Data Analysis</a></li>
<li><a href=""#conclusions"">Conclusions</a></li>
</ul>",4471e513,0.0
420,34fff8ce731b03,cdc8ace8,"## Classificação utilizando Pandas e Scikit-learn

Neste notebook iremos fazer a predizer os clientes inadimplentes utilizando a biblioteca [Scikit-learn](https://scikit-learn.org/) e o Pandas. Iremos desenvolver, neste notebook, um modelo capaz de predizer se o cliente está ou não inadimplente, ou seja, uma tarefa de classificação binária.",6f9e5b2e,0.0
421,3536195ad632ee,10554219,"**INTRO**

MNIST dataset has 70000 28x28 grayscale images and a single label for each image. 

In this kaggle competition the original MNIST dataset splitted into a train dataset with 42000 images and labels and a test dataset with 28000 images only. 
There are no labels in kaggle's test dataset. 

So in this competition for training you should use kaggle's train dataset with 42000 images and not the original dataset with 60000 or even 70000 images as some competitors do.
If you find submission with kaggle score higher than 0.998 in the Leaderboard, it is very likely that more than 42k images were used for training. 
See details about it in section 'How much more accuracy is possible?' in [this fine kernel](http://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist) by Chris Deotte.
",3c26aafc,0.0
422,3597174a998d4d,9aa71826,"This notebook contains three parts:
* the state of hotels
* the analysis of variables related to cancelation
* modelling details",276892ed,0.0
424,2f964d08c25d93,4ad8c1ea,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",1f2e4468,0.0
426,2f47abddfd1928,4e396060,"Before to start this Notebook, I would like to add a quick introductory note.

This is my first Notebook and also my first coding mini-project. I am using the notebook to put in action what I have learnt so far.

Before starting the notebook I have read many Titanic Notebooks to learn coding and machine learning best practicies and also to take ideas to start. Therefore I would like to thank to all the community that shares their Notebooks as it is helping a lot to beginners like me.

Also, I made this Notebook based on many iterations of feature engineering and model selection. Anyone can also check that ""pre-work"" in the next link:

https://github.com/diequies/Titanic_Kaggle

Now let's start with the actual Notebook and please do not hesitate to comment anything that can help my learning.",ae33cc0b,0.0
428,2b39f4ff896f97,98155769,Import neccessary packages,3ddfe182,0.0
430,2b613b2e6aa124,4efea3b0,"!pip uninstall kaggle<br>
!pip install --upgrade pip<br>
!pip install kaggle==1.5.6<br>
!mkdir -p ~/.kaggle<br>
!cp kaggle.json ~/.kaggle<br>
!ls -lha kaggle.json<br>
!chmood 600 ~/.kaggle/kaggle.json<br>",fd4ee989,0.0
432,2bace980aeb34c,2be07ed9,"**This notebook is an exercise in the [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/alexisbcook/pipelines).**

---
",dc05ef6c,0.0
433,2bd6c370695ea7,02d4bce7,## Libraries,cbe6aec8,0.0
435,2c5cb484988da2,1e6c09ec,"# Dataset Information
----
This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.

Content
There are 25 variables:

* **ID**: ID of each client
* **LIMIT_BAL**: Amount of given credit in NT dollars (includes individual and family/supplementary credit
* **SEX**: Gender (1=male, 2=female)
* **EDUCATION**: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
* **MARRIAGE**: Marital status (1=married, 2=single, 3=others)
* **AGE**: Age in years
* **PAY_0**: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)
* **PAY_2**: Repayment status in August, 2005 (scale same as above)
* **PAY_3**: Repayment status in July, 2005 (scale same as above)
* **PAY_4**: Repayment status in June, 2005 (scale same as above)
* **PAY_5**: Repayment status in May, 2005 (scale same as above)
* **PAY_6**: Repayment status in April, 2005 (scale same as above)
* **BILL_AMT1**: Amount of bill statement in September, 2005 (NT dollar)
* **BILL_AMT2**: Amount of bill statement in August, 2005 (NT dollar)
* **BILL_AMT3**: Amount of bill statement in July, 2005 (NT dollar)
* **BILL_AMT4**: Amount of bill statement in June, 2005 (NT dollar)
* **BILL_AMT5**: Amount of bill statement in May, 2005 (NT dollar)
* **BILL_AMT6**: Amount of bill statement in April, 2005 (NT dollar)
* **PAY_AMT1**: Amount of previous payment in September, 2005 (NT dollar)
* **PAY_AMT2**: Amount of previous payment in August, 2005 (NT dollar)
* **PAY_AMT3**: Amount of previous payment in July, 2005 (NT dollar)
* **PAY_AMT4**: Amount of previous payment in June, 2005 (NT dollar)
* **PAY_AMT5**: Amount of previous payment in May, 2005 (NT dollar)
* **PAY_AMT6**: Amount of previous payment in April, 2005 (NT dollar)
* **default.payment.next.month**: Default payment (1=yes, 0=no)

### Inspiration
Some ideas for exploration:

1. How does the probability of default payment vary by categories of different demographic variables?
2. Which variables are the strongest predictors of default payment?

### Acknowledgements
Any publications based on this dataset should acknowledge the following:

Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

The original dataset can be found here at the UCI Machine Learning Repository.",d94f9784,0.0
436,2c8119a4061997,64cbb2be,"# Description
Greetings, this kernel provides fast.ai starter code for Bengali.AI Handwritten Grapheme Classification competition.

The task proposed in this competition is recognition of handwritten Bengali letters. In contrast to similar competitions such as [mnist digit recognition](https://www.kaggle.com/c/digit-recognizer) or the recent [Kannada MNIST](https://www.kaggle.com/c/Kannada-MNIST), Bengali alphabet is quite complex and may include ~13,000 different grapheme variations. Fortunately, each grapheme can be decomposed into 3 parts: grapheme_root, vowel_diacritic, and consonant_diacritic (168, 11, and 7 independent classes, respectively). Therefore, the task of grapheme recognition is significantly simplified in comparison with 13k-way classification. Though, additional consideration may be required for this multitask classification, like checking if 3 independent models, or a single model one head, or a single model with 3 heads (this kernel) works the best. This kernel is mostly based on [my findings](https://www.kaggle.com/c/Kannada-MNIST/discussion/122430) in a recent Kannada MNIST toy competition.

In addition to this kernel I have prepared a [code](https://www.kaggle.com/iafoss/image-preprocessing-128x128) for data preprocessing and generation of cropped images rescaled to 128x128. Because the data has much higher resolution than MNIST or CIFAR, the things that work there may be less applicable to this data, while ImageNet based solutions should be more suitable. So, I've choosen DenseNet121 as a starter network. In addition to this kernel I will prepare a submission kernel posted separately.",1836a79c,0.0
437,2ca509e51a6e4b,990d5c05,"Setting up a few things first, then I'll get into how to encode categorical features with singular value decomposition (SVD).",e63b0ba6,0.0
438,2cb457b60dd246,eba7a951,"Thabks to Marsh for his amazing notebook which helped me a lot! (https://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app)

In this notebook I tried to classify the skin leason images using densenet to make a better result. I also edited the attention module compatable with this piece of code. But it takes too much time to train while using the attention module in kaggle so I had to give that up. I kept the attention module code so that if any one needs it the can take it from here as I had to stuggle to make this part run. Happy kaggeling! 
",339367df,0.0
441,2d40f383473fa4,89da02a2,"![](https://media.giphy.com/media/m2tOKbpjpFMvm/giphy.gif)
# 1. Introduction
In this Kernel is made an approach using three Python modules as the pillars: [fastai](https://github.com/fastai/fastai), [Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling) and [H2O](https://www.h2o.ai/products/h2o/), but why to use these modules? Because they can do so much work writing just a few lines of code speeding up the process, therefore the name ""Fastanic"".<br>

The fastai loads a lot of useful modules, as Pandas and Numpy, Pandas Profiling can generate a detailed Exploratory Data Analysis (EDA) with just one code line and H2O using its AutoML module can train different Machine Learning models and ensemble it to make good predictions, but remember there's [*No Free Lunch*](https://towardsdatascience.com/a-blog-about-lunch-and-data-science-how-there-is-no-such-a-thing-as-free-lunch-e46fd57c7f27), fastai is good but maybe you will need other modules to make your EDA, Pandas Profiling is not good to report in large datasets (I tried to use on [Porto Seguro Safe Driver Prediction Dataset](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) without sucess) and H2O AutoML can take a big amount of time to train all the models.<br>

The Legendary Competition to predict the survivors of Titanic Disaster is a good start point to apply these great modules and understand how to work with them and know their pros and cons. This dataset is small enough to use these tools and give a good idea when to use they and we can dedicate more time to understand the data and apply Feature Engineering (FE) to it.

### 1.1 The Challenge

The [sinking of the Titanic](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic) is one of the most infamous shipwrecks in history.<br>

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.<br>

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.<br>

In this challenge, we ask you to build a predictive model that answers the question: ""what sorts of people were more likely to survive?"" using passenger data (ie name, age, gender, socio-economic class, etc).<br>

It is your job to predict if a passenger survived the sinking of the Titanic or not.<br>
For each in the test set, you must predict a 0 (Dead) or 1 (Survived)  value for the variable.<br>
Your score is the percentage of passengers you correctly predict, the accuracy.<br>

### 1.2 Data
Some variables to take note for insights:<br>
* **pclass**: A proxy for socio-economic status (SES)<br>
1st = Upper<br>
2nd = Middle<br>
3rd = Lower<br>

* **age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>

* **sibsp**: The dataset defines family relations in this way...<br>
Sibling = brother, sister, stepbrother, stepsister<br>
Spouse = husband, wife (mistresses and fiancés were ignored)<br>

* **parch**: The dataset defines family relations in this way...<br>
Parent = mother, father<br>
Child = daughter, son, stepdaughter, stepson<br>
Some children travelled only with a nanny, therefore parch=0 for them.<br>

# 2. Load Modules and Data
Let's start importing our Power Trio (fastai, Pandas Profiling and H2O), define the data path and load the available data.",1da1eff0,0.0
443,2dda7facf3c1e0,723441bb,"<a href=""https://colab.research.google.com/github/asigalov61/Amazon-Deep-Composer/blob/master/Amazon_Deep_Composer.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>",45552d2b,0.0
444,132fa9714f2046,d2bc0ffc,"___

<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>
___
# Matplotlib Exercises 

Welcome to the exercises for reviewing matplotlib! Take your time with these, Matplotlib can be tricky to understand at first. These are relatively simple plots, but they can be hard if this is your first time with matplotlib, feel free to reference the solutions as you go along.

Also don't worry if you find the matplotlib syntax frustrating, we actually won't be using it that often throughout the course, we will switch to using seaborn and pandas built-in visualization capabilities. But, those are built-off of matplotlib, which is why it is still important to get exposure to it!

** * NOTE: ALL THE COMMANDS FOR PLOTTING A FIGURE SHOULD ALL GO IN THE SAME CELL. SEPARATING THEM OUT INTO MULTIPLE CELLS MAY CAUSE NOTHING TO SHOW UP. * **

# Exercises

Follow the instructions to recreate the plots using this data:

## Data",3bb1775f,0.0
445,2e0fd6e937bf79,b16a2a3f,# global variables,6acb965d,0.0
446,2e40928927c0d4,389015e0,**Importing the required libraries**,b6385ef2,0.0
447,2e4b36a0bd7613,6325fe07,"![](https://static.wixstatic.com/media/9145cc_be1ed41c54a244c5acb82a01a8e7f736~mv2.jpg/v1/fill/w_1748,h_1240,al_c,q_85/9145cc_be1ed41c54a244c5acb82a01a8e7f736~mv2.jpg)postcardsisters",cd4d333e,0.0
448,2ef36514eab722,466fdcea,# Albumentations faster geometric transforms,759739cb,0.0
450,2b36742b49c7bc,73806126,"## MLUB МУИС Сорил - 1-р байрны шийдэл (Сургалт)

- Гүйцэтгэсэн Я.Баярцогт [kaggle](https://www.kaggle.com/bayartsogtya) - [github](https://github.com/bayartsogt-ya) - [twitter](https://twitter.com/_tsogoo_) - [linkedin](https://www.linkedin.com/in/bayartsogt-yadamsuren/)
- Энэхүү notebook нь зөвхөн сургалтын дарааллыг харуулсан болно. 
- Хамгийн сайн үр дүнг харуулсан (inference) notebook-ийг [энэ линкээр](https://www.kaggle.com/bayartsogtya/mlub-muis-soril-best-single-model-private-0-9725?scriptVersionId=74875736) орж үзнэ үү!
- Тэмцээний шийдэл дараах үндсэн хэсгүүдээс тогтох болно.
    0. Орчноо бэлдэх
    1. Өгөгдөл боловсруулалт
    2. Гараас өгөгдөх параметрүүд
    3. Сургалт эхлүүлэх
    4. Баталгаажуулалт
    5. Үр дүнгээ илгээх
- Дараах ажлууд байгаагүй бол энэхүү үр дүнг гаргах боломжгүй байлаа. Баярлалаа!
    * https://www.kaggle.com/c/muis-challenge
    * https://huggingface.co/tugstugi
    * https://huggingface.co/tugstugi/bert-large-mongolian-uncased
    * https://huggingface.co/transformers/main_classes/trainer.html
    * https://github.com/bayartsogt-ya/mlub-muis-soril",c8f8a96d,0.0
452,3793c438a71b52,e214aba1,"# Happiness Index Analysis with Prediction

1) Data Preparation - Seperate 5 files
2) Visualization
4) Modeling 
5) Conclusion",13eb76df,0.0
453,10c5a39a87c47e,e45aef26,"## Key Facts of Malaria
- Malaria is a life-threatening disease caused by parasites that are transmitted to people through the bites of infected female Anopheles mosquitoes. It is preventable and curable.
- In 2017, there were an estimated **219 million** cases of malaria in **87 countries**.
- The estimated number of malaria deaths stood at **435 000** in 2017.
- The WHO African Region carries a disproportionately high share of the global malaria burden. In 2017, the region was home to **92%** of malaria cases and **93%** of malaria deaths.
- Total funding for malaria control and elimination reached an estimated **US dollar 3.1billion** 

Malaria is caused by **Plasmodium parasites.** The parasites are spread to people through the bites of infected female Anopheles mosquitoes, called **""malaria vectors.**"" There are 5 parasite species that cause malaria in humans, and 2 of these species – **P. falciparum** and **P. vivax** – pose the greatest threat.
[source](https://www.who.int/news-room/fact-sheets/detail/malaria)

![](https://i.ibb.co/QdQPJkY/malar1.jpg)
source: **thegreatcoursesdaily.com**",09c7337a,0.0
454,3dd4294f903768,c8588e49,"In this kernel, we will try to predict the 'Aggregate rating' based on the other features.",0d89d098,0.0
455,3e325daf577158,ca9617bd,"# Genetic Biomarker prediction with CRNN - train
A lot of this work is inspired from work shared by fellow kagglers, thank everyone for teaching me good stuff !!

### Credits
- https://keras.io/examples/vision/video_classification/
- https://www.kaggle.com/ayuraj/train-brain-tumor-as-video-classification-w-b
- https://www.kaggle.com/sergeybulanov/tf-simple-prediction-with-vgg16#2.-DataLoader
- https://www.kaggle.com/michaelfumery/brain-tumor-transfert-learning-flair-kfold",c873dfec,0.0
456,3f25b363afec54,e5612121,# <center> Healthcare Analytics</center>,bbdaae25,0.0
458,3fb15e6e48aec2,8032885e,# Importing Modules,9d1f4358,0.0
459,400bbcc496138f,56ce9c86,"# Summary


**Most of us have been using a constant threshold based on variance for feature  selection. The current kernel investigates a possible alternative through  a varying threshold  per 'wheezy-copper-turtle-magic' category. The threshold is selected so as to maximise the accuracy on each category dataset.** 

",191b86b8,0.0
460,401338428b2d1c,c35d063b,# Random Forest Classification,e4b768be,0.0
462,410285582f4f7e,7fe0cb02,![](https://cdn-images-1.medium.com/max/1600/1*yZ1LPIcXnnW6Ubmp2M-0rQ.png),d026266b,0.0
464,4278c69fedbe27,f14a6c6a,"Special thanks to @abhishek and @nbroad for their notebooks:

https://www.kaggle.com/abhishek/hello-friends-chaii-pi-lo

https://www.kaggle.com/nbroad/chaii-qa-torch-5-fold-with-post-processing-765

Please upvote!",2e05ce58,0.0
465,42e0005bed28aa,9b924fc6,"---


<center><h1 class=""list-group-item list-group-item-success"">Cataract Prediction</center>

---",5616d451,0.0
466,10b5af05d804ff,299b7bb2,# Get +12 on LB on Dota2 with a magic mirror,4a9b1705,0.0
467,434f930cb58aee,590103e1,"# Introduction and Acknowledgement


Hello, I am a beginner in the field of deep learning and I'm still learning new things everyday. In this challenge of multilabel image classification, we will try to indentify various types human proteins present in the given image. Before starting with the code, I would to like to say thanks to [aakash](https://www.kaggle.com/aakashns) and [jovian](https://jovian.ml/) for conducting very helpful course and competition in for deep learning with pytorch. It really helped me to understand core programming concepts to build various deep learning models. As the course was focused on the programming with pytorch, majority of people used to pytorch for this competitions. I would like to say thanks to [nachiket273](https://www.kaggle.com/nachiket273), [ronaldo](https://www.kaggle.com/ronaldokun),[Mimi Cheng](https://www.kaggle.com/mimicheng) for sharing their work. I have learned lots of new concepts by studying there kernels.

As a beginner, I really had a tough time chosing between which framework to use for deep learning. I started with tesnorflow-keras and a rookie programmer it saved a lot of lines of code for me. Tensorflow is really good when someone has less experience of programming. Unlike, pytorch, it does a lot of things for you. In addition, various type api helps in handling complext tasks like data pipeline, deployment, etc. However, pytorch is gaining more popularity since last year because of its flexibility in code and for being more pythonic. 

I am creating this notebook to help someone who is interested in the tensorflow implementation of the classification challenge. I want to express gratitude to [Dmitry](https://www.kaggle.com/dmitrypukhov) and [Laura](https://www.kaggle.com/allunia) sharing there work related to[ datagenerator with keras](https://www.kaggle.com/dmitrypukhov/cnn-with-imagedatagenerator-flow-from-dataframe) and [multilabel classification](https://www.kaggle.com/allunia/protein-atlas-exploration-and-baseline) respectively.
",0e1d3554,0.0
470,4392956f62c040,223861b7,"## PREMISE

- This kernel is just an experiment to check if two models performs better than one single model.
- The notebook is a edit of [this tutorial](https://www.kaggle.com/julian3833/5-xlm-roberta-torch-s-extra-data-lb-0-749).

All credits to Julián Peller!  


",c3ed519d,0.0
473,44f6a002ecd033,9e23c0ee,# Loan Approval Model,70bbe106,0.0
474,3d905ce4828057,80d07842,"# **CLTV (Customer Lifetime Value)**
![image.png](attachment:d447bb98-997b-4646-bda8-67ee1ed6b3df.png)

It means the lifetime value of a brand to the customer. It allows us to have information about customers. It is the monetary value that a customer will bring to this company during the relationship-communication with a company. In the RFM analysis, we segmented the customers. However, we could only determine special strategies and marketing strategies from these segments. We were not making any predictions for the future. In other words, with CLTV, we can calculate how much added value our customers can provide us from a wider perspective, namely time projection.",5b006cc3,0.0
475,37360278c19104,ad965908,"# ECG Heartbeat Categorization

The goal is to be able to classify heart disease from heartbeat signal. There is a lot of data, let's try to make sens out of it.",21473a41,0.0
476,3d77c1560bd16e,cee78643,"<div align=""center"">
    <h2>TEXAS COVID-19 Vaccination Progress</h2>
    <img src=""https://images.unsplash.com/photo-1513023001678-6927b70dc4a0?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1050&q=80""  width=""500"" height=""200"">
    <br><span>Photo by <a href=""https://unsplash.com/@accrualbowtie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"">Ryan Wallace</a> on <a href=""https://unsplash.com/s/photos/texas-state-capitol?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"">Unsplash</a></span>

</div>
<br>
<div align=""left""> 
This notebook presents and analyzes information about COVID-19 vaccine doses allocated and administered in Texas, USA. 
</div><br>

<div style=""left"">As of Monday, March 29, 2021, everyone age 16 and older is now eligible to receive a COVID-19 vaccine in Texas. <a href=""https://www.dshs.state.tx.us/coronavirus/immunize/vaccine.aspx#eligible""> Source </a></div><br>

<div style=""left"">The COVID-19 vaccine supply has increased to the point that Texas DSHS will no longer allocate vaccine to providers each week. As of May 10, providers can order vaccine from DSHS on a daily basis. The weekly allocations in this data set reflect allocations that were made between Dec. 14, 2020 – May 3,2021.
<a href=""https://www.dshs.state.tx.us/coronavirus/immunize/vaccineallocations.aspx""> Source </a></div><br>


<div style=""background-color:#60cff7; font-size:120%;  font-family:sans-serif; text-align:center""><b>Table of Content</b></div>

* [Summary 📊](#1)
* [Demographic analysis](#2)
* [Details](#3)
* [Vaccine distribution vs population](#4)
* [Vaccine Doses Allocated](#5)
* [Weekly trend](#6)
* [Vaccine types and providers](#7)
* [Texas choropleth maps](#8) ",87c141ca,0.0
477,3cea0f929a2035,fcab6eba,"Statistics about suicide are not easy to collate, and may be inaccurate due to the sensitivity of the issue. However,taking into account the existing data I am going to present the analysis on world suicide data from 1985 to 2016. The following features are present in the data:
*     country-name of the countries
*     year-year for the suicides recorded
*     sex-gender
*     age-age groups
*     suicides_no-number of suicides for the mentioned age groupes
*     population-number of individuals in the corresponding age groups
*     suicides/100k pop-number of suicides per 100k population
*     country-year-column combining data in columns ""country"" and ""year""
*     HDI for year-The Human Development Index for given year (a statistic composite index of life expectancy, education, and per capita income indicators)
*     gdp_for_year - GDP (a monetary measure of all market values of all the goods and services produced in certain period of time)
*     gdp_per_capita - GDP per person (obtained by dividing its GDP for a particular period by its average population for the year)
*     generation-generation for each group

In this kernel we'll see how each of the features is related to the number of suicides worldwide, and which are the countries with high suicide rate.

Let's see what our data looks like.
",04cfbade,0.0
478,37b09262279764,6d69996f,# Problem Statement,37c4c417,0.0
479,37e461081e47c5,b6ddd802,"# Kaggle Competition: Predict Future Sales
#### Available at: https://www.kaggle.com/c/competitive-data-science-predict-future-sales
##### The dataset contains historical sales data, and requires a forecast of the total amount of products sold in every shop in the test set for the next month",b3e6549e,0.0
481,3879ef16f5eb28,16f0e373,"#### The below code is automation of some useful graphs in matplotlib which we use regularly.
#### After Executing this code you need to enter name/path of your csv file to which you want to do plotting, next you need to select a plot and enter column names by reading instructions. You can also give your customized color or css color to your plot.If you want to contine after one plot enter y if not enter n and rerun the code and enter another filename/filepath
#### Plots involved in this are:
### 1.Line PLot
   #### one scatter plotting
   #### two scatter plottings
   #### morethan two scatter plottings
### 2.Bar Plot
   #### Basic barplot
   #### horizontal barplot
   #### vertical stacked barplot
   #### horizontal stacked barplot
   #### Press 5 for grouped barplot
### 3.Scatter Plot
   #### one scatter plotting
   #### two scatter plottings
   #### morethan two scatter plottings
### 4.Histogram PLot
   #### Basic Histogram Plot
   #### more than 1 Histogram Plot
   #### Multiple histogram plots in same grid
   #### cumulative distribution histogram plot
### 5.Pie Plot
   #### Basic pie plot
   #### Multiple pie plot
   #### Exploding a slice of from a pie plot
   #### Exploding more than one slice from a pie plot
### 6.Area Plot
   #### basic area plot
   #### stacked area plot
### 7.Donut Plot
   #### Basic donut plot
   #### Multiple donut plot
   #### Exploding a slice of from a donut plot
   #### Exploding more than one slice from a donut plot
### 8.Stem Plot
### 9.Box Plot
   #### single column box plot
   #### morethan one  column box plot
### 10.Violin Plot
   #### single column violin plot
   #### morethan one violon box plot",784b8d8e,0.0
482,38b79494ac749e,fc0e9c07,# Overfitting and Regularization,39162a40,0.0
483,395ed8e0b4fd17,6477d25c,"**Created by Sanskar Hasija**

**📊 G-Research Plots + EDA 📊**

**3 NOVEMBER 2021**
",7573ea31,0.0
487,3a6274ed72cc00,e3ee425c,"## Introduction: 

Market segmentation is a process of dividing a heterogeneous market into relatively more homogenous segments based on certain parameters like geographic, demographic, psychographic, and behavioural. It is the activity of dividing a broad consumer or business market, normally consisting of existing and potential customers, into sub-groups of consumers (known as segments) based on some type of shared characteristics.

In dividing or segmenting markets, researchers typically look for common characteristics such as shared needs, common interests, similar lifestyles, or even similar demographic profiles. The overall aim of segmentation is to identify high yield segments – that is, those segments that are likely to be the most profitable or that have growth potential – so that these can be selected for special attention (i.e. become target markets). Many different ways to segment a market have been identified. Business-to-business (B2B) sellers might segment the market into different types of businesses or countries. While business-to-consumer (B2C) sellers might segment the market into demographic segments, lifestyle segments, behavioural segments, or any other meaningful segment.

Market segmentation assumes that different market segments require different marketing programs – that is, different offers, prices, promotion, distribution, or some combination of marketing variables. Market segmentation is not only designed to identify the most profitable segments, but also to develop profiles of key segments in order to better understand their needs and purchase motivations. Insights from segmentation analysis are subsequently used to support marketing strategy development and planning. Many marketers use the S-T-P approach; Segmentation → Targeting → Positioning to provide the framework for marketing planning objectives. That is, a market is segmented, one or more segments are selected for targeting, and products or services are positioned in a way that resonates with the selected target market or markets.

In this notebook it is aimed to make segmentation according to the customer information in the dataset. ",51369a2a,0.0
489,3ac432b2cac29c,6030a533,"## Recap
You've built a model. In this exercise you will test how good your model is.

Run the cell below to set up your coding environment where the previous exercise left off.",a358669e,0.0
490,3b5903412fe741,b8f2b841,"# Indexing, selecting, assigning reference

This is the reference component to the ""Indexing, selecting, assigning"" section of the Advanced Pandas track. For the workbook component, [click here](https://www.kaggle.com/residentmario/indexing-selecting-assigning-workbook).",ad231969,0.0
491,123382acf2a162,e5687730,"# Overview
According to this post from [Max Diebold](https://www.kaggle.com/c/airbus-ship-detection/discussion/62376), an empty submission can get you  a score of 0.847.

So, this baseline model will always predict that there is no ship.",a0e20386,0.0
492,3c2033cc99c12c,9784a22e,"# <h1 align=""center""> EDA and Classification Training on credit card fraud detection </h1>  
",dfa22a54,0.0
493,3ca4870f877961,0c1d743d,"#### this notebook is part of the documentation on my HPA approach  
    -> main notebook: https://www.kaggle.com/philipjamessullivan/0-hpa-approach-summary",43ae205c,0.0
494,3cb96bd8eb364b,86cbcaae,# TF - toyproject,3157af7e,0.0
495,117fc0956643d0,d49396b9,"# Extract Excerpt From Abstract Using LDA and Fine-Tuned ALBERT

In the following sections, we explain how we have developed a framework for finding answers for the next question: What do we know about the virus, the human immune response, and predictive models?",68cef9fd,0.0
496,3cc097a5859dc1,742b28f1,# **Importing Libraries and Data**,14380d73,0.0
499,3d08ca7656dec0,96b71732,#       Heart attack analysis and prediction,bd3f87e3,0.0
500,1375dae95a1962,ff2aa5cc,"# Statistik und Wahrscheinlichkeitsrechnung: Hello World
Glückwunsch - Sie haben Ihr erstes Notebook geöffnet! 

Python-Notebooks bestehen aus sogenannten Zellen (engl. ""cells""). Es gibt zwei Typen von Zellen: 
* Markdown-Zellen (diese enthalten einfach nur Freitext im Markdown-Format). Diese Zelle ist zum Beispiel eine Markdown-Zelle.
* Code-Zellen (diese enthalten Python-Code, den man direkt im Notebook ausführen kann).

Dieses Notebook besteht - neben dieser Zelle - aus zwei weiteren Zellen (siehe unten). In beiden finden Sie ein kurzes FIXME. Lösen Sie die FIXMEs, um die Funktionsweise von Python-Notebooks zu lernen.
",6c688a10,0.0
501,2b2b7b3f105eb2,5ce9d255,"## Introduction
An example of how to load the data.",2faea1d5,0.0
502,13c7672da1b571,7dac1f6a,"This notebook is based entirely on the knowledge which I gained from doing the Machine Learning courses on Kaggle and reading sklearn/pandas documentation.  

I tried here to create an automated pipeline for feature preparation, which does:
- remove numerical columns with a large amount of missing data,
- impute missing data in categorical columns and in the numerical columns which have most entries and a small part of missing data,
- removes features have small mutual information with the target,
- creates new features by applying PCA to numerical features,
- performs One Hot Encoding on the categorical data.

Next, the preprocessed data is fed into a simple XGBRegressor model, which can be run with early stopping rounds, when performed on the train/validations sets.

",002d3ec0,0.0
503,19e3d0e5900aff,94defaa8,"![](https://github.com/microsoft/FLAML/raw/main/docs/images/FLAML.png)

**FLAML** is a lightweight Python library (https://github.com/microsoft/FLAML) that finds accurate machine learning models automatically, efficiently and economically. It frees users from selecting learners and hyperparameters for each learner. It is fast and economical. The simple and lightweight design makes it easy to extend, such as adding customized learners or metrics. FLAML is powered by a new, cost-effective hyperparameter optimization and learner selection method invented by Microsoft Research.",6321da83,0.0
504,1a0bd2f72bbe36,ac08be21,# EDA ON AMAZON TOP SELLING BOOKS 📚📚📚:,2fa311dc,0.0
505,1a222fee3089d2,0ee8d597,"# Titanic - Exploratory Data Analysis and Machine Learning from Disaster

Author: **Renato Holzlsauer Mattos Macedo**<br>
Contact: **renato.holzlsauer@gmail.com** - [**GitHub**](https://github.com/Holzlsauer) - [**LinkedIn**](https://www.linkedin.com/in/holzlsauer/)

",59ab8894,0.0
506,1a285e4c830f3f,f85cd2d3,"## Cardiovascular Disease Prediction
**Verwandtes Notebook von Benan AKCA, https://www.kaggle.com/benanakca
PhD Student, Marmara University İstanbul, Turkey** <a id=""0""></a>

## Einführung <a id=""1""></a>
<mark>[Return Contents](#0)
<hr>
Basierend auf nummerischen und kategorischen Merkmalen des ""cardiovascular disease datasets"" soll eine Supportvektormaschine optimiert werden. Wir laden erst mal die Daten:",360b50e9,0.0
507,1b5c395d894e90,f4ba71c1,![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRQbzAowHZUu3P3qgDtpREmiydgOWkW5PQ4_g&usqp=CAU)amazon.com,3a04e44d,0.0
508,1bd6cc83c02681,227ec152,# MOVIE RECOMMENDATION SYSTEM,17ff92f0,0.0
511,1c381451c17150,f9939e7e,"# Pythonic Python Script for Making Monty Python Scripts
The python language was named after Monty Python's Flying Circus so why not use python to generate an unlimited supply of new Monty Python scripts? This notebook is a compressed version of my text generating AI. Text generator like this one require a lot of computational power so it only became really feasible to do them on Kaggle Kernels when they upgraded to have a GPU and a 6 hour computational limit. Even so, 6 hours is still kind of lean for an LSTM text generator but I can make it work quite well anyways. 

The goal of this notebook is to serve as a introduction to text generation NLPs. These LSTM text generator are actually not that difficult to make. However, most tutorials on the topic are incomplete and/or generate poor results. I'll try to talk about every step of the process thoroughly and clearly. Other than that, this notebook is pretty easy to adapt to any text generation you might want to do. Just pop in any sizeable txt file and the model will learn to make more text in that style. Things like Shakespeare are common and work well for this type of text generation. Make sure that GPU is enabled in settings. Now lets make an AI generate something completely different.

## Imports
As always, a block of imports.
",e79b530f,0.0
512,1c5aaf7bea6414,e45e2ce8,"# Introduction

I prefer medium.com for writing. You can access the medium story that relevant with this notebook from [here](https://medium.com/@mebaysan/what-is-rfm-analysis-an-applied-example-in-python-1979b9853f0b). This notebook will cover just codes.

Kind regards.",34d8f42d,0.0
513,1c7dacc7f36c8c,84b3c93c,"### 'f4'컬럼의 값이 'ESFJ'인 데이터를 'ISFJ'로 대체하고, 'city'가 '경기'이면서 'f4'가 'ISFJ'인 데이터 중 'age'컬럼의 최대값을 출력하시오!
- 데이터셋 : basic1.csv
- 오른쪽 상단 copy&edit 클릭 -> 예상문제 풀이 시작",871a833c,0.0
514,1cd8be6e679620,4a0830c4,## OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction,3ce15a43,0.0
515,1d1598b6fa2aa7,2602896b,"![](https://revistas.isfodosu.edu.do/public/enlaceinteres/Plotly-logo-01-square.png)

## Introduction to Plotly

**[Plot.ly](http://https://plotly.com/)** - is a Python graphing library makes interactive, publication-quality graphs and plots. Plotly is free and open source and you can view the source, report issues or contribute on [GitHub](http://https://github.com/plotly). You can use it for Python without signing up to any service, use it in offline, without being connected to the internet, use Dash is for free. Plotly also has commercial offerings, such as Dash Enterprise and Chart Studio Enterprise.

### Goal

This is a kernel for showing the awesome capabilities of the popular visualiztion library. We will start from simple charts and goes to more difficult examples. The main goal is to get you interested in trying to use this library in your projects.

### Why Plotly?

*- We have ```matplotlib``` and ```seaborn```, why do we need to use one more library?*

**Plotly** provides a wide range of interactive plotting options and is one of the most interactive python visualization libraries. It can be used not only in Jupyter Notebooks but you can deploy it on web sites using Dash. Dash is a productive Python framework for building web analytic applications. On the other hand, ```matplotlib``` and ```seaborn``` focus on bulding ""static"" visualisations without interactive elements. The real beauty of interactive plots is that they provide a user interface for detailed data exploration. For example, you can see exact numerical values by mousing over points, hide uninteresting series from the visualization, zoom in onto a specific part of the plot, etc.

The analogue of **Plotly** is **Bokeh**. It is another library for creating interactive visualizations for Python. But **Plotly** has advantages, it is ease of learning and use. 

Begin with importing necessary modules.",e066accf,0.0
518,1dd9c6aa74d289,68131726,"# Introduction
The analysis is used to make the plots seen in my Medium Article [""How long could it take to climb my first 8a?""](https://kate-d.medium.com/how-long-could-it-take-to-climb-my-first-8a-d841f2573518)

With the dataset, I put forward three questions that I want to know most as a climber.
1. How long could it take to climb my first 6a, 7a or 8+?
2. How is my height compared with most climbers?
3. Where are the popular climbing places?

So, this notebook will be divided into three parts accordingly. They are all about climber statistics.

## Reference
Special thanks for the following efforts, so I can make this notebook. 
- How long before you get ""Good"" as bouldering, [kaggle kernel](https://www.kaggle.com/aarontrefler/how-long-before-you-get-good-at-bouldering), by Aaron Trefler
- Plotting progression times per grade, [kaggle kernel](https://www.kaggle.com/durand1/plotting-progression-times-per-grade), by Durand D'souza
- Climber-characteristic-analysis, [gitHub repo](https://github.com/stevebachmeier/climber-characteristic-analysis), by stevebachmeier",5ef9a1be,0.0
519,1eb62c5782f2d7,58d9714b,# Mencari total area (Cumulative) dibawah kurva Standard Normal Distribution,bb69f147,0.0
520,1f3295ed0d4e4a,eeb23f4c,"[Dask](https://docs.dask.org/en/latest/)
====

> Dask is a flexible library for parallel computing in Python.

> Dask is composed of two parts:
> 
> 1. Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.
> 2. “Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.",b0aeb172,0.0
521,1fac5edd4063ba,de09c992,"On May 5, 1809, Mary Kies became the first woman to receive a patent in the United States. (It was for her technique of weaving straw with silk.)

Of course, women inventors existed before this time, but 
#The property laws in many states made it illegal for women to own property on their own. This led some women to apply for patents #in their husbands’ names if they decided to apply at all.

As of last year,
#only 10 percent of U.S. patent holders were women, although women account for half of doctoral degrees in science and engineering. #This disparity is due in part to the the U.S. Patent and Trademark Office being more likely to reject patents with women as sole #applicants.

#Further, when patents sought by women are approved, they are more likely to have added parameters that made the description of the #patents far more detailed. These revisions tend to lower the scope of the patent, making it weaker and less valuable.

It’s no secret that women face more scrutiny in STEM fields, which is why the women on this list have more to be congratulated on than their inventions. They are pioneers in their fields – some of them were kids when they invented their first product – and have often overcome tougher hurdles based on their sex than their male counterparts. Read the complete list:
https://www.usatoday.com/story/money/2019/03/16/inventions-you-have-women-inventors-thank-these-50-things/39158677/",04bc01e0,0.0
522,1fcf9c261518d6,420cf022,# I. Importing Required Libraries and Data,be646fb0,0.0
524,19aae4a6ede288,d5dbdf3e,"# <center>Tabular Playground Series - June/2021<center>
## <center>Wide and Deep Neural Network with Keras<center>
---

In the spirit of trying different approaches for Neural Networks in this month’s competition, this notebook presents an attempt with a Wide and Deep Network, easily created using [tf.keras.experimental](https://www.tensorflow.org/api_docs/python/tf/keras/experimental) module. This same approach can be found in the [bonus lesson from the Kaggle’s course ‘Intro to Deep Learning’](https://www.kaggle.com/ryanholbrook/detecting-the-higgs-boson-with-tpus).
<br>
<br>
<br>    
    
![Wide and Deep NN](https://i.imgur.com/Jzj75Nv.png)
 <center>Wide and Deep NN. [Cheng, H. et al. Wide and Deep Learning for Recommender Systems. (2016)](https://arxiv.org/pdf/1606.07792.pdf)<center>
<br>",56934674,0.0
528,1645979263c148,f97bb2c1,"# Chronic Kidney Disease
Data has 25 feattures which may predict a patient with chronic kidney disease",fa11663e,0.0
529,1660daf8867980,b97dc80a,"# Reinforcement Learning Chess 
Reinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI. 

[**Notebook 1: Policy Iteration**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-1-policy-iteration)  
[**Notebook 3: Q-networks**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-3-q-networks)  
[**Notebook 4: Policy Gradients**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-4-policy-gradients)  
[**Notebook 5: Monte Carlo Tree Search**](https://www.kaggle.com/arjanso/reinforcement-learning-chess-5-tree-search)  ",42d7cffc,0.0
530,1667a100fc8b42,8fdd4282,"# Introduction
- The aim of this notebook is not to get a good position on the leaderboard,instead it is to learn about PCA and how to apply it.",6c8cd6b6,0.0
531,166a62ebb4fc3a,ba4c5c72,"# Breast Cancer Prediction

Breast cancer is a type of cancer that starts in the breast. Cancer starts when cells begin to grow out of control. Breast cancer cells usually form a tumor that can often be seen on an x-ray or felt as a lump. 

Depending on the types of cells in a tumor, it can be:

1. **Benign** - The tumor doesn’t contain cancerous cells.
2. **Malignant** - The tumor contains cancerous cells.

![](https://gotalktogetherdotcom.files.wordpress.com/2016/05/cancerbenignmalig1.jpg)",db48a079,0.0
532,16862cb02d73d5,d6648ddc,"Hi everyone.
This is my first kernel in kaggle and thought of writing about **anomaly detection** and its **visualization**. First I would like to thank **Sudalai Rajkumar** for his kernels which are great inspiration to me.",d7ffa1a6,0.0
533,169177b6e9edea,6f08a2cc,"<h1> Projeto da Casa de IA - Competição do Titanic
    <p>",ca42152f,0.0
534,1691c9f2c2f656,21b40f35,"# What does Donald Trump tweet about?


![https://www.casino.org/news/wp-content/uploads/2019/07/1533207630043-trumpKim.jpeg](https://www.casino.org/news/wp-content/uploads/2019/07/1533207630043-trumpKim.jpeg)",70433e76,0.0
535,16ca1123840e9f,921f8d8d,"# A deep dive into Space Missions!🚀 



![](https://inteng-storage.s3.amazonaws.com/images/uploads/sizes/Space-exploration_resize_md.jpg)

###### We humans have always looked up into the night sky and dreamed about space. In the latter half of the 20th century, rockets were developed that were powerful enough to overcome the force of gravity to reach orbital velocities, paving the way for space exploration to become a reality. We have been venturing into space since October 4, 1957, when the Union of Soviet Socialist Republics (U.S.S.R.) launched Sputnik, the first artificial satellite to orbit Earth. In this notebook let's explore more on the various space missions happended since 1957.#####",e8b8f086,0.0
537,16071987c1cb40,350d961c,"# TPS Feb 2021
Starter Notebook

## Deleverables
1. EDA
    - What's going on?
    - Show me the data...
2. Model
    - Baseline...
    - Simple...
    - Evaluation...
    - Improvement...
3. RAPIDS Bonus
    - Apply RAPIDS ([Starter Notebook](https://www.kaggle.com/tunguz/tps-feb-2021-rapids-starter))
    - Replace pandas with [cuDF](https://github.com/rapidsai/cudf) & sklearn with [cuML](https://github.com/rapidsai/cuml)
    - Context: [What is RAPIDS?](https://medium.com/future-vision/what-is-rapids-ai-7e552d80a1d2?source=friends_link&sk=64b79c363beeffb9923e16482f3977cc)
    
    
#### Troubleshooting
- [Data](https://www.kaggle.com/c/tabular-playground-series-feb-2021/data)
- [Overview](https://www.kaggle.com/c/tabular-playground-series-feb-2021/overview)
- [RF Starter Notebook](https://www.kaggle.com/warobson/tps-feb-2021-rf-starter)
- [ML repo on GitHub](https://github.com/gumdropsteve/intro_to_machine_learning)
- [Most simple RAPIDS Notebook submission](https://www.kaggle.com/warobson/simple-rapids-live) (Has stuff like `train_test_split()` with cuml..)
    
#### Load Data",835687fc,0.0
538,171494b45650a2,78320308,"# **Pizza Price Prediction**
## *Exploratory Data Analysis*

------

**Arinal Haq**",9c8cc578,0.0
541,1750367e54f407,44aa1f33,# EfficientNet+Augmentation for Cassava Disease Classification using TF.Keras,a8e655b2,0.0
542,17a24d566ffa59,c0cd8805,"# Dimensionality Reduction 

##### Author: Alex Sherman | alsherman@deloitte.com ; Sai Revannth Vedala | revedala@deloitte.com


##### Agenda
- PCA
- SVD
- Latent semantic indexing (LSI/LSA)
- Latent dirichlet allocation (LDA)",89049e56,0.0
545,188731d7fa0604,da8e43ad,"## 전자상거래 배송 데이터
### 제품 배송 시간에 맞춰 배송되었는지 예측모델 만들기
학습용 데이터 (X_train, y_train)을 이용하여 배송 예측 모형을 만든 후, 이를 평가용 데이터(X_test)에 적용하여 얻은 예측값을 다음과 같은 형식의 CSV파일로 생성하시오(제출한 모델의 성능은 ROC-AUC 평가지표에 따라 채점)

![image.png](attachment:f70c3a4b-9984-4656-af95-dac047a900cb.png)

[시험용 데이터셋 만들기] 코드는 예시문제와 동일한 형태의 X_train, y_train, X_test 데이터를 만들기 위함임

(유의사항)
- 성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, 피처엔지니어링, 분류알고리즘, 하이퍼파라미터 튜닝, 모형 앙상블 등이 수반되어야 한다.
- 수험번호.csv파일이 만들어지도록 코드를 제출한다.
- 제출한 모델의 성능은 ROC-AUC형태로 읽어드린다.",7cc543d3,0.0
546,18a864b56ac3b8,3ba52f65,"# Task: Classification task with NN

The goal of the task is to build a coin classifier using a neural network. In this notebook I will demonstrate progressive resizing to achieve the most accurate possible results. My goal for this models accuracy was 97% as the owner of the dataset had a model that achieved 96% accuracy.

As one quick note: I'm using Luis Moneda's original dataset, instead of VolodymyrGavrysh's copy (which is the dataset that has the task attached to it). For whatever reason I had greater difficulty getting the classifier to run well on the other dataset (my final accuracy was around 93% and this model did better), it might not be that I wasn't using the best method to untar the .jpg files.

Thanks to:
* Luis Moneda for the dataset https://www.kaggle.com/lgmoneda/br-coins
* VolodymyrGavrysh for the task https://www.kaggle.com/volodymyrgavrysh/brazilian-coins-dataset-classification25k-images/tasks?taskId=395",f3ca0a7c,0.0
547,18a96bb5711ed9,1663819a,"<center> <img src=""https://i1.wp.com/www.middleeastmonitor.com/wp-content/uploads/2018/01/2016_8-30-Mediterranean-sea-migrantsCrHJ_O_WEAAFoSO.jpg?resize=1200%2C800&quality=75&strip=all&ssl=1""  height=""600px"" width=""900px""> </center>",e79768db,0.0
549,15eb884262ba09,d0a76aa6,**A brief visualization of the world's continents in one of the worst ways imaginable.**,d703bdab,0.0
550,20b372b6e4e276,f3b7e148,"<a class=""anchor"" id=""0""></a>
# [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)",ec8b0860,0.0
551,20c9a2456e494a,2b15b6aa,"# Training CIFAR10


* 1. Introduction
* 2. Data preparation
* 3. Model Training (CNN)
* 4. Feature extraction
* 5. Evaluation",3e487f55,0.0
552,20e1ba19eb9b5e,59b238d3,"# DESCRIPTION
In this notebook we try to predict the sale price of residential homes in Ames, Iowa. The dataset comes from a Housing Prices Competition held at Kaggle (https://www.kaggle.com/c/home-data-for-ml-course ). We focus on implementing the most popular gradient boosted decision trees algorithm - XGBoost. The final score of predicition on *test data* usually falls into TOP 4% on public leaderboard (https://www.kaggle.com/c/home-data-for-ml-course/leaderboard)",4569bfc1,0.0
553,2640fa3bf01b3d,9ec1cacf,"# Getting Started with the Abstraction and Reasoning Challenge

The following code might be useful to those getting started with the ARC.  The code creates a ""noop"" submission, in that the answers produced are just copies of the test question.",c82adeae,0.0
554,268a610bbc64b4,a8eaa541,# 0. Importing data,8a16f301,0.0
555,26b93b6f4dc148,46816740,### Importing Libraries,6f667d22,0.0
556,2730840089c8eb,2c309fe8,This lesson will be a double-shot of essential Python types: **strings** and **dictionaries**.,34d27dac,0.0
557,274b32da3b19a8,ccfa68d0,"# [Train notebook Here](https://www.kaggle.com/durbin164/chaii-baseline-training) 
",408f7268,0.0
558,27778055896e17,9157a4a2,"# **Credit Card Fraud Detection**

**The aim of this notebook is to build a classifier that can detect credit card fraudulent transactions. We will use a variety of machine learning algorithms that will be able to discern fraudulent from non-fraudulent one. By the end of this machine learning project, you will learn how to implement machine learning algorithms to perform classification.**",1dbe0165,0.0
561,28b5e5683108e1,b597762a,"**On the initial basis, I conducted some tests and got good results.**",cc9fe4d1,0.0
562,29437539745aa5,93c7e191,"<h1 style=""text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #74d5dd; background-color: #ffffff;"">Human Protein Atlas - Single Cell Classification</h1>
<h2 style=""text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;"">Categorical Classification At a Cellular Level [INFERENCE]</h2>
<h5 style=""text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;"">CREATED BY: DARIEN SCHETTLER</h5>
",c17b490a,0.0
563,2975a21f5ce2ae,7fd67a4b,"This kernel is a part of a [**post**](http://mlwhiz.com/blog/2018/12/17/text_classification/)  written by me on my [**blog**](http://mlwhiz.com/blog/2018/12/17/text_classification/) to try to learn text classification using Deep learning. I have tried to explain some of the models in an intutive way. Do take a look. 

### Import Libraries",4d6a6aa0,0.0
565,1466e61d45b718,b70982b6,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",b062d92b,0.0
566,145a0f699c2d4d,e7d5062b,<h1>MNISTSuperpixels Interpretation</h1>,55a70522,0.0
567,2a123b4e8f9433,5dfb7596,# Joseph Stewart,0a082218,0.0
568,2a377ced98d67a,ac7c34bf,# Exploratry Data Analysis and Regression Model for Admission Chance Prediction,262231a8,0.0
569,2a56d6b0e153f2,32ea37c4,"
# IMPORTING LIBRARIES",8dc315e6,0.0
570,2a724fb7835cdc,e05ba8e9,"**Fast and Basic Solution to Movie Review Sentiment Analysis using LSTM (forked from Ahmet Erdem)
**

I have used some of my previous code from Quora Duplicate Question Competition. https://github.com/aerdem4/kaggle-quora-dup",c38ac61d,0.0
571,2a9a149f306b6c,0dd06ee7,"# Skin cancer classification with fastai library

Check out the awesome [fastai](https://docs.fast.ai) libary and the great [online courses](https://course.fast.ai/). 

The obtained accuracy with fastai is >90% using transfer learning with a pre-trained resnet50 and just a few lines of code!",295c52ce,0.0
573,2ada0305b68956,b647e91f,"# What is pairplot?

By design, this feature creates a grid of axes such that each numeric variable in the data is shared between the y-axes in a single row and the x-axes in a single column. Diagonal plots are treated differently: a univariate distribution plot is drawn to indicate the marginal distribution of the data in each column.

It is also possible to display a subset of variables or to map various variables on rows and columns.",133e26f4,0.0
576,2597e45509d551,9f3573ac,"One of the main issues that make Kaggle (and for tahat matter any other) predictive modeling tricky are the discrepancies between the training and the test datasets. In order to get an idea of the magnitude of these differences, one of the more valuable tools to use is adversarial validation. With aversariel validation we try to build an auxiliary model that predicts whether given data points belong to the train and the test set. If we can make predictions with such a model with a high degree of confidence, then that usually means that the train and test sets are significantly different, and we need to be careful to make a model that will take that into the account.

We will make this adversarial validation notebook with the Rapids library. [Rapids](https://rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https://www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. 

Rapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https://rapids.ai/start.html).

For the modeling part we'll use the latest version of XGBoost, which allows for GPU accelerated calculation of Shapely Values. We'll use these ""SHAP"" values to calculate correct feature importances. Starting with the version 1.3, XGBoost supports fast calculation of the SHAP values on GPU. ",4ef1bf15,0.0
577,254cccd5145725,81d296e6,<h1> Problem</h1>,a49b4037,0.0
578,20e523830aab51,39c9831e,"# Task for Today  

***

## Orange/Grapefruit Classification  

Given *data about citrus fruits*, let's try to classify the **type** of a given fruit.  
  
We will use a logistic regression model to make our predictions.",810b8785,0.0
579,2105f2c5132866,c5eb4e1e,"In this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. 

The notebook walks us through a typical workflow for solving data science competitions at sites like Kaggle.

There are several excellent notebooks to study data science competition entries. However many will skip some of the explanation on how the solution is developed as these notebooks are developed by experts for experts. The objective of this notebook is to follow a step-by-step workflow, explaining each step and rationale for every decision we take during solution development.",bfe8023d,0.0
582,21bce4ec54b3fa,b2869711,"# Feature selection

The purpose of this post is to explore feature selection techniques, namely:
    * Correlation based
    * K-best
    * In-built features importance of Random Forest
    * Permutation importances",35546e30,0.0
583,21c1e34efd71b8,cdaefcf5,"This is Part 2 - 
In this part I'll show how the Near Miss and SMOTE CV improves the Recall Accuracy of the models as compared to the Stratified Kfold.

Please note that few steps like reading the file and normalizing/standardizing the Amount and Time columns will be repeated.",23b2cdd6,0.0
584,2259c048379b36,4afd62a7,"# This notebook is about measuring the variability in a time series. 
## So given two time series, how can I determine which one is more variable? 
### In the following lines, two time series will be compared in order to find which one is more variable by using the percentage change and then the standard deviation of such percentage change. 
### The time series with the highest standard deviation is also the most variable one.  
",43558d11,0.0
585,225b4fe5d3894a,e936bec2,"## KEY NOTE

This notbook is complete guide to end to end machine learning problem from scratch. if you are beginner, it might hep you have an insight on how to start and how to approach a ML problem. Since the dataset is fairly simple it is very good to start your handson with.

I followed the book by Aurelien Geron, The steps described by him are really detailed, so I decided to replicate it on my own and experiment with the concepts.",4b4197b3,0.0
587,22bd95f4807a23,77f898b7,"# Data Import

In this script, we use a dataset that represents women's clothing reviews. This script will look at a surface level analysis of the data. We will be predominantly using the the `pandas` data analysis libary.",c05d356f,0.0
588,450fda47b03baa,37329190,Gerekli kütüphaneleri yükleyelim.,62c04adb,0.0
591,2409b2d74a9871,955d3cde,"This notebook is a detailed exploration of the data in the 'Newyork Airbnb dataset' as well as the price prediction accuracy testing with different models.<br> 
**Problem Definition**: Predict the prices of the room on the basis of the given parameters to rightly decide the costing to increase profit and improve customer service.",b0a6c313,0.0
592,241cf32abb22d8,91c02d43,"# Predicting Student Performance in Mathematics

The obejctive of this project is to build and compare three binary classifiers for predicting student performance in Mathematics, using the data collected from two public schools in Portugal during the school year 2005/06. The dataset was retrieved from the UCI Machine
Learning Repository (Cortez & Silva, 2008). The descriptive features include 5 numeric, 17 nominal and 10 ordinal features.
The target feature, G3, is a numeric variable, which shows the final grade. The values of G3 range from 0 to 20, where 0 represents the lowest grade while 20 represents full marks. For a binary classification, G3 values that are greater or equal to 10 represent ""Pass"", else ""Fail"". 

## Outline:
- [Section 1 (Overview)](#1)
- [Section 2 (Data Preparation)](#2)  
- [Section 3 (Hyperparameter Tuning)](#3) 
- [Section 4 (Performance Comparison)](#4) 
- [Section 5 (Summary)](#5) 
- [Section 6 (References)](#6) ",47157066,0.0
593,245c89d02f3f5f,79cbb0f5,"# Παίζοντας Atari με βαθιά ενισχυτική μάθηση

Προσοχή: για να μπορέσει να τρέξει αυτό το notebook στο Kaggle πρέπει να ενεργοποιήσετε το Internet και την GPU για τον πυρήνα σας. Αυτό απαιτεί την επιβεβαίωση του κινητού σας με SMS. Πηγαίντε στο δεξί sidebar στα settings και θα το δείτε. 

Τελικά πρέπει να έχετε αυτή την εικόνα:

![](https://i.imgur.com/Ek5hOIo.jpg)",61a1eacd,0.0
595,2473d004f92592,cfb7c343,**Import Python libraries**,18d3b6ee,0.0
596,24cc078fe66245,bc4fd79e,"# Intrusion Detection Evaluation Dataset (CIC-IDS2017)
By AliK604 

Intrusion Detection Systems (IDSs) and Intrusion Prevention Systems (IPSs) are the most important defense tools against the sophisticated and ever-growing network attacks. Due to the lack of reliable test and validation datasets, anomaly-based intrusion detection approaches are suffering from consistent and accurate performance evolutions.",04dcf5fe,0.0
598,2500c5fe8497ee,5f3cdacb,"# 🥩🥩🥩 Worldwide Meat Consumption Analysis


## Introduction:

Meat consumption is related to living standards, diet, livestock production and consumer prices, as well as macroeconomic uncertainty and shocks to GDP. Compared to other commodities, meat is characterised by high production costs and high output prices. Meat demand is associated with higher incomes and a shift - due to urbanisation - to food consumption changes that favour increased proteins from animal sources in diets. While the global meat industry provides food and a livelihood for billions of people, it also has significant environmental and health consequences for the planet. This indicator is presented for beef and veal, pig, poultry, and sheep.

Meat consumption is measured in thousand tonnes of carcass weight (except for poultry expressed as ready to cook weight) and in kilograms of retail weight per capita. Carcass weight to retail weight conversion factors are: 0.7 for beef and veal, 0.78 for pigmeat, and 0.88 for both sheep meat and poultry meat.

(Source: https://data.world/oecd/meat-consumption/workspace/project-summary?agentid=oecd&datasetid=meat-consumption)

## Objective

Exploratory Data Analysis of the worldwide meat consumption to get insights about the data.

## Data Feilds

1. Location: The country code
2. Subject: The type of meat ('BEEF' 'PIG' 'POULTRY' 'SHEEP' etc.)
3. Measure:
    * KG_CAP: KG per person annually
    * THND_TONNE: Annual consumption in thousand of tonnes
4. Time: The Year the data recorded
5. Value: The Value, according to the Measure",855355f0,0.0
599,149cb8d3489224,5fab2979,"<center><h1>Explore Vaccines Tweets</h1></center>


<center><img src=""https://images.unsplash.com/photo-1605377347958-e8bd4c00ba1d?ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=700&q=80"" width=400><img></center>

# Introduction


The Dataset we are using here is collected using Twitter API, **tweepy** and Python package.

The following vaccines are included:  
* Pfizer/BioNTech;   
* Sinopharm;  
* Sinovac;  
* Moderna;  
* Oxford/AstraZeneca;   
* Covaxin;   
* Sputnik V.  

",116858e7,0.0
600,2343dc02ffb96a,dd887ba7,"# In Kaggle, ensure the correct files are in the input directory.",29aa95a4,0.0
601,722cd844dfbe8f,074dca0d,![brain_baner](http://www.mf-data-science.fr/images/projects/brain_baner.jpg),0cedb385,0.0
603,45921c50ac56fa,54a9f72e,"# Introduction to Latent Dirichlet Allocation 
 I've made this notebook to showcase the capability of Latent Dirichlet Allocation( LDA ).  
 I have used this dataset's training data to demonstarate LDA and how to implement it using   
 <b> Gensim and pyLDAvis </b>.
    
We will use LDA to perform Topic modelling.  
Topic modelling refers to the task of identifying topics that best describes a set of documents.     
These topics will only emerge during the topic modelling process (therefore called latent).   
To tell briefly, LDA imagines a fixed set of topics. Each topic represents a set of words.   
And the goal of LDA is to map all the documents to the topics in a way, such that the words  
in each document are mostly captured by those imaginary topics.",465973eb,0.0
605,631cd434fc3aa2,b2e25313,"## Intro
To help me build my first Kernel here are some greate notebooks that I've read:
* [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by Serigne.
* [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by Pedro Marcelino.",2b74febb,0.0
607,639e8aae4e046e,036149f6,"**Summary**

   - **Problem:** predict the total revenue from each visitors to Google Store
   - **Data:**  ""per visit"" information is provided.  Each visitor may have several visits to the store.
   - **Startegy**:
       - Classify each session based on having/not-having revenue
       - Predict each session's revenue
       - Sum up revenue's from each visitor",77deb4cb,0.0
608,63b44c85e32c1f,bf8392d3,# Data Structures,fb9b9562,0.0
609,63d0d9b9a8c7d2,33fd7934,# Importing Required Packages,e32e5933,0.0
610,64169805aacf17,90013678,# **[diavlex Art+AI](http://www.libreai.com/diavlex): Text to Paint using Neural Painters and CLIP**,1f12ded0,0.0
611,6471597c5d2f66,836683bd,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",a41b4abe,0.0
613,64a336ac34d95c,cb5781f8,"
# Customer Churn Prediction
## Problem Statement
### Acquiring a new customer typically costs anywhere between 5 and 25 times as much as it does to retain an existing customer, making it critical for any business to minimize customer churn. That’s why it’s so important for product managers to take a proactive approach to user retention, to both minimize acquisition cost and increase the chances of mass adoption over the long haul. But any product manager knows that they’re going to experience serious roadblocks to adoptions, and in turn, retention. The more you can forecast churn, the better you can prevent it. 
## What to achieve?
### With machine learning models,what’s specifically causing churn.",be73a990,0.0
614,64c5c66fd713b1,2f6cec95,"## How well is Ohio handling COVID-19

Background: I live in Cleveland Ohio and our Governor was one of the first in the US to shut down schools, entertainment and dining venues, and scale large gatherings to less than 50 people. 

I am interested in measuring the efficacy of these measures against the spread of COVID-19 globally. 

As of this draft 1.0 on 2020_03_16 I will be looking at initial spread rates compared to protective measures in Hubei, China and Italy vs Ohio.

## Population - Area - Population Density

****

### Hubei

<img src=""https://www.google.com/maps/vt/data=v_Cz891VEP0Pp80anEu_urZ1oIqJg1YC6TxIO5hX_m4YRWNhYo3PUz0xeaTMkWSzwdZ-V_Gg0Vilws9bpqcuL2gdbfyVu_DbhvfIoK1x2fMnMepj-jDRW5bOKMdr5NJR0r4hHQ6MGGzJM4gYHOx12hrDTd-yHTJwVyZuZ9Ju7OgeZ271hxQ6UnV8d4SX65WUuf0VBRoojbsK3P5nDITPmVdnk45upyILcB83tZxacAf-QW5C9v-4zLE-"" width=""50%"" height=""50%"">

#### Population 

58.5 million 2015

#### Area 

Area: 71,776 mi²

#### Population Density

820 per square mile
Hubei is the size of Washington State where 7.5 million Americans live. 
However, the Chinese province has a population 58.5 million people and an average population density of 820 per square mile.
Feb 13, 2020

****

### Italy

<img src=""https://www.google.com/maps/vt/data=hKYfMKIcQ1TbBwCUL4DuZ9KoBFOaniAh745n79zTGIFrYOgmEZrhbAlpGEehkEe-l58ttI8eRgnTVQ7h6Js42n3HLL6J5tYYJrScRYPFy6w0lzbuwI1aPFwZ-cA5h-eHxrsjBgi9tiM7QXc_8eqXCsPGAZSGSpNCPbmFKZnHPjQfbLaOBHw8ilngIekuuL-nT2W0aFnBdRasvt20TSnbo_zyTpfh1LF-Y8T2B8FDJ1mCH1EAifEwsQxh"" width=""50%"" length=""50%"">

#### Population 

60.48 million (2018)

#### Area

116,347 mi²

#### Population Density 

532 people per square mile
The population density in Italy is 206 per Km2 (532 people per mi2). The median age in Italy is 47.3 years.

Italy Population (2020) - Worldometer

**** 

### Ohio

<img src=""https://www.google.com/maps/vt/data=7lVrMlyOFY_pn6RCFrjKXRWTyMx7Xqbfs8Uofof7cHQPLuX2Aln1AzKzlVjVrNGmp_rVsUwQ4I8ZogFNAnFgnvcPzWq7YcwbhB1HgkimT-W0aw0YjHV35NNbzBd2T7GIb_alHxifLDm28JNeipjyQYcOd1uca_mJzoMWWBLgnFLTpc1xfVioay4YzUXjCZzBHdYU2I3ls8yCyFA0e_-1wlJGT6geXxsnZVG5rXT01vRflXoFR8qfB0AF"" width=""50%"" length=""50%"">

#### Population

11.69 million (2018)

#### Area 

44,825 mi²

#### Population Density

Ohio has a population density of 282.3 people per square mile, ranking 10th in the nation, with a total land area of 44,825 square miles, which ranks 34th. Ohio's population is spread throughout the state with many major cities. Feb 17, 2020

Ohio Population 2020 (Demographics, Maps, Graphs)",5814b2b3,0.0
615,0e09587faffa8f,1c8030ba,"# New York City: Parking Tickets Analysis
![New York City](https://blog-www.pods.com/wp-content/uploads/2019/04/MG_1_1_New_York_City-1.jpg)",0d563d61,0.0
618,656185a18260be,e5532c43,"# Data Recipes and Augmentations Training Demo

My goal for this notebook is to demonstrate my training pipeline, especially the data recipes and image-like augmentations. I trained my models locally, so I won't be able to reproduce exactly the same results on Kaggle notebook. ",0318cab5,0.0
620,659f5f3ef8aa0e,e6575d7f,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",3654c2d0,0.0
621,663bbc9eaf267b,29da4efe,# BMW Used Car Price Prediction,32445529,0.0
622,669ce946943d60,ff138fa5,"# Implementation of MCRMSELoss

In this notebook, MCRMSE is implemented according to [this resource](https://www.kaggle.com/c/stanford-covid-vaccine/overview/evaluation) and some examples are shown.
Example section is based on the [@hiroshun's notebook](https://www.kaggle.com/hiroshun/pytorch-implementation-gru-lstm). Thank you for publishing a good implementation.

Using this MCRMSELoss and `SN_filter` `df = df[df.SN_filter == 1]`, you can simulate the LB score. However you got to be carefull that the private score does not use `SN_filter`.",0f63c4ce,0.0
623,675b60eaf415a6,24b39cca,"# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**
![alt text](https://www.vision.ee.ethz.ch/datasets_extra/food-101/static/img/food-101.jpg)",68c0b725,0.0
624,67b7354e96113a,2675dae2,## About the problem,dca94250,0.0
626,67efe818cb2372,e6f01c13,"**Introduction**

I have some experience in classification neural nets from the 1990s (feed-forward backprops, adaptive resonance etc - all written in C or C++ from scratch) which took vectors as inputs. 
I'm completely new to both Python (as a programming language) and the new net architectures (CNN etc) and frameworks (tensorflow, keras, sklearn) so armed with a couple of books and the on line documentation here's my first attempt at image classification using keras.
",f28a2a34,0.0
628,62037c5832129c,43914338,"*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017

Code Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition

Code License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)

# Python Machine Learning - Code Examples

# Chapter 6 - Learning Best Practices for Model Evaluation and Hyperparameter Tuning

Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).

*The use of `watermark` is optional. You can install this IPython extension via ""`pip install watermark`"". For more information, please see: https://github.com/rasbt/watermark.*",61474350,0.0
629,5e1d001f8764e0,272fdce2,"# FAKE NEWS CLASSIFIER USING LSTM

IN THIS NOTEBOOK WE WILL CLASSIFY WHETHER THE NEWS IS FAKE OR NOT USING LSTM MODEL.

1--> REAL

0--> FAKE

WE WILL PERFORM SOME TEXT PREPROCESSING( STEMMING, REMOVAL OF STOP WORDS, CONVERTING TEXT INTO VECTORS)

THEN WE WILL BUILD OUR LSTM MODEL TO CLASSIFY THE NEWS",62be464c,0.0
631,5ea840754577e3,1020c165,"There are a lot of notebooks in here on titanic. So I'll add mine to the lot as well.

## Features in the data set

* PassengerId: Passenger identifier. Not really necessary for any analysis
* Survived: If the passenger survived the tragedy or not
* Pclass: Socio-economic status of the passenger.
    * Were passengers with higher socio-economic status given higher priorities to access to life boats
    * Did socio-economic status and gender combined create more priority access
* Name: name of the passenger. Not needed for analysis because it will not add any value
* Sex: Male/Female
    * How did gender affect the survivability of a passenger?
* Age: Age of the passenger in years
    * How did gender affect the survivability of a passenger?
    * Were children and old age given a higher priority?
* SibSp: Number of siblings / spouses
    * Did number of siblings / spouses impact the chances of survival?
* Parch: Number of parents / children
    * Did number of parents / children impact the chances of survival?
* Ticket: Ticket number of the passenger
    * Did ticket number indicate any higher status?
* Fare: How much the passenger had paid for his/her travel
    * Does fare paid matter in survivability?
* Cabin: The cabin number of the passenger
* Embarked: The place where the person had embarked from
    * Does place of embarking affect survivability?",9cf9b73f,0.0
634,5f32117bcd5255,734d016c,"# HISTORY

#### New Hycean Planets, Near K2-18 - b

* The star has an exoplanet, called K2-18b, a super-Earth located within the habitable zone of K2-18. It is the first exoplanet in the habitable zone, albeit a gas giant, to have water discovered in its atmosphere. The star also has a second planet K2-18c, which is proven by system tidal simulation to be a small gas giant

* K2-18b, also known as EPIC 201912552 b, is an exoplanet orbiting the red dwarf K2-18, located 124 light-years (38 pc) away from Earth. The planet, initially discovered with the Kepler space telescope, is about eight times the mass of Earth, and is thus classified as a super Earth. It has a 33-day orbit within the star's habitable zone.

* In September 2019, two independent research studies, combining data from the Kepler space telescope, the Spitzer Space Telescope, and the Hubble Space Telescope, concluded that there are significant amounts of water vapor in its atmosphere, a first for an exoplanet in the habitable zone.

* A new class of 'waterworld' exoplanets have been found by astronomers, who say that despite being hot, ocean-covered and hydrogen-rich - could support life.

* They have been dubbed 'Hycean' world's by the team from the University of Cambridge, who say they 'greatly increase our chance of finding alien life.'

* In the search for life elsewhere, researchers have mostly looked for planets of a similar size, mass, temperature and atmospheric composition to Earth.

* Astronomers now say there may be more promising candidates out there, and these Hycean worlds are more numerous than Earth-like planets.

* The habitable exoplanets are hot, ocean-covered worlds with hydrogen-rich atmospheres, and are more observable than Earth-like planets due to their temperature, making them easier to detect with current telescopes.

* Dr Nikku Madhusudhan from Cambridge's Institute of Astronomy, who led the research, said they open a whole new avenue of search for life elsewhere.

* Many of the prime candidates of these planets identified by the researchers are bigger and hotter than Earth.

* This data includes FITS files of previously discovered habitats. It will be used for research.",85882abf,0.0
635,5f4ae633cfd090,83c05b9a,"# UFC WINNER PREDICTION

This is a UFC dataset which uses bets and odds to favour who will win a particular fight.
Although originally intended for predicting 'bets', I will be using this to predict who will win the fight.

The following notebook is divided into 3 parts:
1. Baseline model - Building a simple baseline model with basic modifications
2. Data Visualization - Using the power of visualization to gain a deeper insight into our data.
3. Feature Engineering and building the final model - After both the steps above are done, using feature engineering to strengthen our input and build a model more powerful than the baseline

Note: Originally the dataset is built for a **Regression** model, but I have used it to for the purpose of **Classification**.",a30a16e2,0.0
636,0e2a23fbe41ca9,181dabc0,"Elo Merchant Category Recommendation
=====

# Objective

Develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. We have to predict a loyalty score for each ```card_id``` represented in ```test.csv```.

# Evaluation Metric

Evaluation metric is RMSE - $\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$, 
where $\hat{y}$ is the predicted loyalty score for each ```card_id```, and $y$ is the actual loyalty score assigned to a ```card_id```.

# Data

- **train.csv**: Training set having the information of cards. It also has the loyalty score column.
- **test.csv**: Testing set having the information on cards
- **historical_transactions.csv**: Up to 3 months' worth of historical transactions for each card_id
- **merchants.csv**: Additional information about all merchants / ```merchant_id```s in the dataset.
- **new_merchant_transactions.csv**: two months' worth of data for each ```card_id``` containing ALL purchases that ```card_id``` made at ```merchant_id```s that were not visited in the historical data.
",64e4762c,0.0
637,5f544a32fb2ce9,8d29e5bf,# Load Packages,616f33af,0.0
638,5f674175839b32,30147ce5,# Video games SALES ANALYSIS,53a2e343,0.0
639,5fc2f23dfbeeb1,2bf4a8ec,# Natural Language Processing with Disaster Tweets,f37b4110,0.0
640,5ffe6aa38958a1,c3f7b2bc,"**Beginning with Kaggle**

[ Reference: https://www.kaggle.com/niklasdonges/end-to-end-project-with-python#Random-Forest]



",11f5412e,0.0
641,6014a2010722f2,e7294415,"# Getting Twitter Data and Create Dataset - Without API

<center><img src=""https://help.twitter.com/content/dam/help-twitter/brand/logo.png"" alt=""Twitter Image""></center>

In this simple notebook, i am gonna use Twint - https://pypi.org/project/twint/ to get Twitter data.

Different from Tweepy, this is a relatively new package that manages to get around Twitter's API.

If you want to implement this library, i recommend you to use it carefully

In this notebook, i will cover:

- Searching for tweets
- Extracting it's data and then save it

According to Twint's team, some of the benefits of using Twint vs Twitter API:

- Can fetch almost all Tweets (Twitter API limits to last 3200 Tweets only);
- Fast initial setup;
- Can be used anonymously and without Twitter sign up;",abd38dca,0.0
642,601e18072783b4,f5f78b42,"In this notebook, I have combined rating data from IMDb and matched it with films/TV shows in the Netflix data. My aim is to do data visualization of Netflix content with IMDb rating. I have restricted data analysis to India & US.
 ",36b2b1fa,0.0
643,60d500d196eb42,33b4b462,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",2ad55f3f,0.0
644,60da9bbfe39c4b,e98704f0,"# Exploratory Data Analysis
* Exploratory Data Analysis of 'Covid-19 Status in Bangladesh'.
* In this EDA, describe all informations of coivd-19 of Bangladesh",b0dd8ad6,0.0
645,6116b13d4464d0,4855c3bc,"# Background

Here I provide code for concatenating the .wav files from each of the birdcall directories into a single larger .wav file.

I decided to do so after reading the winning solution in the BirdCLEF 2019 competition. [Bird Species Identification in Soundscapes](http://ceur-ws.org/Vol-2380/paper_86.pdf)

In his write-up after the competition, Mario Lasseck describes a data augmentation technique in which he uses a single large file composed of background noise from various soundscapes. Similarly, I decided to use a similar scheme in which I concat a large wav file and pick 5 sec segments out rather than dealing with more complicated logic to account for wav files that are less than 5 seconds or (not evenly divisible into 5 second intervals).

It took me a few hours to figure out how to call ffmpeg from within Python, so I provide the code here in case it's of assistance to someone. Additionally, I've provided the dataset that was produced from this code. More details on the dataset at the links below:

# Dataset
[A to B](https://www.kaggle.com/smodad/birdcall-no-background-concat-a-b)

[C to F](https://www.kaggle.com/smodad/birdcall-no-background-concat-cf)

[G to M](https://www.kaggle.com/smodad/birdcall-no-background-concat-gm)

[N to R](https://www.kaggle.com/smodad/birdcall-no-background-concat-nr)

[S to Y](https://www.kaggle.com/smodad/birdcall-no-background-concat-sy)

[Background noise](https://www.kaggle.com/smodad/birdcall-background-concat)

",e2826468,0.0
647,614ba9f0c62677,4b1ca3cb,"CNN is tried with Fashion Dataset of MNIST

2019 - November",b8551335,0.0
648,6181cffbfcc35a,9d61bc75,"For the sake of initial prototyping, I provide rescaled images: 
* 256x256: https://www.kaggle.com/konradb/resized-data-256
* 512x512: https://www.kaggle.com/konradb/resized-data-512
",2cc9bd1e,0.0
650,62487bcd70b199,ad7f3d0b,"# Campaign for selling personal loans.

This case is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are liability customers (depositors) with varying size of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.

The department wants to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.

The file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.",f6ae50af,0.0
653,6903d3f38c6a66,33d727f1,![MAt.png](attachment:MAt.png),6067ce5e,0.0
654,6d29650083cbde,8cab1205,"**1. INTRODUCTION**

I've had for about a year now data on football performance of all players in the 5 'Big Leagues' (i.e. England, Spain, Italy, Germany & France) between 2009-2010 and 2016-2017 and I decided to try to fit a random forest on the data and see how much I can predict future success. 

I focus for now on strikers, which appear to be much more homogeneous in terms of performance. 

My goal is to find a way to predict which players are 'relatively' under the radar and become rated in the top 10% of their league the following season.

I will first present my data, then perform a random forest analysis to link period *t+1* rating with period *t* performance. I follow this by a concrete example of what my data predicts as the best 2018 strikers, and link this result to actual performance data. I finally wrap this up by forecasting the 2019 talents.

Please do comment and point me towards improvements !",e65fd993,0.0
655,0d8df2c2983694,2f30674c,# Building a Logistic Regression,9bf7fa4e,0.0
656,6d66ced0028dea,de7f3461,"# Бейзлайн для курса ""Библиотеки Python для Data Science: Numpy, Matplotlib, Scikit-learn""",f50aae52,0.0
658,6e28c4f557f736,9c211a4b,"### Predicting real disaster tweet using GloVe and LSTM
To determine whether a person’s words are actually announcing a disaster is important: 
Dataset description:",021fdf75,0.0
659,6e472c6c591c7d,c3e1a25a,"**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**

---
",65532a3d,0.0
661,0d59a3e0130db0,482488a2,"Hello to all Russian speakers and everyone interested! 

Recently in the NLP course, I learned that convolutional neural networks can be used to classify text and decided to check this on this wonderful dataset. Let's see what happens :)",285f04b2,0.0
662,6e9b4020644836,1ca1f738,"# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Emma Raducanu</p>
 
![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRI1CyNcqT7Hzu6KZ_aSdWtOlSbH5xubiByjg&usqp=CAU) 

### **Who Emma Raducanu?**
    Raducanu, who was born in Canada to Ian and Renne, with her father Ian being of Romanian origin and mother Renne of Chinese origin, moved to England with her parents at the age of two years and started playing tennis at the age of five years. She would later train under coach Nigel Sears.",5ad41fc6,0.0
665,6f4795cfdc96c7,50b0c1c8,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",1f3ab82f,0.0
669,70b7a24d522250,e85c8bb7,![](https://techinsight.com.vn/wp-content/uploads/2020/08/chatbot.jpg)LaptrinhX.com,83f3c002,0.0
672,71b75664517244,fc0abbad,"# Introduction

Hai kagglers, this is my tutorial on how to access all insight Premier League All Season Dataset.
We will perform some EDA and visualization, lets get into it.",fc905af5,0.0
673,71c3c1eab0377d,b1f02ee5,# Basic Implementation of H2O Gradient Boosting for Classification on Titatnic DataSet,52b4e360,0.0
674,71d3e4aee86e3e,554161de,"# India Coronavirus(Covid-19) EDA,Visualizations and Comparisons

<a></a>
# 1. Introduction 
[](##Dataset_Imports)
<font size=""2"">The COVID-19 pandemic in India is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first case of COVID-19 in India, which originated from China, was reported on 30 January 2020. India currently has the largest number of confirmed cases in Asia and has the second-highest number of confirmed cases in the world after the United States,with the number of total confirmed cases breaching the 100,000 mark on 19 May,and 1,000,000 confirmed cases on 17 July 2020. On 29 August 2020, India recorded the global highest single-day spike in COVID-19 cases with 78,761 cases, surpassing the previous record of 77,368 cases recorded in the US on 17 July 2020. India currently holds the single day record for largest increase in cases, set on September 17, with an additional 97,894 and has sustained highest number of daily cases spike since then. September 25 marked 1.49 million tests in a single day; the highest in the world.</font>

![](https://drive.google.com/uc?export=download&id=1tiSW79M8ERZ_pEKFEhXFZro6xJK8MqNo)


### This notebook aims at exploring COVID-19 situation in India through data analysis and projections.

## Covid-19 Data Sources:
* CSV Datasets [COVID19-India API](https://api.covid19india.org/documentation/csv/)
* Official Sources are [MOHFW](https://www.mohfw.gov.in/), [ICMR](https://icmr.nic.in/node/39071).
* Learn more from [World Health Organization](https://www.who.int/emergencies/diseases/novel-coronavirus-2019)

> <font size=""2"">Don't PANIC, follow your nation and WHO guidelines.
> Stay safe. Join the cause!</font>

## Feel free to drop your feedback below.Last update: 07/19/2021. Updates to the data table.

",69706f0b,0.0
675,6d1c11d28bb481,19e02c12,"Source:

https://docs.python.org/3/library/codecs.html#standard-encodings

https://www.kaggle.com/devghiles/step-by-step-solution-with-f1-score-as-a-metric

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html

To rename:
https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas

To change cols:
https://stackoverflow.com/questions/12329853/how-to-rearrange-pandas-column-sequence/23741704

https://rajacsp.github.io/mlnotes/python/data-wrangling/advanced-custom-lambda/
",62cf76ed,0.0
677,6cade0b6a41ba2,58c550a6,"# INDEX
- ## 1. Library Management
- ## 2. Data Sourcing
- ## 3. Data Cleaning
- ## 4. Data Preparation (For Modelling)
- ## 5. Data Modelling",e6110293,0.0
678,6cac6d4743088f,17f18c98,"<div align=""center"">
 <img src=""https://raw.githubusercontent.com/matheusmota/dataviz2018/master/resources/images/logo_facens_pos.png"" width=""150px"">
 <h1> Dataviz - Data Science Specialization Program - FACENS</h1>
</div>
<br><br>
# Exercício 1 - Primeiro contato com o Kaggle
(valendo nota)

* **Data de entrega:** até o final da aula 
* **Professor:**  Matheus Mota
* **Aluno:** Vinícius Targa Gonçalves
* **RA:** 191228",55fb02fa,0.0
679,690e3b3b5cf0b4,4a4aed58,The idea of this Notebook is to easily upload the pretrained model darknet53.conv.74 as download can take several minutes and I could not find a public notebook with the file. Add the notebook in your input. Size is 155Mb. You can use it to train your Yolo model with your custom object.Includes also the whole darknet.,80eb8ed7,0.0
681,6998861ff6ff01,8c6e01ed,"### All days of the challange:

* [Day 1: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)
* [Day 2: Scaling and normalization](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)
* [Day 3: Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)
* [Day 4: Character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)
* [Day 5: Inconsistent Data Entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)
___
Welcome to day 3 of the 5-Day Data Challenge! Today, we're going to work with dates. To get started, click the blue ""Fork Notebook"" button in the upper, right hand corner. This will create a private copy of this notebook that you can edit and play with. Once you're finished with the exercises, you can choose to make your notebook public to share with others. :)

> **Your turn!** As we work through this notebook, you'll see some notebook cells (a block of either code or text) that has ""Your Turn!"" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). You can also click in a cell and then click on the right ""play"" arrow to the left of the code. If you want to run all the code in your notebook, you can use the double, ""fast forward"" arrows at the bottom of the notebook editor.

Here's what we're going to do today:

* [Get our environment set up](#Get-our-environment-set-up)
* [Check the data type of our date column](#Check-the-data-type-of-our-date-column)
* [Convert our date columns to datetime](#Convert-our-date-columns-to-datetime)
* [Select just the day of the month from our column](#Select-just-the-day-of-the-month-from-our-column)
* [Plot the day of the month to check the date parsing](#Plot-the-day-of-the-month-to-the-date-parsing)

Let's get started!",ea9e72cf,0.0
682,69ac33d79f5130,fb518f64,"# US Accidents data Analysis

TODO - Talk about EDA

TODO - talk about the dataset (Source, what it contain, how it will be usefull)

 * Kaggle
 * Information about US accidents
 * Can useful to prevent accidents.
 * New york data not present.",9d760d2a,0.0
683,69d50f5e1373f1,dba6eafb,"# Abstraction and Reasoning Starter Notebook

This notebook will get you started on on the basics of this competition",ec7545ee,0.0
685,69e2428808c415,c3c5a584,# Importing important libraries and dataset ,ae798c16,0.0
686,6a05614abce6d9,3cdb321b,"## CatBoost Tabular Prediction (Oct 2021)
In this Notebook, I will:
* Build a Tabular Prediciton Model using Catboost.
* Make this Notebook available to train on Google Colab or Other Platfroms or your Computer. (It may need to adjust code and configuration), The Competition result can be submited automatically.",c0c9da16,0.0
687,6a1ae8234c7653,25e2a3d2,**Getting started with Kaggle**,2d643c72,0.0
688,6a1d04e8153df3,bc8f2103,"# Introduction

Hy viewer/ Kaggler ,
Haberman’s data set contains data from the study conducted in University of Chicago’s Billings Hospital between year 1958 to 1970 for the patients who undergone surgery of breast cancer.

**Objective**
-I am going to perform some analysis according to survival status.",38572b05,0.0
689,0dd3ac2d55efd7,3744f137,***After my previous notebook on Bow and Tf-idf models this notebook takes you through word embeddings and different methods to create word embeddings and using those embeddings with a Sequential model using bidirectional LSTM layer which classifies our text achieving a submission score of >0.8 .***,e9aa2cc2,0.0
690,6a49325ea305e2,9b2adff9,"# RAPIDS CuML
The RAPIDS library is now available in all Kaggle notebooks. Hooray! Simply type `import cuml` or `import cudf` to load the two most popular packages.

RAPIDS is described [here][1]. RAPIDS `cuDF` accelerates dataframe operations using GPU and has a similar api as Pandas. RAPIDS `cuML` accelerates machine learning algorithms using GPU and has a similar api as Scikit-Learn. Since RAPIDS ML algorithms are so fast, we can do things that were never possible like applying genetic algorithms to ML hyperparameter searchs!

[1]: https://rapids.ai/",119fdcf9,0.0
691,6a80f915608fc2,ba620372,# MoA or Not MoA : that is an easier question?,636938eb,0.0
695,6b54e39f86bdb5,9478833a,"# CNN for MNIST digits classification (99.285% test accuracy, top 25%)

In this notebook I will use a modified LeNet5 implementation using tensorflow keras API for the problem of handwritten digits classifcation. This is tribute to he man who created the data set and the performance of this CNN is very satisfying for not an acceptable number of parameters.",198084bc,0.0
696,6b65d81a5743dd,ef5487c1,# 1. Introduction,4080a2d2,0.0
697,0d9a2067267ba1,b733f8e5,"### Objective :
We aim to predict the energy consumption instensity by building using climate and weather features .
",abc194fb,0.0
698,6b7c80ed7bd03d,7f4ce653,"# 0. Introduction

Welcome to the competition, '<a href=""https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/overview"">RSNA-MICCAI Brain Tumor Radiogenomic Classification</a>'.  
Also, welcome to this source code.  
This source code is constructed for the following goals.  
* Providing the converting functions from DICOM (DCM) to Numpy Array (npy).  
* Visualizing the converted array in 2D and 3D plot.  
  
Try this source code and upvote if you like it!  

I hope always good luck to you.",7bba27db,0.0
699,6b955982396c14,7927eced,## 강의 영상 : https://youtu.be/Jh3rJaZlEg0,2b4cb71b,0.0
700,6cacdcf8daf400,5740bbca,"Image augmentaion library인 imgaug를 사용해보는 예제입니다.

**Reference **  
https://www.kaggle.com/fulrose/3rd-ml-month-car-model-classification-baseline  
https://www.kaggle.com/tmheo74/3rd-ml-month-car-image-cropping-updated-7-10  
https://www.kaggle.com/easter3163/3rd-ml-month-keras-efficientnet  
https://www.kaggle.com/seriousran/what-0-95121-model-failed  
https://www.kaggle.com/janged/3rd-ml-month-xception-stratifiedkfold-ensemble  
https://www.kaggle.com/ratan123/aptos-2019-keras-baseline  
https://github.com/aleju/imgaug",83939b53,0.0
701,5e02999ca74e7e,9728ab9e,"# **Import Libraries**

Import the module that we want to use for this research.",b69da28e,0.0
702,5d6d539f8e7121,0ec0204c,"## 결측치 제거 및 그룹 합계에서 조건에 맞는 값 찾아 출력
- 주어진 데이터 중 basic1.csv에서 'f1'컬럼 결측 데이터를 제거하고, 'city'와 'f2'을 기준으로 묶어 합계를 구하고, 'city가 경기이면서 f2가 0'인 조건에 만족하는 f1 값을 구하시오
- 데이터셋 : basic1.csv
- 오른쪽 상단 copy&edit 클릭 -> 예상문제 풀이 시작",79340a85,0.0
704,4bbe953f82d29b,fae6df2c,"## Добрый день, дорогие друзья.

Этот ноутбук содержит код для статьи ""Извлекаем признаки из временных рядов"" на портале NewTechAudit.  
Разделы тетрадки повторяют содержание текста.  ",772301f2,0.0
705,4c47839b067546,d669d996,"

# Проект: Предсказание цены автомобиля на основе его характеристик



Предположим, что в Москве есть компания, которая продает автомобили с пробегом. Основная задача компании и её менеджеров — максимально быстро находить выгодные предложения (проще говоря, купить ниже рынка, а продать дороже рынка). 

Руководство компании просит вашу команду создать модель, которая будет предсказывать стоимость автомобиля по его характеристикам.

Если такая модель будет работать хорошо, то вы сможете быстро выявлять выгодные предложения (когда желаемая цена продавца ниже предсказанной рыночной цены). Это значительно ускорит работу менеджеров и повысит прибыль компании.

Исторически сложилось, что компания изначально не собирала данные. Есть только небольшой датасет с историей продаж за короткий период, которого для обучения модели будет явно мало. Его мы будем использовать для теста, остальное придется собрать самим.

### Условия и инструменты:

- разрешено применение каких-либо методов или библиотек ML,кроме Deep Learning;
- решение должно быть воспроизводимо;
- метрика качества модели: MAPE - Mean Absolute Percentage Error;
- нужно добиться минимального процента ошибки.",1f517b02,0.0
706,1011899b959f44,215fb2cd,# Game of Thrones: Pandas Tutorial,0b112382,0.0
707,4c55891bcb068d,aff742b1,# Load files,01b9cd67,0.0
708,4cd25e50c7e007,197e829b,"# Problem Statement

A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a ""dock"" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.


A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. 


In such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.


They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:

Which variables are significant in predicting the demand for shared bikes.
How well those variables describe the bike demands
Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ",ceb0c525,0.0
710,4d91e84c564cbe,f2dc2c0d,"# Lists

Lists in Python represent ordered sequences of values. Here is an example of how to create them:",355a43e3,0.0
712,4dd47072617594,083517dd,# A short introduction to NLP,44ff1d11,0.0
713,4f69b7bb1ca287,0a4027f1,**Credits:** Help taken from [Abhishek Thakur's notebook](https://www.kaggle.com/abhishek/competition-part-1-baseline/notebook?select=train_folds.csv): ,34abc6a1,0.0
714,4fa553c2b837d4,040ab2ed,"# Learn Machine Learning
This notebook will guide you to kick start with you first Machine Learning Model.
You can apply these techniques in other datasets for your practice. (Ex: Titanic Dataset)

> **If this helps you in anyways, then Please Upvote.**",c65a23e9,0.0
716,5083d7a61f2426,4f98bb72,# Time Series LSTM - FORECAST NEW DATA,541a0fec,0.0
721,0fc0cbf884acd6,63449127,"# XG boost modelling by month for Time Series Store forecast 
* By Alex Dance https://www.linkedin.com/in/alex-dance/
* This notebook is one of several notebooks for a project to improve store and product forecasts
1.	EDA – Exploratory Data Analysis – includes working with annual forecasts
2.	Main Modelling
3.	XG Boost modelling by Month
4.	Weighted average
5.	ARIMA – Month and Other Modelling
6.	Deep Learning

* This workbook is XG Boost for further forecasts ",064949f1,0.0
722,510b8303776bb6,47c08641,"**If you like my notebook, please upvote my work!**

**If you use parts of this notebook in your scripts/notebooks, giving some kind of credit for instance link back to this notebook would be very much appreciated. Thanks in advance!** :) 

**P.S:**
1. The scripts in lines 22 and 25 show errors when run on kaggle(And have therefore been commented) but will run perfectly fine after downloading and running the script on local machine.

2. Please make sure that you have plotly installed on your local machine.

Lastly if anyone knows how to fix the above problem please let me know. Thankyou! :)
Hope you like my work!
",18080db8,0.0
723,513ce405d7f6a3,5951e51d,# This notebook shows how to implement different Machine learning algorithms to do sentiment analyses.,8461e086,0.0
724,514d8de15cb7ef,be781ba0,## Text Classification using NLP for Various types of Wines -- Part 3,cfe111b2,0.0
726,5169abdc647412,33d7ad2e,# Titanic Prediction using Desicion Tree classifier,28efc68d,0.0
727,4bb488ee3d75c5,768a5af9,"### Winning solutions of kaggle competitions: ###

Awesome work from [SRK](https://www.kaggle.com/sudalairajkumar).

Generally at the end of every Kaggle competition, winners and other toppers share their solutions in the discussion forum. But it is a tedious task to search for the competitions and then for the solutions of these competitions when we need them. I always wanted to get the links to the solutions of all past Kaggle competitions at one place. I thought it would be a very good reference point many a times. Thanks to Kaggle team for the Meta Kaggle dataset, now I am able to get them in one single place through this notebook .

Thanks a lot to [@sban](https://www.kaggle.com/shivamb) for this [wonderful kernel](https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle/notebook) without which I wouldn't have got this notebook.

I have inlcuded only ""Featured"" & ""Research"" competitions and the competitions are ordered based on recency.

Have fun and Happy Kaggling.! 

P.S : There are few outlier discussions (which are not actually solutions) too ;)
",9a261f31,0.0
729,45cf7099ebd023,bde0b0a6,"# Total Ranking of all participants of COVID19 Global Forecasting Challenges
* [COVID19 Global Forecasting (Week 5)](https://www.kaggle.com/c/covid19-global-forecasting-week-5)
* [COVID19 Global Forecasting (Week 4)](https://www.kaggle.com/c/covid19-global-forecasting-week-4)
* [COVID19 Global Forecasting (Week 3)](https://www.kaggle.com/c/covid19-global-forecasting-week-3)
* [COVID19 Global Forecasting (Week 2)](https://www.kaggle.com/c/covid19-global-forecasting-week-2)
* [COVID19 Global Forecasting (Week 1)](https://www.kaggle.com/c/covid19-global-forecasting-week-1)",0d292462,0.0
732,47012add33109f,27a855c0,"<div align=""center"">
<img src=""https://user-images.githubusercontent.com/48846576/121096632-33afc800-c7b8-11eb-970a-0af7433de15a.png"" alt=""fastai"" width=""300"" height=""200""/> 
<p>In this notebook we will use fastai library to classify the <a href=""http://https://www.kaggle.com/c/siim-covid19-detection"">SIIM-FISABIO-RSNA COVID-19 Detection</a> images into the following categories</p>
<div align=""left"">    
<ul>
    <li>Negative </li>
    <li>Typical </li>
    <li>Atypical </li>
    <li>Indeterminate</li>
    </ul>
    </div>    
</div>

I'm using resized images from my other notebook https://www.kaggle.com/rajsengo/image-resize-siim-covid-19-detection using which I learnt the image resize techniques.
",b4c2e4e2,0.0
733,47a1b1fe51b4ad,991aa4bd,"## Titanic: Machine Learning from Disaster

**Objective**: Predict if a passenger survived the sinking of the Titanic or not. Predict a 0 or 1 value for the variable.

**Metric**: Percentage of passengers correctly predicted (Accuracy).",331ded2f,0.0
734,47b2c9be5e31cb,060b70de,"## Introduction
Greetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue ""Edit Notebook"" or ""Fork Notebook"" button at the top of this kernel to begin editing.",7d4afe56,0.0
735,47c519cb88e1c2,24f47656,"# Acknowledgements

1. Inspired by article [Applying Machine Learning to Connect Four](http://cs229.stanford.edu/proj2019aut/data/assignment_308832_raw/26646701.pdf)
2. Inspired by kernel [Scoring connect-x agents](https://www.kaggle.com/petercnudde/scoring-connect-x-agents) by Peter Cnudde, Industry Fellow at UC Berkerly
3. We plan to obtain a set of training data by querying https://connect4.gamesolver.org
4. We obtain the training data for educational purpose only.",f2c992fa,0.0
736,47e3a1925754c2,b10b59b2,# <U>COVID-19 DATA ANALYSIS AND VISUALIZATIONS</U>,ecc615b7,0.0
737,485de87c50af82,4b39a6fd,### Loading Libraries,a5bd438e,0.0
739,4883314a96dc34,3b07c04e,The aim is to predict success rate (%) in reaching the Mount Rainier peak given (1) the route and (2) the weather condition.,50d36836,0.0
740,4913b61a68d355,6c894139,# Gravity Spy,6e269c6a,0.0
743,49ee86d074de69,b0b5ac02,"<font color = 'red'>
<h1>Analysis of Absenteeism<h1>
    
<hr>",71ccc6d3,0.0
744,49f2274c1dd516,2727ec04,"# Introduciton

This notebook seeks to be a landing page for people looking to start working with the information made available in the UNCOVER dataset. It begins with an analysis of the basic data located in each of the folders examining the columns that exist.

**Please up vote to let me know this is useful or comment on other ways to make this a valuable resource**",06b0ffee,0.0
745,4ae464582bac51,19bd4e02,# IMPORTS,ca6a52ce,0.0
746,4ae6a182abac64,64f3656d,"![ ](https://scontent-arn2-2.xx.fbcdn.net/v/t1.0-9/57429716_2030860760555937_2750062083545497600_n.jpg?_nc_cat=100&ccb=2&_nc_sid=8bfeb9&_nc_ohc=y5d_WudgJy0AX9PNi5o&_nc_ht=scontent-arn2-2.xx&oh=a630ca82a715594c37e77c981b2f0b12&oe=6004F26F)
                                   ",418676c5,0.0
747,4b4117cf42ef8d,10c81405,# Import Necessary Libraries,457cd6f4,0.0
748,4b64dc653fb7eb,432f23ab,"## [Workbook 1](https://www.kaggle.com/sabasiddiqi/workbook-1-text-pre-processing-for-beginners) - Text Preprocessing for Beginners - Data Cleaning
<br>
**Level** : Beginner

This notebook discusses **Text Data Preprocessing** for **NLP Problems** using Toxic Comment Classification Dataset. Data comprises of large number of Wikipedia comments which have been labeled by human raters for toxic behavior

Data is available via following link.
[Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)

Next Workbook : [Workbook 2 - Text Preprocessing for Beginners - Feature Extraction](https://www.kaggle.com/sabasiddiqi/workbook-2-text-preprocessing-feature-extraction) 

To skip the initial steps (reading data, text extraction from data), Jump to [Text Pre-Processing Steps](#jump).",57675cc2,0.0
749,4b7039cb44a54c,f096d832,"# About This Notebook

This implementation is based on a vanilla **swin_large_patch4_window12_384** in Pytorch for the Pawpularity Competition.  
This model uses **both images and dense features** for score prediction.  
This scores around 18.847 LB and 18.02 CV.

Training Params: -
1. **Dataset**: - 3-channel RGB Images (384x384) with separate dense features
2. **Augmentations**: - Resize, Normalize, HorizontalFlip, VerticalFlip, RandomBrightness, RandomResizedCrop, HueSaturationValue, RandomBrightnessContrast
3. **Optimizer**: - AdamW
4. **Scheduler**: - CosineAnnealingLR
5. **Model**: - swin_large_patch4_window12_384
6. **Initial Weights**: - Imagenet
5. **Max Epochs**: - 8 (~22 min per epoch on P100 PCIE GPU)
6. **Saved Weights**: - 10-fold ensemble. Weights having highest OOF score on RMSE metric were saved.

This notebook only contains the inference for the model as described above. If you are looking for the training notebook please follow the link below.

Baseline Model Notebook:- https://www.kaggle.com/manabendrarout/pawpularity-score-starter-image-dense-train

![SETI](https://www.petfinder.my/images/cuteness_meter.jpg)  

**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. 😊**",24e806af,0.0
751,519e936017c30a,83271c12,"Actualmente, la mayoría de las personas piensan que la industria de los videojuegos se ha incrementado con el paso del tiempo, debido a la influencia de la tecnología, ya que cuanto más avanza, mejor es el ocio para los individuos.

Por ende, en este proyecto vamos a estudiar como evolucionan las ventas de este campo de trabajo a nivel global y por zonas geográficas, las 20 mejores ventas de videojuegos, y un estudio de una plataforma al azar y su comparación con otra.

Comenzamos el análisis de nuestra base de datos, cargando los correspondietes comandos que vamos a utilizar y la base de datos ""videogamesales"" de la plataforma ""Kaggle"". Cabe resaltar, que el valor de las ventas está expresado en millones, de forma simplificada, y en dólares.",dc34915d,0.0
752,51a46d0a7597f5,ea1f902f,"**Overview and Motivation**

This Project is an exploration of Fifa19 Player Dataset as part of the Udacity's Data Science Nanodegree Introduction to DataScience Blog post. Corresponding BlogPost for project can be found here -> https://towardsdatascience.com/exploring-fifa-277f469c8cdc 

Exploration of FIFA player 19 dataset gave me an opportunity to merge my interest of DataScience and Soccer. When I saw the dataset, I wanted to ask myself several questions almost immediately and hence started the exploration to find the answer to such questions. This Project helped me satiate my Curiosity.



**<font size=5 color=""aa7700"">Welcome to FIFA EDA</font>**<br/><br/>
<span style=""color:#D4AF37"">
**<font size=3>Questions Asked</font>**
<br/>

<A href=""#section1""><font color='blue' size=4></font></A>
<A href=""#section1.1"">1. Is there a relationship between Market Value and Wages of Players?</A><br/>
<A href=""#section1.2"">2. What is the preferred Foot among the players and how does it affect their positioning? </A><br/>
<A href=""#section1.3"">3. Can we predict the Value of a player based on its attributes (like accuracy, shot power, reactions, dribbling etc)?</A><br/>
<A href=""#section1.4"">4. Clubs with the highest median wages (Top 11)?</A><br/>
<A href=""#section1.5"">5. Players with largest release clause (Top 11)?</A><br/>",e9e25b17,0.0
754,585c280865b46e,8e6989fd,"# What is about 

Here we analyse sciPlex2 subdataset looking at histone genes.
There are 76 histone genes (according to wikipedia).
Main part of histone genes produces protein from which nucleosomes are made (histone octomers).
They play important role in many processes in cell.
In particular the paper dicusses a lot drugs ""HDAC"" (Histone deacetylase)

See for futher info on histones:
https://www.kaggle.com/alexandervc/singlecell-rnaseq-exposed-to-multiple-compounds/discussion/228144

In particular histones play role at cell cycle and some histones should be expressed at S-phase.


------------------


#### Conclusions: 

At the moment, unfortunately, expected outcomes not yet obtained:


The hope was to see ""S""-phase of cell cycle (since replication-dependent histones are expressed at S-phase) , but seems it is not so easy.



Correlations of histone genes is not so big - max is 0.56 for H1FX H1F0 (both are linker histones - not those which construct nucleosome and not S-phase expressed). 

We see some higher levels of expressions some histone genes for cell affected by SAHA (HDAC) drug, does it correspond to bio sense ? 

--------------------

With mygene package we get the following: 

**l_histone_replication_dependent** 58 ['HIST1H1A', 'HIST1H1B', 'HIST1H1C', 'HIST1H1D', 'HIST1H1E', 'HIST1H1T', 'HIST1H2AA', 'HIST1H2AB', 'HIST1H2AC', 'HIST1H2AD', 'HIST1H2AE', 'HIST1H2AG', 'HIST1H2AI', 'HIST1H2AJ', 'HIST1H2AK', 'HIST1H2AL', 'HIST1H2AM', 'HIST2H2AA3', 'HIST2H2AC', 'HIST1H2BA', 'HIST1H2BB', 'HIST1H2BC', 'HIST1H2BD', 'HIST1H2BE', 'HIST1H2BF', 'HIST1H2BG', 'HIST1H2BH', 'HIST1H2BI', 'HIST1H2BJ', 'HIST1H2BK', 'HIST1H2BL', 'HIST1H2BM', 'HIST1H2BN', 'HIST1H2BO', 'HIST2H2BE', 'HIST1H3A', 'HIST1H3D', 'HIST1H3E', 'HIST1H3F', 'HIST1H3G', 'HIST1H3H', 'HIST1H3I', 'HIST1H3J', 'HIST2H3C', 'HIST3H3', 'HIST1H4A', 'HIST1H4B', 'HIST1H4C', 'HIST1H4D', 'HIST1H4E', 'HIST1H4F', 'HIST1H4G', 'HIST1H4H', 'HIST1H4I', 'HIST1H4J', 'HIST1H4K', 'HIST1H4L', 'HIST4H4']

**l_histone_replication_independent** 15 ['H1F0', 'H1FNT', 'H1FOO', 'H1FX', 'H2AFB1', 'H2AFB2', 'H2AFB3', 'H2AFJ', 'H2AFV', 'H2AFX', 'H2AFY', 'H2AFY2', 'H2AFZ', 'H2BFM', 'H2BFWT']


------------------

#### Dataset reminder:

It is the second ""toy"" subdataset, while sciPlex3 is the main very huge subdataset.

Here - only one cell line - A549 - human lung adenocarcinoma.

Exposed to 4 different compounds DEXA, nutlin, BMS, SAHA (HDAC).

24 hours across SEVEN doses. 

24262 cells x 58347 genes


'GSM4150377_sciPlex2_pData.txt' - contains information on: cell+drugs+doses+etc... There are 24262 rows - one row - one cell.
The index (=row number) of cell in that file corresponds to its index in the countmatrix (after transforming csv file with count matrix to sparse countmatrix)

Count matrix (in list of edges format), cell ids, genes ids are in the files (respectively):
'GSM4150377_sciPlex2_A549_Transcription_Modulators_UMI.count.matrix',
 'GSM4150377_sciPlex2_A549_Transcription_Modulators_cell.annotations.txt',
 'GSM4150377_sciPlex2_A549_Transcription_Modulators_gene.annotations.txt',
 
 ----------
 
 Version 6:  change normalization / log order
 
 ",4d6056f1,0.0
755,58c6c7dba7b83f,16bbd1b7,"# PneuNetV2
Algoritmo de Aprendizaje Profundo para clasificación de imágenes por radiografía hacia un diagnóstico preciso de enfermedades comúnes de pulmón, ya sean virales o bacterianas.

Este modelo usa aprendizaje por transferencia en el modelo EfficientNetB0 de clasificación mediante redes neuronales convolucionales (CNNs), entrenado previamente en el set de datos ImageNet. Esto hace que su error sea aún menor que entrenar un modelo desde el principio. Además, el haber sido entrenado en un set de datos más extenso previamente, hace que se puedan utilizar menos imágenes y aún así tener una precisión mayor. Incluso, al ser un modelo desarrollado con la librería de aprendizaje profundo TensorFlow y TensorFlow Lite, permite una compresión de 25-50% de su tamaño original para uso en sistemas embedidos, microcontroladores, celulares, o con poca capacidad de procesamiento, manteniendo la misma precisión.

## Set de datos
Las radiografías utilizadas para este modelo usan el set de datos de radiografías de pacientes del ***Guangzhou Women and Children’s Medical Center***, en China. Este set de datos es público y está alojado en la plataforma de ciencia de datos [Kaggle](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia). Cuenta con +5000 radiografías, divididas en sets de entrenamiento y de prueba.",d73c318c,0.0
757,593d1d3d1df05a,74f59661,"# License Plate Detection
",bc682ffe,0.0
760,598b6228760590,c208798c,"<div style=""color: #fff7f7;
           display:fill;
           border-radius:10px;
           border-style: solid;
           border-color:#424949;
           text-align:center;
           background-color:#8ac6d1 ;
           font-size:20px;
           letter-spacing:0.5px;
           padding: 0.7em;
           text-align:left"">  
<center> <h1>Titanic machine learning 🚩</h1> </center> 
</div>",be30ab66,0.0
762,5a8c553e21c70f,88aa0df7,"In this notebook, we experiment with **neural networks** and observe how the predictions change as we apply modifications to training set. Why are **neural networks** defined as **high variance** machine learning methods? We will answer this question.

Later, we look for a remedy to high variance problem and use an ensemble of neural networks. Each member of the ensemble is trained on a slightly different subset of training data. Each subset is obtained by resampling the original training set with replacement, a method known as **bootstrapping**. For inference, the decisions coming from each ensemble member are aggregated. Whole method is called **bootstrap aggregating** or **bagging**.

* Load Data
* Split Data
* Visualization
* Outlier Check with Isolation Forest
* Outlier Check with IQR
* Minority Class Upsample with SMOTE
* Majority Class Downsample with RandomUnderSampler
* Standardization
* Correlation Analysis
* Permutation Feature Importance
* Bootstrap Samples
* Training and Validation
* Testing
",9ebd9d8f,0.0
763,5af9bf52e5f17c,081b58b6,"This notebook is a quick data visualization notebook, still very much a work in progress.

Currently it shows confirmed cases and fatalities by country/region and province/state.",f98ab90d,0.0
765,5ba4207c371899,24a25305,"The purpose of this notebook is a basic exploration of the NSL-KDD dataset. Here are the goals of this exploration:

* Gain a basic understanding of the data set
* Look at how the data set might be used to predict network anomalies or attacks
* Walk through some fundemental concepts of building machine learning models like Logistic Regression and Decision Tree classifier
* Compare the performance of selected feature engineering techniques.",187b1451,0.0
766,5be39e4e35cec7,f24a0210,"# Introduction
The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

<font color = ""blue"">
Content:
    
1. [Load And Check Data](#1)
1. [Variable Description](#2)
    * [Univariate Variable Analysis](#3)
        * [Categorical Variable](#4)
        * [Numerical Variable](#5)
1. [Basic Data Analysis](#6)        
1. [Outlier Detection](#7)
1. [Missing Value](#8)
    * [Find Missing Value](#9)
    * [Fill Missing Value](#10)",14d617c9,0.0
771,5cb7f999fd1ecb,10695b9e,# Used Cars Data Analysis and Visualization (EDA),88b54f70,0.0
772,5ce12be6e7b90e,f61d1184,"# Python tutorial

## Credit for this notebook goes to : [Yoav Ram](http://yoavram.com)",c0ab62dd,0.0
774,5d2a3e82679cf3,fa78e1a7,# 1) IMPORT THE DATA,9e60b1e3,0.0
775,582cb872d19026,93046e13,# Explotary Data Analysis (EDA) for Top Games on Google Play Store,8d966d69,0.0
777,57740be713cf12,066c05ef,"# **Pizza Price Prediction**
## *Model Development*

------
**Arinal Haq**",ac122df5,0.0
778,0f5085b162bd9f,c174577b,"# Compare various clustering algorithms on the iris dataset

* **k-Means and Mini Batch k-Means**
accuracy: 0.89 ... 
silhouette:  0.696

* **DBSCAN and Optics**
accuracy: with 3 clusters there are 114 outliers with DBSCAN and similar w OPTICS (?!)

* **Affinity Propagation**
accuracy: 0.90 ... 
silhouette:  0.696

* **Mean Shift**
accuracy: 0.79 ... 
silhouette:  0.635

* **Spectral Clustering**
accuracy: 0.84 ... 
silhouette:  0.661

* **Agglomerative Clustering**
accuracy: 0.89 ... 
silhouette:  0.688

* **Gaussian Mixture Clustering**
accuracy: 0.97 ... 
silhouette:  0.606

* **Birch**
finds only 2 clusters


* Based on https://scikit-learn.org/stable/modules/clustering.html
",a3d989ee,0.0
780,52cfd66e9ec908,332d3d88,# Lyft: Understanding the data  and EDA,c74adcdf,0.0
781,52ee792e228d54,c3a3fce5,"### Context
This case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.<br>
### Attribute Information:
 ID  : Customer ID<br>
 Age : Customer's age in completed years<br>
 Experience : #years of professional experience<br>
 Income : Annual income of the customer (\\$000)<br>
 ZIP Code : Home Address ZIP code.<br>
 Family : Family size of the customer<br>
 CCAvg : Avg. spending on credit cards per month (\\$000)<br>
 Education : Education Level.<br>
    &emsp;1. Undergrad<br>
    &emsp;2. Graduate<br>
    &emsp;3. Advanced/Professional<br>
 Mortgage : Value of house mortgage if any. (\\$000)<br>
 Personal Loan : Did this customer accept the personal loan offered in
the last campaign?<br>
 Securities Account : Does the customer have a securities account with
the bank?<br>
 CD Account : Does the customer have a certificate of deposit (CD)
account with the bank?<br>
 Online : Does the customer use internet banking facilities?<br>
 Credit card : Does the customer use a credit card issued by
UniversalBank?<br>",5096094e,0.0
782,535da6591a3246,958ec6e9,# Beginner approach to campus palcement prediction,9ef8d0c7,0.0
783,53f302571cd4ac,adbd7fbc,"## Object-Oriented Programming:
Object-oriented programming (OOP) is a programming paradigm based on the concept of ""objects"", which can contain data and code:<br>
data in the form of fields (often known as attributes or properties) and, <br>
code in the form of procedures (often known as methods or behaviors ).",62c28443,0.0
785,541d0fa0e26b80,3dc37466,# Students Exam Performance - EDA,a29e0f29,0.0
786,5472f083b56184,d70a96ef,"

**Hi ! Welcome to my Kernel**

> **** This the first part of a series of kernels that wil familiarize you with the naunces of Machine Learning using Python

* I created this kernel so that beginners can use this to learn the syntax and data types in Python.

* Others may just use this code as a refresher !!

**We will look at the following topics in this kernel**

1. Basic Operations in Python
2. Creating variables in Python and working with them
3. Python Lists, Functions and Packages
4. Introduction to Numpy
",c313f45f,0.0
787,548f961125248d,b46fa56e,"# Catboost & Hyperopt : Amazon employees dataset

* **Information from [Kaggle](https://www.kaggle.com/c/amazon-employee-access-challenge/overview)**
    - When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role.
    - **Given data about current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company.** These auto-access models seek to minimize the human involvement required to grant or revoke employee access.
    
    
* **Catboost**: 
    - **Yandex, the developers of Catboost, claim that default Catboost provides ~20% logloss improvement over LightGMB & XGBoost. Tuning further improves performance of the model.**
    - **I will be testing these claims.**
    - Catboost uses gradient boosted trees. Great for working on catgorical data and mixed data (with both categorical and numerical features)
    - Data is quantized into bins. The algorithm decides bin 'borders'(We can set our own values too). This quantization supports faster integration into parallel processing workflows. 
    - Symmetric gradient boosted trees are built, each subsequent tree improves the performance of the previous set of trees. 
    - Categorical preprocessing steps like One-Hot-Encoding, text preprocessing steps like tokenization, Bag of Words models can be performed within the Catboost algorithm (No need for additional preprocessing.)  
    
    
* **RESULT**:
    - **One of the columns had duplicated information. After removing this column - the default algorithm gave the best loss publicised by Yandex (~0.137). A kaggle submission showed 90% AUC score.**
    - Hyperopt tuning did not improve scores. 
    - Yandex's claims were proven. It had the best loss among the boosting models as shown in table below. 
    
    
<table>
  <tr>
    <th>Model</th>
    <th>Logloss from default</th>
  </tr>
  <tr>
    <td>Catboost</td>
    <td>0.13516505504697254</td>
  </tr>
  <tr>
    <td>Xgboost</td>
    <td>0.1554555542790197</td>
  </tr>
  <tr>
    <td>LightGBM</td>
    <td>0.16383632381872779</td>
  </tr>
</table>
    
    
    
## Table of Contents
* [Imports & Read in file](#first)
* [Explore data](#second)
* [Preprocessing](#third)
* [Baseline Model](#fifth)
* [Test set performance](#eighth)
* [Hyperparameter tuning](#sixth)
* [Model validation](#seventh)
* [Other Boosting Algorithms](#ninth)

<img src=""https://images.freeimages.com/images/large-previews/753/go-to-work-1189863.jpg"" alt=""Drawing"" style=""width: 300px;""/>


* **Description of Features:**
    
<table>
  <tr>
    <th>Label</th>
    <th>Description</th>
  </tr>
  <tr>
    <td>ACTION</td>
    <td>ACTION is 1 if the resource was approved, 0 if the resource was not</td>
  </tr>
  <tr>
    <td>RESOURCE</td>
    <td>An ID for each resource</td>
  </tr>
  <tr>
    <td>EMPLOYEE ID</td>
    <td>The EMPLOYEE ID of the manager of the current EMPLOYEE ID record; an employee may have only one manager at a time</td>
  </tr>
  <tr>
    <td>ROLE_ROLLUP_1</td>
    <td>Company role grouping category id 1 (e.g. US Engineering)<td>
  </tr>
  <tr>
    <td>ROLE_ROLLUP_2</td>
    <td>Company role grouping category id 2 (e.g. US Retail)</td>
  </tr>
  <tr>
    <td>ROLE_DEPTNAME</td>
    <td>Company role department description (e.g. Retail)</td>
  </tr>
  <tr>
    <td>ROLE_TITLE</td>
    <td>Company role business title description (e.g. Senior Engineering Retail Manager)</td>
  </tr>
  <tr>
    <td>ROLE_FAMILY_DESC</td>
    <td>Company role family extended description (e.g. Retail Manager, Software Engineering)</td>
  </tr>
  <tr>
    <td>ROLE_FAMILY</td>
    <td>Company role family description (e.g. Retail Manager)</td>
  </tr>
  <tr>
    <td>ROLE_CODE</td>
    <td>Company role code; this code is unique to each role (e.g. Manager)</td>
</table>",d8c5e8b8,0.0
790,55a5e31d03df9f,0fd21dc8,"# 📚 Learning Deep Learning & Tensorflow with LEGO - Image Classification 🖥👁👁

[Daniel Beltrán](https://www.linkedin.com/in/danielbeltranpardo/) - Oct 2021

----------
Motivation: This notebook is motivated by the [TensorFlow Developer Certificate in 2021: Zero to Mastery](https://www.udemy.com/course/tensorflow-developer-certificate-machine-learning-zero-to-mastery/) which covers almost all these topics, a fantastic way to understand a topic is to teach. This notebook then pretends to capture all the knowledge and teach in the kaggle environment about Tensorflow, Deep Learning, and Image classification.

---------
<center><img src=""https://3.bp.blogspot.com/-QZVBl08fmPk/XhO909Ha1dI/AAAAAAAACZI/q1a1UykGKe0KDUZ_ZITtWmM7bBJFRrvPQCLcBGAsYHQ/s1600/tensorflowkeras.jpg"", width = 400px></center>

# Introduction 

Computer vision has advanced in terms of recognition and tracking thanks to machine learning. There are some sets of problems that the most common like:
* Image Classification: The use of computer vision allows machines to analyze and categorize what they ""see"" in photos or videos. A classification problem generally involves classifying images into 2 or more classes (single-label problem) or if there is no limit to how many classes an instance can be allocated to it's the multi-label problem.
* Object Localization and Detection: Object localization and detection is a computer vision issue in which an algorithm is given an image and is asked to determine the positions of one or more target items in the image or video frame, producing bounding boxes for each.
* Image segmentation: Image segmentation is used in a variety of applications (robotics, satellite imagery, medical imagery, and so on) to not only comprehend the positions of items in pictures and video frames but also to map the borders between distinct objects in the same image more precisely.

In this Notebook we will focus in the **image classification task** on single label problem.

**<span style=""color:Green"">📝 Note: This kernel has been a work of more than 10 days If you find my kernel useful and my efforts appreciable, Please Upvote it, it motivates me to write more Quality content</span>**

<a id=""top""></a>

<div class=""list-group"" id=""list-tab"" role=""tablist"">
<h3 class=""list-group-item list-group-item-action active"" data-toggle=""list"" style='background:black; border:0; color:white' role=""tab"" aria-controls=""home""><center>Contents</center></h3>

* [1. Problem statement](#problem-statement)
* [2. Data Exploration](#eda)
    * [2.1 Visualizing random images](#randomviz)
* [3. Data preparation](#dataprep)
    * [3.1 Loading data](#dataload)
* [4. Multi Layer Perceptron](#mlp)
    * [4.1 Create and compile a MLP](#mlpcompile)
    * [4.2 Callbacks and fitting an MLP](#mlpcallbfit)
    * [4.3 Evaluating the MLP model](#mlpeval)
* [5. Convolutional Neural Network](#cnn)
    * [5.1 Create and compile a CNN](#cnncreate)
    * [5.2 Evaluate a CNN](#cnneval)
* [6. Transfer learning](#transferlearn)
    * [6.1 Creating a model with TensorflowHub](#tfhubcreate)
    * [6.2 Visualizing results from TFHub model](#tfhubresults)
    * [6.3 Creating a model with Keras Application](#kerasmodel)
    * [6.4 Visualizing results from KerasApplication model](#kerasresult)

In each of these contents, I will try to make a brief explanation of the topics and concepts that get presented and immediately do a code implementation.  

    
## <a name=""problem-statement"">Problem statement 🎯</a>
Before we start going we are going to write what we want to achieve: **We want to develop a Machine Learning algorithm that is capable to identify LEGO® Minifigures images and classify them correctly**, to accomplish this we will use the imagery data available from the LEGO Minifigures dataset.

## <a name=""eda"">Data exploration 🕵🏻‍♂️</a>

For this first part, we will use the motto of **Visualize, Visualize and Visualize** this will help us to understand what data we have available, what preprocessing steps we need to do, what lego Minifigures we have available, what are the classification can be done with our current data.

Let's answering the following questions:
* How many images we have available?
* What information does the CSVs contain?
* How are the folders organized? (This is extremely useful we will cover it later on)
    

",06dce00f,0.0
792,5626e84c4e6bf8,fae52708,"<img src=""https://i.imgflip.com/2jxxia.jpg"" title=""made at imgflip.com"">
# Unsupervised Deep Learning
We, data scientists regularly use **DNNs, CNNs and RNNs** for most applications of deep learning but that's only the **supervised side** of the neural networks family but there is also the more sophisticated and less talked about **unsupervised side** which is just as or even more intriguing than the conventional supervised architectures. These unsupervised models enable the neural networks to perform tasks like **clustering, anomaly detection, feature selection, feature extraction, dimensionality reduction and recommender systems**. Some of these neural networks are: **Self organizing maps, Boltzmann machines, Autoencoders**

### Self Organizing Maps
<img src=""https://www.researchgate.net/profile/Damian_Jankowski3/publication/291834232/figure/fig3/AS:553741877481472@1509033759154/Self-organizing-map-structure.png"">

### Boltzmann Machines
<img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Boltzmannexamplev1.png/330px-Boltzmannexamplev1.png"">

### Autoencoders
<img src=""https://cdn-images-1.medium.com/max/1000/1*ZEvDcg1LP7xvrTSHt0B5-Q@2x.png"" height=""400"" width=""500"">",e2ecb669,0.0
793,565ad413cd802f,7e2053ed,"## Human Protein Multi Label Image Classification 

This is a starter notebook for the competition [Zero to GANs - Human Protein Classification](https://www.kaggle.com/c/jovian-pytorch-z2g)",397b074e,0.0
794,0fa9979b5690e9,21f6352d,"## Scikit-Learn é uma biblioteca que auxilia na validação do modelo.

A separação de dados em treino e teste é muito importante para ter-se condições de avaliar o desempenho de um modelo em dados que ainda não foram vistos. Na verdade, dados ainda não vistos são o foco de aprendizado de máquina, visto que ao desenvolver-se sistemas, o principal objetivo é colocá-los em produção e ter performance aceitável (confiável).

Existe uma subdivisão dos dados de treino que pode ser o conjunto de validação. O conjunto de validação pode, por exemplo, participar de um processo de melhoria do modelo. Uma vez que o modelo está aperfeiçoado suficiente, então ele é avaliado no conjunto de teste.",c26eea94,0.0
795,567022e9487930,ae2b0dbe,# Kaggle House Prices Prediction,ffd36c75,0.0
796,56785caebaa256,3fec6f3b,# Dataset [COVID-19 in Ukraine: daily data](https://www.kaggle.com/vbmokin/covid19-in-ukraine-daily-data),a792961a,0.0
798,56cc8fb47bef6a,3f43e8b3,"<p style=""text-align: center;""><span style=""font-size:18px""><span style=""color:#2f4f4f""><strong><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Spam detection - Multinomial Naive Bayes</span></strong></span></span></p>",652d6670,0.0
799,56e58d53ac9c57,909af1d5,# By Elnur Shahbalayev,90e2ab8e,0.0
801,55c34673c1f760,9de29945,# Tarea Semana 11,2663c47f,0.0
802,cf46cd6f7c55c0,4d66db2d,"# Pseudo Labeling with QDA scores LB 0.969
Roman posted a kernel introducing pseudo labeling [here][1]. I would like to explain why it works, make a few improvements, and demonstrate the power of this technique! This kernel is the same as my support vector machine kernel [here][2] with QDA and pseudo labeling added. It achieves an impresive CV 0.970 and LB 0.969

In Santander's Customer Transaction competition, team Wizardry used pseudo labeling to increase their private LB by 0.0005 allowing them to beat second place by 0.0002 for the $25,000 prize! [here][3]

In ""Instant Gratification"" competition, we are building 512 models where each model's train data has approximately 512 rows and 40 columns. With 40 variables, a quadratic model would like more rows. With pseudo labeling, we can get an additional 400 rows of training data and increase CV and LB by 0.005!

[1]: https://www.kaggle.com/nroman/i-m-overfitting-and-i-know-it/notebook
[2]: https://www.kaggle.com/cdeotte/support-vector-machine-0-925
[3]: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003",191b86b8,0.0
804,e323e594ef918f,7fc426bf,"# CAM - Class Activation Map Explained

I'm using this notebook to learn more about [CAM](https://arxiv.org/pdf/1512.04150.pdf). I will use a dataset of 512x512 trainset images converted into jpg format to quickly train a simple classifier (resnet18), and then use it to analyse class activation maps.

![CAM.png](attachment:CAM.png)

The basic idea is as follows:
- After a series of convolutions, we end up with a small grid and a large number of channels. In our case, starting with 448 image and resnet18, after all the convolutions we end up with a 14x14 grid and 512 channels.
- We can think of the grid as downsampled image and the channels as different features discovered in that image.
- We then average the channels across that grid and multiply them with a set of weights to come up with the final prediction for each class (global average pooling followed by linear layer).
- To understand which region of the image contributed to the final prediction the most, we can apply the weights to each element of the 14x14 grid (before averaging).
- The regions with highest value contributed the most. 
- We can then overlay the activation map with the original image to visualize this. 

This notebook is largely based on chapter 18 in [Deep Learning for Coders with Fastai and PyTorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).",6e829ab6,0.0
805,d58491f2896fc1,fdef16ee,# Genetik Algoritma Nedir?,514bfdff,0.0
807,ee23a565163388,16f3022c,# **Introduction**,88aacbc4,0.0
811,00d295edcd117e,0a399841,# 训练一个分类器,f5810f4b,0.0
812,e25c0f830df3f4,8cc0ae93,# Importing python packages,fdcf7189,0.0
813,eb33e05704d647,c834be74,"Code Credit: https://github.com/kentaroy47/frcnn-from-scratch-with-keras Thank you kentaroy for you nicely documented github repo.

Here I am going to train Faster RCNN with 90% of images datasets.Where all the required data preprocessing I have done in Part 1: EDA and Data Processing Kernal(https://www.kaggle.com/kishor1210/eda-and-data-processing)

prerequisite: We need to create annotation.txt which you can find in part 1 kernal.
              We need to download pretrained weight 
 
Pretrained Weight : https://github.com/fchollet/deep-learning-models/releases/tag/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5

Part 1: https://www.kaggle.com/kishor1210/eda-and-data-processing",cd80436d,0.0
814,d5dcd3f3253568,fe9e5140,# Searching for an intuitive way to set the parameters,7c1555a3,0.0
819,eda49464dd6d1b,90f6d2d0,![image.png](attachment:image.png),8421f81f,0.0
820,ff3a8ce61fab6a,d125db95,# Hello everyone in this kenel we will learn together Tensorflow and Tensorflow tools,9afe1654,0.0
824,eb800c50fcfbb2,968e522b,"# Introduction

The idea presented in this notebook is to find the dissimilarity between a pair of satellite images of the same geograhical region. A pair of satellite image consist of an image taken during flood, and an image taken few days before the flood.

An example image pair is given below,
",e7173f4d,0.0
825,dbe40fdf51456d,6e944292,"# Causality: Double ML examples using EconML and Causal Forests

Much of what we do on kaggle falls under the category of [predictive modelling](https://en.wikipedia.org/wiki/Predictive_modelling); we are given a 
sample of data with which to build a model, and assuming that the data is [stationary](https://en.wikipedia.org/wiki/Stationary_process) and that the data that the model was trained on, and the data that the model will be used on, have the same underlying distributions, we can use that model to then predict the outcome (*i.e.* the target) of new, previously unseen out-of-sample data. 

It is very easy to obtain the correlations between features in ML, but [**correlation does not imply causation**](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation). (For some wonderful and amusing examples of where where things can go very wrong see the website [Spurious Correlations](https://tylervigen.com/spurious-correlations)). Put simply, it is entirely possible that a correlation is the result of coincidence. The recent blog post [""*Be careful when interpreting predictive models in search of causal insights*""](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html) is well worth reading.
 
There are situations where we wish to study [causality](https://en.wikipedia.org/wiki/Causality), or cause and effect.  The gold standard when it comes to studying cause and effect is the experiment. In the field of machine learning such experiments often take the form of [A/B tests](https://en.wikipedia.org/wiki/A/B_testing) where changes are compared and contrasted with the results of randomized control groups. However, there are numerous circumstances where this is impractical:

* Historical non-experimental observational data: No more data can ever be obtained (an example is the Titanic)
* Impossibility of performing experiments, for example in the economic or social sciences
* We may wish to select a few potentially interesting treatments before actually performing possibly expensive and/or time consuming A/B tests

For the purposes of this notebook we shall define causality as the influence a single continuous treatment (i.e. feature) has on the outcome (i.e. the predictions) whilst keeping all of the other features, or ""confounders"" ($W$) constant. We shall adopt the nomenclature of the feature under investigation as the **treatment $T$**, and the effect is the **outcome** $Y$.

We shall assume that there is no **unobserved confounding** going on. There may well be a feature(s) that has a significant contribution to the target that was not included in the original dataset. Here we assume that all of the relevant features have been measured. Unobserved confounding is the bane of causality; if we do have unobserved confounders then there is little else to do other than go out and obtain data regarding these missing features for our dataset. 

## Double ML
Double ML is an ingenious development by [Victor Chernozhukov](https://www.mit.edu/~vchern/) and co-workers, of particular note is the publication [""*Double/debiased machine learning for treatment and structural parameters*""](https://doi.org/10.1111/ectj.12097). The 'double' comes from the use of primary and auxiliary predictive models.

The overall process can be summarised as:

*     Sample splitting: the data is divided into two equal sets
*    Model the response based on the covariates for both splits
*    Model the treatment based on the covariates for both splits
* For both of the above models we shall be using the scikit-learn [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
*    Calculate the response and treatment residuals $ \tilde{Y}, \tilde{T}$
* Finally fit the response residuals w.r.t. the treatment residuals using ML to obtain the overall treatment effect from

$$ \tilde{Y} = \theta(X) \cdot \tilde{T} + \epsilon $$

## EconML
[EconML](https://www.microsoft.com/en-us/research/project/econml/) is a python package  developed by the ALICE team at Microsoft Research. In the following examples we shall be using the Causal Forests ([`CausalForestDML`](https://econml.azurewebsites.net/_autosummary/econml.dml.CausalForestDML.html)) routine, where the residuals are fitted using a non-parametric estimator.
",f3e8a1e4,0.0
828,d52eea97c2aee5,7816c11c,# Analyze your model performance by confusion matrix,c9fa2014,0.0
829,dbd96dd275dc60,53b1c58a,"# 🚜 Predicting the Sale Price of Bulldozers using Machine Learning
In this notebook, we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.

### 1. Problem defition
How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?

### 2. Data
The data is downloaded from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data

There are 3 main datasets:

* Train.csv is the training set, which contains data through the end of 2011.
* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Our score on this set is used to create the public leaderboard.
* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.


### 3. Evaluation
The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.

For more on the evaluation of this project check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation

Note: The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises RMSLE.

### 4. Features
Kaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https://docs.google.com/spreadsheets/d/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI/edit?usp=sharing",1ed493a8,0.0
830,f3d5d8917ce5df,f9b8b508,"# Leaderboard update - TOP 50% !!!
Funny thing... this notebook that does a very simple random forest with no feature engineering, or hyperparamater tuning at all, shot way up the leaderboard from bottom 75% in the public version, to top 50% in the private version!!

Just a pretty good lesson that doing a simple random forest should probably be the first thing that you do, and makes for a pretty good baseline for most structured data tasks.",e45112f8,0.0
832,02773bdc5d3c7a,d69631d2,# Credit Card Fraud Detection,86245f35,0.0
833,ed83604018af52,18c4bfed,# Ornamental Plants Classify Torch Conv2d,838163a4,0.0
834,d5f78aa381f58d,44ca85c7,# Heart Disease Analysis and Prediction,d60f358f,0.0
835,efd44ce2c08541,b8a59860,# Install dependencies,ebc2d00c,0.0
836,d905cde3391d2b,90658c0c,"Originally published in [Kaggle](https://www.kaggle.com/nowke9/statistics-1-summarizing-quantitative-data)

# Table of contents

* Introduction
* Measuring center
    * Mean
        * Arithmetic Mean
        * Geometric Mean
    * Median
    * Mode
* Measuring spread (variability)
    * Range
    * Interquartile Range (IQR)
        * Percentiles
    * Variance and Standard deviation
        * Comparison with IQR
    * Distribution graph
    * Mean Absolute Deviation (MAD)
    * Box and whisker plots",067dba39,0.0
839,eb0ecd6bebeb15,a89befae,## Keşifçi Veri Analizi | Becerileri Pekiştirme,d7b93a60,0.0
840,f35bf4df70d310,dc9d60f5,"# Exploring Principal Component Analysis (PCA)

A notebook by Jonathan Kristanto

All rights reserved

*PS : If you found this notebook helpful, please consider to upvote this notebook. Thank you :D*",10bb859a,0.0
841,db5a369894fef6,26fb5f5e,"# COVID Data Visualization - Part 1
[Edward Toth, PhD, University of Sydney]

- e-mail: eddie_toth@hotmail.com
- Add me on: https://www.linkedin.com/in/edward-toth/ 
- Join the community: https://www.meetup.com/Get-Singapore-Meetup-Group/

Using different data visualization tools, we attempt to understand the spread of COVID through pretty pictures. In these tutorials you learn more about:
- the spread of COVID
- explore the following Python libraries for visualizing data

__PART 1:__

    - pandas (quick plots for data frames/series)
    - matplotlib (basic plotting library in Python)
    - plotly (interactive plots)
    

<!-- ggplot (follows `R`'s ggplot2, concepts from 'The Grammar of Graphics'), web-based plot for dash, Gleam (inspired by R's Shiny package for interactive plotting apps) -->
  
For more detail on data visualization libraries: https://mode.com/blog/python-data-visualization-libraries/

Dataset from: 
https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset
- 8 `.csv` files
- records of confirmed cases, death and recovered patients
- Short description of patients (gender, age, location, etc.)

## In Part 1 of Visualizing COVID
We will visualize data mainly from:
- `time_series_covid_19_confirmed.csv`
- `time_series_covid_19_deaths.csv` 
- `time_series_covid_19_recovered.csv`",065aaf61,0.0
842,e4525eb0c96f28,a3aa70d8,"# Analyzing Video Game Sales
by Andy Qu and Evan Song",2093a1f1,0.0
844,f2f2db16a2f86c,0c3208fa,# **Estimation of House Value in California**,ffc6a115,0.0
846,e424c111c44669,f2a9af00,"![image.png](attachment:image.png)
",d9fccfba,0.0
847,e4c6dd957eb5ce,f465a10d,"## Welcome to my kernel on Meta Kaggle Dataset

Finally I understood that Meta-Kaggle is a dataset; <br>I ever saw amazing kernels that the source was <b>[Dataset no longer available]</b> and only now I became to here. ",2e383665,0.0
849,d369f200a84c2a,0a228daa,"<a id='0'></a>
# <p style=""background-color:lightgray; font-family:newtimeroman; font-weight: 700; font-size:300%; text-align:center; border-radius: 50px 50px;"">Capsule Networks </p>",8fef4d48,0.0
850,d1ff7e10ee0102,9bbf0dcc,"# COMPREHENSIVE DATA EXPLORATION WITH PYTHON
[Pedro Marcelino](http://pmarcelino.com) - February 2017

Other Kernels: [Data analysis and feature extraction with Python
](https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python)

----------",2cc71c3c,0.0
852,f2e5e9fb9eaaf7,17ae65d0,"# Table of Contents
<a id=""table-of-contents""></a>
- [1 Introduction](#1)
- [2 Preparations](#2)
- [3 Datasets Overview](#3)
    - [3.1 Train dataset](#3.1)
    - [3.2 Test dataset](#3.2)
    - [3.3 Submission](#3.3)
- [4 Features](#4)
    - [4.1 Missing values](#4.1)
       - [4.1.1 Preparation](#4.1.1)
       - [4.1.2 Individual features](#4.1.2)
       - [4.1.3 Individual rows](#4.1.3)
       - [4.1.3 Dealing with missing values (reference)](#4.1.4)
    - [4.2 Distribution](#4.2)
- [5 Target](#5)
    - [5.1 Distribution](#5.1)
    - [5.2 Target & missing value](#5.2)
- [6 Model](#6)
    - [6.1 Base model](#6.1)
        - [6.1.1 XGBoost Classifier](#6.1.1)
        - [6.1.2 LGBM Classifier](#6.1.2)
        - [6.1.3 Catboost Classifier](#6.1.3)
    - [6.2 Base model & feature engineering](#6.2)
        - [6.2.1 Log](#6.2.1)
        - [6.2.2 Minimum of features](#6.2.2)
        - [6.2.3 Maximum of features](#6.2.3)
        - [6.2.4 Sum of features](#6.2.4) 
        - [6.2.5 Multiplication of feature](#6.2.5) 
        - [6.2.6 Prorate of features](#6.2.6) 
        - [6.2.7 Exponential of features](#6.2.7) 
    ",048e0d08,0.0
853,ee43c91f90dcd5,aa142391,This notebook is a fork of https://www.kaggle.com/awsaf49/vinbigdata-cxr-ad-yolov5-14-class-train,56c45a1b,0.0
855,e169603b62be56,82bcb278,**Importing Libraries**,8c311ec1,0.0
856,db7890856ec28b,468a04db,# Spam Email Classification Model,106f6dca,0.0
858,dbccf99c49570f,0344c4fa,## Importing Required Libraries,c20fc09e,0.0
860,f3c6048d1058e3,3b641fe4,**Sentiment Analysis of IMDB Movie Reviews**,1d9056b0,0.0
864,d42518f6cb0995,8335d4c7,"# Riiid! Answer Correctness Prediction
**Concept taken from @Ilia Start Notebook**
## Introduction
In this competition you will predict which questions each student is able to answer correctly. You will loop through a series of batches of questions. Once you make that prediction, you can move on to the next batch.

This competition is different from most Kaggle Competitions in that:
* You can only submit from Kaggle Notebooks
* You must use our custom **`riiideducation`** Python module.  The purpose of this module is to control the flow of information to ensure that you are not using future data to make predictions.  If you do not use this module properly, your code may fail.

## In this Starter Notebook, we'll show how to use the **`riiideducation`** module to get the test features and make predictions.
## TL;DR: End-to-End Usage Example
```
import riiideducation
env = riiideducation.make_env()

# Training data is in the competition dataset as usual
train_df = pd.read_csv('/kaggle/input/riiideducation/train.csv', low_memory=False)
train_my_model(train_df)

for (test_df, sample_prediction_df) in iter_test:
    test_df['answered_correctly'] = 0.5
    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])```
Note that `train_my_model` and `make_my_predictions` are functions you need to write for the above example to work.",26913a9b,0.0
865,d4c5aaa4b36810,764066da," # Predicting the Happiness of a Nation Based on it's Development
This project aims to explore the link between the happiness of a nation and it's economic development. ",65441f28,0.0
866,f998cece696659,387c8a0e,"# Introduction 

1. [Loading and Checking Data](#1)
1. [Linear Regression](#2)
1. [Multiple Linear Regression](#3)
1. [Polynomial Linear Regression](#4)
1. [Decision Tree Regression](#5)
1. [Random Forest Regression](#6)
1. [Evaluation Regression Models](#7)",7964297e,0.0
867,fe118026267a88,7a2a207c,"**[Pandas Micro-Course Home Page](https://www.kaggle.com/learn/pandas)**

---
",612efa48,0.0
868,e3fb4c6300cb56,a3564097,"# INTRODUCTION

There are many death cases by police officers accidentally or other in the US. So, in this dataset, I examine by using visualization methods

1. Read datas
1. Poverty rate of each state
1. Most common 15 Name or Surname of killed people
1. High school graduation rate of the population that is older than 25 in states
1. Percentage of state's population according to races that are black,white,native american, asian and hispanic
1. High school graduation rate vs Poverty rate of each state
1. Kill properties
    * Manner of death
    * Kill weapon
    * Age of killed people
    * Race of killed people
    * Most dangerous cities
    * Most dangerous states
    * Having mental ilness or not for killed people
    * Threat types
    * Flee types
    * Having body cameras or not for police
1. Race rates according to states in kill data 
1. Kill numbers from states in kill data
<br>
<br>
Plot Contents:
* [Bar Plot](#1)
* [Point Plot](#2)
* [Joint Plot](#3)
* [Pie Chart](#4)
* [Lm Plot](#5)
* [Kde Plot](#6)
* [Violin Plot](#7)
* [Heatmap](#8)
* [Box Plot](#9)
* [Swarm Plot](#10)
* [Pair Plot](#11)
* [Count Plot](#12)",8ebbdf89,0.0
869,eb0854a6601407,d1e9db52,"# Ubiquant Market Prediction
Twitch Stream EDA.

1. This notebook was create during a live coding session on twitch. follow for past and future broadcasts here: [here](https://www.twitch.tv/medallionstallion_) ",6d107747,0.0
870,d46508f983e086,4c5e900d,"**Large Shoe Dataset (UT Zappos50k)**

* Data Importing
* Defining Labels (Classes)
* Train, Validation and Test
* Augmenting the Image Dataset
* Building a Model
* Testing Model",454138b8,0.0
872,f06fd8f5916431,3a34627c,"# ***Disclaimer:*** 
Hello Kagglers! I am a Solution Architect with the Google Cloud Platform. I am a coach for this competition, the focus of my contributions is on helping users to leverage GCP components (GCS, TPUs, BigQueryetc..) in order to solve large problems. My ideas and contributions represent my own opinion, and are not representative of an official recommendation by Google. Also, I try to develop notebooks quickly in order to help users early in competitions. There may be better ways to solving particular problems, I welcome comments and suggestions. Use my contributions at your own risk, I don't garantee that they will help on winning any competition, but I am hoping to learn by collaborating with everyone.",854fa433,0.0
873,051b118f751e77,ad544777,"# Introduction
This notebook consists of small EDA (with DABL) and comparison of performance of 11 different models.

<span style=""color:red"">If you like my work, please don't forget to upvote this notebook!</span>

<span style=""color:blue"">If you don't, atleast leave a comment on what should I do to improve it!</span>",9fad25fd,0.0
875,f4514ec092a771,f03ca3fc,"### About this kernel
This kernel ONLY describe the data preparation scripts for preparing necessary files to run on Kaldi and it cannot run end to end on Kaggle kernel. I will not say any further about how to run it on Kaldi. There is a detail tutorial [here](http://kaldi-asr.org/doc/kaldi_for_dummies.html).
I'm also getting familar with Kaldi for nearly a month, so please feel free to discuss and make question or suggestion to have a better result.
The folder to put on running script (run.sh, data, etc...) is uploaded on this [Github repository](https://github.com/minhnq97/asr-commands).",3739ab1e,0.0
878,d78988cb5a1b02,0aa2ed9c,"# **ROC-AUC CURVE**
**ROC stands for Receiver Operating Characteristic, AUC stands for area under curve. ROC-AUC  is used as a metrics for classification problems. It is used to pick the correct threshold for the classification problem. The area under curve varies between 0-1. AUC-1 is the model with high accuracy and with 0 is the worst model. 0.5 is not a good model as it will not correctly predict the classes. The higher the area under the curve, the higher the model performance**",233f3a92,0.0
880,f67c61c7d50810,5efd8007,"## Overview

[The result dataset is here.](https://www.kaggle.com/kokitanisaka/unified-ds-wifi-and-beacon)<br>
<br>
In this notebook, I show one way to make a dataset, Wi-Fi features and Beacon features. <br>
And also, I tried to utilize timegap of Wi-Fi and Beacon from the nearest waypoints. <br>
<br>
The fundamental idea of this dataset is, make samples based on waypoints. <br>
So the number of samples is same as number of waypoints. <br>
Which is much less than [this dataset](https://www.kaggle.com/kokitanisaka/indoorunifiedwifids).<br>
<br>
We can have similar result only with Wi-Fi features in [this dataset](https://www.kaggle.com/kokitanisaka/unified-ds-wifi-and-beacon) as [this dataset](https://www.kaggle.com/kokitanisaka/indoorunifiedwifids).<br>
As it has less samples, the training speed is much faster.<br>
<br>
With beacon features, I wasn't able to achieve a better result. <br>
So if you are interested in it, feel free to do some experiments. <br>

## Attention
Not all the samples can have beacon features. Because in some paths, there's no beacon signals are observed.<br>
we need to take it into account when we train a model. <br>
",2bfe0598,0.0
881,f652abd9973afa,3a353f8c,# How top 1% play the game differently ?,ab2f9cc1,0.0
883,dcb7050d126707,d25b0239,## Prepare environment,0fe949be,0.0
884,020c28a360b0cd,5d7d2283,# Project: Boolean Variables & Operators,2ba397f0,0.0
885,061d6757dfbce0,6cc7e21f,"# Introduction: ASHRAE - Great Energy Predictor III

## Data    

The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.

### Files
> #### train.csv
- building_id - Foreign key for the building metadata.
- meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, hotwater: 3}. Not every building has all meter types.
- timestamp
- meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.


> #### building_meta.csv
- site_id - Foreign key for the weather files.
- building_id - Foreign key for training.csv
- primary_use - Indicator of the primary category of activities for the building based on [EnergyStar](https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/identify-your-property-type) property type definitions
- square_feet - Gross floor area of the building
- year_built - Year building was opened
- floor_count - Number of floors of the building


> #### weather_[train/test].csv
Weather data from a meteorological station as close as possible to the site.

- site_id
- air_temperature - Degrees Celsius
- cloud_coverage - Portion of the sky covered in clouds, in [oktas](https://en.wikipedia.org/wiki/Okta)
- dew_temperature - Degrees Celsius
- precip_depth_1_hr - Millimeters
- sea_level_pressure - Millibar/hectopascals
- wind_direction - Compass direction (0-360)
- wind_speed - Meters per second


> #### test.csv
The submission files use row numbers for ID codes in order to save space on the file uploads. test.csv has no feature data; it exists so you can get your predictions into the correct order.
- row_id - Row id for your submission file
- building_id - Building id code
- meter - The meter id code
- timestamp - Timestamps for the test data period

All floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload. There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored. 
",c0c2915a,0.0
886,d77e6d61ad2e8b,7bff831b,# Installing PyCaret,03fd0e96,0.0
887,f6488772605bb5,18ea914e,"* ***User***: [@manishshah120](https://www.kaggle.com/manishshah120)
* ***LinkedIn***: https://www.linkedin.com/in/manishshah120/
* ***GitHub***: https://github.com/ManishShah120
* ***Twitter***: https://twitter.com/ManishShah120

> *This Notebook was created while undergoing a course ""[Deep Learning with PyTorch: Zero to GANs](https://jovian.ml/forum/t/start-here-welcome-to-deep-learning-with-pytorch-zero-to-gans/1622)"" from ""jovian.ml"" in collaboratoin with ""freecodecamp.org""*",068d4697,0.0
889,ed5c03987493eb,61f17a66,"This notebook helps you to get started with Tensorflow Probability, a module for probabilistic reasoning within the DL framework.  
Why do we want to think probabilistically?  The simplest answer is that we want to have the ability to make prediction with uncertainty estimation. This would help us understand the data better and to make decision based on probabilistic reasoning.  
Another application for Tensorflow Probability is in generative model. In models such as GANs or Variational Autoencoder, we need the probability module to have random initialization.  
In this notebook, we will start with an autoencoder. Next, we will build a variational autoencoder that take advantages of Tensorflow Probability model. Lastly, we will make a probablistic regression model.",bea97744,0.0
891,ff9142eb631dd5,7aaf5ad8,"## Install RAPIDS for faster feature engineering on GPU
https://www.kaggle.com/cdeotte/rapids",453131ac,0.0
892,f015d0147e8fbf,0d2ef127,"# Home Credit Default Risk Competition

## Why I Wrote this Kernel

This was my first Kaggle competition, and through several months of practice, research, trial and error, as well as extensive exploration of forum advice and kernels written by expert Kagglers, I was able to build a featureset/model that earned a final private leaderboard score of `0.79506` as a solo submission, which translated to final rank of 561 out of 7,198 -- just inside the top 8%.

Along the way, I had to figure out from scratch several things that seasoned Kagglers probably take for granted. These included things like:
* Are my Pandas operations as efficient as can be?
* How do I keep my memory use from ballooning out of control?
* What kind of cross validation should I use? And how do I implement it?
* Choosing a learning rate and tuning hyperparameters.
* Deciding how many boosting rounds to use during training.
* Should I use only one, or multiple training rounds?
* If more than one, how do I blend predictions from each training round?
* And finally, exactly how do I create that CSV file that I'll need to submit?

I wanted to publish this kernel in order create an example (both for my future self and for other beginners) of what it looks like to implement all the steps (preprocessing --> feature engineering --> cross validation --> training --> prediction generation --> post processing) that are necessary to build a (somewhat) competitive prediction algorithm. In other words, I wanted to demonstrate what it looks like to *""put it all together.""*

This is just single model LightGBM kernel, without any stacking. I'm planning to dive much deeper into stacking/blending/etc. in my next competition : )

I am indebted to the generosity of the community at large, and in particular, to the wisdom and techniques shared by folks like  [Silogram](https://www.kaggle.com/psilogram), [olivier](https://www.kaggle.com/ogrellier), and [Laurae](https://www.kaggle.com/laurae2).

Although I still consider myself to be a beginner who has a lot more to learn, these folks and their generosity have inspired me to do what little I can to give back. I hope that certain aspects of my code below, such as how my cross-validation method includes target encoding in a way that (I believe) prevents data leak, will be useful to others in the community. 

Finally, I welcome any tips and or feedback on my approach and implementation that follows below. Having one's blindspots pointed out to them is the surest way to growth and improvement  : )",518954fb,0.0
893,de577c910a687d,b65a36e4,"# Welcome to the September 2021 Tabular Playground Competition! #

In this competition, we predict whether a customer will make an insurance claim.

# Step1: Import Helpful Libraries #",c3ebadbc,0.0
894,04ff2af52f147b,5f676b8b,"**Titanic Competition Introduction:**

Our problem is to predict which passengers survived the Titanic's maiden voyage given the following information about them:

- *Age* in years old
- *Sex* $\in$ {female, male}
- *SibSp* representing the number of siblings and spouses travelling with the passenger
- *Parch* representing the number of parents and children travelling with the passenger
- *Pclass* representing the passenger's ticket class $\in$ {1st class, 2nd class, 3rd class}
- *Ticket* number
- *Cabin* number
- *Fare* paid for ticket
- *Embarked* representing the port where the passenger embarked $\in$ {C = Cherbourg, Q = Queenstown, S = Southampton}
- *Survived* $\in$ {0 = died, 1 = survived} for the training set, target for test 

This represents a binary classification problem as the passengers either survived, or they did not.",d5f37be9,0.0
895,ddbe4806ed061e,6272f74b,"<span style=""color: #2486c7; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px"">PetFinder - EDA + Resized Training Images 🛠️</span>

<div style=""font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 16px"">
Currently, PetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.
<br><br>
In this competition, we’ll analyze raw images and metadata to <strong>predict the “Pawpularity” of pet photos</strong>. We'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.
</div>",a0d5fd77,0.0
896,ed8009f482b380,98e47392,## Stroke Prediction,e99941fa,0.0
897,d8d227c158d883,958f5b87,**https://www.kaggle.com/takaishikawa/no-ml-modeling**,3391b4a7,0.0
901,ff029d7b52ae1d,2d805cf2,"## Introduction to Auto_ViML
We are going to try to use Auto_ViML on this data set and see what rank we get. The github for Auto_ViML is located here:
https://github.com/AutoViML/Auto_ViML

The way to use Auto_VIML is to install it first using this command:
pip install autoviml
",c987f868,0.0
902,edc19e349fe80a,c384bbf0,# Notebook mẫu của nhóm: **The Simp Duo**,7882221a,0.0
903,ddcdecdd6a3b6d,2654af70, # 深度卷积神经网络（AlexNet） ,90831448,0.0
907,fda19edaf5c621,cbc865bb,# People get young as they grow old,5cfff76e,0.0
908,016abae0483764,71d20ab5,### Simple Model for Prdicting the COVID-19 test result or possibility of COVID infection with the given data.,bc9f289b,0.0
909,fc8e0042411c46,fff81f79,# Lead Scoring ,af476c2a,0.0
911,f5ca8fb6a465f3,0b9ef4b5,"**This notebook has been forked from** [VinBigData-CXR-AD YOLOv5 14 Class [infer]](https://www.kaggle.com/awsaf49/vinbigdata-cxr-ad-yolov5-14-class-infer)

Please do visit the notebook and support the original author.",56c45a1b,0.0
913,f4b603905215b7,7927317a,"In this Kernel, I tried different methods for Credit Card Fraud Detection. 
![image.png](attachment:image.png)
Check the last Cell for knowledge sources.",efe1d587,0.0
914,f05342aabe2b59,054ba8e4,# More Candy !  -  Santa 2020,cfbb391f,0.0
916,d6cbd7160961dc,885145f9,"<span style='color:Red ; font-size: 250%'> Benford's Law - A tool to detect data manipulation </span> 
#### **Author**: [Rafael Klanfer Nunes](https://www.linkedin.com/in/rafaelknunes/)
#### **Date**: 07/apr/2020
#### **Disclaimer**: This notebook was produced in order to explain our utility script [RKN Module - Benford Law](https://www.kaggle.com/rafaelknunes/rkn-module-benford-law).
#### **References**:
* Nigrini, Mark J. ""I’ve got your number: How a mathematical phenomenon can help CPAs uncover fraud and Rother irregulaties."" AICPA Journal of Accountancy Online Journal 5 (1999).
* Benford, Frank. ""The law of anomalous numbers."" Proceedings of the American philosophical society (1938): 551-572.
* PEARSON, K. (1935). Statistical Tests. Nature, 136(3434), 296–297.

#### **Kaggle Notebook**: https://www.kaggle.com/rafaelknunes/rkn-module-benford-law-tutorial/notebook",36d74664,0.0
917,fbb1f9d3818830,f70ba429,"# Deep Feature K-Means

This notebook is a tutorial for using K-Means clustering to visualize the quality of features/representations learned by a CNN model.",c7027f86,0.0
919,d8ff894670d506,e2184b99,"**School Donation Analysis** 🏫

The below notebook is an attempt to use various tools available in **python** to work on the dataset

**School_Donations** which contains over*4687884* record of the **schools**

their various departments, the **teachers** involved in the donation drives

and the *states* of **United States** which organized projects in the school

for the purpose of **donation generation**
",eb0fb7de,0.0
921,fa02c409161192,9fc1366b,# The MNIST dataset,e97077f7,0.0
922,e16860fce156b0,f4218414," # **<span style=""color:#346888;"">Acknowlegdements</span>**
 
 
SFU Data Science Research Group - Simon Fraser University

https://sfu-db.github.io/dataprep/user_guide/eda/plot.html
 
 
https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc ",2054f1ce,0.0
925,fe6750354fb64f,f5ded30f,"## <center><font size=""20"" class=""serif"" color=""teal""> India against COVID-19 </font> </center>",271741f0,0.0
926,038abade89e59f,50c79420,"# RANZCR 1st Place Solution Inference

Hi all,

We're very exciting to writing this notebook and the summary of our solution here.

Our final pipeline has 4 training stages but the minimal pipeline I show here has only 2 stages.

The 5-fold model trained with this minimal pipeline is sufficient to achieve CV 0.968-0.969 and pub/pvt LB 0.972

I published 3 notebooks to demonstrate how our MINIMAL pipeline works.

* Stage1: Segmentation (https://www.kaggle.com/haqishen/ranzcr-1st-place-soluiton-seg-model-small-ver)
* Stage2: Classification (https://www.kaggle.com/haqishen/ranzcr-1st-place-soluiton-cls-model-small-ver)
* Inference (This notebook)

This notebook shows how we can inference my minimal pipeline to make a valid submission, using my 5 fold segmentation models (b1 w/ input size 1024) and 5 fold classification models (b1 w/ input size 512)


Our brief summary of winning solution: https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/226633



# Thanks!",cb32a3fe,0.0
927,e03eb63c1f725d,30bf61b8,"<font size=""+4"" color=teal><u><center>Fake News Classifier </center></u></font>",e204b7e3,0.0
930,ec296e2859e62c,5fb00417,"Just a very simple baseline applying a RoBerta model to predict the readibility of excerpts. If you have any questions, please let me know.

The notebook for inference can be found here: https://www.kaggle.com/hannes82/commonlit-readability-roberta-inference/",0b0fed10,0.0
933,fdc3afd309b850,b2ed5ed5,# Price Prediction - Brasília Apartments,966bde38,0.0
934,f50dc95483c98f,6e6be495,"### **Importing All Essential Libraries**
* A (software) library is a collection of files (called modules) that contains functions for use by other programs.
* May also contain data values (e.g., numerical constants) and other things.
* Library’s contents are supposed to be related, but there’s no way to enforce that.
* The Python standard library is an extensive suite of modules that comes with Python itself.
* Many additional libraries are available from PyPI (the Python Package Index).",cd9e9621,0.0
937,f7436bc492474c,0705aac9,"## Introduction

This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).",328fd235,0.0
938,fb5c6021d127ef,b5a61e82,"### This exercise is designed to pair with [this tutorial](https://www.kaggle.com/rtatman/bigquery-machine-learning-tutorial). If you haven't taken a look at it yet, head over and check it out first. (Otherwise these exercises will be pretty confusing!) -- Rachael ",dd05cbd3,0.0
940,f4b9042e693b6c,49cdd225,"# SETI PyTorch XLA/TPU starter
![image.png](attachment:image.png)

### If you found this helpful, please give it an upvote!

## Introduction

[PyTorch XLA](https://pytorch.org/xla/master) is a PyTorch library for XLA support. XLA (Accelerated Linear Algebra) is a domain-specific compiler that was originally meant for compiling and accelerating TensorFlow models. However, other packages, like JAX and now PyTorch XLA can compile program with XLA to accelerate code. TPUs can be programmed with XLA programs and PyTorch XLA provides this interface with TPUs by compiling our PyTorch code as XLA programs to run on TPU devices.

In this kernel, I provide an in-depth look into how you can use PyTorch XLA to **train a PyTorch model on the TPU** for the SETI Breakthrough Listen - E.T. Signal Search competition.",676cacc9,0.0
941,ffc9490c4f6c38,bed166bd,"# Search 'useless columns'

In this kernel, suggest 'useless' columns.
I believe these should be drop but may not be correct.


based on [LR great kernel](https://www.kaggle.com/cdeotte/logistic-regression-0-800).
",ae7bbbb3,0.0
943,e0e19e91579432,8093a6bc,# Hyper Parameter tuning using keras tuner,0c8a0755,0.0
944,e04e5204572e7e,7169ef97,"#### It is one of the code script implementations used in the paper:
[Ensemble Algorithm using Transfer Learning for Sheep Breed Classification](http://https://ieeexplore.ieee.org/document/9465609)",6c888be9,0.0
945,fdc9f4863744b1,d310dae6,"## Introduction

The purpose of this analysis is to reinforce the data science methotology and process from reviewing the problem in question, getting and cleaning the data, analyzing it and further creating a predictive model. This report will revolve around the use case of New York Real Estate market. 

I am interested in automating the process of figuring out estimated price of a real estate properly in the recent future. Can I predict the sale price of a property within a certain area? Can I figure out estimated price of a property within a certain neighbourhood? Can I predict the future up and coming neighborhood within a borough? These are some of the questions I will try to answer with my analysis.

I will start downloading my libraries.",b4529365,0.0
946,dd3721cb49c1fd,94cdcad9,"<div style=""margin: 0px; padding: 10px; background-color: #43a047;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px;"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white; text-align:center""> Greykite - Time Series Forecast of Microsoft Stock </h1>
      
<!--       <div style=""color:white"">I am a square root.</div> -->
<!--       <h1>$\sqrt{4}$</h1> -->
  </div>
</div>",1a53fdd9,0.0
948,e9b9663777db82,3d27a0c2,# Home Sales Data,648e8507,0.0
951,0504abe8519634,f94228c1,"#### This notebook compares different CountVectorizer techniques used in Natural Language Processing. 
#### We will look into the following three types for now:
 1. #### with stopwords, without ngrams
 2. #### without stopwords, without ngrams
 3. #### without stopwords, with ngrams",46df846a,0.0
952,f14f6708035916,162c1fd2,# Revolut Spending Analysis,ca22d04b,0.0
954,e8a3483b83a26e,8adcf2de,"A Simple Linear Regression model for predicting salaries of the people according to their years of experience, more work experience means more salary.
Dataset consists of two features(years of experience is independent and other salary is dependent)",0c5db5fd,0.0
955,f166950fa915f8,27179fdc,# Twitter Sentiment Analysis,a7f6ca5e,0.0
959,d128317750d689,e7439c2d,"In this work I classify images of sign language letters with convolutional network using PyTorch. 

First, let's import everything we need for work: ",d87f7428,0.0
960,ee9ddc756b2d4a,534bf762,"# Brain tumor detection in mri brain images

## Machine Learning 2019/2020 ",e367eab3,0.0
961,d96e03a9e7c030,c5b826b3,"**Introduction**

This kernel relates to the Kaggle competition put on by PASSNYC, a not-for-profit organization which, according to their website, is dedicated to broadening educational opportunities for New York City's talented underserved students.

As part of that charter, PASSNYC sponsored the Kaggle competition found [here](http://www.kaggle.com/passnyc/data-science-for-good) to encourage NYC 8th graders to participate in the entrance exam (called the Specialized High Schools Admissions Test, or SHSAT) to NYC's eight specialized schools . As one of the PASSNYC representatives, Max, explains:  
> Only a third (roughly) of eligible students take the SHSAT, and our goal is to drive more test takers (you can't get in if you don't sit for the test!). The education space is full of non-profits like ours with limited resources. So the research question is, given limited resources, where (at which schools) can you target your intervention efforts to make an impact on those participation numbers. [link to quote](http://www.kaggle.com/passnyc/data-science-for-good/discussion/59680#348922).

Existing research from the [Research Alliance for New York City Schools](http://steinhardt.nyu.edu/scmsAdmin/media/users/sg158/PDFs/Pathways_to_elite_education/WorkingPaper_PathwaystoAnEliteEducation.pdf) from 2015 indicated that if you are a child in NYC that is female, black, hispanic or eligible for free lunch, you are underrepresented in SHSAT participation compared to the groups' respective citywide average. 

**Datasets**

To help work towards a solution to this problem, I leaned heavily on the Research Alliance's paper, augmenting their findings by exploring a few other data sets available online. These data sets were:
* The School Explorer file provided as part of the Kaggle competition
* New York Times article which supplied data on the number of students who took the SHSAT and subsequently received an offer in 2017, which can be found [here](http://www.nytimes.com/interactive/2018/06/29/nyregion/nyc-high-schools-middle-schools-shsat-students.html). 
* NYC Department of Education data from its website for every school
* Socrata's NY School Safety Report (2010-2016) data set, found [here](https://www.kaggle.com/new-york-city/ny-2010-2016-school-safety-report)
* Socrata's NYC Council Discretionary Funding data set, found [here](http://www.kaggle.com/new-york-city/new-york-city-council-discretionary-funding)

In the case of collecting the NYC Department of Education, the script took a number of hours to complete, as it goes to the Department of Education website for every school in New York City, simulates clicks and other events on the page to subsequently pull data. An example of such a website can be found for the Christa McAuliffe Middle School [here](https://tools.nycenet.edu/guide/2017/#dbn=20K187&report_type=EMS). The output of this script finds data for every NYC school, such as: 
* Student Population and Characteristics (e.g., number of 8th graders enrolled in 2017 and previous years, percentage of students with disabilities, etc.)
* School Conditions and Practices (e.g., ratings for 2017 and previous years in constructs like how rigorous the instruction is, how collaborative teachers are, etc.)
* Student Achievement and Outcomes (e.g., student proficiency by different demographics on the New York state-wide tests, percentage of students earning high school credit,etc)

In many cases, the data available for each school's NYC DoE website overlaps with the standard datasets provided as part of the competition, but the DoE website data provides data from prior years, along with additional data not included in the 2016 School Explorer data set.

#Methodology / Approach
**Variables**
We'll get into the code and output shortly, but first I wanted to summarize the general approach in recommending targeted interventions to specific schools. The output of this process is the predicted number of *additional* students at each NYC school that would have been expected to take the SHSAT test based on the following variables:
1. Average ELA Performance at each school (from School Explorer dataset)
2. City-wide difference of the percentage of students at each school that scored a 3 or 4 on the Math statewide exam (from the Dept of Ed data)
3. Percentage number of students at each school that have an attendance rate greater than 90% (from the Dept of Ed data)
4. City-wide difference of the percentage of 8th graders at each school that have attained at least some high-school credit (from the Dept of Ed data)
5. Minimum distance from each school to one of the so-called 'Big Three' specialized high schools -- Stuyvesant HS, Bronx HS of Science, and Brooklyn Tech (calculated using the School Explorer data set)

These 5 variables were chosen after exploring all variables in the combined data set, including those from the Kaggle competition, external Socrata data, and data collected from the Dept of Education. A couple additional notes on these final variables:
* To illustrate what the 'city-wide difference' variables (#2 and #4 in the above list) are, let's use an example. If the city-wide average of students that scored a 3 or 4 on the Math statewide exam is 50%, and a particular school has 60% of its students score a 3 or 4, the city-wide difference (referenced as `pct_math_level_3_or_4_2017_city_diff` going forward) variable would be 10%.  Similarly, if another school had 30% of its students score a 3 or 4 on Math, its citywide difference would be -20%.
* As all the above variables are numerical, they were standardized before usage in the final model. To extend the example from the above bullet, if the mean of the `pct_math_level_3_or_4_2017_city_diff` variable is 0 with a standard deviation of .1 (i.e., 10%), then the standarized version of `pct_math_level_3_or_4_2017_city_diff` (referenced as `pct_math_level_3_or_4_2017_city_diff_stdized` going forward) of the two schools above would be 1 and -2 (since one school had 1 standard deviation above the mean for the raw numerical variable, and the other school had 2 standard deviations below the mean for the raw numerical variable
* Notice that **none** of the variables take into account any data regarding the demographic population of the school. This was done on purpose; the thinking is that if we only use variables regarding school performance (statewide scores, attendance rate) or logistics (how far would these students have to travel to get to one of the more prestigious specialized school), we can identify schools that have favorable performance/logistics characteristics regardless of whether their populations have been underrepresented in the past.

**Response Variable**
These five variables were used to create a linear regression model to predict the percentage of students to take the SHSAT at each school. This variable was easily create using the Grade 8 enrollment at each school from the Dept of Ed, along with the number of testtakers taken from the NYT article

**Model and Output**
The model chosen for this purpose was a linear regression model, which, as mentioned previously, outputs a predicted percentage of students expected to take the SHSAT. We can then use the actual number of students enrolled in 8th grade at each school to calculate the number of *additional* (or, in the cases when predicted percentage is lower than the actual, *fewer*) students at each school expected to take the SHSAT. Using these numbers in conjunction with demographic information, we can estimate the number of students in underrepresented groups that might be expected to take the test.

>**OK, apologies for all the words above. Let's begin by looking at the output of the entire process detailed below. We'll go through all the steps needed to created these visualizations in just a minute**",d2b72ced,0.0
962,e95239c8f38005,0fccef6f,Here we call os module to know all about data and folders in dataset. Os module will let you walk through the doors fo dataset foder,e2632312,0.0
963,efbcfe95cd7fde,ea11dd29,"Cuales son los factores incide en sufrir o no un paro cardíaco?
Es importante tener encuenta que partimos de unos datos relacionados Enfermedades cardiaca, y con esto intentamos identificar si existe alguna relacion entre los posibles resultados y se con los mismos se puede llegar a establecer si de acuerdo edad y el sexo, con dolores en el pecho, cuales son las signatorias mas preponderantes",54be281a,0.0
966,d1f92a87a0a1a5,b19ae739,# Earthquake Predict Nankai Pycarat Historical Data,2cd610b2,0.0
967,e8750f24c66614,82520dbc,"1) Based on Version 6 of https://www.kaggle.com/stanleyjzheng/baseline-nn-with-k-folds kernel.

2) Add KFold, a few seeds averaging.

3) Add WeightNormalization, Lookahead, ReduceLROnPlateau.

4) Select features by permutation importance.

5) Put zeros for `ctl_vehicle` predictions.
",bdebc727,0.0
969,ea4e559a86d613,4fc1d577,"# The Images 

There are 2 types of images containing the same information:
1. `.dcm` files: [DICOM files](https://en.wikipedia.org/wiki/DICOM). It's saved in the ""Digital Imaging and Communications in Medicine"" format. It contains an image from a medical scan, such as an ultrasound or MRI + information about the patient.
2. `.jpeg` files: the DICOM files converted into .jpeg format
3. `.tfrec` files: [The TFRecord file format is a simple record-oriented binary format for ML training data.](https://docs.databricks.com/applications/deep-learning/data-prep/tfrecords-to-tensorflow.html#:~:text=The%20TFRecord%20file%20format%20is,part%20of%20an%20input%20pipeline.)",eff47843,0.0
970,f0fab078f8533b,b4e90945,"# Steps followed in this notebook :

1. Importing required packages

2. Preparing Data for analysis:
     
    a. Creating a new column 'year_added'
    
    b. Taking only last 10 year data
    
    c. Replacing 'nan' values in ['director', 'cast', 'country', 'rating'] col with 'Other'
    
    d. Collecting a list of all directors, actors, genres and countries
    
    e. Converting 'listed_id' and country to a list of show types and list of countries respectively
    
    f. Converting duration into a bin with different lengths
    
3. Analysing the data:

    a. Bar Plot to see which (Movie / TV Shows) has the most number of contents
    
    b. Which country has the most number of content in the last 10 years, also the top genres made in the last 10 years.
    
    c. A function 'top_actor_or_director' with i/p parameter dataframe, country (can take any of the countries) and attribute ('director' or 'cast) which returns a pie chart of top (parameter 'top' take any int n) n 'director' or 'actor' in that specific country
    
    d. Bar plot showing number of content (movie and TV show) added per year
    
    e. Bar plot showing the total number of Movies and TV Shows per ratings respectively
    
    f. A function 'genre_actor_or_director' with i/p parameter dataframe, name (of actor / director) and a bool value indicating Ture for actor or director rspectively plots a pie chart with the total number of movies / TV shows done in each genre by the actor/director
    
    g. Bar chart showing when is the most content added during the year
  ",bdb5ea32,0.0
971,d0080e3a39bc5c,9c8a1097,![](https://i.imgur.com/uAAVYel.png),2fcde4cf,0.0
973,d07915a6e6992e,9c34f3b6,"The guided approach explained here will help you to understand how you should design and approach Data Science problems. Though there are many ways to do the same analysis, I have used the codes which I found more efficient and helpful.

The idea is just to show you the path, try your own ways and share the same with others.",2b912140,0.0
974,f13534449a3750,17c7e98c,"#  🚢 Ship Detection: Image Visualizations and EDA

<img src= ""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR-Yxy_PxDspM7V39A-jp2dBv29geaV4OiDxg&usqp=CAU"" alt =""Ships"" style='width: 400px;'>

The goal of this challange is to detect and mask ships from satellite images. There are both trainig and test data available with masks on the ships encoded with a run-lenght encoding style, as we will se later. The main difficult is due to the fact that the dataset is very big and unbalanced in a sense of presence of ships in the images. Furthermore the images are from satellities and the ships rapresent only a small section over the total if we consider the pixels. We will follow a summary as follow:",8b7f3332,0.0
975,dac3c8204a2d1b,7571284c,![](https://image.freepik.com/free-vector/opened-magic-book-realistic-image-with-bright-sparkling-light-rays-illuminating-pages-floating-balls-dark_1284-29035.jpg),b0d2d0dc,0.0
978,fd4017c1514157,7d684d41,"![""](https://storage.googleapis.com/kaggle-competitions/kaggle/25954/logos/header.png?t=2021-03-19-18-32-57)
> # Complete EDA🔎📊📈 for BirdCLEF 2021 - Birdcall Identification Challenge
",fd8f0896,0.0
981,f15eac23fbcc9d,1f841275,# Fast.ai Random Forest approach to solve MNIST ,ea46d8af,0.0
982,d0f6276d5b628c,5e79dd29,"# Statistical Rating analysis and Customer behavioural Analysis:

This project is based on one of the most interesting topic : **Movie Recommendations**. In this notebook we are going to check whether we can find top movies as per rating and popularity and other features.

As we are going through the rating we will also try to find the best audiences and prepare medias for them which will be a **GOAL** of this project.",c64f5ce5,0.0
983,cf4d1c1ad1476c,54e5052b,"
# What are TPU's?

TPU's are holy grail of computers for any Machine Learning Practitioners! A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning. TPUs are hardware accelerators specialized in deep learning tasks. In this code lab, you will see how to use them with Keras and Tensorflow 2. Cloud TPUs are available in a base configuration with 8 cores and also in larger configurations called ""TPU pods"" of up to 2048 cores. The extra hardware can be used to accelerate training by increasing the training batch size.
Why TPUs?

Modern GPUs are organized around programmable ""cores"", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.

The following video from Kaggle explains the main components of TPU like systolic arrays and bfloat16 number formats, and how these two components of TPUs help reduce deep learning model training times",768c1a59,0.0
984,e777353e35d291,6f5bff5e,"The code below is analyzing COVID-19 Cases and COVID-19 Deaths within the United States. I provide a Top-Ten Deadliest States, Top-Ten Deadliest Counties, and Top-Ten Deadliest Counties per State.",228f479b,0.0
988,ff83da40bcdb19,f16fc710,"**WE ARE MAKING A CNN FOR PREDICTING WALK OR RUN**

1. First we are importing libraries, we are giving the path of the datas and we are defining other stuff which we will use in the CNN.

2. Making a fucntion for training data.

3. Doing same things for the testing data (It's has a little bit differences than the second step)

4. Giving the parameters of our model.

5. Dividing the train data for training.

6. Training the model.

7. Predicting and ploting",36b5ec8c,0.0
991,da199f8fb59439,73b336a3,# **Netflix Exploratory Data Analysis**,baaa665d,0.0
992,f18e737fcc4b06,a9bb5328,"# Introduction

The sinking of Titanic is one of the terrible accident in the sea story. We will compare statistics datas about Ticanic crew and passengers

<font color = ""blue"">
Content :
    
    1.Load Datas",087b8637,0.0
995,063a35f644e3c5,ae1c7e90,"<br>

# Capstone Project (Mini project)",1c30fb0a,0.0
996,fdbbd573ba31c2,fb35bde1,# Preparation,f7c28d74,0.0
1000,2ada0305b68956,fe91510f,### **Seaborn has 170 different palette choices to start with. So Let's Jump in the visualisations..!!**,133e26f4,0.002857142857142857
1001,fc8e0042411c46,f954ceef,## Problem Statement,af476c2a,0.003134796238244514
1004,9ceb7278784462,069b01f5,# <a id='0'> Dataset Introduction</a>,3768a567,0.004032258064516129
1005,fdc3afd309b850,65a416ca,"<a id=""introduction""></a>

# Introduction ",966bde38,0.004629629629629629
1006,4c47839b067546,19aba541,# 1. Импорт библиотек и создание функций,1f517b02,0.005319148936170213
1007,396bc36edb95d3,e3a9cd05,"### Problem 2: CART-RF-ANN

An Insurance firm providing tour insurance is facing higher claim frequency. The management decides to collect data from the past few years. You are assigned the task to make a model which predicts the claim status and provide recommendations to management. Use CART, RF & ANN and compare the models' performances in train and test sets.",965e4f8f,0.005555555555555556
1008,81712ee7510ac5,58f1bc96,**Data Types**,c4685e79,0.005714285714285714
1009,2ada0305b68956,7f4846da,![](https://image.freepik.com/free-vector/banner-with-rainbow-coloured-low-poly-design_1048-12754.jpg),133e26f4,0.005714285714285714
1010,30fdc4a6e3c1db,75afaa4b,"# Content:
1. Loading necessary libraries
2. Reading the dataset
3. Summary Statistics
4. Time Series Views
5. Impact of Events and SNAP days on sales
6. Analysis on prices changes",6111ddee,0.005847953216374269
1011,5ce12be6e7b90e,261d2203,"# Hello Jupyter!

To execute code in Jupyter notebook press `Shift+Enter` (or `Shift+Return`) or press `Control+Enter` (or `Command+Return`). The former will execute and advance, the later will execute and stay.
You can also use the ▶️ button on the command pallete above.",c0ab62dd,0.005847953216374269
1012,6a80f915608fc2,2ffe19ea,"## Overview
This notebook messes around with the data sets from the [MoA Prediction](https://www.kaggle.com/c/lish-moa) competition.  For simplicity a simple classifier is implemented to assign sig_ids as ""MoA"" or ""not MoA"".
In v15+ the classifier is implemented as 3 separate XGB models: one for each value of cp_time. The models use the same hyper-parameters and the same reduced set of features (v19+: c-ave, c-std, c-5%, c-95%, c-38, c-65, c-70, c-48, g-95%, g-hif, and 11 specifically selected g values).

(v15) The classifier, with threshold=0.45, labels ~ 2000 of the ~ 22000 non-control train sig_ids as being ""not MoA"" with 89% precision, and, changing the threshold to 0.32, we find ~ 6000 sig_ids are determined as ""MoA"" with 95% precision.  These still leave about 14000 non-control sig_ids whose status is ambiguously determined, although 61% of them are actually ""MoA"". 

Of course the features and ML used here are simple, even so it suggests that there may not be enough information in the data to determine ""MoA"" or ""not MoA"" for many of the sig_ids.

#### The Notebook's LB Score
The notebook's initial score of 0.02363 (v3) comes from the simplest model of setting the controls' targets to 0 and the non-control sig_ids' targets are set to their average as seen in the non-control training set, e.g., the nfkb_inhibitor target is set to 0.037908.

One motivation for wanting to determine the not-MoA sig_ids is that then all of their target values could be set (closer) to 0 and the score would improve. With so few cleanly determined non-MoAs the improvement possible seems at/below the 5th decimal digit - so much for that idea ;-)

**In (v19+)** the classifier is switched to have y=1 correspond to MoA>0. Rows predicted to have MoA>0 have all their target values increased by a calculated factor (e.g., x1.3) and the other rows have their targets decrease by another factor (e.g., x0.95). <br>
**In v20--31**, instead of using all of the targets, the classifying and factors adjustments are restricted to a subset of the targets, i.e.: the ""tSNE-9"" easy-to-detect ones (v21), the ""big 2"" (v22), the high-MoA ones w/o the big 2 (v24), the cdk_inhibitor itself (v26), the ones whose g-vectors seem 'detectable' (v28), etc. See details in the <a href=""#DiaryRecent"">recent Diary entries</a> and in the <a href=""#OutputKaggle"">Output Kaggle Predictions</a> section.<br><br>
**In v32** all the targets are used again, with results not so different from v15: about 3000 ids are classified as not MoA and perhaps 6000 as MoA, leaving 13000 that are not well classified, as seen by the overlap in the confusion-dots plot.",636938eb,0.005952380952380952
1014,fc8e0042411c46,1570d59b,"An education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. 

 

The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. 

 

Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as ‘Hot Leads’. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:


Lead Conversion Process - Demonstrated as a funnel
As you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.

 

X Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers. The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance. The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.

 

Data
You have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page. Another thing that you also need to check out for are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?).",af476c2a,0.006269592476489028
1015,917957c6c4065f,95c37892,"## Data Description(kaggle) 
https://www.kaggle.com/datasnaek/youtube-new  
by byeongjoon

### Context
YouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.

This dataset is a daily record of the top trending YouTube videos.

Note that this dataset is a structurally improved version of this dataset.

### Content
This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.

EDIT: Now includes data from RU, MX, KR, JP and IN regions (Russia, Mexico, South Korea, Japan and India respectively) over the same time period.

Each region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.

The data also includes a category_id field, which varies between regions. To retrieve the categories for a specific video, find it in the associated JSON. One such file is included for each of the five regions in the dataset.

For more information on specific columns in the dataset refer to the column metadata.

### Acknowledgements
This dataset was collected using the YouTube API.

### Inspiration
Possible uses for this dataset could include:

* Sentiment analysis in a variety of forms
* Categorising YouTube videos based on their comments and statistics.
* Training ML algorithms like RNNs to generate their own YouTube comments.
* Analysing what factors affect how popular a YouTube video will be.
* Statistical analysis over time.

### Columns
* video_id
* trending_datetrending-list에 게재된 날짜
* title
* channel_title
* category_idcategory_id.json 참조
* publish_timeupload 시각
* tags
* viewstrending_date 까지의 누적 조회수
* likestrending_date 까지의 누적 좋아요 수
* dislikestrending_date 까지의 누적 싫어요 수
* comment_counttrending_date 까지의 누적 댓글 수
* thumbnail_link
* comments_disabledif TRUE → comment_count = 0
* ratings_disabledif TRUE → likes = 0 and dislikes = 0
* video_error_or_removed
* description",55b8ed68,0.006535947712418301
1019,726833f92fb87a,7308abeb,"The following project focus on the analysis of a dataset 'Bank Marketing' which contains data about customers and aims to get useful insights from the data and predict if a new customer will accept a deposit offer or not.<br>

The project is structured as follows:

- Data Cleaning + Feature Engineering
- Exploratory Data Analysis
- Data preparation for ML algorithms (encoding)
- XGBOOST training and Hyperparameter optimization
- Results Summary",7dc5e1b6,0.006711409395973154
1020,5f32117bcd5255,c7b0f678,"# K2-18b

* Discovery date

2015

* Detection method

Transit

* Semi-major axis

0.1429 (+0.0060−0.0065) AU

21,380,000 km

* Eccentricity

0.20 (±0.08)

* Orbital period

32.939623 (+0.000095−0.000100) d

* Inclination

89.5785° (+0.0079°−0.0088°)

* Argument of periastron

−0.10 (+0.81−0.59) rad


* Semi-amplitude

3.55 (+0.57−0.58) m/s
	

* Mean radius

2.610 (±0.087) R⊕

* Mass

8.63 (±1.35) M⊕

* Mean density

4.01 g/cm3

* Surface gravity

≤1.66 g

* Temperature

265 ± (5 K) (−8 ± 5 °C)",85882abf,0.006711409395973154
1021,63b44c85e32c1f,a1432d88,"Data Structure is a way of collecting and organising data in such a way that we can perform operations on these data in an effective way. Data Structures is about rendering data elements in terms of some relationship, for better organization and storage. 

In simple language, Data Structures are structures programmed to store ordered data, so that various operations can be performed on it easily. It represents the knowledge of data to be organized in memory. It should be designed and implemented in such a way that it reduces the complexity and increases the efficiency.

**For Example:**
- Arrays
- Lists
- Sets
- Trees
- Graphs",fb9b9562,0.006756756756756757
1022,fdc9f4863744b1,511915c3,## Importing Modules and Libraries,b4529365,0.00684931506849315
1023,738bfced935b69,f3a19ac6,"## Table of contents

1. [Introduction](#Introduction)
2. [Data Cleaning](#Data-Cleaning)
3. [Data with Nine Columns](#Data-with-Nine-Columns)
 -  3.1 [Exploratory Data Analysis](#Exploratory-Data-Analysis)
 -  3.2 [Data Analysis by tax](#Data-Analysis-by-tax)
 -  3.3 [Label Encoding](#Label-Encoding)
 -  3.4 [Data Modeling](#Data-Modeling)
4. [Data with Seven Columns](#Data-with-Seven-Columns)
 - 4.1 [Exploratory Data Analysis](#Exploratory-Data-Analysis)
 - 4.2 [Label Encoding](#Label-Encoding)
 - 4.3 [Data Modeling](#Data-Modeling)
5. [Conclusion](#Conclusion)  
 

",2d3c592d,0.00684931506849315
1024,eda49464dd6d1b,38755e15,"<font size=""+3"" color='#053c96'><b> Problem Statement</b></font>",8421f81f,0.006993006993006993
1025,b61ab8f81dc03d,49979807,"<a id=""history""></a>
# History",64d05394,0.0070921985815602835
1026,56785caebaa256,0f1d1ade,"<a class=""anchor"" id=""0""></a>
# COVID-19 in Ukraine: long-term forecast with the Prophet for several months in the future",a792961a,0.0070921985815602835
1027,957e035ba5b9d5,a71bc391,Blog post URL: https://stephanosterburg.github.io/deep_learning_art_images/,778ab3d3,0.0070921985815602835
1028,a566b5b7c374e7,fd0f7c0b,## Import Libraries,b3dc5545,0.007194244604316547
1032,3c2033cc99c12c,5aca3092,"
**Note:** *The course project of SDSC2001--Python for Data Science*   

*SID: 56641800&emsp; &emsp; Name: Du Junye*   

*This project is only for the use of course requirement of sdsc2001*  

",dfa22a54,0.0072992700729927005
1033,c80939c7c626cf,a3144a7f,# load train and test data sets using pandas,b9ac31e2,0.0072992700729927005
1034,7f74a04ae75792,2ffdd80f,### Load the Data into Pandas Dataframe,d01e91da,0.007352941176470588
1036,20b372b6e4e276,e3d59fe9,"# Overview

In this notebook, I analyze and visualize the outliers of the NLP solution from very good notebook ""[TSE2020] RoBERTa (CNN) & Random Seed Distribution""(https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution) using the functions from my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert) including PCA processing, Kmeans clustering, WordCloud and others. More over I try to improve the original solution.

Add chapters ""**Subtext analysis**"" and ""**Metric analysis**"" from the commit 10.",ec8b0860,0.007462686567164179
1037,ba4b3bd184acbb,6f7e3922,***,0f5de724,0.007518796992481203
1038,ee23a565163388,d4687f18,"The heart is the muscular organ made of cardiac muscles and other tissues. The primary function of the heart is to pump the blood through the vessels of circulatory system. There are many ailments which a human can suffer because of disfunctioning of this vital organ. One of such ailments is the **heart failure**. Heart failure happens when the heart fails to do its function of pumping blood.

There could be multiple reasons which can leave heart in such a condition like narrowing of the arteries or high blood pressure. Certain treatments and change in lifestyle can help cure this disease.
",88aacbc4,0.007633587786259542
1039,09751c520b0616,5a910b13,- <b> Import Libraries,a4d0c7e9,0.007692307692307693
1040,d07915a6e6992e,e917aacf,"**I will be trying the classic ""LESS IS MORE"" approach.**

In my previous versions, I have used ""**ALL In**"" approach - using all variables with feature engineering for prediction. Please refer to versions 25 or earlier to review the same.",2b912140,0.007692307692307693
1042,a4f8ad33c823c5,f4959485,# Load Libraries,fcd48307,0.007692307692307693
1043,0932046e1f485d,7203b01f,"This notebook covers the data cleaning and initial exploration of the ""Google Play Store Apps"" dataset.

Each EDA I do provides an opportunity for me to learn and improve. During this specific analysis, I have scratched the very surface of NLP, explored the Treemap plot type and played with the lambda function.

In the future, I plan to dwell deeper into NLP to get a firm understanding of it.",218cc7a3,0.0078125
1045,9ceb7278784462,26ba10ee,"![](https://media.giphy.com/media/xT8qBt3pdiCZrk3erS/giphy.gif)
* This dataset holds a list of 270 books in the field of computer science and programming related topics.
The list of books was constructed using many popular websites which provide information on book ratings an of all the book in those websites the 270 most popular were selected.

* Inside that dataset, you will find general information about the book including the number of pages in the book, the book types, the book descriptions, and the book prices.",3768a567,0.008064516129032258
1046,e19e307b3fd188,23e4089d,# Objective,2173955b,0.008130081300813009
1047,979f1e99f1b309,6c5a4db3,"# PUBG Finish Placement Prediction Project
#### In this project we will be working with a Pubg finish placement data set, indicating the finishing placement of players. We will try to create a model that will predict finishing placement in future based of the features of the past placement.
#### This data set contains the following features:

> - DBNOs - Number of enemy players knocked.
- assists - Number of enemy players this player damaged that were killed by teammates.
- boosts - Number of boost items used.
- damageDealt - Total damage dealt. Note: Self inflicted damage is subtracted.
- headshotKills - Number of enemy players killed with headshots.
- heals - Number of healing items used.
- Id - Player’s Id
- killPlace - Ranking in match of number of enemy players killed.
- killPoints - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a “None”.
- killStreaks - Max number of enemy players killed in a short amount of time.
- kills - Number of enemy players killed.
- longestKill - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.
- matchDuration - Duration of match in seconds.
- matchId - ID to identify match. There are no matches that are in both the training and testing set.
- matchType - String identifying the game mode that the data comes from. The standard modes are “solo”, “duo”, “squad”, “solo-fpp”, “duo-fpp”, and “squad-fpp”; other modes are from events or custom matches.
- rankPoints - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API’s next version, so use with caution. Value of -1 takes place of “None”.
- revives - Number of times this player revived teammates.
- rideDistance - Total distance traveled in vehicles measured in meters.
- roadKills - Number of kills while in a vehicle.
- swimDistance - Total distance traveled by swimming measured in meters.
- teamKills - Number of times this player killed a teammate.
- vehicleDestroys - Number of vehicles destroyed.
- walkDistance - Total distance traveled on foot measured in meters.
- weaponsAcquired - Number of weapons picked up.
- winPoints - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a “None”.
- groupId - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.
- numGroups - Number of groups we have data for in the match.
- maxPlace - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.
- winPlacePerc - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.",d1bfebbf,0.00819672131147541
1048,e9b9663777db82,ab12a9be,![Home%20Sales%20Picture.png](attachment:Home%20Sales%20Picture.png),648e8507,0.008264462809917356
1049,2f47abddfd1928,5d1bcd86,"# Titanic Survival Kaggle Dataset

To import all necessary libraries for each stage.",ae33cc0b,0.008264462809917356
1050,62487bcd70b199,47ec39da,"# Index:

- <a href='#1'>1. Data</a>
    - <a href='#1.1'>1.1. Data overview</a>
    - <a href='#1.2'>1.2. Profile Report of data for better understanding</a>
- <a href='#2'>2. Data Manipulation</a>
- <a href='#3'>3. Statistical Tests</a>
    - <a href='#3.1'>3.1. Distribution check on Numerical Features</a>
    - <a href='#3.2'>3.2. Dependancy check across Features</a>
    - <a href='#3.3'>3.3.Performing an Independent t-test for Income and Loan</a>
    - <a href='#3.4'>3.4.Performing ANOVA for Family and Loan</a>
- <a href='#4'>4. Exploratory Data Analysis</a>    
    - <a href='#4.1'>4.1. Personal loans distribution in data</a>
    - <a href='#4.2.'>4.2. Age and Income in Personal Loan</a>
    - <a href='#4.3.'>4.3. Age and Experience in Personal Loan</a>
    - <a href='#4.4.'>4.4. Family and Income in Personal Loan</a>
    - <a href='#4.5.'>4.5. Family and Mortgage in Personal Loan</a>
    - <a href='#4.6.'>4.6. Income and Mortgage in Personal Loan</a>
    - <a href='#4.7.'>4.7. CCAvg and Family in Personal Loan</a>
    - <a href='#4.8.'>4.8. Distribution of Categorical Variables across Loan and Non Loan Cusotmers</a>
	- <a href='#4.9.'>4.9. Distribution of Numerical Features across Loan and Non Loan Cusotmers</a>
- <a href='#5'>5. Data Preprocessing</a>
	- <a href='#5.1.'>5.1. Data Split into Test and Train</a>
	- <a href='#5.2.'>5.2. Normalise the data</a>
- <a href='#6.'>6. Model Building</a>
	- <a href='#6.0.'>6.0. Functions to be used across all models</a>
	- <a href='#6.1.'>6.1. KNN Classifier</a>
		- <a href='#6.1.1.'>6.1.1.Finding best K value</a>
		- <a href='#66.1.2.'>6.1.2. KNN Model</a>
	- <a href='#6.2.'>6.2. LogisticRegression Model</a>
	- <a href='#6.3.'>6.3. GaussianNB Model</a>
	- <a href='#6.4.'>6.4. DecisionTreeClassifier Model</a>
	- <a href='#6.5.'>6.5. RadiusNeighborsClassifier Model</a>
- <a href='#7.'>7. Model Performances </a>
	- <a href='#7.1'>7.1. Model Performance Metrics</a>
	- <a href='#7.2'>7.2. Compare Model Metrics</a>
- <a href='#8.'>8. Ensemble Methods </a>
	- <a href='#8.1'>8.1. Bagging Classifier</a>
		- <a href='#8.1.1.'>8.1.1. KNeighborsClassifier</a>
		- <a href='#8.1.12.'>8.1.2. LogisticRegression</a>	
		- <a href='#8.1.3.'>8.1.3. GaussianNB</a>
		- <a href='#8.1.4.'>8.1.4. DecisionTreeClassifier</a>
	- <a href='#8.2'>8.2. Boosting Methods</a>
		- <a href='#8.2.1.'>8.2.1. AdaBoostClassifier</a>
			- <a href='#8.2.1.1.'>8.2.1.1. LogisticRegression</a>
			- <a href='#8.2.1.2.'>8.2.1.2. GaussianNB</a>
			- <a href='#8.2.1.3.'>8.2.1.3. DecisionTreeClassifier</a>
			- <a href='#8.2.1.4.'>8.2.1.4. default AdaBoostClassifier</a>
	- <a href='#8.2.2.'>8.2.2. GradientBoostingClassifier</a>
	- <a href='#8.3'>8.3. VotingClassifier</a>
	- <a href='#8.4'>8.4. RandomForestClassifier</a>
	- <a href='#8.5'>8.5. lightgbm</a>
	- <a href='#8.6'>8.6. xgboost</a>
- <a href='#9.'>9. Ensemble Performances </a>
	- <a href='#9.1'>9.1. Ensemble Performance Metrics</a>
	- <a href='#9.2'>9.2. Compare Ensemble Metrics</a>",f6ae50af,0.008333333333333333
1051,37b09262279764,849b6397,"- In this challenge we need to predict whether a passenger survived or did not survived on the Titanic
- This is a <b>Classification</b> problem
- URL of the dataset https://www.kaggle.com/c/titanic",37c4c417,0.008333333333333333
1053,4ae6a182abac64,151111ee,"
 1. EXPLORATORY DATA ANALYSIS
   
   - 1.1 Libraries
   
   - 1.2 Acquire the data
  
   - 1.3 Descriptive statistics
  
   - 1.4 Data visualisation
   

2. Feature Engineering

   - 2.1 Filling missing Values
  
   - 2.2 Binning the categorical features
  
   - 2.3 Creating New Features
   
   - 2.4 Removing irrelevant variables
   
   - 2.4 Creating dummy variables
   
  

3. Pre-Modeling Tasks


   -  3.1 Defining Features in Training/Test Set
   
   -  3.2 Splitting the dataset
   

4. Modeling
 
  
   - Random Forest Model
   
   
5. Evaluating the performance of the model
     
     - Confusion Matrix
     - Classificarion Report
     - Accuracy Score
     - Precision Score
     - ROC Curve
     
6. Submission


- Useful resources
  ",418676c5,0.008403361344537815
1054,9169c4e9c33c90,c2d8a22c,"This notebook is a brief EDA on the Top 50 Amazon best-selling books dataset (https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019).

This can be considered a work in progress, so any feedback or criticism is welcome; I'd like to improve for future notebooks!",725bf880,0.00847457627118644
1055,1294fb4c86f993,8567645e,"<a id='intro'></a>
## Introduction

<br>",4471e513,0.00847457627118644
1057,49ee86d074de69,16af459a,"# Introduction
* Feature Engineering Technics

<font color = 'red'>
* <h4>If you like, Please don't forget to UPVOTE <h4>

<br>
<br>
<font color = 'blue'>
<b>Content: </b>

1. [Load Libraries](#1)
1. [Load Dataset](#2)
1. [Basic Data Analysis](#3)
1. [Feature Engineering Part-1](#4)
    * [Remove Constant Features](#5)
    * [Remove Quasi-Constant Features](#6)
    * [Remove Duplicate Features](#7)
    * [Remove Correlation Feature](#8)
    * [Remove Unnecessary Features](#9)   
1. [Observe Reason for Absence](#10)
1. [Feature Engineering Part-2](#11)
1. [Logistic Regression](#12)
    * [Standardize The Data](#13)
    * [Train-Test Split of Data](#14)
    * [Model](#15)
    * [Finding The Intercept & Coefficients](#16)
    * [Save The Model](#17)  


    
<hr>
   ",71ccc6d3,0.008547008547008548
1059,2ada0305b68956,701aec2b,"**Seaborn makes it easy to use colors that are well-suited to the characteristics of your data and your visualization goals. Seaborn arguably has one of the most rich visualization packages for python. It contains beautiful colors with powerful controls of parameters for a wide array of plots. While exploratory data analysis is one of the most important steps in the machine learning pipeline, interpretation and sharing of data can be considered an even more vital component of data science. In data visualization color is necessarily involved, and colors have an influence on their observer. In this guide we will display the full range of color palettes offered by Seaborn to give anyone intending to visualize data a comprehensive perspective of their options.**",133e26f4,0.008571428571428572
1060,f3c6048d1058e3,7c4f8287,"**Problem Statement:**

In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models.",1d9056b0,0.008620689655172414
1061,71c3c1eab0377d,8488220d,"## The First Part is Data Preprocessing & Feature Engineering and the Second Part is H2O-GBM implementation

## Important Note:  You can leave the first part because the objective of this Notebook is to learn some implementation basics of H20.",52b4e360,0.008695652173913044
1062,fe7360cddc13e5,3aa53f22,"""""""License

Copyright (c) 2020 Mustafa Batuhan

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.""""""",8979e423,0.008771929824561403
1064,6cade0b6a41ba2,c69954e4,# 1. Library Management,e6110293,0.008771929824561403
1065,c9b4e282e4e2c1,5ac86048,"Let's focus now on the task in hand. The main question is: What conclusions can we extract from this data? What can we improve? These are the basics steps to do a correct data analysis:

A)Preprocessing the data.
* What type of are the attributes?
* Are there any NaN values?
* Do we need to normalize any attribute?
* ...

B)Visualization of the data.
* Using Seaborn or pandas.

C)Studying the data. 
* Make your theories : what phenomenas seems to be happening? Is there any relation between some attributes?

D)""Play with the data"".
* Create your own attributes, make predictions, etc.

E)Conclusions.
* The main goal of this study is to extract some useful conclusions which would serve to improve the life quality!",f44d339f,0.008849557522123894
1066,a5a419dc7245b0,fe1ecb5c,"**Evaluation Metric to be Used:** 

**The evaluation metric for this analysis and model should be F1 Score.**",4279726e,0.008849557522123894
1067,ac1abfe1dfe815,8b2de9a9,# Introduction  ,6529dbcb,0.008849557522123894
1069,ab6da5994949a3,328f27b9,### Importing the Libraries,fae6b91d,0.009259259259259259
1070,fdc3afd309b850,5cfac568,"The main objective of the project was to work with a messy data set, extracted from the web and even with a relatively small dataset, build a good price predictor with a regression model.

In this notebook we will use Web Scraping technique on the Villa Real website. We will get a few features from the main page of the site. So, we will try to don't waste any information however we will spend a lot of time in the feature engineering processes to be able to build a regression model that gives us a very good score.


:)

Have fun!",966bde38,0.009259259259259259
1071,2f0f808765fc67,d2573e8b,# Read Training and Test Data,fd1f6494,0.009259259259259259
1072,c84925c8171900,5188aed3,"<h2>   
      <font color = blue >
            <span style='font-family:Georgia'>
            Table of Contents:
            </span>   
        </font>    
</h2>
<span style='font-family:Georgia'>
    <ul>
        <li><a href='#intro'>1. Introduction</a></li>
        <ul>
            <li><a href='#background'>1.1 Background</a></li>
            <li><a href='#data'>1.2 Data Dictionary</a></li>
        </ul>
        <li><a href='#libraries'>2. Python Libraries</a></li>
        <ul>
            <li><a href='#python'>2.1 Import Python Libraries</a></li>
            <li><a href='#warning'>2.2 Supress Warnings</a></li>
        </ul>
        <li><a href='#import'>3. Reading & Understanding the data</a></li>
        <ul>
            <li><a href='#import'>3.1 Importing the input files</a></li>
            <li><a href='#inspect'>3.2 Inspecting the dataframes</a></li>
            <li><a href='#nullcal'>3.3 Null Value Calculation</a></li>
        </ul>      
        <li><a href='#datacleaning'>4. Data Cleaning</a></li>
        <ul>
            <li><a href='#yearimpute'>4.1 Year Imputation</a></li>
            <li><a href='#pubimpute'>4.2 Publisher Imputation</a></li>
        </ul>
        <li><a href='#eda'>5. Exploratory Data Analysis</a></li>
        <ul>
            <li><a href='#stat'>5.1 Overall Statistics</a></li>
            <li><a href='#yearwise'>5.2 Year Wise Analysis</a></li>
            <li><a href='#publisher'>5.3 Publisher Wise Analysis</a></li>
            <li><a href='#platform'>5.4 Platform Wise Analysis</a></li>
            <li><a href='#genre'>5.5 Genre Wise Analysis</a></li>
            <li><a href='#numvar'>5.6 Global & Regional Sales Wise Analysis</a></li>
        </ul>
    </ul>
</span>",e21ff7ec,0.009345794392523364
1073,fc8e0042411c46,f41d5c5d,## Goals,af476c2a,0.009404388714733543
1074,43e60eb1362f5c,ac888d31,# Reading the DATA SET,87934234,0.009433962264150943
1078,7454fdc444df16,dc3cbbfb,"This is a forked version of the [Kernel](https://www.kaggle.com/allunia/breastcancer) by [Allunia](https://www.kaggle.com/allunia). The kernel has been modified so that some of the functions are more computationally efficient, and shorter and easier to understand. 

Instead of using deep learning, we will be using Support Vector Machines (SVMs) as a classifier combined with Principle Component Analysis (PCA). 

Some of the techniques used and considerations taken in this kernel, were directly inspired by [Joni Juvonen's](https://www.kaggle.com/qitvision) [kernel](https://www.kaggle.com/qitvision/a-complete-ml-pipeline-fast-ai).
",a7818ef5,0.009523809523809525
1081,44f6a002ecd033,a13bf0df,"**Purpose:** The purpose of this notebook is to work with machine learning techniques to make the best model for approving or denying loan applications.

**Result:** Able to produce a model with approximately 80% accuracy.",70bbe106,0.009615384615384616
1086,98a6794067932a,fefd6082,"# 1. Introduction : Présentation du contexte et du but du projet

**1.1 Mise en contexte**

Dans le cadre de ce projet de session pour le cours ""Analytique de la chaîne logistique"", notre équipe de quatre étudiants a été en mesure de mettre la main sur une base de données représentant les ventes effectuées par une chaîne de superstore se situant aux États-Unis. Pour des raisons de confidentialité, nous n'avons pas eu accès au nom exact de l'entreprise qui avait fourni ces données. Il est possible de retrouver des informations par rapport à quatre années de ventes différentes au sein de cette base de données, soit les années 2015, 2016, 2017 et 2018.

Selon les informations extraites dans la base de données, nous avons pu constater qu'il s'agissait d'une entreprise bien établie au niveau de la vente de produits de bureau. Parmi les sous-catégories présentes dans la base de données, on retrouve des accessoires, des appareils, de l'art, des cartables, des bibliothèques, des chaises, des photocopieuses, des enveloppes, des attaches, des meubles, des étiquettes, des machines, du papier, des téléphones, des outils de rangement, de l'équipement divers et des tables. Bref, on constate que ce superstore offre une gamme complète de produits afin de subvenir aux besoins de ses clients. Au niveau des clients, on constate que ceux-ci sont divisés sous trois catégories différentes, soit les entreprises, les particuliers et les bureaux à la maison. Comme nous le verrons plus tard dans ce rapport, les clients de l'entreprise se retrouvent en très grande majorité aux États-Unis à l'exception de quelques-uns se trouvant à l'international. Finalement, il nous a été possible de constater que l'entreprise offre quatre niveaux de livraison possible à ses clients, soit la journée même, la première classe, la deuxième classe ou la classe standard.

Ensuite, de manière plus précise, cette base de données à laquelle nous avions accès est divisée en 18 colonnes représentant le numéro d'identification de la commande, le numéro de la ligne, la date de commande, la date d'envoi, le type d'envoi, l'identifiant du client, le nom du client, le type de client, le pays, la ville, l'état, le code postal, la région, le numéro d'identification du produit, la catégorie du produit, la sous-catégorie du produit, le nom du produit et finalement, le montant de la vente. En ayant accès à toutes ces informations, nous étions confiants d'être en mesure de pouvoir tirer plusieurs analyses pertinentes de cette base de données. 

**1.2 But du projet**

Après avoir observé de manière globale la base de données à laquelle nous avions accès, nous avons déterminé qu'un but intéressant pour ce projet serait de créer un outil d'analyse descriptive permettant d'évaluer la situation des ventes de l'entreprise au cours des quatre années représentées dans la base de données. Par la suite, grâce à cet outil d'analyse, les dirigeants de l'entreprise pourraient être en mesure de mieux orienter leurs décisions stratégiques face à la localisation de leurs futurs centres de distribution. Pour ce faire, différents aspects sont évalués avec l'aide de cet outil d'analyse, soit les niveaux de commandes clients, les ventes et les expéditions en fonction des régions et le niveau de fidélité des clients au fil des années. Bien que cet outil d'analyse permettra aux dirigeants de l'entreprise d'effectuer eux-mêmes leurs propres constats, nous procéderons tout de même à plusieurs suggestions face aux résultats tirés de cet outil d'analyse afin d'en démontrer son utilité.

",08600fe2,0.009708737864077669
1087,52cfd66e9ec908,ddee163c,![](https://neurohive.io/wp-content/uploads/2019/07/Screenshot-from-2019-07-25-01-30-57.png),c74adcdf,0.00980392156862745
1088,7cfd96218dd933,2d64dcb4,"#### WHAT IS LONGITUDE AND LATITUDE

* Latitude and longitude are a system of lines used to describe the location of any place on Earth. Lines of latitude run in an east-west direction across Earth. Lines of longitude run in a north-south direction. Although these are only imaginary lines, they appear on maps and globes as if they actually existed.

#### ACQUISITION TIME EXAMPLE

Based on UTC time. You have to think in decimal.

GOING FROM AM TO PM

FOR EXAMPLE

* 911 = 9.11
* 1032 = 10.32
* 0034 = 00.34",7c34d96c,0.00980392156862745
1089,71b75664517244,e7c8749f,## Import Modules,fc905af5,0.00980392156862745
1090,842547b2def18c,0da24e6e,"# Titanic Data Science Solutions (Japanese ver.)


### このノートブックは [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312) の姉妹作です. 

このノートブックでは，Kaggleのようなサイトにあるデータサイエンスコンペを解くための典型的なワークフローを一通り説明します．

いくつかの同様の素晴らしいノートブックがありますが，その多くは""ExpertがExpertのために作った""ようで，「その解決法がどのように構築されたのか」という点に対する説明がスキップされています．
このノートブックの目的は，ワークフローの各ステップを順に追い，それぞれのステップを説明すると共に，問題解決の際に我々(Kaggle上級者)がとった全ての決定に対する理論的根拠を説明することです．

## ワークフローの各ステージ

コンペに対するワークフローは，""Data Science Solutions book""で述べられている7つのステージからなります．

1. 問いかけ/要件の定義． (Question or problem definition．)
2. train/testデータの取得． (Acquire training and testing data.)
3. データの補完，準備，整頓． (Wrangle, prepare, cleanse the data.)
4. パターンの特定/分析 および データの探索/調査． (Analyze, identify patterns, and explore the data.)
5. モデル，予測，問題解決(Model, predict and solve the problem.)
6. 問題解決にいたる各ステップと結論の可視化，伝達，説明 ．(Visualize, report, and present the problem solving steps and final solution.)
7. 結果の提供/提出． (Supply or submit the results.)

このワークフローは，「どのようにして，各ステージが他ステージを相補するか」に対する一般的な流れを提示していますが，例外的なユースケースもあります．

- 複数のワークフロー・ステージを結合してもよい．データ可視化によって分析してもよい．
- 提示されたものよりも簡単に，ステージを実行する．データ補完の前後にデータ分析をしてもよい．
- ワークフローの中で，ステージを複数回実行する．ステージを可視化することは，複数回行われてもよい．
- あるステージを完全にやめる．コンペのデータセットを商品化/サービス化するためのステージを提供する必要はないかもしれない．


## 問いかけ/要件の定義

Kaggleのようなコンペ・サイトは，解決すべき問題や，モデルを訓練するためのtrainデータを提供するとき，およびモデルの予測結果をtestデータに対して検証するときに，問いかける質問を定めます．
""Titanic Survival competition""における問題/質問の定義は，[Kaggleで次のように述べらています．](https://www.kaggle.com/c/titanic).

> タイタニック号事故に生存したor生存しなかった乗客を表形式にまとめたサンプルのtrainデータセットから，我々のモデルは，生存情報を含まないtestデータセットに基づいて，""testデータセットの乗客が生存したかor生存しなかったか""，を決定することができる． <br><br> Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.


我々は，問題の領域についての簡単な理解を構築したいとも思うかもしれません．これについては，[Kaggle competition description page](https://www.kaggle.com/c/titanic)で述べられています．
以下は，特筆すべきハイライトです．

- 1912/4/15，初めての航海で，タイタニック号は氷山に衝突した後に沈没した．2224名の乗客/船員のうち1502名が死亡しました．すなわち32%の生存率です．
- このような人命事故をまねいた難破の1つの理由は，乗客/船員のための十分な数の救命ボートがなかった点です．
- 沈没から生存することには運の要素もあるものの，女性・子供・上流階級などのグループは他よりも生存しやすかったようです．


## ワークフローの目標

データサイエンス・ワークフローは7つの主要目標を解決します．

- **分類する(Classifying)** <br>
我々はサンプルデータを分類/カテゴライズしたいと思うことでしょう．また，我々は異なるクラスどうしの背後関係or相関関係を理解したいと思うことでしょう．

- **関連づける(Correlating)** <br>
trainデータセットの中で利用可能な特徴量に基づいて問題にアプローチできます．データセットの中で，どの特徴量が我々の目標変数(Survived)に対して意義深く貢献するのか？統計的には，統計量と目標変数(Survived)との間に[相関関係(Correlation)](https://en.wikiversity.org/wiki/Correlation)がないでしょうか？特徴量の値が変わるならば，解決状態も変わるのでしょうか？これは与えられたデータセットの中で数値変数/カテゴリ変数の両方に対して検証されます．
ワークフロー・ステージとその次の目標のために，我々は，""生存したか""以外の特徴量の中の相関関係を決定したいと思うことでしょう．特徴量を作成・補完・訂正する際に，ある特徴量を相関させることが役立つでしょう．

- **変換する(Converting)** <br>
モデリングを行う段階では，データを前処理する必要があります．モデル・アルゴリズムの選択に依存して，すべての特徴量を数値的に等しい値に変換させる(正規化する)必要があるかもしれない．たとえばテキスト・カテゴリ変数を数値変数に変換します．

- **補完する(Completing)** <br>
データの前処理として，特徴量の中のすべての欠損値を推定する必要があります．モデル・アルゴリズムは欠損値がない場合に最もよく働くでしょう．

- **訂正する(Correcting)** <br>
我々は，特徴量の中で欠損値or不正な値のために，trainデータセットを分析するでしょうし，これらの値を訂正しようとするor欠損をもつサンプルを排除しようとするでしょう．
これを行うための1つの方法は，サンプルor特徴量の中のすべての外れ値を区別することです．また，もしある特徴量が分析に寄与しないor結果を有意に歪めるかもしれないならば，我々はその特徴量を完全に捨てるでしょう．

- **作成する(Creating)** <br>
新しい特徴量が相関関係・変換・完全性に従うように，我々は既存の特徴量or特徴量集合から新しい特徴量を作成することができます．

- **可視化(Charting).** <br>
どのように正しい可視化の描画/図表を選択するかは，データの性質と目標変数(Survived)に依存します．",b8efde6d,0.00980392156862745
1091,629f2918807a9b,e903f30e,"### Use Machine Learning and Data Sciences to help explore these ideas:

Q1: What is the best-selling book?

Q2: Visualize order status frequency

Q3: Find a correlation between date and time with order status

Q4: Find a correlation between city and order status

Q5: Find any hidden patterns that are counter-intuitive for a layman

Q6: Can we predict number of orders, or book names in advance?",be56dc84,0.00980392156862745
1092,4cd25e50c7e007,4950e1e9,"# Business Goal:

You are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.",ceb0c525,0.01
1093,b10bd75889dad9,29b8c6e7,## Data Exploration,ee00ceee,0.01
1094,83df814455f06c,d3a1737c,"**As always, I hope you find this kernel useful and your <font color=""red""><b>UPVOTES</b></font> would be highly appreciated**.",c9cff71a,0.01
1097,8ec771f5600a61,3f90004d,# IMPORTING REQUIRED FILES,48364c1f,0.010309278350515464
1098,225b4fe5d3894a,a4e41f32,"<a id='top'></a>
<div class=""list-group"" id=""list-tab"" role=""tablist"">
<h3 class=""list-group-item list-group-item-action active"" data-toggle=""list""  role=""tab"" aria-controls=""home"">Notebook Navigation</h3>

[1. Project Skeleton](#1)   
[2. Load the Data](#2)  
[3. Take a Quick Look at Data Structures](#3)   
[4. Create a Test Set](#4)    
[5. Discover and Visualize Data to Gain Insights](#5)  
&nbsp;&nbsp;&nbsp;&nbsp;[a. Visualizing Geographical Data](#5a)   
&nbsp;&nbsp;&nbsp;&nbsp;[b. Looking for Correlations](#5b)       
&nbsp;&nbsp;&nbsp;&nbsp;[c. Experimenting with Feature Combinations](#5c)     
[6.Preparing Data for Machine Learning Algorithms](#6)     
&nbsp;&nbsp;&nbsp;&nbsp;[a. Data Cleaning](#6a)     
&nbsp;&nbsp;&nbsp;&nbsp;[b. Handling Text and Categorical Features](#6b)     
&nbsp;&nbsp;&nbsp;&nbsp;[c. Column Transformers](#6c)     
&nbsp;&nbsp;&nbsp;&nbsp;[d. Transformation Pipelines](#6d)     
[7. Select and Train a Model](#7)     
&nbsp;&nbsp;&nbsp;&nbsp;[a. Training and Evaluating on Training Set](#7a)     
&nbsp;&nbsp;&nbsp;&nbsp;[b. Better Evaluation Using Cross Validation](#7b)     
[8. Fine-Tune a Model](#8)  
&nbsp;&nbsp;&nbsp;&nbsp;[a. Grid Search](#8a)     
&nbsp;&nbsp;&nbsp;&nbsp;[b. Analyse the Best Models and Their Errors](#8b)       
[9. Evaluate Your System on Test Set](#9)    
[10. References](#10)   ",4b4197b3,0.010309278350515464
1099,2a123b4e8f9433,076cea7b,# Score = 0.86286,0a082218,0.010309278350515464
1100,063a35f644e3c5,4e518dcf,> **By: Atharv Chaudhari**,1c30fb0a,0.010309278350515464
1101,840534f2908a9c,c77d548b,**1. DATA EXPLORATION**,8081c3cc,0.010526315789473684
1103,169177b6e9edea,26c17035,![image.png](attachment:image.png),ca42152f,0.010526315789473684
1105,f6648e47713411,67b88619,"# Nội dung

* [<font size=4>EDA</font>](#1)
    * [Chuẩn bị dữ liệu, lập biểu đồ](#1.1)
    * [Một số ảnh ví dụ từ tập dữ liệu](#1.2)
    * [RGB Analysis](#1.3)
    * [Parallel categories plot](#1.3)
",f4af4d1c,0.010638297872340425
1110,b0c2805cd5c087,60c65f69,"1. R&D 
   (arXiv; Github; MAG; NESTA; Scopus.)
2. Conferences
   (Ethics at AI Conference)
3. Technical Performance
   (Compute Economics; Language; Omniglot; Summary; Vision)
4. The Economy
   (Jobs; Investment; Corporate Activity)
5. Education
   (AI Brain Drain; Course Enrollment; Coursera; CRA; Faculty Diversity; Udacity)
6. Autonomous Vehicles
   (Bloomberg Philanthropy; Collision, Disengagement Report and Data, State Law)
7. Perception
   (Authenticity; Campaign to Stop Killer Robots; Congress Mentions-BloombergGov; GDELT and Google Search; Mckinsey     Government Mention; News and Web Search; Prattle; SIPRI)
8. Societal
   (Mckinsey; PwC; Quid)
9. Composite Measure
   (Global AI Vibrancy; NAISR)
",0446f327,0.011111111111111112
1115,3597174a998d4d,a5406fbb,"The dataset has 32 variables. And the task is to predict the possibility of a booking for a hotel, which means that the variables should have been obtained before customers check in or cancel the booking, **so I drop the variables reservation_status and reservation_status_date.**",276892ed,0.011111111111111112
1116,312135b445bd23,d17f7767,![](https://www.ovpm.org/wp-content/uploads/2020/03/chla-what-you-should-know-covid-19-1200x628-01.jpg),8ced381f,0.011235955056179775
1120,d1ff7e10ee0102,c4b0255b,"<b>'The most difficult thing in life is to know yourself'</b>

This quote belongs to Thales of Miletus. Thales was a Greek/Phonecian philosopher, mathematician and astronomer, which is recognised as the first individual in Western civilisation known to have entertained and engaged in scientific thought (source: https://en.wikipedia.org/wiki/Thales)

I wouldn't say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, it's easy to overlook this initial step and jump too soon into the water.

So I tried to learn how to swim before jumping into the water. Based on [Hair et al. (2013)](https://amzn.to/2JuDmvo), chapter 'Examining your data', I did my best to follow a comprehensive, but not exhaustive, analysis of the data. I'm far from reporting a rigorous study in this kernel, but I hope that it can be useful for the community, so I'm sharing how I applied some of those data analysis principles to this problem.

Despite the strange names I gave to the chapters, what we are doing in this kernel is something like:

1. <b>Understand the problem</b>. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.
2. <b>Univariable study</b>. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.
3. <b>Multivariate study</b>. We'll try to understand how the dependent variable and independent variables relate.
4. <b>Basic cleaning</b>. We'll clean the dataset and handle the missing data, outliers and categorical variables.
5. <b>Test assumptions</b>. We'll check if our data meets the assumptions required by most multivariate techniques.

Now, it's time to have fun!",2cc71c3c,0.011363636363636364
1123,81712ee7510ac5,23ae00e9,**Numbers**,c4685e79,0.011428571428571429
1124,2ada0305b68956,34801265,"**Here in this notebook we are going to see different pairplot palette color combination with IRIS data set.**<br>
**My other work on IRIS data set:** 
* **IRIS EDA: https://www.kaggle.com/sunaysawant/iris-eda**
* **IRIS PCA: https://www.kaggle.com/sunaysawant/iris-eda-pca**
* **IRIS Clustering: https://www.kaggle.com/sunaysawant/iris-eda-k-means-clustering**",133e26f4,0.011428571428571429
1125,ee9ddc756b2d4a,0f73d4b6,Data comes from kaggle: https://www.kaggle.com/ahmedhamada0/brain-tumor-detection ,e367eab3,0.011494252873563218
1126,2c5cb484988da2,ceccdf31,"### Import
Import all the libraries for start to work, since EDA until modeling creation. Additional, we import the CSV (UCI Credit Card).",d94f9784,0.011494252873563218
1127,d5f78aa381f58d,f6fb9e1c,"In this notebook we trying to predict whether a patient should be diagnosed with Heart Disease or not. This is a binary outcome:<br>
1. **Positive (1):** patient diagnosed with Heart Disease
1. **Negative (0):** patient not diagnosed with Heart Disease<br>

Multiple machine learning Models will be applied to see which yields greatest accuracy.",d60f358f,0.011494252873563218
1129,d96642860ab3dd,4056b799,Kaggle DataSets :: https://www.kaggle.com/c/titanic/data,98419d48,0.011627906976744186
1131,30fdc4a6e3c1db,cdb58de6,# 1. Loading necessary libraries,6111ddee,0.011695906432748537
1133,513ce405d7f6a3,0c72712a,# Please Upvote this notebook if you find it useful,8461e086,0.011764705882352941
1134,e93a41c03638fe,ed16790f,# **Extracting data and preprocessing :**,7363527b,0.011764705882352941
1135,869a39a3d4dea2,1a629875,"### Table of Contents
* [Setup](#setup)
* [Image Basics](#image_basics)
    * [Manipulate Image](#manipulate_image)
* [Drawing](#drawing)
* [Image Processing](#image_processing)
    * [Image Transformation](#image_transformation)
    * [Image Arithmetic](#image_arithmetic)
    * [Bitwise operations](#bitwise)
    * [Masking](#masking)
    * [Splitting and Merging Channels](#splitandmerge)
    * [Color spaces](#colorspace)
* [Histogram](#histogram)
    * [Gray Scale](#grayhist)
    * [Color Scale](#colorhist)
    * [Equalization](#equalization)",9020daf8,0.011764705882352941
1136,6a80f915608fc2,9f401ac0,"## <a id=""Index"">Index</a>
<a href=""#Diary"">Diary and Score history</a><br>
 . . .   --> <a href=""#DiaryRecent"">most recent</a><br>

<a href=""#Preliminaries"">Preliminaries</a><br>
<a href=""#DataProcessing"">Reading and Processing csv Data Files</a><br>

<a href=""#TargetSummary"">Looking at the Targets</a><br>
<a href=""#FeatureSummary"">Looking at the Features</a><br>

<a href=""#gVectors"">g-Vectors for each Target MoA</a><br>
<a href=""#tSNEfeatures"">t-SNE on Features shows some Target clusters</a><br>

<br>
<a href=""#MachineLearning"">Machine Learning</a><br>
<a href=""#HyperSearch"">Hyper-Parameter Search</a><br>
<a href=""#FeatureImportance"">Feature Importance</a><br>
<a href=""#ROC"">Model Quality and ROC</a><br>

<br>
<a href=""#OutputKaggle"">Output Kaggle Predictions</a><br>
<a href=""#TheEnd"">The End</a><br>
<br>",636938eb,0.011904761904761904
1137,87e94f864d74be,0a7827d8,![NNN.png](attachment:NNN.png),294bfe9f,0.011904761904761904
1141,9ceb7278784462,977fc6d1,# <a id='1'> 1.Importing Libraries</a>,3768a567,0.012096774193548387
1142,0b01138ad120fc,72ec583f,## Recurrent Neural Networks,0b4b72e6,0.012195121951219513
1143,9c26c5dcd46a25,a36f0fee,"## <font color=""#00afe6"">Sommaire</font>

1. [Analyses univariées](#section_1)    
    1.1. [Analyse des dates de création et modification de produits](#section_1_1)   
    1.2. [Les contributeurs à la base OpenFoodFacts](#section_1_2)     
    1.3. [Répartition des Nutriscores et ANOVA](#section_1_3)     
    1.4. [Analyse des corrélations linéaires](#section_1_4)     
2. [Analyses multivariées : Régression linéaire multivariée](#section_2)     
    2.1. [Baseline](#section_2_1)     
    2.2. [Première régression linéaire](#section_2_2)     
    2.3. [Régression linéaire avec catégorie produits](#section_2_3)          
3. [Réduction dimensionnelle](#section_3)     
    3.1. [Eboulis des valeurs propres](#section_3_1)     
    3.2. [Cercle des corrélations](#section_3_2)     
    3.3. [Projection des produits sur les plans factoriels](#section_3_3)     
    3.4. [Qualité de représentation de la réduction de dimension](#section_3_4)     ",1bbbb677,0.012195121951219513
1144,fd4017c1514157,0c7b000e,"





---",fd8f0896,0.012195121951219513
1145,faa8e6c8ab9246,7f4a2b2f,Now import train data and test data.,2bea1419,0.012345679012345678
1146,4883314a96dc34,ca524e02,# Import Libraries,50d36836,0.012345679012345678
1147,254cccd5145725,0f844f1e,"The data for this competition is provided in two files: train.csv and test.csv. The training set has 9557 rows and 143 columns while the testing set has 23856 rows and 142 columns. Each row represents one individual and each column is a feature, either unique to the individual, or for the household of the individual. The training set has one additional column, Target, which represents the poverty level on a 1-4 scale and is the label for the competition. A value of 1 is the most extreme poverty.",a49b4037,0.0125
1148,5ffe6aa38958a1,27691470,"# 1.  Objectives

## 1.1 Learning to code ML project 
Entering some Kaggle competitions may be a good idea to get started. Things I am looking to learn: 

1. More systematic use of iPython notebooks
2. Using pandas for python 
3. Data visualization
4. Creating and running a model using existing frameworks 
5. Creating and running a model from scratch (extended objective)

## 1.2 Learning how Kaggle works
Well, I am completely new to everything here .. except with some background in private Jupyter notebooks and python and numpy. 

1. Learn my way around
2. How to write a good Kernel
3. How to import datasets, train and submit results
",11f5412e,0.0125
1150,3dd4294f903768,bd7dd0b5,**First we will import some important libraries which we will use in the pre-processing and EDA**,0d89d098,0.0125
1151,fc8e0042411c46,b5db4c37,"There are quite a few goals for this case study.


Build a model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.
There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. ",af476c2a,0.012539184952978056
1157,80ad12f326ab70,2698b190,"## Problem Statement
The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America’s most vulnerable learners continue to grow.",da404a16,0.01282051282051282
1159,663bbc9eaf267b,ab992e03,![bmw-logo-car-brand-brand.jpg](attachment:6aaedf8a-3e06-4ef1-9fb9-85072b280f4f.jpg),32445529,0.012987012987012988
1164,241cf32abb22d8,e0688a25,"# Overview <a class=""anchor"" id=""1""></a> 

## Methodology

I build three classification models, K-Nearest Neighbors (KNN), Decision Trees (DT) and Naive Bayes (NB), to predict whether students pass or fail in Mathematics.

I start by transforming the dataset. The categorical features are encoded into numerical features and the whole descriptive features are scaled using Min-Max Scaling. The dataset is then partitioned into two parts at a 70:30 ratio for training and test.

Then, given the large number of columns of descriptive features after transformation, applying feature selection could be beneficial before fitting the model. I select the top 10 features by Random Forest Importance and F-Score. Then, I compare the performance of these two feature selection methods and continue with the better one for further model fitting.

After the feature selection, I train the models with hyperparameter search in a pipeline with 5-fold repeated stratified cross-validation based on the train data with full features and the same train data but only with the top 10 features selected in the previous stage. 

Stratification is necessary throughout the model fitting and selection as the binary target classes are imbalanced.

In the end, I fit the best models identified from the hyperparameter search on the test data with a 5-fold repeated stratified cross-validation and compare the model performance by a paired t-test to determine if these models yield any significant differences. The comparison is initially based on the metric area under curve (AUC), and I integrate other evaluation metrics, such as recall, precision, and F1-score, for a comprehensive and in-depth comparison.",47157066,0.012987012987012988
1165,90691864eb68c7,4ebc39e6,# 1. Importing Libraries,3555ef9b,0.012987012987012988
1166,722cd844dfbe8f,76cfd1ea,"<h1 style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"">Context</h1>

The goal of this competition, initiated by the **Radiological Society of North America *(RSNA)*** in partnership with the **Medical Image Computing and Computer Assisted Intervention Society *(the MICCAI Society)*** is to predict the methylation of the **MGMT promoter**, which is an important gene biomarker for treatment of brain tumors.

These predictions will be based on a database of **MRI *(magnetic resonance imaging)*** scans of several hundred patients.

<h1 style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"">Data</h1>

Each independent case has a dedicated folder identified by a five-digit number. Within each of these “case” folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:

- Fluid Attenuated Inversion Recovery (FLAIR)
- T1-weighted pre-contrast (T1w)
- T1-weighted post-contrast (T1Gd)
- T2-weighted (T2)

| ![brain_baner](http://www.mf-data-science.fr/images/projects/brain_tumor_types.png) | 
|:--:| 
| *Examples of the four MR sequence types included in this work* |

<h1 style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"">Acknowledgement</h1>

This Notebook is inspired from *Ammar Alhaj Ali* work :
- [🧠Brain Tumor 3D [Training]](https://www.kaggle.com/ammarnassanalhajali/brain-tumor-3d-training)
- [🧠Brain Tumor 3D [Inference]](https://www.kaggle.com/ammarnassanalhajali/brain-tumor-3d-inference)",0cedb385,0.012987012987012988
1167,917957c6c4065f,04164160,"## 상황

### 가정
1. 한국에서 유튜브 채널을 시작하려함
2. 채널의 방향을 어떻게 잡아야 인기 동영상에 갈 수 있을지 알아보고자함


### 궁금한 것
1. 인기 동영상의 기준은 무엇인가  
2. 현재 인기동영상의 전반적인 상황은 어떠한가
    2.1. views  
    2.2. likes  
    2.3. dislikes  
    2.4. comment_count  
    2.5. tag_count  
    2.6. Ratio
    2.7. title_length  
    2.8. description_length  
    2.9. 하루 평균 인기 동영상의 개수    
    2.10. 게시되고 어느 정도 지나서 인기 영상에 갔는가  
    2.11. category 별 동영상의 수  
    2.12. 채널 별 상황은 어떠한가 
3. 조회수와 연관이 있는 항목들은 무엇인가  
4. 결론

*******",55b8ed68,0.013071895424836602
1171,37e461081e47c5,be0d5d47,# Import Modules,b3e6549e,0.013333333333333334
1172,2bd6c370695ea7,4da85fde,## Data Loading,cbe6aec8,0.013333333333333334
1174,7e1da639035ac5,8bbf377d,"- <a href='#1'>1. Introduction</a>  
- <a href='#2'>2. Loading libraries and retrieving data</a>
- <a href='#3'>3. Data impressions</a>
- <a href='#4'>4. Data cleaning</a>
- <a href='#5'>5.  Data preparation</a>
- <a href='#6'>6.  Economic Need Index</a>
    - <a href='#6.1'>6.1  Distribution of Economic Need Index</a>
    - <a href='#6.2'>6.2  Economic Need Index vs. School Income Estimate</a>
    - <a href='#6.3'>6.3  Average Economic Need Index in different cities</a>
- <a href='#7'>7. School Income Estimate</a>
    - <a href='#7.1'>7.1 School Income distribution for community schools</a>
    - <a href='#7.2'>7.2 Bubble chart depicting estimated income and economic need index on map of school locations</a>
- <a href='#8'>8. Racial distribution analysis</a>
    - <a href='#8.1'>8.1 Average racial distribution in schools of different cities</a>
    - <a href='#8.2'>8.2 Comparing distribution of races with each other with Economic Need Index</a>
    - <a href='#8.3'>8.3 Racial distribution in community and private schools</a>
- <a href='#9'>9. Rigorous instruction analysis</a>
    - <a href='#9.1'>9.1 Rigorous instruction % distribution</a>
    - <a href='#9.2'>9.2 Rigorous instruction ratings statistical analysis</a>
- <a href='#10'>10. Collaborative teachers analysis</a>
    - <a href='#10.1'>10.1 Collaborative teachers % distribution</a>
    - <a href='#10.2'>10.2 Collaborative teachers ratings statistical analysis</a>
- <a href='#11'>11. ELA and math proficiency analysis</a>
    - <a href='#11.1'>11.1 ELA and math proficiency distribution</a>
    - <a href='#11.2'>11.2 ELA proficiency vs math proficiency distribution</a>
- <a href='#12'>12. Supportive Environment analysis</a>
    - <a href='#12.1'>12.1 Supportive Environment % distribution</a>
    - <a href='#12.2'>12.2 Supportive Environment ratings statistical analysis</a>
- <a href='#13'>13. Effective School Leadership analysis</a>
    - <a href='#13.1'>13.1 Effective School Leadership % distribution</a>
    - <a href='#13.2'>13.2 Effective School Leadership ratings statistical analysis</a>
- <a href='#14'>14. Strong Family-Community Ties analysis</a>
    - <a href='#14.1'>14.1 Strong Family-Community Ties % distribution</a>
    - <a href='#14.2'>14.2 Strong Family-Community Ties statistical analysis</a>
- <a href='#15'>15. Trust analysis</a>
    - <a href='#15.1'>15.1 Trust % distribution</a>
    - <a href='#15.2'>15.2 Trust ratings statistical analysis</a>
- <a href='#16'>16. KMeans clustering on explored features</a>
    - <a href='#16.1'>16.1 Elbow method</a>
    - <a href='#16.2'>16.2 KMeans clustering implementation and visualization</a>
    ",120b6c23,0.013333333333333334
1175,cb570c7b7f0501,736dc89e,"<a id='intro'></a>
## Introduction
Patients book appiontments with doctor, but when it's time to see the doctor -somehow- they don't show.
In Brazil around 20% of patient doesn't attend their appiontments with doctor after booking.
we are going to dive together into a hostpital dataset for more than 100K appointments from 29/4/2016 
till 6/6/2016 to discover what is the reason behind Not showing !?
",a200a0ec,0.013333333333333334
1177,67b7354e96113a,f6f8f4ec,"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.",dca94250,0.013333333333333334
1178,726833f92fb87a,7d81c4ec,# Campaign Success Prediction results dashboard:,7dc5e1b6,0.013422818791946308
1179,5f32117bcd5255,84a2676c,"# MATH SHEET

#### FUNDAMENTAL UNIT OF DISTANCE IN ASTRONOMY

""A star with a parallax of 1 arcsecond has a distance of 1 Parsec.""

* 1 parsec (pc) is equivalent to:

206,265 AU

3.26 Light Years

3.086x1013 km

* 1 light year (ly) is equivalent to:

0.31 pc

63,270 AU

#### GENERAL CONSTANTS

* Speed of Light = c = 3 ˆ 108 m/s
* Gravitational Constant = G = 6.67 * 10 ** -11N m2/kg2
* Mass of Earth = MC = 5.97 * 10 ** 24 kg
* Radius of Earth = RC = 6378 km
* Mass of the sun = M@ = 1.99 * 10 ** 30 kg
* Radius of the sun = R@ = 6.96 * 10 ** 5 km
* Effective Temperature of the sun = T@ = 5778 K
* Luminosity of the sun = L@ = 3.9 * 10 ** 26W
* Mass of the Moon = MK = 7.346 * 10 ** 22 kg
* Radius of the Moon = RK = 1738.1 km
* Mass of a proton = mp = 1.6726 * 10 ** -27 kg
* Mass of an Electron = me = 9.109 * 10 ** -31 kg
* Hubble’s Constant = H0 = 70 km/s/Mpc (Note: There is ongoing debate about the actual value)
* Planck’s Constant = h = 6.626 * 10 ** -34 J s
* Type Ia Supernova Absolute Magnitude = -19.3

#### GENERAL CONVERSIONS

* 1 Astronomical Unit = 1 AU = 1.5 * 10 ** 8 km
* 1 Parsec = 1 pc = 3.09 * 10 ** 13km = 3.26 ly
* 1 Light Year = 1 ly = 9.46 * 10 ** 12 km
* 1 Mega Light Year (Mly) = 10 ** 6 Light Year - 306,601 parsecs - 9.4607304725808 × 1018 km
* Arc Minute = 11 = p
* 1 year = 31.557.600 seconds
* 1 Megaton (energy) = 1.000.000 tons of TNT = 4.184 * 10 ** 15 J
* MPC = MEGAPARSEC (A megaparsec is a measurement of distance equal to one million parsecs or 3.26 million light years)

#### LIGHT INTENSITY

* LI = 1 / DISTANCE FROM EARTH ** 2

#### VACUUM FREQUENCY

f = c/λ

* f = Wave frequency (Hz)
* c = Speed of Light (m/s)
* λ = wavelength (m)

#### ORBIT ECCENTRICITY

e = c/a

* e = Orbital Eccentricity
* c = Distance from focus to center
* a = semimajor axis

#### PARALLAX

d = 1/p

* d = Distance (pc)
* p = Parallax Angle (arcseconds)

https://lco.global/spacebook/sky/using-angles-describe-positions-and-apparent-sizes-objects/

#### HUBBLE'S LAW

v = H0 . d

* v = Recessional velocity (km/s)
* H0 = Hubble’s Constant
* d = Distance

#### WHAT IS TRANSIT DETECTION

* The transit method is a photometric method that aims to indirectly detect the presence of one or more exoplanets in orbit around a star. In 1999, the method was used to confirm the existence of HD209458b, a planet that had been discovered almost at the same time by the radial velocity method. This discovery, published in 2000 in a study led by David Charbonneau, among others, was made using a 10-cm diameter telescope installed in the parking lot of a building in the United States and paved the way for a whole new field of research on exoplanets. The first new detection was OGLE-TR-56b, discovered in 2003.

![](https://i2.wp.com/www.exoplanetes.umontreal.ca/wp-content/uploads/2020/02/transit2.png?resize=768%2C458)

* The transit method consists of regularly measuring the luminosity of a star in order to detect the periodic decrease in luminosity associated with the transit of an exoplanet. The transit happen when a planet passes in front of its star. On the other hand, when the planet passes behing the star, it is called an eclipse. The effect measured during a transit is quite small. For a star the size of the Sun, the transit of a Jupiter-size planet will cause a decrease in apparent luminosity of about 1%, while this decrease will be of about 0.001% for a planet the size of the Earth.

![](https://planetary.s3.amazonaws.com/web/assets/pictures/_1200x716_crop_center-center_82_line/20130108_Planetary_transit.jpg.webp)

* How much a star dims during a transit directly relates to the relative sizes of the star and the planet. A small planet transiting a large star will create only a slight dimming, while a large planet transiting a small star will have a more noticeable effect. The size of the host star can be known with considerable accuracy from its spectrum, and photometry therefore gives astronomers a good estimate of the orbiting planet's diameter, but not its mass. This makes photometry an excellent complement to the radial-velocity method, which allows an estimate (a lower limit) of a planet's mass, but provides no information on the planet's diameter. Using both methods, combining mass and diameter, scientists can calculate the planet's density. Density, in turn, can suggest whether a planet is rocky, gassy, or in between.

![](https://planetary.s3.amazonaws.com/web/assets/pictures/_1200x581_crop_center-center_82_line/283591/20200424_TPS_20_01_Exoplanet-InfoSushi_Final_1920_Light-2.jpg.webp)

* Transit photometry is currently the most effective and sensitive method for detecting extrasolar planets. It is a particularly advantageous method for space-based observatories that can stare continuously at stars for weeks or months. It also can be performed from the ground with quite small telescopes; the TRAPPIST telescopes only have 60-centimeter primary mirrors.

* Transits provide scientists with estimates of planet diameters, a physical property not otherwise measurable. Because transiting exoplanets orbit in orbital planes that are necessarily edge-on to Earth-based observers, using both the transit method and the radial-velocity method to observe the same planet can provide the planet's mass and therefore its density and likely composition. Transits can provide scientists with a great deal of infFirst and foremost the ""dip"" in a star's luminosity during transit is directly propotionate to the size of the planet. Since the star's size is known known with a high degree of accuracy, the planet's size can be deduced from the degree to which it dims during transit.

* If the transiting planet has an atmosphere, some of the light from the star passes through the planet's atmosphere on its way to Earth. Some wavelengths of that starlight are preferentially blocked by gases in the atmosphere. By studying the spectrum of a star both during a transit and outside a transit, astronomers can find telltale dips in the spectrum of starlight that are diagnostic of the presence of atmospheric gases. Water vapor is one molecule that can be observed using transit spectroscopy.

* Finally, transit photometry searches can operate on a massive scale. Transit surveys (both ground- and space-based) can simultaneously watch as many as 100,000 stars at a time.

#### WHAT IS SEMI-AMPLITUDE

* The astrometric amplitude of the wobble of a host star induced by its companion in au is derived straightforwardly from balance of the star/planet system about its center of mass. The distance to the system then determines the angular size of the projected motion on the sky.

#### WHAT IS SURFACE GRAVITY

* The surface gravity, g, of an astronomical object is the gravitational acceleration experienced at its surface at the equator, including the effects of rotation. The surface gravity may be thought of as the acceleration due to gravity experienced by a hypothetical test particle which is very close to the object's surface and which, in order not to disturb the system, has negligible mass. For objects where the surface is deep in the atmosphere and the radius not known, the surface gravity is given at the 1 bar pressure level in the atmosphere.

* Surface gravity is measured in units of acceleration, which, in the SI system, are meters per second squared. It may also be expressed as a multiple of the Earth's standard surface gravity, g = 9.80665 m/s². In astrophysics, the surface gravity may be expressed as log g, which is obtained by first expressing the gravity in cgs units, where the unit of acceleration is centimeters per second squared, and then taking the base-10 logarithm. Therefore, the surface gravity of Earth could be expressed in cgs units as 980.665 cm/s², with a base-10 logarithm (log g) of 2.992.

* The surface gravity of a white dwarf is very high, and of a neutron star even higher. A white dwarf's surface gravity is around 100,000g (9.84 ×105 m/s²) whilst the neutron star's compactness gives it a surface gravity of up to 7×1012 m/s² with typical values of order 1012 m/s² (that is more than 1011 times that of Earth). One measure of such immense gravity is that neutron stars have an escape velocity of around 100,000 km/s, about a third of the speed of light. For black holes, the surface gravity must be calculated relativistically.


#### WHAT IS ORBITAL ECCENTRICITY

* In astrodynamics, the orbital eccentricity of an astronomical object is a dimensionless parameter that determines the amount by which its orbit around another body deviates from a perfect circle. A value of 0 is a circular orbit, values between 0 and 1 form an elliptic orbit, 1 is a parabolic escape orbit, and greater than 1 is a hyperbola. The term derives its name from the parameters of conic sections, as every Kepler orbit is a conic section. It is normally used for the isolated two-body problem, but extensions exist for objects following a rosette orbit through the galaxy.",85882abf,0.013422818791946308
1180,e4525eb0c96f28,b42d1b54,"## Introduction: Why analyze Video Game Sales?

What makes a popular video game? Publicity? Is it the genre? ESRB rating? Platform? Developer, critic evaluation, country of development or year created? Or a combination of these?

Analyzing video game sales attempts to find properties of Video Games that affect sales. The most popular games are oftentimes published by big publishers such as Nintendo and EA, however every now and then an indie/solo title published by a small company breaks the charts. If the publicity and advertising of a famous publisher isn't the only variable in a games success, then what is it that makes games popular? 

This tutorial will walk through our thought processes in data analysis and how we approach analyzing the dataset to discover the ""secret"" of a successful video game.",2093a1f1,0.013513513513513514
1182,63b44c85e32c1f,483566a8,"---
## Lists",fb9b9562,0.013513513513513514
1184,62037c5832129c,add1b367,# Make sure your Internet is On in Settings,61474350,0.013513513513513514
1185,91473a39b85068,dae05f7f,"### Business Objectives and Constraints
Predict as many tags as possible with high precision and recall.
So for this problem we should get high precision and high recall rates. 

For example, let’s assume that we have a title, description with 4 tags. If we want to predict any of the tags we should have a high precision value i.e, we have to be really sure that the predicted tag belongs to the given question. Also, we want to have a high Recal rate, which means If the tag actually supposed to be present, we want it to be present most number of times.",6e3d91c2,0.0136986301369863
1187,738bfced935b69,e3789959,"## Introduction
This Data set for more than 100000 UK used car in 10 brand with many models from this brands with date of  manufacture , fuel type, transmission,  engine size, price, etc....

Lets import libraries and loading all dataset:",2d3c592d,0.0136986301369863
1188,1eb62c5782f2d7,fdaef214,"## 1. Area di sebelah kiri point z-score
![5_1_graph_left_of_z.png](attachment:5_1_graph_left_of_z.png)",bb69f147,0.0136986301369863
1189,3d08ca7656dec0,8da3819e,"**Import necessary tools**
* pandas
* numpy
* matplotlib
* seaborn
* sklearn
* scipy",bd3f87e3,0.0136986301369863
1190,596389bed473be,59772c73,# Dataframe,5f8af156,0.0136986301369863
1191,166a62ebb4fc3a,42af4a89,"In this notebook, we are going to predict whether a breast tumor is benign or malignant based on 30 features in the dataset. This prediction can be useful in diagnosing patients with suspected breast cancer.",db48a079,0.013888888888888888
1194,fdc3afd309b850,277c211e,"<a id=""data""></a>
# Data ",966bde38,0.013888888888888888
1195,593d1d3d1df05a,b8927132,# Header Files for Pre Processing Image,bc682ffe,0.013888888888888888
1196,c01049afb6d307,59ef37ac,"# Attribute Information
* **ID**
* **Reason for absence**
    1. Certain infectious and parasitic diseases
    1. Neoplasms
    1. Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism
    1. Endocrine, nutritional and metabolic diseases
    1. Mental and behavioural disorders
    1. Diseases of the nervous system
    1. Diseases of the eye and adnexa
    1. Diseases of the ear and mastoid process
    1. Diseases of the circulatory system
    1. Diseases of the respiratory system
    1. Diseases of the digestive system
    1. Diseases of the skin and subcutaneous tissue
    1. Diseases of the musculoskeletal system and connective tissue
    1. Diseases of the genitourinary system
    1. Pregnancy, childbirth and the puerperium
    1. Certain conditions originating in the perinatal period
    1. Congenital malformations, deformations and chromosomal abnormalities
    1. Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified
    1. Injury, poisoning and certain other consequences of external causes
    1. External causes of morbidity and mortality
    1. Factors influencing health status and contact with health services.
    1. Patient follow-up
    1. Medical consultation
    1. Blood donation
    1. Laboratory examination
    1. Unjustified absence
    1. Physiotherapy
    1. Dental consultation
    1. 0 = There is no reason for absence

* **Month of absence**
* **Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6))**
* **Seasons (summer (1), autumn (2), winter (3), spring (4))**
* **Transportation expense**
* **Distance from Residence to Work (kilometers)**
* **Service time**
* **Age**
* **Work load Average/day**
* **Hit target**
* **Disciplinary failure (yes=1; no=0)**
* **Education (high school (1), graduate (2), postgraduate (3), master and doctor (4))**
* **Son (number of children)**
* **Social drinker (yes=1; no=0)**
* **Social smoker (yes=1; no=0)**
* **Pet (number of pet)**
* **Weight**
* **Height**
* **Body mass index**
* **Absenteeism time in hours**",d37d3b5d,0.013888888888888888
1198,eda49464dd6d1b,c62340d8,"Our client is an Insurance company that has provided Health Insurance to its customers now they need your help in building a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.

An insurance policy is an arrangement by which a company undertakes to provide a guarantee of compensation for specified loss, damage, illness, or death in return for the payment of a specified premium. A premium is a sum of money that the customer needs to pay regularly to an insurance company for this guarantee.

For example, you may pay a premium of Rs. 5000 each year for a health insurance cover of Rs. 200,000/- so that if, God forbid, you fall ill and need to be hospitalised in that year, the insurance provider company will bear the cost of hospitalisation etc. for upto Rs. 200,000. Now if you are wondering how can company bear such high hospitalisation cost when it charges a premium of only Rs. 5000/-, that is where the concept of probabilities comes in picture. For example, like you, there may be 100 customers who would be paying a premium of Rs. 5000 every year, but only a few of them (say 2-3) would get hospitalised that year and not everyone. This way everyone shares the risk of everyone else.

Just like medical insurance, there is vehicle insurance where every year customer needs to pay a premium of certain amount to insurance provider company so that in case of unfortunate accident by the vehicle, the insurance provider company will provide a compensation (called ‘sum assured’) to the customer.",8421f81f,0.013986013986013986
1199,631cd434fc3aa2,a9efa2d0,## Setup,2b74febb,0.014084507042253521
1200,9bcfa825c8b2e6,8496e184,![image.png](attachment:014d44d4-706d-490a-8020-48c1ec403f2e.png),220f36e4,0.014084507042253521
1202,06c7ba9203293f,86e19668,"# **“To alcohol: the cause of, and solution to, all of life's problems.” - Homer J. Simpson**

Small pay-check? sucky job? bad boss? dodgy colleagues? missed promotions?
judgy in-laws? nosy neighbors? noisy kids? 

And all you want is nothing but get away from daily dramas that you'd rather
step on a piece of Lego?
Do you want to drain your pain, and shake off your sorrows 
like a dog drenched to the bone?

Then look no further lads and lasses... You are at the right place!

'Pub Hopping in Melbourne' is a data driven guide to Pubs in Melbourne for all you legendary 
Wednesday warriors who want to crack a cold one and be yourself.

**Attribute Info:**
* id: This is a unique field. I believe it's the registration ID
* alias: alternate Name
* name: Well… name
* image_url: NA
* is_closed: status
* url: webpage address
* review_count: No. of reviews
* categories: Category under which the place is registered
* rating: Rating as on Yelp
* coordinates: coordinates of the Pub
* transactions:?
* price: How expensive is this place
* location: Address
* phone: Contact number
* display_phone: Contact number
* distance: Distance from Melbourne city centre (CBD)

**Questions:**
* Carrying out an EDA on the Dataset
* Cleaning and categorizing the Dataset
* Predicting missing Price values.


**Questions:**
* Where are these pubs located?
* How far is the farthest pub from the CBD?
* How are the pricing and reviews stacked up?



 

",1e1a2b48,0.014084507042253521
1203,d8ff894670d506,f37fbd69,"**Importing the Libraries**

**numpy** for working on arrays

**pandas** for working and creating our own datasets

**matplotlib, pyplot, cufflinks** for visualisation of results

**datetime** Datetime module supplies classes to work with date and time. These classes provide a number of functions to deal with dates, times and time intervals.

*An extensive use of **plotly and cufflinks** has been done*",eb0fb7de,0.014084507042253521
1204,bddd799cdbbae8,5098afbd,# <a id='1'> 1. Import Library</a>,b44e3c08,0.014084507042253521
1205,56785caebaa256,76b5501c,"# Acknowledgements

### Datasets:
- my dataset [COVID-19 in Ukraine: daily data](https://www.kaggle.com/vbmokin/covid19-in-ukraine-daily-data) - it is recommended to follow the updates
- official data of Ukraine (https://covid19.rnbo.gov.ua/)
- dataset [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data) (including dataset [Oxford COVID-19 government response tracker](https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker) and dataset [NOAA](https://www.ncei.noaa.gov/)) : @article{Wahltinez2020,author = ""Oscar Wahltinez and Matt Lee and Anthony Erlinger and Mayank Daswani and Pranali Yawalkar and Kevin Murphy and Michael Brenner"", year = 2020, title = ""COVID-19 Open-Data: curating a fine-grained, global-scale data repository for SARS-CoV-2"", note = ""Work in progress"",  url = {https://github.com/GoogleCloudPlatform/covid-19-open-data},} 
- my dataset with holidays data [COVID-19: Holidays of countries](https://www.kaggle.com/vbmokin/covid19-holidays-of-countries) - it is recommended to follow the updates

### Notebooks:
- notebook [COVID in UA: Prophet with 4, Nd seasonality](https://www.kaggle.com/vbmokin/covid-in-ua-prophet-with-4-nd-seasonality)
- notebook [COVID-19-in-Ukraine: Prophet & holidays tuning](https://www.kaggle.com/vbmokin/covid-19-in-ukraine-prophet-holidays-tuning)
- notebook [COVID-19 Novel Coronavirus EDA & Forecasting Cases](https://www.kaggle.com/khoongweihao/covid-19-novel-coronavirus-eda-forecasting-cases) from [@Wei Hao Khoong](https://www.kaggle.com/khoongweihao)

### Libraries from GitHub:
- https://facebook.github.io/prophet/
- https://facebook.github.io/prophet/docs/
- https://github.com/facebook/prophet
- https://github.com/dr-prodigy/python-holidays",a792961a,0.014184397163120567
1206,b61ab8f81dc03d,b4756895,"## Titanic
RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.

Titanic was under the command of Captain Edward Smith, who also went down with the ship. The ocean liner carried some of the wealthiest people in the world, as well as hundreds of emigrants from Great Britain and Ireland, Scandinavia and elsewhere throughout Europe, who were seeking a new life in the United States. The first-class accommodation was designed to be the pinnacle of comfort and luxury, with a gymnasium, swimming pool, libraries, high-class restaurants, and opulent cabins. A high-powered radiotelegraph transmitter was available for sending passenger ""marconigrams"" and for the ship's operational use. Although Titanic had advanced safety features, such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people—about half the number on board, and one third of her total capacity—due to the maritime safety regulations of those days. The ship carried 16 lifeboat davits which could lower three lifeboats each, for a total of 48 boats. However, Titanic carried only a total of 20 lifeboats, four of which were collapsible and proved hard to launch during the sinking.

After leaving Southampton on 10 April 1912, Titanic called at Cherbourg in France and Queenstown (now Cobh) in Ireland, before heading west to New York. On 14 April, four days into the crossing and about 375 miles (600 km) south of Newfoundland, she hit an iceberg at 11:40 p.m. ship's time. The collision caused the hull plates to buckle inwards along her starboard (right) side and opened five of her sixteen watertight compartments to the sea; she could only survive four flooding. Meanwhile, passengers and some crew members were evacuated in lifeboats, many of which were launched only partially loaded. A disproportionate number of men were left aboard because of a ""women and children first"" protocol for loading lifeboats. At 2:20 a.m., she broke apart and foundered with well over one thousand people still aboard. Just under two hours after Titanic sank, the Cunard liner RMS Carpathia arrived and brought aboard an estimated 705 survivors.

The disaster was met with worldwide shock and outrage at the huge loss of life, as well as the regulatory and operational failures that led to it. Public inquiries in Britain and the United States led to major improvements in maritime safety. One of their most important legacies was the establishment of the International Convention for the Safety of Life at Sea (SOLAS) in 1914, which still governs maritime safety. Several new wireless regulations were passed around the world in an effort to learn from the many missteps in wireless communications—which could have saved many more passengers.

The wreck of Titanic was discovered in 1985 (73 years after the disaster) during a Franco-American expedition and United States Military mission. The ship was split in two and is gradually disintegrating at a depth of 12,415 feet (2,069.2 fathoms; 3,784 m). Thousands of artefacts have been recovered and displayed at museums around the world. Titanic has become one of the most famous ships in history, depicted in numerous works of popular culture, including books, folk songs, films, exhibits, and memorials. Titanic is the second largest ocean liner wreck in the world, only being surpassed by her sister ship HMHS Britannic, however, she is the largest sunk while in service as a liner, as Britannic was in use as a hospital ship at the time of her sinking. The final survivor of the sinking, Millvina Dean, aged two months at the time, died in 2009 at the age of 97.

![image.png](attachment:image.png)

https://en.wikipedia.org/wiki/Titanic",64d05394,0.014184397163120567
1207,957e035ba5b9d5,c34744af,# Import necessary libraries,778ab3d3,0.014184397163120567
1209,675b60eaf415a6,374870b2,"### **Overview** 
* **Download and extract Food 101 dataset**
* **Understand dataset structure and files** 
* **Visualize random image from each of the 101 classes**
* **Split the image data into train and test using train.txt and test.txt**
* **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**
* **Fine tune Inception Pretrained model using Food 101 dataset**
* **Visualize accuracy and loss plots**
* **Predicting classes for new images from internet**
* **Scale up and fine tune Inceptionv3 model with 11 classes of data**
* **Model Explainability**
* **Summary of the things I tried**
* **Further improvements**
* **Feedback**",68c0b725,0.014285714285714285
1210,2730840089c8eb,391edc7b,# Strings,34d27dac,0.014285714285714285
1211,38b79494ac749e,d0a6e5fd,## Imports,39162a40,0.014285714285714285
1212,36c35f0a9f70f7,716f3169,## Load Modules and helper functions,67358bc7,0.014285714285714285
1214,2ada0305b68956,5f643506,**Let us start with our work on Iris Dataset**,133e26f4,0.014285714285714285
1216,9c044fa3072552,7e39ca0d,"# Data Preparation and Cleaning

- Load the file using Pandas
- Look at some information about the data and the columns
- Fix any missing or incorrect values",1362842e,0.014285714285714285
1219,548f961125248d,d490cba6,"## Imports & Read in file <a class=""anchor"" id=""first""></a>",d8c5e8b8,0.014492753623188406
1224,a1a31459abf078,e784e3ce,"
I came across this interesting competition on Kaggle about a month ago, and its been a great learning experience for me so far as I had to go through lot of challenges with the size of the data and full test dataset not being available to us. In the process of working through this competition, I got a chance to learn about GPUs and data processing and modelling libraries which use GPUs to process data faster. 

Through some of the great notebooks written so far, I came across RAPIDS framework created by NVIDIA which allows for GPU based acceleration for analytics workflows. It also allowed me to use some of my weekly quota of GPUs for the first time on Kaggle. Please read through this kernel and provide your feedback in the comments. 

",66fc0f54,0.014492753623188406
1226,7e275c8d5ff2a0,a7cc6b65,"For the prediction purpose we have use Prophet library produced by Facebook which is used for Time series Forecasting. Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data.

Prophet is available in both Python and R but in this project we have use python. Prophet() function is used do define a Prophet forecasting model in Python.  Input to Prophet is a dataframe with minimum two columns : ds and y.  ds is datestamp column and should be as per pandas datatime format, YYYY-MM-DD or YYYY-MM-DD HH:MM:SS for a timestamp and y is the numeric column we want to predict or forecast. We can get a suitable dataframe that extends into the future a specified number of days using the helper method Prophet.make_future_dataframe. By default it will also include the dates from the history.

Prophet time series = Trend + Seasonality + Holiday + error. 
 Trend models non periodic changes in the value of the time series.
 Seasonality is the periodic changes like daily, weekly, or yearly seasonality.
Holiday effect which occur on irregular schedules over a day or a period of days.
Error terms is what is not explained by the model.

",b3afcc98,0.014492753623188406
1231,3c2033cc99c12c,882cbbbd,"<b>Before we Begin:  </b>   
In order to run the code, necessary pckages are required,make sure the following packages have already been installed in your computer: 
+ Basic scientific calculation libraries: Numpy, Pandas, Matplotlib  
+ Advanced Visualization libraies: Seaborn  
+ Packages for machine learning and deep learning: Sklearn, Pytorch(GPU version)  
+ Packages for dynamic graphs: Cufflinks, Plotly

<b> Introduction </b> 
<b> Our Goals: </b>  
The goal of the project is to analyze the data distribution and build a classification model of the credit card fraud detection. 

<b> Outline: </b>  



>I. <b>Data Exploration and Data Cleaning</b><br>
>> <b>Brief view of the dataset</b><br>
>> <b>Nan value processing</b><br>
>> <b>Outlier processing</b><br>
>> <b>Combat Imbalanced Classes in the Dataset</b><br>  

>II. <b>Data Visualization</b><br>   
>> <b> Heat map<b>  
>> <b> Visualization of data distribution<b>  
>> <b> Table and barchart of statistical values<b>    
    
>III. <b>Dimension Reduction</b><br>  
>> <b>Principle Component Analysis<b><br>
>> <b>Singular Value Decomposition<b><br>
>> <b>T-distributed Stochastic Neighbor Embedding visualization<b><br> 
>> <b>Comparison between different methods<b><br>
    
>IV. <b>Classification</b><br> 
>> <b>Sampling Method to deal with the imbalanced data<b><br>
>> <b>Logistic Regression in Scikit Learn</b><br>
>> <b>Support Vector Machine</b><br>
>> <b>Deep Learning Method in Pytorch</b><br> 
>> <b>The accuracy analysis and model evaluation<b><br>  
    
>V.<b>Summary<b><br>  
>> <b>The findings during the process<b><br>
>>



<b> References: </b>
<ul> 
<li>Hands on Machine Learning with Scikit-Learn & TensorFlow by Aurélien Géron (O'Reilly). CopyRight 2017 Aurélien Géron  </li>
<li>Reference Lecture Note of SDSC2001 
",dfa22a54,0.014598540145985401
1232,9cec5ddf8b6f49,f2c3ba17,## 1.a) Load Libraries,d39fc8e7,0.014705882352941176
1233,eb0ecd6bebeb15,9bf61d0e,Aşağıda ihtiyacımız doğrultusunda kullanacağımız kütüphaneleri yükleyelim.,d7b93a60,0.014705882352941176
1236,e4c6dd957eb5ce,68d7b00c,"# Objective:
I will explore this interesting that talk about us =D

- How many users are registered on Kaggle?
- How many of them are actively on the platform?
- What's the distribution of the tiers?
- When the plataform reached the mark of 1 million registrations?
- Which are the mean average rate of registrations by day?
- Which was the peak of registrations?
- How long time in mean takes to get the Tiers?

And many more interesting questions that probably will raise",2e383665,0.014705882352941176
1238,99821bc6a45be6,fe7021c5,"# **Math 3094: Midterm Project**


**Group Members: Aaron Spaulding & Timothy O'Reilly**",b9d59346,0.014705882352941176
1239,3d905ce4828057,fea0f919,**CLTV = (Customer Value / Churn Rate) * Profit Margin**,5b006cc3,0.014705882352941176
1240,02b7e38902069e,358000c6,![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMcqi3PiHDWsNk5YrwALttBllQQdpvfgsOxg&usqp=CAU)amazon.in,726a03a0,0.014705882352941176
1243,1a222fee3089d2,168f141c,# Setup,59ab8894,0.014925373134328358
1245,21413205980558,1671f067,"# Catalog
> ## Section Ⅰ Business analyse(业务分析)
>> ###   1.1  Basic attributes(基本属性)
>> ###   1.2  Business contact（业务联系）
>> ###   1.3  Marketing activities（营销活动）
>> ###   1.4  Target data(目标数据)
> ## Section Ⅱ Basic data processing(数据基本处理)
>> ###   2.1  data description(数据描述)
>> ###   2.2  Data cleaning and filtering(数据清洗及过滤)
> ## Section Ⅲ EDA
>> ###   3.1  Job
>> ###   3.2  Balance
>> ###   3.3  Marital
>> ###   3.4  Education
>> ###   3.5  Housing and Loan
>> ###   3.6  Deposit
> ## Section Ⅳ Marketing analysis
>> ###   4.1  Deposit business user age（存款业务用户年龄）
>> ###   4.2  Deposit business user job（存款业务用户职业）
>> ###   4.3  Marketing month analysis（营销月份分析）
> ## Section Ⅴ Prediction Model
>> ###   5.1  Logistic Regression Model（逻辑回归模型）
>> ###   5.2  Random Forest model（随机森林模型）
> ## Section Ⅵ Summary",84197de0,0.014925373134328358
1246,20b372b6e4e276,48a9a11e,"# Results of analysis:
1. Outlier analysis of the best solutions on basic roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155419
2. Analysis of the predictions with the worst score=0 from roBERTa - pls. see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/155616
3. New (commit 22): **analysis of 3 or more repetitions of characters in words**",ec8b0860,0.014925373134328358
1248,ba4b3bd184acbb,cb019e84,"# Introduction

Pandas is an open source Python package that can makes working with data more intuitive and faster.

The primary Pandas objects are **Series** and **DataFrames**.

**Series** are 1D arrays where the elements have index labels.

**DataFrames** are 2D arrays with labeled columns and indexed rows, comparable to an SQL table but with more features.

This tutorial focuses on the Pandas DataFrame, so we first need to import pandas",0f5de724,0.015037593984962405
1251,ee23a565163388,112acb99,# **Symptoms**,88aacbc4,0.015267175572519083
1253,3cb96bd8eb364b,0ae534ff,## Project,3157af7e,0.015384615384615385
1254,a8c042af6b7245,11892c5c,### Loading packages,2487ac62,0.015384615384615385
1256,2d75fd881827b8,13469742,**Introduction**,107b5299,0.015384615384615385
1259,f2f2db16a2f86c,47b5f758,### **Importing Libraries and Dataset**,ffc6a115,0.015384615384615385
1260,c115e287523aab,0baadf13,"# Idea:
* Basic idea of this notebook is to use only **Image** Feature.
* Tabular data will be merged on later Notebooks. 
* **Wandb** is integrated hence we can use this notebook to track which experiemnt is peforming better and also do error analysis using **Grad-CAM** at the end.
",feb1288b,0.015384615384615385
1261,d07915a6e6992e,d872a74e,"<h1 id=""tocheading"">Table of Contents</h1>
<div id=""toc""></div>",2b912140,0.015384615384615385
1264,0932046e1f485d,511a43d1,**Importing of libraries**,218cc7a3,0.015625
1265,ff3a8ce61fab6a,ae311ff9,![images.jpg](attachment:images.jpg),9afe1654,0.015625
1269,ce9ed5e2d601d7,46fb1c31,"## Fine tuning
Fine tune the system using the hyperparameters and configs below:
* FOLD - 5, 10, 15, 20.
* SAMPLE - Set it to True for full sample run. Max sample per class.
* BEST_OR_FOLD - True: use Best model, False: use KFOLD softvote
* TPU - Only works on save version.
* selu love lecun_normal",f58a2f43,0.015748031496062992
1273,f3d5d8917ce5df,5c2ed4a1,"# Overview of this notebook! <img src = ""https://upload.wikimedia.org/wikipedia/commons/d/d1/HerdQuit.jpg"" align = ""right"" width = 300 alt=""Wrangling"">

### 1. Unpivot
The format of the data provided in this contest is not so great for data science! In order to get the historic data in the format that a model can use to make predictions requires a lot of wrangling. 

The given layout of the data is that each product/store combination is a row, and each date is a column (going out to 1900+ date columns...). It look like this:
![image.png](attachment:image.png)

###### (cows are to data as cowboys are to code? hmm...)",e45112f8,0.015873015873015872
1274,06ecf7a304c309,85e79f72,"## 1. Introduction

### 1.1 오토인코더란 무엇인가?

오토인코더는 입력을 그대로 똑같이 출력을 만드는 특별한 유형의 신경망입니다. 오토인코더는 비지도 학습으로 입력 데이터에 대한 low-level의 성질을 학습하기 위해 사용합니다. 이런 low-level 성질은 실제 데이터로 재구성하는데 도움을 줍니다. 

오토인코더는 네트워크가 입력을 예측하도록 요구하는 회귀작업이라고 볼 수 있습니다. 이 네트워크는 중간에 병목 현상이 있고, 이런 병목은 후에 디코더로 원본 데이터로 다시 변환하기 위해 필요한 입력 데이터를 효과적인 표현으로 압축해주는 효과를 가집니다.

이런 오토 인코더는 3가지 구성 요소가 있습니다.

- **Encoding Architecture** : 인코더 구조는 일련의 레이어로 구성되고, 이를 이용해 노드를 감소시킵니다. 그리고 잠재 공간 표현의 크기를 매우 줄여줍니다.

- **Latent View Representation** : 잠복 공간은 줄어든 입력 정보가 보존되는 가장 낮은 레벨 공간을 나타냅니다.

- **Decoding Architecture** : 인코딩 구조와 대칭되는 거울상이지만, 모든 레이어의 노드 수가 증가하고 궁극적으로 거의 유사한 입력을 출력합니다.

> Latent View를 저는 다른 문서에서 많이 사용하는 latent space라 생각하여 잠재공간 로 번역했습니다.

![autoencoder example](https://i.imgur.com/Rrmaise.png)

매우 섬세하게 조정된 오토인코딩 모델은 첫 번째 레이어에서 전달된 동일한 입력을 재구성 할 수 있어야합니다. 이 커널(필사)에서는 오토인코더와 그 구현 방법을 설명합니다.

오토인코더는 이미지 데이터에 많이 사용되고,  다음과 같은 사례에서 사용할 수 있습니다.

- 차원 축소
- 이미지 압축
- 이미지 노이즈 제거
- 이미지 생성
- 특성 추출",714de627,0.015873015873015872
1277,57070ad5e0f94f,049b3670,# **Importing Mandatory Libraries**,d97edc41,0.016129032258064516
1278,ad26c020235dfc,55a50979,"# Libraries
We load some standard libraries and packages of sklearn.",bf766e48,0.016129032258064516
1281,e19e307b3fd188,b41085c5,"In this notebook, you must do an exploratory data analysis , feature engineering, data treatment and application of Machine Learning models to predict the **RENT AMOUNT (R$)**.
I will follow this script:
1. Perform an EDA in order to gain insights and choose the best features;
2. Preprocess the data;
3. Test models and choose the best one;
4. Perform the final test with the chosen model.",2173955b,0.016260162601626018
1284,bcd7e398c4d0ec,153cee89,# 1. Data Exploration,77a143f6,0.01639344262295082
1285,0858e1bb3cbaca,58621a93,"This is a guide introducing potential ways of using pandas to interpret sales dataset. First of all, let's import the dataframe we are using by

**pd.read_csv(' ')**

the string in the brackets should be the route to access your file",78548374,0.01639344262295082
1288,e9b9663777db82,bac4a7d8,"Since residential real estate is a property with many different features, these properties are of great importance in determining the price. The absence of precise price determining rules in housing purchase and sale transactions causes serious price differences to occur even in the purchase and sale transactions of houses located in the same region. This situation increases the need for systems that will make price estimates according to the properties of the house.

In this dataset, the data of the houses offered for sale in Istanbul in July and October 2020 are presented. There are __34844__ records with a total of __179__ qualifications. This dataset can be used in house price estimation studies.",648e8507,0.01652892561983471
1290,541d0fa0e26b80,8bda3c9f,![image.png](attachment:image.png),a29e0f29,0.016666666666666666
1291,07f5853e4db8f8,e94f5fe4,"## Table of Content
# Preprocss 
    `- District dataset
    
     - Product dataset
     
     - Engagment dataset
 # Visualize
 
 # Train model
 # Conclusion ",d13c2c32,0.016666666666666666
1293,5b92c712910a11,98a60d4c,"# [Data Set](https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-1-for-beginners-bag-of-words)
The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.

# File descriptions
* labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  
* testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. 
* unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. 
* sampleSubmission - A comma-delimited sample submission file in the correct format.
# Data fields
* id - Unique ID of each review
* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews
* review - Text of the review
",e1d17100,0.016666666666666666
1296,f6488772605bb5,d3b249e1,# **Image Classification using Convolutional Neural Networks in PyTorch Minimal**,068d4697,0.016666666666666666
1297,396bc36edb95d3,a01ba04e,"#### Data Dictionary

1. Target: Claim Status (Claimed)
2. Code of tour firm (Agency_Code)
3. Type of tour insurance firms (Type)
4. Distribution channel of tour insurance agencies (Channel)
5. Name of the tour insurance products (Product)
6. Duration of the tour (Duration in days)
7. Destination of the tour (Destination)
8. Amount worth of sales per customer in procuring tour insurance policies in rupees (in 100’s)
9. The commission received for tour insurance firm (Commission is in percentage of sales)
10.Age of insured (Age)",965e4f8f,0.016666666666666666
1298,712198370d5521,5e6dd46a,"# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">Customer Segmentation</p>

<img src=""https://github.com/KarnikaKapoor/Files/blob/main/Colorful%20Handwritten%20About%20Me%20Blank%20Education%20Presentation.gif?raw=true"">

In this project, I will be performing an unsupervised clustering of data on the customer's records from a groceries firm's database. Customer segmentation is the practice of separating customers into groups that reflect similarities among customers in each cluster. I will divide customers into segments to optimize the significance of each customer to the business. To modify products according to distinct needs and behaviours of the customers. It also helps the business to cater to the concerns of different types of customers.


   <a id='top'></a>
<div class=""list-group"" id=""list-tab"" role=""tablist"">
<p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">TABLE OF CONTENTS</p>   
    
* [1. IMPORTING LIBRARIES](#1)
    
* [2. LOADING DATA](#2)
    
* [3. DATA CLEANING](#3)
    
* [4. DATA PREPROCESSING](#4)   
    
* [5. DIMENSIONALITY REDUCTION](#5) 
      
* [6. CLUSTERING](#6)
    
* [7. EVALUATING MODELS](#7)
    
* [8. PROFILING](#8)
    
* [9. CONCLUSION](#9)
    
* [10. END](#10)
",5882e04c,0.016666666666666666
1299,c18267b203f28a,966636e3,"### 各模型Baseline指标记录



| model |image size |  epoch | train acc | val acc| lb | 备注 | 
|:----|----|----|----|----|----|----|
|InceptionV3|512|25/25|0.9023|0.8504|0.843 | |
|ResNet50|512|23/25|0.9301|0.8658| 0.851 | |
|ResNet101|512|16/25|0.9119 |0.8667 | |之后过拟合 |
|ResNet152|512|17/25|0.9268|0.8640| - |-|
|ResNet101V2|512|19/25|0.9322|0.8400| 0.827 |之后过拟合|
|ResNet152V2|512|23/25|0.9707|0.8339|  |-|
|InceptionResnetV2|512|22/25|0.8749|0.8369|  |-|
|DenseNet121 |512| 24/25| 0.8916| 0.8755 | 0.8620 |曲线看着可以, 25轮未完全收敛|
|EfficentNetB0|300| 30/32|0.9030|0.8488|  |图片太大GPU爆内存|
|Xception|512|25/25|0.8082|0.8105|  |25轮未完全收敛|

",09ca8efb,0.016666666666666666
1300,bc058fe14d3d1b,654c0687,## 0.Before Start,d0273670,0.016666666666666666
1301,37b09262279764,259e83fc,### Importing Libraries,37c4c417,0.016666666666666666
1304,c4386b8a01d66e,c027c80b,# EDA,dc732bf5,0.01680672268907563
1305,4ae6a182abac64,4a9d79fc,# 1. EXPLORATORY DATA ANALYSIS,418676c5,0.01680672268907563
1307,dac3c8204a2d1b,c35f6537,"**Introduction**

In this notebook, I will analyse Amazon's top 50 bestselling books of each year from 2009 to 2019. ",b0d2d0dc,0.01694915254237288
1308,9169c4e9c33c90,4b6486c9,"<a id=""Top""></a>",725bf880,0.01694915254237288
1309,a81661cc35d8d2,2657cc2c,![banner-918218_1920.jpg](attachment:banner-918218_1920.jpg),3331f113,0.01694915254237288
1311,f2e5e9fb9eaaf7,244c1fc5,"[back to top](#table-of-contents)
<a id=""1""></a>
# 1 Introduction

Kaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.

The goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.

The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.

This competition will asked to predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.

Submissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target.",048e0d08,0.01694915254237288
1312,ed8009f482b380,b5d0b6b7,Predicting whether or not a person would have a stroke. The dataset is acquired from kaggle (https://www.kaggle.com/fedesoriano/stroke-prediction-dataset),e99941fa,0.01694915254237288
1313,1294fb4c86f993,98a8268c,"### The project utilizes two database files. One includes data from NICS firearm checks (loaded as `guns.csv`) and the other (`census.csv`) includes population aspects that I will try to link to the former data file.
***
### Guns.csv
### As per Github link to this database __[Github](https://github.com/BuzzFeedNews/nics-firearm-background-checks/blob/master/README.md)__ descriping The NICS system :

>Mandated by the Brady Handgun Violence Prevention Act of 1993 and launched by the FBI on November 30, 1998, NICS is used by Federal Firearms Licensees (FFLs) to instantly determine whether a prospective buyer is eligible to buy firearms or explosives. Before ringing up the sale, cashiers call in a check to the FBI or to other designated agencies to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. More than 100 million such checks have been made in the last decade, leading to more than 700,000 denials.",4471e513,0.01694915254237288
1314,b660910fcc2954,e7a9ba92,"# Prepare the analysis


## Load packages",80b74f88,0.01694915254237288
1315,149cb8d3489224,d46b7f70,"# Data preparation

## Load packages",116858e7,0.01694915254237288
1316,a077820f7ab459,af6b88f6,"## compare the result with Transfer Learning for RPS Classification 
https://www.kaggle.com/stpeteishii/transfer-learning-for-rps-classification",05a43104,0.01694915254237288
1322,49ee86d074de69,45466962,"<a id = ""1""></a><br>
## Load Libraries",71ccc6d3,0.017094017094017096
1327,434f930cb58aee,543c790f,"There are lots of things which I want to try out in this notiebook. I will try to update the notebook at least once or twice in a week based on how much time I get for this notebook. Version 8 successfully runs all the classification. In addition, Some of the things which I want to try are as fallows:

* Use of tf.data to build dataset
* Use of tf.autotune for optimal use of GPU and CPU
* a loss function based on the F1_score 
* EDA related to data which is available 
* variable thresholding for different classes",0e1d3554,0.017241379310344827
1328,1750367e54f407,59895d8f,"This notebook presents a full pipeline to load the data, apply advanced data augmentation, train an EfficientNet and use the model to predict over the test images. To make it possible to run within the allocated time for notebooks, this notebook will only present a single fold with a split of 80% for training and 20% for validation. Due to the original image size of 600x800 pixels, we will randomly crop 512x512 images from original images in order to keep the highest image resolution possible for our model training. Previous versions of this notebook used resized images and the results were extremely poor in comparison (~0.42 accuracy).",a8e655b2,0.017241379310344827
1332,84127ade6fde87,87cfd492,https://github.com/deep-learning-with-pytorch/dlwpt-code,f55d05b6,0.017241379310344827
1333,00001756c60be8,448eb224,**Импортируем необходимые для работы функции и классы**,945aea18,0.017241379310344827
1334,20e1ba19eb9b5e,5913cb73,"* **1. Loading the data**
    * 1.1 Load data
    * 1.2 Study the dataset
* **2. Data Processing**
    * 2.1 Outliers
    * 2.2 Target variable
    * 2.3 İmpute null and missing values
    * 2.4 Label Encoding
    * 2.5 Split training and valdiation set
* **3. XGBoost**
    * 3.1 Define the model
    * 3.2 Parameter tuning
* **4 Prediction and submition**
* **5. References**",4569bfc1,0.017241379310344827
1335,d42518f6cb0995,663b81bb,"## In-depth Introduction
First let's import the module and create an environment.",26913a9b,0.017241379310344827
1336,1cd8be6e679620,ba4b4cbd,![vaccine-image.jpg](attachment:vaccine-image.jpg),3ce15a43,0.017241379310344827
1339,71c3c1eab0377d,d031f858,# Part 1,52b4e360,0.017391304347826087
1342,c2a9f2fb3e1594,3b84ef56,"<a id=""ch1""></a>
# How a Data Scientist Beat the Odds
It's the classical problem, predict the outcome of a binary event. In laymen terms this means, it either occurred or did not occur. For example, you won or did not win, you passed the test or did not pass the test, you were accepted or not accepted, and you get the point. A common business application is churn or customer retention. Another popular use case is, healthcare's mortality rate or survival analysis. Binary events create an interesting dynamic, because we know statistically, a random guess should achieve a 50% accuracy rate, without creating one single algorithm or writing one single line of code. However, just like autocorrect spellcheck technology, sometimes we humans can be too smart for our own good and actually underperform a coin flip. In this kernel, I use Kaggle's Getting Started Competition, Titanic: Machine Learning from Disaster, to walk the reader through, how-to use the data science framework to beat the odds.
   
*What happens when technology is too smart for its own good?*
![Funny Autocorrect](http://15858-presscdn-0-65.pagely.netdna-cdn.com/wp-content/uploads/2016/03/hilarious-autocorrect-fails-20x.jpg)",53411c04,0.017543859649122806
1344,fe7360cddc13e5,ad3fa5fa,# Buy ‘Til You Die” Yaklaşımı,8979e423,0.017543859649122806
1345,c3498779cda661,6892f35a,# Importart Bibliotecas,0f531b65,0.017543859649122806
1347,3fb15e6e48aec2,2f037905,# Loading Data,9d1f4358,0.017543859649122806
1348,9e27af2600925c,f234d678,"## 1 - Packages ##

First, let's run the cell below to import all the packages that you will need during this assignment. 
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.
- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.
- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.",9b556435,0.017543859649122806
1350,e03eb63c1f725d,839707fb,"<a id=""top""></a>

<div class=""list-group"" id=""list-tab"" role=""tablist"">
<h3 class=""list-group-item list-group-item-action active"" data-toggle=""list""  role=""tab"" aria-controls=""home"">Table of contents</h3>

* [Introduction](#intro)
* [Data cleaning and Feature extraction](#data)
* [1.  Title - Word Clouds ](#1)
* [2.  Length - Title/Text ](#2)
* [3.  Ngrams - Title words](#3)
* [4.  Removal of stopwords](#4)
* [5.  Count Vectorizer](#5)
* [6.  Passive Aggressive Classifier Classifier for CountVectorizer](#6)
* [7.  Hyper Paramterization with Multinomial NB for CountVectorizer](#7)
* [8.  TfidfVectorizer](#8)	
* [9.  Hyperparameterization (with MultinomialNB) for TfidfVectorizer](#11)
* [11. PassiveAggressiveClassifier for TfidfVectorizer](#12)
* [12. Hashing Vectorizer](#13)
* [13. Comparison Table](#14)
* [14. LSTM](#15)
    
",e204b7e3,0.017543859649122806
1351,54004b32784b68,9db5ab78,# Data Load,27213ca9,0.017543859649122806
1354,c9b4e282e4e2c1,7cb65908,Let's get started:,f44d339f,0.017699115044247787
1355,ac1abfe1dfe815,3e5a2747,"## Objective: 
- To clean, analyze and apply a supervised models to Twitter US Airlines Sentiment  

## Content:  
- Import data and libraries.
- Data Preprocessing
- Analyzing and Visualising
- Wordcloud plots for positive, neutral, and negative tweets.
- Cleaning Tweets.
- TF_DIF
- Modeling",6529dbcb,0.017699115044247787
1356,a5a419dc7245b0,84ef7828,### Dataset Description,4279726e,0.017699115044247787
1360,f13534449a3750,1e024588,"<a id=""introduction""></a>
## Segmentation in Image Processing

In image processing and computer vision image segmentation is a very important and useful task. It consists in partitioning the image into multiple segments, or sets of pixels that belong to a patrticular class. In particular the task is to assing a particular label to every pixel in the image based on some shared characteristics, the results produces a mask, a set of segments that cover the entire or part of the image. The applications of this kind of machine learning task are huge for example in the medical imaging of face recogniction.

There are two main segmentation approches:
* **Semantic Segmentation**: all pixels belonging to a particular class are considered together as one mask.
* **Instance Segmentation**: the pixels are divided into different objects, even if belong to the same class.

In our case the problem is a Instance Segmentation one, in particular we have to match every ship in the images and find the pixels belonging to that particular class. 




",8b7f3332,0.017857142857142856
1362,6a80f915608fc2,2aebcaca,"## <a id=""Diary"">Diary and Scores History</a>
Back to <a href=""#Index"">Index</a> <br>

**(vN) LB-score** <-- These entries mark each commit and its LB score (if submitted). <br>
(v0) Read in the data and looked at the Target values: they are very sparse... Most common one has fraction 3.5%. Many ids, 40%, have 0 active targets; 51% have 1; 6% have 2; and about 3% for 3 and more targets active in an id. Compared with a random distribution there are more 2s and 3+s than expected, i.e., there's some anti-correlation between targets being active.<br>
**(v1,2) 0.02398** Expected a score something like 0.021, OK. (v2: Had to disable internet.)<br>
11Sep2020: Started looking through the Discussion posts, from oldest to most recent posted, things I learned: `cp_type=='ctl_vehicle'` are [controls](https://www.kaggle.com/c/lish-moa/discussion/180304) and have no active targets; `cp_time, cp_dose` have [6 possible combinations](https://www.kaggle.com/c/lish-moa/discussion/180588), so the same drug (may?must?) appear 6 times in ids; a very nice [EDA notebook](https://www.kaggle.com/datafan07/mechanisms-of-action-what-do-we-have-here) includes scatter-matrix plot in PC space, color-coded by cp_type: shows the controls have less feature variation.<br>
12Sep2020: Separated out the Treatment ids and modified the fraction calculation to use these (no controls) to estimate the active fractions. Also set the Test control ids to have 0s for their predictions.<br>
**(v3) 0.02363** Expected 0.02045.<br>
No obvious change in target fractions when selecting on cp_time or cp_dose, though a [cp_time effect](https://www.kaggle.com/c/lish-moa/discussion/180981) was seen by others. Posted 0,1,2,3+ actual and expected percentages to a [discussion](https://www.kaggle.com/c/lish-moa/discussion/180500).<br>
13Sep2020: Added df_aug_feats with additional useful columns, e.g., numMoA. Looked at the c features and it does seem they have similar variation [as reported](https://www.kaggle.com/c/lish-moa/discussion/181798); with this in mind, create two c-summary features: c-ave and c-std. A c-std vs c-ave plot colored by numMoA has structure. Add the c-ave,std to the test features too.<br>
14Sep2020: Added g-ave and g-std features too. g-ave seems not useful, mostly near 0. Also plotted g-std vs c-ave and c-std vs g-std; added coloring by cp_time which has patterns, coloring by cp_dose seems intermixed with no patterns. These suggest that the features: c-ave, c-std, g-std, and cp_time could be used in predicting numMoA. Use an xgb classifier to decide if numMoA=0 (y=1) based on the features: cp_time, c-ave, c-std, g-ave, g-std.<br>
(v4) (not submitted, should be the same as v3) <br>
If we identify an id as having MoA=0 then we can half, say, all 206 of its targets predictions improving its summed score contribution (by 206 x 0.005 ish ~ 1.0); if we were wrong then the one (or two) active the target(s) in that id will increase their contribution to the summed score by 0.69. So the number we can select/detect with a high precision (> 66%, say) is measure of how well we're doing.<br>
**(v5) 0.02363** Train: 233+102, 69.5% precision, ROC 0.633. Test: 35 assigned MoA=0. <br>
**(v6) 0.02363** Train: 407+149, 73.2% precision, ROC 0.642. Test: 72 assigned MoA=0.<br>
Changed xgb params: min_child_weight=1(was 4), colsample_bytree=0.8(was 0.4).   <br>
15Sep2020: The C-std vs C-ave plot of the MoA=0 ML-selected ids in (v6) seems like it is doing as well as could be expected from the few features used.<br>
Look into using the (772!) g-values, to identify specific targets? Made average g-vectors from ids with a common MoA to see if any fixed pattern results. Seems kind of variable: some MoAs have many significant g-averaged values, others have just a few.  Add two more features: fraction of gs > 2 (g_hif) and fraction < -2 (g_lof) in an id; they generally track each other. Make and look at their ratio, g_hilof: very close to 1. Do ML adding only g_hif to previous features; makes just about no change to the ML performance.<br>
(v7,8) 0.02363  Many ids with MoA=1 have c-ave, c-std of (0,0.5), the same as controls - so with these simple features we have no way to decide if MoA=1 or 0 (or even if they are control.) But there is a line-segment region of mostly MoA=0 ids in the c-std va c-ave space: what's special about these?<br>
18Sep2020: Started creating the average g-vectors of the ids having a given MoA target and the g-vector for controls. <br>
**(v9) 0.02363** No change, just saving this as a version for the record. <br>
19Sep2020: Assembled a dataframe (i.e., matrix) of the average g-vectors for each of the MoAs (targets). Using the g-vector for each MoA, we can get a ranking of which MoAs might be most predictable, the top 24 (out of 0 to 205) are: <br>
163, 136, 110, 139, 46, 142, 194, 103, 133, 63, 127, 65, 169, 34, 199, 153, 47, 35, 112, 12, 166, 36, 175, 82.<br>
In a similar way, 22 of the g features were selected that have the highest variation across the MoAs:<br>
g-392, g-100, g-158, g-50, g-231, g-91, g-744, g-75, g-37, g-175, g-257, g-178, g-672, g-38, g-489, g-332, g-411, g-65, g-761, g-723, g-58, g-131.  Including these as features for the ML classifier gives some improvement but there are still a lot sig_ids with high MoA-or-not ambiguity.<br>
**(v10) 0.02363** Train: 481+111, 81.3% precision, ROC 0.699 Test: 65 assigned MoA=0.<br>
Improved the XGB hyper-parameters, max_depth=8, learning_rate=0.05, etc.:<br>
**(v11) 0.02365** Train: 1504+85, 94.6% precision, ROC 0.849. Test: 149 assigned MoA=0.<br>
22Sep2020: Add g-5% and g-95% and run with the better hyper-parameters, but don't use the gs_to_use features (these encourage overfitting?). Note the XGB on Kaggle is different from my local version, numbers are updated for the Kaggle-version result.<br>
**(v12) 0.02364** Train: 866+137, 86.3% precision, ROC 0.758. Test: 123 assigned MoA=0.<br>
What could be a better submission for (v13) than setting all targets to 2E-15: all of the target=1 values will contribute an error score of 33.85 for an average score of ~ 4.1E-5 for each target=1 in test. So, using the score lets us determine the number of targets in the test set (yes, it's test knowledge that we should NOT use in predictions.) If proportional number of 1s in test (expect about 2800 of them), then expect a score around: 0.11500, that should make a splash on the leader board ;-)<br>
**(v13) 0.12811** (expected ~ 0.11500 when all targets = 2E-15.) So, sum(Test MoA) ~ 3125, which is about 11% more than expected.<br>
Next, same as v12 (i.e., no gs_to_use features) but with changed model params (based on local GSCV: min_child=1, max_depth=6, colbytree=0.9, learningrate=0.03, n_ests=100)<br>
**(v14) 0.02363** Train: 401+94, 81.0% precision, ROC 0.679. Test: 50 assigned MoA=0.<br>
(Small oops: n_ests was 100 for the v14 run, but should have been 120.)<br>
24Sep2020: Add in c-5% and c-95% features; replace g-hilof with g-hilopc. Look at correlations between the 22 gs_to_use: looks like a subset of 11 are most unique, so reduce gs_to_use to that subset: g-392, g-100, g-158, g-91, g-231, g-175, g-178, g-75, g-65, g-332, g-50. Look at their histograms over the 206 average target vectors.<br>
25Sep2020: Selecting and fitting each cp_time's data separately looks like it gives better results (the numerical/categorical cp_time feature was not being used well by XGB.) Use three separate models for the 3 cp_time values and combine their results for the final submission...<br>
**(v15) 0.02367** Train: 1651+185, 89.9% precision, ROC 0.821. Test: 229 assigned MoA=0.<br>
(Run on my setup: Train: 1715+214, 88.9% precision, ROC 0.832. Test: 272 assigned MoA=0.) Well, the score increased :(  <br>
OK, try more 'regularization' by setting min_child_weight = 2 and max_depth = 5, and use threshold = 0.47 for more certainty...:<br>
**(v16) 0.02363** Train: 600+66, 90.1% precision, ROC 0.767. Test: 73 assigned MoA=0.<br>
(Run on my setup: Train: 584+66, 89.8% precision, ROC 0.770. Test: 76 assigned MoA=0.) There are too few MoA=0 sig_ids that can be clearly identified and that makes a too-tiny effect on the score.<br>
27Sep2020: Based on the [t-SNE discussion post](https://www.kaggle.com/c/lish-moa/discussion/186919) and
[Notebook](https://www.kaggle.com/nelsonewert/using-t-sne-to-identify-clusters-in-the-data), I ran t-SNE on the training data to see what kind of grouping appears, color-coded output by: controls, MoA=0,1, and specific target MoA. There are some all-MoA ""islands"" in the plot and these are each dominated by a particular target,
see [t-SNE section below.](#tSNEfeatures), so expect these targets could be well identified. <br>
1-Oct2020: Selected 3 c- features to include in ML as well...<br>
**(v17) 0.02364** Train: 594+59, 90.9% precision, ROC 0.776. Test: 82 assigned MoA=0.<br>
(Run on my setup: Train: 582+65, 89.9% precision, ROC 0.768. Test: 84 assigned MoA=0.) Feels like it is over-fitting, remove many of the general g- features leaving just g-hif and g-95% plus the 11 selected specific gs (along with the 4 general cs and, now, 4 specific c features: 21 features in all.)<br>
**(v18) 0.02363** Train: 538+57, 90.4% precision, ROC 0.763. Test: 64 assigned MoA=0.<br>
(Run on my setup: Train: 531+50, 91.4% precision, ROC 0.771. Test: 57 assigned MoA=0.)<br>
<br>
 . . . 2-Oct2020: Slight ""ah-ha"" moment: I focussed on the MoA=0 ones because then I'd know that *all* of their targets should be near 0, so all of them in the row could be reduced.  I'd thought that knowing MoA=1 for a row is not so useful because we wouldn't know *which* of the MoAs is the one to 'set'. However, since this is a probability metric we can instead increase all the targets in the MoA=1 rows by some amount and reduce targets in all the other rows by some amount. So I've switched to having y=1 mean MoA >= 1 for that row. Note that the ROC stays the same when switching the 'sign' of the binary classification.<br>
**(v19) 0.02355** Train: 3161+70, 97.8% precision, ROC 0.763. Test: 486-18 assigned MoA=1.<br>
(Run on my setup: Train: 3149+61, 98.1% precision, ROC 0.771. Test: 493-20 assigned MoA=1.)<br>
 . . . Since it seems some targets are more detectable than others, limiting the MoA>0 classifier to a subset of targets improves its performance. The classification is then used to adjust the probabilities for just the targets in the subset. <br>
Using **the subset of 9 ""t-SNE island"" targets**, fewer MoA>0 are detected but larger probability corrections are made for that subset of targets - improving the score a bit more? -yes. <br> 
**(v20) 0.02284** Train: 1782+30, 98.3% precision, ROC 0.960. Test: 262-1 assigned MoA=1. <br>
(Run on my setup: Train: 1789+28, 98.4% precision, ROC 0.962. Test: 263-1 assigned MoA=1.)<br>
 . . . In v20, adjustments to the prediction probabilities were made only to the targets in the target subset;<br> now also make appropriate changes to the not-in-subset target values. (The ML is the same.)<br> 
**(v21) 0.02271** Train: 1782+30, 98.3% precision, ROC 0.960. Test: 262-1 assigned MoA=1. <br>
(Run on my setup: Train: 1789+28, 98.4% precision, ROC 0.962. Test: 263-1 assigned MoA=1.)<br>
 . . . 4-Oct2020: As an extreme version of the subset method, try **the target subset: nfkb_inhibitor, proteasome_inhibitor**; these are both very common, detectable, and often occur together, set y=1 for numSub>1 to find where both are set (there are 718 of these in the training set). This is fewer than the previous subset, but they are set much closer to 1 (less error score) than in a diluted 9-target set. Let's see...<br>
**(v22) 0.02228** Train:  712+5, 99.3% precision, ROC 0.999. Test: 138-0 assigned y=1. <br>
(Run on my setup: Train:  712+5, 99.3% precision, ROC 0.999. Test: 138-0 assigned y=1.)<br>
 . . . Made some t-SNE summary values and plots trying to identify targets with low spread in t-SNE space, not so convincing... As another subset demo use **the subset ""4: low t-SNE rms""** these are higher-counts ones in the set of 9 and have a different location in c-std--c-ave space than the ""big 2"" used in (v22).<br>
**(v23) 0.02331** Train:  820+25, 97.0% precision, ROC 0.974. Test: 91-0 assigned y=1. <br>
(Run on my setup: Train:  823+27, 96.8% precision, ROC 0.974. Test: 89-0 assigned y=1.)<br>
 . . . 6-Oct2020: Another subset: use **22 targets with an average MoA rate > 0.01**, but exclude the ""big 2"" targets since they are unique (they occur together and are more common.) Show this subset highlighted in one of the t-SNE plots too.<br>
**(v24) 0.02348** Train:  1125+30, 97.4% precision, ROC 0.778. Test: 136-1 assigned y=1. <br>
<a id=""DiaryRecent"">Recent activity:</a><br>
 . . . 10Oct2020: Put the y_yhat_plots() routine in an external file and import it. Use just **1 target, nfkb_inhibitor**... <br>
**(v25) 0.02310** Train: 716+2, 99.7% precision, ROC 0.974. Test: 137-0 assigned y=1. <br>
(Run on my setup: Train: 716+2, 99.7% precision, ROC 0.974. Test: 138-0 assigned y=1.)<br>
 . . . Some cleaning up, and set threshold for ~ 97% precision for the single-MoA classifying.  Do the **cdk_inhibitor** by itself as another example:<br>
**(v26) 0.02349** Train: 283+8, 97.2% precision, ROC 0.979. Test:  31-0 assigned y=1. <br>
(Run on my setup: Train: 283+8, 97.2% precision, ROC 0.979. Test:  31-0 assigned y=1.)<br>
 . . . Looking at other individual targets, each of the ""tSNE-9"" are very detectable (good Recall with 97% Precision) whereas other targets (even ones that have a high number of MoAs, or are expected to be 'detectable') are not clearly detected.<br>
To add some 'regularization' fit the models on 4x the X,y with added random noise in c- and g- features, use std = 0.5.<br>
 . . . The subset of **16 targets that have a > 0.01 (but are not in the tSNE-9)** have very little signal in the features. Use all 22 of the initially selected ""gs_to_use"" and set a 95% precision (threshold 0.311(me), 0.319(kaggle)):<br>
**(v27) 0.02358** Train: 872+41, 19% Recall @ 95.5% Precision, ROC 0.865. Test: 100-12 assigned y=1. <br>
(Run on my setup: Train: 954+49, 21% Recall @ 95.1% Precision, ROC 0.856. Test: 104-8 assigned y=1.)<br>
 . . . The subset of **7 g-detectable w/o tSNE-9** have g-vectors with signal, do them at 95% precision:<br>
**(v28) 0.02354** Train: 417+22, 56% Recall @ 94.9% Precision, ROC 0.972. Test:  50-0 assigned y=1. <br>
(Run on my setup: Train: 417+22, 56% Recall @ 94.9% Precision, ROC 0.972. Test:  50-0 assigned y=1.)<br>
 . . . Feels like the 22 gs_to_use don't have much information on the previous 16(v27) and 7(v28) target sets. Look at the g-vectors just for these 23 targets and find the ones with most variation across them and add those g-vectors to the features (10 more addded.)  Re-do v28 with these extra gs-added:<br>
**(v29) 0.02353** Train: 485+23, 65% Recall @ 95.5% Precision, ROC 0.978. Test:  56-0 assigned y=1. <br>
(Run on my setup: Train: 484+25, 65% Recall @ 95.1% Precision, ROC 0.980. Test:  59-1 assigned y=1.)<br>
. . . Use different hyper-parameters: min_child=2, maxdepth=10, colsample=0.8; go back to the 11 gs_to_use (instead of 22) and include the 10 more 'do-better' features:<br>
 **(v30) 0.02363** Train: 667+35, 89% Recall @ 95% Precision, ROC 0.998. Test:  97-2 assigned y=1. <br>
   . . . Well, that is overfitting... use child=4, depth=8, colsample=0.7:<br>
**(v31) 0.02354** Train: 505+26, 68% Recall @ 95% Precision, ROC 0.988. Test:  64-2 assigned y=1. <br>
<BR>
 . . . OK, go back to using all targets to classify as MoA or notMoA<br>
**(v32) 0.023??** Train: 5768+65, 40% Recall @ 99% Precision, ROC 0.913. Test: 852-80 assigned y=1. <br>
 . . . **HALT**",636938eb,0.017857142857142856
1363,3cd78d8d6d56e4,3f3e1d1b,"# Introduction - MNIST Training Competition
Link to the topic: https://www.kaggle.com/c/digit-recognizer/data

This is another Notebook to take a look into annother algorithm. Here I want to give the Deep Neural Network with the Framework Keras a try. As already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform/prepare the data rightaway.

If you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:
- Another MNIST Try: https://www.kaggle.com/skiplik/another-mnist-try
- First NN by Detecting Handwritten Characters: https://www.kaggle.com/skiplik/first-nn-by-detecting-handwritten-characters
...


",9f632e94,0.017857142857142856
1365,f0fab078f8533b,68c4a836,## 1. Importing required packages,bdb5ea32,0.01818181818181818
1366,016abae0483764,b5ded1c6,"We will be using a dataset of simple covid-19 based attributes. The dataset is simple with alphanumeric values..
<br></br>
Let's start with importing the libraries",bc9f289b,0.01818181818181818
1371,ac04ba639d1c93,b5a7b68d,"# Predicting Molecular Properties


####  Updates:
 > Predicting Fc as before, but also pdo,dso and sd<br>
 > Create a sum of predictions (pdo,dso,sd and fc)
 > Create and shift for FC, because i notice that it had a offset compared with scc.<br>
 > Create some Extra Features:<br>
    * closest <br> 
    * coseno <br>
    * Tryed Dihedral Angle but had poor score.
     


<h3 style=""color:red"">If this Kernel Helps You! Please UP VOTE! 😁</h3>

<h3> Can you measure the magnetic interactions between a pair of atoms? </h3>

This kernel is a combination of multiple kernels. The goal is to organize and explain the code to beginner competitors like me.<br>
This Kernels creates lots of new features and uses lightgbm as model",748059d5,0.018518518518518517
1375,fdc3afd309b850,9f46ffe8,"The data that we will use was extraction using Web Scraping technique from Viva Real Website, one of the Brazilians biggest real estate seller websites. We extracted information about apartments from all over Brasília, the capital of Brazil. After query about apartments in Brasília in the website each container shows:
The address, a title, the area in m², the number of rooms, number of bathrooms, number of garages, the condo fees, the price and amenidades items as shown in the image below.

<br>
<br>


![Screen%20Shot%202020-12-02%20at%2017.51.15.png](attachment:Screen%20Shot%202020-12-02%20at%2017.51.15.png)

<br>
<br>

Ps: Most of the time those informations are not completely filled for the apartaments owners.",966bde38,0.018518518518518517
1378,135122550b6483,8b861ba1,"Reference for EDA:
https://www.kaggle.com/ambrosm/tpsjan22-01-eda-which-makes-sense
",6592d6d8,0.018518518518518517
1379,c84925c8171900,011f24da,"<a id=""intro""></a>
<h2>   
      <font color = blue >
            <span style='font-family:Georgia'>
            1. Introduction:
            </span>   
        </font>    
</h2>",e21ff7ec,0.018691588785046728
1380,fdbbd573ba31c2,8f5bfeaa,# Exploratory Data Analysis ,f7c28d74,0.01875
1382,43e60eb1362f5c,b10a1621,"
In this notebook, I developed a model aimed at predicting flight delays at the Destination Airport. The purpose is to create a Data set which can be used for visualization and model Building so as to predict the Delays of Flights. I did the visualization so as to get better inferences about the data. For, model fitting I have seperated the Dataset into training Data and Testing Data so that prediction can be done on the testing Data. I also showed how to import Tableau and make visualization more crisp and clear.

Technical aspect Covered:

visualization: matplolib, seaborn, Tableau

data manipulation: pandas, numpy

modeling: sklearn

class definition: regression, Boosting, Bagging

For EDA I used some part of Python coding and Tableau Visulization so as to get a brief insight and inference from the data. Various Plots are created so as to get a great idea of whats happening in the Dataset and what is the most important variable affecting the dalays of the airlines. Feature scaling is a method used to normalize the range of independent variables or features of data and this concept is used.

",87934234,0.018867924528301886
1383,07bfec3562f9b3,6b76a45c,### Imports,327e7d5b,0.018867924528301886
1385,f3c8651cb08234,8e9e59f3,# Car Price Prediction,37f86e36,0.018867924528301886
1387,510b8303776bb6,cb85ed3f,# Importing important libraries,18080db8,0.018867924528301886
1388,4dd47072617594,379555bb,"In this kernel I explore some fundamental NLP concepts and show how they can be implemented using the increasingly popular nltk package in Python. This post is for the absolute NLP beginner, but knowledge of Python is assumed.

> **The two significant libraries used in NLP are NLTK and spaCy. There are substantial differences between them, which are as follows:**

- NLTK provides a plethora of algorithms to choose from for a particular problem which is boon for a researcher but a bane for a developer. Whereas, spaCy keeps the best algorithm for a problem in its toolkit and keep it updated as state of the art improves.
- NLTK supports various languages whereas spaCy have statistical models for 7 languages (English, German, Spanish, French, Portuguese, Italian, and Dutch). It also supports named entities for multi language.
- NLTK is a string processing library. It takes strings as input and returns strings or lists of strings as output. Whereas, spaCy uses object-oriented approach. When we parse a text, spaCy returns document object whose words and sentences are objects themselves.
- spaCy has support for word vectors whereas **NLTK does not**.
- As spaCy uses the latest and best algorithms, its performance is usually good as compared to NLTK. As we can see below, in word tokenization and POS-tagging spaCy performs better, but in sentence tokenization, NLTK outperforms spaCy. Its poor performance in sentence tokenization is a result of differing approaches: NLTK attempts to split the text into sentences. In contrast, spaCy constructs a syntactic tree for each sentence, a more robust method that yields much more information about the text.

I used NLTK for this kernel. You can find these steps in below.
> 1. Text Preprocessing
> 2. Text Visualization
> 3. Sentiment Analysis
> 4. Feature Engineering
> 5. Sentiment Modeling",44ff1d11,0.018867924528301886
1389,f015d0147e8fbf,8f4f9f54,"## Summary of My Kernel

Single model LightGBM with 1,275 features in total. Solo submission. This was my best performing kernel, though not my final submission (more later on as to why this was the case). `0.79581` local CV score. `0.79524` private LB score. `0.79831` public LB score:


* I incorporated features from all seven of the data tables. I one-hot encoded categorical features from the bureau and previous application data tables. All other categorical features were left as categorical, and were ultimately target encoded. Some categorical features in the bureau and previous application tables were also target encoded.


* I tried my best to isolate unhelpful features and drop them. This proved to be time-consuming and it was soon clear to me that the process of vetting features one-by-one would not scale to the size of the competition's featureset. I had tried using LightGBM and SelectKBest feature importances to guide me, but found that they were more of a red herring than anything else, in that those importances did not reliably predict how a feature's absence would affect my local CV's ROC AUC score.


* I experimented with various types of scaling and normalization of numerical features, such as log-normalization, replacing NaN entries with 0, -1, -999999, etc., but found that none of these tweaks helped the performance of my LightGBM model. (This makes sense since LGBM is a tree-based model.)


* Several of my engineered features were simple aggregations, using mean, sum, min, max, etc. At the same time, I created a handful of bespoke features that made intuitive sense to me. 


* Stratified 5-fold CV for model selection.


* In addition to trying to add/remove features one-by-one, at times I tried adding/removing features in bulk (in the interest of saving time). It's not clear to me if one approach is necessarily better than the other all the time.

  I would find that there were times I would add one feature, see my CV score drop, then add several more features, re-tune my model parameters, see the CV score improve, then try and remove that first feature that had originally lowered my CV score, only to see the CV score fall after the feature was removed. This experience only strengthened my hunch that successful data science has an element of artfulness and intuition.
  

* I used LightGBM's built-in CV for feature selection and hyperparameter tuning. It trains and makes predictions on each validation fold in parallel, so much time is saved over running serial K-fold CV. Unfortunately, lightgbm.cv doesn't currently support the kind of preprocessing inside CV folds that would be necessary to properly perform target encoding during CV without leakage. (Believe me, I tried.)


* I found that using target encoding added just under 0.001 to my local CV and public LB scores. (Because lightgbm.cv doesn't support target encoding preprocessing for each fold, I had to use standard serial K-fold CV to do an apples-apples comparison of the performance of target encoding vs. merely using lightgbm's default categorical feature handling.)

  Interestingly, it ultimately turned out that the private LB score of my model that used target encoding (with number of boosting rounds determined by serial CV) was only just under 0.0001 better than that of my model that used LightGBM's default handling of categorical features (and had its number of boosting rounds determined by lightgbm.cv). If I had it to do over again, I'm not sure I'd use target encoding.
  

* To generate test set predictions, I trained my final model five times, each time using a different random seed for my LightGBM parameters, and generated five sets of test predictions. I used mean ranking to blend the sets of predictions. The number of boosting rounds was equal to 110% of the average round of highest score across each of the CV folds.

  Had I not used target encoding, I could have used lightgbm.cv and found the number of the actual single round where the average CV score across all folds was the highest. As it was, I had to settle for finding the round number of highest score for each of my five CV folds, and then take the average of those five round numbers.


* I didn't experiment with any other sort of ensemble methods such as blending different model types, or stacking. I am saving that for my next competition :)


### A Lesson I Learned About Overfitting to the Training Set:
As mentioned above, although this was my best performing kernel, it was only my second-to-final submission to the competition. In the final four days of the competition, I went on a spree of feature engineering/aggregation that more than doubled my feature count, arriving at a grand total of 2,782 features. This of course substantially decreased the speed of my data preprocessing and model training, but it did increase my local CV score from `0.79581` to `0.79665`. I figured the higher score was worth it and tagged that set of predictions as my final submission.

Unfortunately, while doing this raised my public LB score from `0.79831` to `0.80003`, it turned out that it would eventually lower my private LB score from `0.79524` to `0.79506`. Thankfully, this wasn't too huge a drop, and I was still able to achieve a bronze medal and score within the top 8% of the competition with my `0.79506` submission. 

In retrospect, although my local CV score did increase after adding all those extra features, I should have been suspicious because the difference between training and validation scores on each fold of my CV increased by nearly 50%. This was likely evidence that my model was now doing some serious overfitting in order to achieve that slight bump in local CV score. ",518954fb,0.018867924528301886
1392,04bac111ffbe9c,ea002e4c,"# Data Exploration
*  Describing the data
*  Finding missing values
*  Quality, completeness and tidiness issues
    -  Search if any data type is falsely assigned.
*  Cocluding relations between features. (heatmap)
    -  Distplot for numerical data
    -  Countplot for categorical data
*  Outlier detection",82576b17,0.01904761904761905
1394,7454fdc444df16,6b300ccb,"# Motivation 

### My aunt has recently been diagnosed with breast cancer, this kernel is dedicated to her in the hopes that I can learn from this experience and one day build an improved version of this classifier.

",a7818ef5,0.01904761904761905
1395,7a058705183598,e4b2c04b,Removing ID from iris dataframe,b0ead917,0.01904761904761905
1396,bbaa07ad21cf4e,ffdc72f6,"## 2. Download data <a class=""anchor"" id=""3""></a>


",3ab6b254,0.01904761904761905
1397,582cb872d19026,9150c9e5,"## Introduction

As it is known, Google Play Store allows many games to be downloaded by users. Thousands of users install millions of apps every day. The Play Store offers millions of applications in many categories to users for free or paid.
To determine which games are more popular among users, we need to examine the ratings users give for games.
Thus, Google can examine which of the games it offers is more popular, the effects of the price of the game on its popularity and make improvements.

In addition, application performance and efficiency can be increased by developers. The analysis will not only be useful for developers, but also the user. The content of Google play store apps will be characterized on a certain scale. Thus, as a result of this analysis, it can be determined that it will be profitable to upload advertisements to the game. ",8d966d69,0.019230769230769232
1399,44f6a002ecd033,ec2bd2b0,"## Table of Contents

* **[Analyzing the Data](#Analyzing-the-Data)**
* **[Cleaning the Data](#Cleaning-the-Data)**
* **[Modeling the Data](#Modeling-the-Data)**",70bbe106,0.019230769230769232
1402,d0f6276d5b628c,ff17e8c8,# UPVOTE if you like this notebook :),c64f5ce5,0.019230769230769232
1403,6f1481148352e9,64ee535d,"# Introduction

**Analyzed data by:**
* year - the year when the fires occurred;
* state - the state in which the fires occurred;
* month - month in which the fires occurred;
* number - the number of fires;
* date - date of fire.


**In the process, I will add the following data:**
* longitude;
* latitude;
* half_year - half year in which the fires occurred;
* season - the season of the year in which the fires occurred.

**I'm going to analyze the data by state, by period and by ""season"".**",7cfbdb8f,0.019230769230769232
1407,98a6794067932a,df7982f6,"# 2. Méthodologie : Application et présentation de l'outil analytique

Dans la deuxième partie de ce rapport, nous allons vous présenter les différents codes utilisés afin de procéder à nos analyses. Chacune de ces cellules de code sera expliquée en détail dans le but d'expliquer le fonctionnement des différentes lignes de codes. L'analyse des différents résultats obtenus grâce aux nombreuses lignes de codes sera plutôt effectuée lors de la partie 3 de ce rapport.

**2.1 Initialisation du document**

Dans cette section, nous allons explorer les différentes étapes effectuées afin d'importer la base de données csv et les différents modules de python qui nous permettront d'élaborer nos analyses dans les sections suivantes.
",08600fe2,0.019417475728155338
1409,52cfd66e9ec908,05dd96c2,"### Credits:

**https://www.kaggle.com/t3nyks/lyft-working-with-map-api**<br>
**https://www.kaggle.com/jpbremer/lyft-scene-visualisations**<br>
**https://www.kaggle.com/pestipeti/pytorch-baseline-train**",c74adcdf,0.0196078431372549
1411,1a0bd2f72bbe36,3560a95e,"<img style=""float: center;""  src=""https://static.amazon.jobs/business_categories/15/thumbnails/Amazon_Books.jpg?1465411410"" width=""800px"">",2fa311dc,0.0196078431372549
1413,fa02c409161192,84ae9af5,"This is my first go at creating a neural network (NN) to create a classifier for the MNIST dataset. The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.

The MNIST database contains 60,000 training images and 10,000 testing images.",e97077f7,0.0196078431372549
1415,7cfd96218dd933,1c9a8a81,"# AREA OF INTEREST

* 40.25528   38.01956,

* 38.89297   38.50296,

* 36.82755   38.54691,

* 35.06973   38.41507,

* 33.75137   37.75589,

* 32.91641   37.22855,

* 30.85098   37.668,

* 29.66446   38.01956,

* 28.08243   37.93167,

* 27.11563   37.84378,

* 25.79727   37.668,

* 25.09415   38.23929,

* 24.1713   38.98636,

* 22.32559   38.85452,

* 19.82071   38.37112,

* 18.41446   37.88773,

* 16.65665   38.23929,

* 14.98672   38.6348,

* 13.14102   38.72269,

* 10.76797   38.32718,

* 8.83438   37.88773,

* 8.30704   36.92093,

* 8.13126   35.77835,

* 8.61465   35.07523,

* 8.6586   33.49319,

* 9.18594   32.61429,

* 10.89981   31.95511,

* 12.8334   31.60355,

* 14.15176   30.98831,

* 15.86563   30.54886,

* 17.75528   29.93362,

* 18.76602   29.49417,

* 20.30411   29.66995,

* 21.18301   30.32913,

* 21.7543   30.98831,

* 23.11661   31.25198,

* 24.34708   31.12015,

* 25.0502   31.60355,

* 27.07169   30.68069,

* 28.47794   30.02151,

* 30.54337   29.84573,

* 32.78458   29.14261,

* 34.89395   29.7139,

* 35.64102   30.94437,

* 35.11368   32.30667,

* 35.59708   33.36136,

* 35.77286   34.32816,

* 36.25626   35.42679,

* 37.57462   35.73441,

* 38.89297   35.60257,

* 40.69473   35.38284,

* 41.39786   36.61331,

* 41.79337   37.36038,

* 40.25528   38.01956",7c34d96c,0.0196078431372549
1418,917957c6c4065f,63c09127,# 전처리 ,55b8ed68,0.0196078431372549
1420,d0080e3a39bc5c,507467be,**INTRODUCTION**,2fcde4cf,0.0196078431372549
1422,64169805aacf17,d4ea9ad4,"### **Let us know if you have any comments, questions, or want to share your creations. Cheers! Ernesto [@vedax](https://twitter.com/vedax/)**

# Intro

Artists combine colors and brushstrokes to paint their masterpieces, they do not create paintings pixel by pixel. However, most of the current generative AI Art methods based on Machine Learning (ML) are still centered to teach machines how to ‘paint’ at the pixel-level in order to achieve or mimic some painting style, e.g., GANs-based approaches and style transfer. This might be effective, but not very intuitive, specially when explaining this process to artists, who are familiar with colors and brushstrokes.

Our goal is to teach a machine how to paint using a combination of colors and strokes by telling it in natural language what to paint. How can we achieve this?

# Materials and Approach
We need two basic ingredients:

1. A ML model that knows how to paint using colors and strokes. To this end we will use a *Neural Painter* <a href=""#neural-painter-diavlex"">[1]</a>,<a href=""#neural-painter-reiichiro"">[2]</a>

1. A ML model that connects text with images, that is, it should be able to associate text with  visual concepts. We use CLIP (Contrastive Language–Image Pre-training) for this task. <a href=""#clip"">[3]</a>


## TL;DR 

The following steps capture the essence of our idea, note that we use pseudocode based on Python, which does not necessary reflect the models' API. Please have a look to the notebook itself for the actual code and methods used. 

1. Specify what to paint, e.g., 
`prompt = ""black sheep""`

1. Encode the text using CLIP's language portion to obtain the text features 
`text_features = clip_model.encode_text(prompt)`

1. Initialize a list of brushstrokes or `actions` and ask the `neural painter` to paint on a canvas. At the beginning the canvas will look random.
`canvas = neural_painter.paint(actions)`

1. Use the vision portion of the CLIP model to extract the image features of this initial `canvas`.
`image_features = clip_model.encode_image(canvas)`

1. The goal is to teach the neural painter to modify the strokes (i.e., its actions) depending on how different is what it is painting to the initial text request (`prompt`). For example, in the perfect case scenario, the cosine similarity between the text and image feature vectors should be 1.0.
Using this intuition, we use as the loss to guide the optimization process the the cosine distance, that measure how different the vectors are. The cosine distance in our case corresponds to 
`loss = 1.0 - cos(text_features, image_features)`.

1. We minimize this loss adapting the neural painter `actions` that in the end should produce a canvas as close as possible to the original request.

# Enjoy Neural Painting! ;) 

---
## References

<a id=""neural-painter-diavlex""></a>
[1] *The Joy of Neural Painting*. Ernesto Diaz-Aviles, Claudia Orellana-Rodriguez, Beth Jochim. Libre AI Technical Report 2019-LAI-CUEVA-X01. 2019. 
* Paper: https://arxiv.org/abs/2111.10283 
* Blogpost: https://www.libreai.com/the-joy-of-neural-painting/
* Code Repo [MIT License]: https://github.com/libreai/neural-painters-x – Part of the code is used in this notebook.

<a id=""neural-painter-reiichiro""></a>
[2] *Neural Painters: A learned differentiable constraint for generating brushstroke paintings*. Reiichiro Nakano. 2019. 
* Paper: https://arxiv.org/abs/1904.08410
* Blogpost: https://reiinakano.com/2019/01/27/world-painters.html
* Code Repo [MIT License]: https://github.com/reiinakano/neural-painters-pytorch – Part of the code is used in this notebook.

<a id=""clip""></a>
[3] *Learning Transferable Visual Models From Natural Language Supervision. CLIP.* Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. OpenAI. 2021. 
* Paper: https://arxiv.org/abs/2103.00020
* Blogpost: https://openai.com/blog/clip/
* Code Repo [MIT License]: https://github.com/openai/CLIP


",1f12ded0,0.0196078431372549
1423,2ada0305b68956,27aa30a3,**Load the dataset now**,133e26f4,0.02
1426,0687cd5c8597db,ff91dd1b,"<img src=""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRgEwrABViJwTOdkOPaFYAC0qAtZwr3FZmK6g&usqp=CAU"" 
     style=""display: block;margin-left:auto;margin-right:auto;width:50%;""/>",4edec76a,0.02
1427,4cd25e50c7e007,c9a01617,![0_Dgkc35WIkcAVHbMC.jpg](attachment:0_Dgkc35WIkcAVHbMC.jpg),ceb0c525,0.02
1429,7dd46c750653eb,7fe391c4,"Thank you Vitaliy Malcev for providing this dataset. 

**Objectives**:
* To find the seasonality of Birth and Death Rate in Moscow
* To find seasonality of Marriages and Divorces in Moscow",c2644713,0.02
1430,10c5a39a87c47e,9449c629,"In this kernel, I have shown the stepwise process of creating Convolution Neural Network Model using keras library for classifying images of Normal and Malaria parasite infected human blood sample. 

Apart from that, I have also deployed a flask based web app. The link for **LIVE WEB APP** : [Malaria Detection Web App ](https://malaria-detection-app.herokuapp.com/).Github repo :[malaria_detector](https://github.com/sid321axn/malaria-detection-app)   .Below is the snapshot of the web app deployed over internet.
![](https://i.ibb.co/gM7qfp4/malaria.png)

## Stepwise Approach
Following steps have been followed while building the CNN based model
1. [Step 1 : Importing Essential Libraries](#step-1)
2. [Step 2 : Loading data](#step-2)
3. [Step 3: EDA -> Checking sample of Infected and Uninfected images](#step-3)
4. [Step 4: Data Preprocessing (Labeling & Resizing of images)](#step-4)
5. [Step 5: Train Test Split](#step-5)
6. [Step 6: Normalization](#step-6)
7. [Step 7: Label Encoding](#step-7)
8. [Step 8 : Model Building: CNN](#step-8)
9. [Step 9: Compiling the model](#step-9)
10. [Step 10: Setting Callbacks](#step-10)
11. [Step 11: Model Fitting](#step-11)
12. [Step 12: Model Evaluation (Testing accuracy, confusion matrix, classification report)](#step-12)
13. [Step 13: Plotting ROC-AUC Curve](#step-13)
14. [Step 14: Plotting Sample Prediction (Groundtruth vs Prediction)](#step-14)
15. [Conclusion & Future Improvements](#conc-fut)
",09c7337a,0.02
1431,83df814455f06c,d32d6247,"<a class=""anchor"" id=""0.1""></a>
# **Table of Contents**


1.	[Introduction to Decision Tree algorithm](#1)
2.	[Classification and Regression Trees](#2)
3.	[Decision Tree algorithm terminology](#3)
4.	[Decision Tree algorithm intuition](#4)
5.	[Attribute selection measures](#5)
    - 5.1 [Information gain](#5.1)
    - 5.2 [Gini index](#5.2)
6.	[Overfitting in Decision-Tree algorithm](#6)
7.	[Import libraries](#7)
8.	[Import dataset](#8)
9.	[Exploratory data analysis](#9)
10.	[Declare feature vector and target variable](#10)
11.	[Split data into separate training and test set](#11)
12.	[Feature engineering](#12)
13.	[Decision Tree classifier with criterion gini-index](#13)
14.	[Decision Tree classifier with criterion entropy](#14)
15.	[Confusion matrix](#15)
16.	[Classification report](#16)
17.	[Results and conclusion](#17)
18. [References](#18)
",c9cff71a,0.02
1434,726833f92fb87a,7adc7578,"**NOTE: The dataset contains lots of missing, unknown or 'other' values for categorical features, which is bad for prediction. It is recommended to the company to collect better the data and have less uncertain values.**",7dc5e1b6,0.020134228187919462
1435,5f32117bcd5255,e8c1dd4b,# PACKAGES AND LIBRARIES,85882abf,0.020134228187919462
1438,63b44c85e32c1f,ee6f31a9,"Lists are the most commonly used data structure. Think of it as a sequence of data that is enclosed in square brackets and data are separated by a comma. Each of these data can be accessed by calling it's index value.

Lists are declared by just equating a variable to '[ ]' or list.",fb9b9562,0.02027027027027027
1441,087e21401d7dfc,0e647c66,# Load Data,42000489,0.02040816326530612
1444,12f4d16fc21645,d3227318,<h1 style='color:blue'>Import libraries</h1>,c7752038,0.02040816326530612
1447,f35bf4df70d310,d16beb82,## 0. Importing Library,10bb859a,0.02040816326530612
1448,5a04b504932f52,9564be27,# Imports,39b55fdc,0.02040816326530612
1450,fdc9f4863744b1,79e1fcf5,"Besides these standard libraries, i will import sklearn, warnings for my predictive model and itertools moduole for iteration and looping.",b4529365,0.02054794520547945
1451,225b4fe5d3894a,ac9e4174,"<a id=""1""></a>
## 1. Project Skeleton
Before starting out any project, we must first plan our steps and have clarity on what type of problem we are tackling and what tools can be used and what cannot be used and why not?. This ""why not"" question will help you gain more insights on your ML journey. The following are key points I took into consideration.

Staircase
* What kind of ML problem statement is it? Try to define it
* Understand the type of data?
* Keep a test data aside for EDA
* Relationships between various features, ie EDA 
* Try your intuition about the field: 
   * What can be important features that effect a house price? Bedrooms? Area? Population?
* Data preprocessing: Building a pipeline for it
* Applying models to predict
* What must be the evaluation metric?
* Evaluate the model on Test data",4b4197b3,0.020618556701030927
1452,063a35f644e3c5,45029230,#### Importing Necessary Libraries,1c30fb0a,0.020618556701030927
1453,2a123b4e8f9433,154a7595,"Kaggle stopped letting me use notebook submission and isn't updating ""best score.
Score submitted using download and upload of SVC w/ grid search of sub_data.
Leaderboard score shows 0.86286",0a082218,0.020618556701030927
1460,7325ce9461a814,8cef8933,### import Libraries,c73a7825,0.020833333333333332
1462,2a377ced98d67a,9180e527,"![](https://www.stoodnt.com/blog/wp-content/uploads/2018/07/US-Universities-1024x443.jpg)

Credit: https://www.stoodnt.com/blog/fall-2019-application-deadlines-and-gre-requirements-for-ms-in-us/",262231a8,0.020833333333333332
1463,e82462cdc998a7,9c536a6c,"<a class=""anchor"" id=""0""></a>
# [Mechanisms of Action (MoA) Prediction](https://www.kaggle.com/c/lish-moa)",b39bf244,0.020833333333333332
1465,386c42a7fb27a4,15403b34,## Read Data,9e9f6974,0.020833333333333332
1467,eda49464dd6d1b,0687413e,"<font size=""+3"" color='#053c96'><b>Bussiness Goal</b></font>",8421f81f,0.02097902097902098
1469,f91f58d488d4af,54cb9afc,## Create a Simple model that can clasify any image as a 3 or 7.,5df1bbf3,0.021052631578947368
1470,840534f2908a9c,0a92ead5,"**Training data has 55M rows. In this kernel, we shall only read in 5M rows**",8081c3cc,0.021052631578947368
1472,73893f0467d5e3,71059795,# Missing Values ,279787c6,0.02127659574468085
1477,c7e5f658090347,9eb3bc19,<a id='Enviro_Setup'></a> ,43c78e7d,0.02127659574468085
1478,3f25b363afec54,24288734,<center><img src='https://raw.githubusercontent.com/AIVenture0/-JanataHack---Healthcare-Analytics/master/img.png'/></center>,bbdaae25,0.02127659574468085
1479,56785caebaa256,22ec0340,"<a class=""anchor"" id=""0.1""></a>
## Table of Contents

1. [Import libraries](#1)
1. [Download data](#2)
1. [Selection data with holidays](#3)
    - [Holidays with a shift](#3.1)
    - [Additional dates of anomalies as holidays](#3.2)    
        - [The weakening of quarantine](#3.2.1)
        - [Very comfortable conditions for rest](#3.2.2)
        - [Holidays as days of less efficient work of laboratories](#3.2.3)
        - [Weekend quarantine as holidays](#3.2.4)        
1. [EDA](#4)
    - [Plots - Confirmed cases over time](#4.1)
    - [Statistics](#4.2)
    - [Set initial values for tuning](#4.3)
1. [Tuning Prophet model and holidays parameters](#5)
    - [Stage 1 - Tuning holiday parameters](#5.1)
        - [Model training, forecasting and evaluation](#5.1.1)
        - [Results visualization](#5.1.2)
    - [Stage 2 - Tuning seasonality parameters](#5.2)
        - [Model training, forecasting and evaluation](#5.2.1)
        - [Results visualization](#5.2.2)
    - [Results of all tuning](#5.3)
1. [Prediction](#6)
1. [Comparison with previous forecasts](#7)",a792961a,0.02127659574468085
1480,04e6b0d3c70f46,6c0ffaf7,### Import Required Libraries,56344f77,0.02127659574468085
1481,0caaec057f7184,674f100f,# Input data information,b875533e,0.021505376344086023
1483,9b5de3823ad5ab,ddfdf07e,## Imports,33e48774,0.021739130434782608
1486,7e2644d6b415bc,d8138cb7,"# Questions And Answer

## 1. Which factor influenced a candidate in getting placed?

# Ans: Status is highly related with salary, then ssc percentage(ssc_p)

## 2. Does percentage matters for one to get placed?

# Ans: Yes, percentage matters for one get placed

## 3. Which degree specialization is much demanded by corporate?

# Ans: Marketing and Finance is much demanded by corporate

## 4. Play with the data conducting all statistical tests.",52de7ef0,0.021739130434782608
1488,73ca9abcc2034e,6117c961,"**Short summary:**
The average title length is 11 words.
The average title length is 120 words.
The most popular words are, without a surprise: gme, buy, robinhood, hold, amc.
The most popular tickers are: gme, know, one, hold, see, time, big, amc",cec3446c,0.021739130434782608
1491,b01ee6cb674fa3,cf03da7b,"as colunas Unnamed:0 e Unnamed 0.1 são iguais ao índice, então a decisão é de dropar essas duas colunas
",a8ffd35e,0.021739130434782608
1492,a6b9837940ee38,803c2c5b,# Load and process data,52d2acc7,0.021739130434782608
1493,3c2033cc99c12c,cd019baf,## Import relevant packages ,dfa22a54,0.021897810218978103
1494,c80939c7c626cf,57b52890,"# 2 Exploratory Data Analysis
first five rows of train dataset",b9ac31e2,0.021897810218978103
1497,7f74a04ae75792,33ad1a33,"# <font color=green>Data Cleaning<font>
Checking the existance of missing values, the type of variables, or integrity of data.",d01e91da,0.022058823529411766
1498,c65a65d4041018,d91b4cbe,### Responders in different countries,824fb229,0.022058823529411766
1499,3597174a998d4d,d012e467,"There are 4 variables with missing values. 

* For children, agent, company variables, the missing values means there is no child, agent or company related to the booking. So their missing values can be filled with 0.
* For country variable, the number of missing values is small. So its missing values can be deleted.",276892ed,0.022222222222222223
1503,188731d7fa0604,540da567,"## [참고]작업형2 문구
- 출력을 원하실 경우 print() 함수 활용
- 예시) print(df.head())
- getcwd(), chdir() 등 작업 폴더 설정 불필요
- 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가

### 데이터 파일 읽기 예제
- import pandas as pd
- X_test = pd.read_csv(""data/X_test.csv"")
- X_train = pd.read_csv(""data/X_train.csv"")
- y_train = pd.read_csv(""data/y_train.csv"")

### 사용자 코딩

### 답안 제출 참고
- 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용
- pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)",7cc543d3,0.022222222222222223
1504,42e0005bed28aa,46f6ddf1,"# Description
Ocular Disease Intelligent Recognition (ODIR) is a structured ophthalmic database of 5,000 patients with age, color fundus photographs from left and right eyes and doctors' diagnostic keywords from doctors.

This dataset is meant to represent ‘‘real-life’’ set of patient information collected by Shanggong Medical Technology Co., Ltd. from different hospitals/medical centers in China. In these institutions, fundus images are captured by various cameras in the market, such as Canon, Zeiss and Kowa, resulting into varied image resolutions.
Annotations were labeled by trained human readers with quality control management. They classify patient into two labels:

<li>Normal (N)</li><br>
<li>Cataract (C)</li>
",5616d451,0.022222222222222223
1505,b0c2805cd5c087,2b97c3df,"*  Human Predictions
*  Graphics",0446f327,0.022222222222222223
1507,4fd4b6a80d40e3,ead3d5fa,"## Neural Network Example

![image.png](attachment:image.png)",f6913cc3,0.022222222222222223
1508,892be0a523578c,8f12c698,  #### 1. dailyActivity_merged.csv,b0e8d7c0,0.022222222222222223
1510,396bc36edb95d3,d8c97a43,"#### 2.1 Read the data and do exploratory data analysis (4 pts). Describe the data briefly. Interpret the inferences for each (2 pts). Initial steps like head() .info(), Data Types, etc . Null value check. Distribution plots(histogram) or similar plots for the continuous columns. Box plots, Correlation plots. Appropriate plots for categorical variables. Inferences on each plot. Summary stats, Skewness, Outliers proportion should be discussed, and inferences from above used plots should be there. There is no restriction on how the learner wishes to implement this but the code should be able to represent the correct output and inferences should be logical and correct.",965e4f8f,0.022222222222222223
1513,6fad63bfd45ef9,8496a8a8,Control Variables,b3c6f1d6,0.022222222222222223
1514,d58491f2896fc1,321d82e0,"Genetik algoritma; Charles Darwin tarafından ortaya atılan ve en uygun olan canlının hayata devam etmesini destekleyen doğal seleksiyon teorisi üzerinden ortaya çıkmış bir algoritmadır. Doğal seleksiyon teorisinde yavrular arasında hayatta kalmaya en uygun bireyler ebeveynlerinin genleri doğrultusunda seçilmekte ve yine hayatta kalma şansını arttırabilmek adına çeşitli çaprazlama ve mutasyon işlemleri kullanılmaktadır. Doğal seleksiyon ile birlikte yeni yavrular hayatta kalma şansı daha yüksek ve günün şartlarına optimize edilmiş varlıklara dönüşürler. Yapması gerekenler (beslenme, barınma, doğal koşullar karşısında hayatta kalabilme vs.) konusunda daha gelişmiş ve işlevsel olurlar. Bu tamamen biyolojinin bize sunduğu bir nimettir ve genetik algoritmada da buradan esinlenerek zorlu matematiksel problemlerin üstesinden gelinmiştir. Genetik algoritmalarda her nesilde bir değerlendirme süreci bulunmaktadır. Bu süreç içerisinde bireylerin hayatta kalmaya ne kadar elverişli oldukları yani problemin çözümü için ne kadar uygun oldukları saptanmaktadır. Uygunluk değeri (**fitness**) kavramı genetik algoritmalardaki en kilit noktalardan birisidir. Karmaşık matematiksel problemler ya da modeller üzerinde çalışacak bir genetik algoritma sistemi tasarlandığında uygunluk değeri üzerinden bu karmaşıklığın üstesinden gelinebilmektedir. Uygunluk değerini ölçebilmek adına uygunluk fonksiyonu oluşturulmalıdır ve bu fonksiyonun oluşturulması problemin yapısına göre farklılılık gösterebilmektedir.",514bfdff,0.022222222222222223
1517,d905cde3391d2b,cc6e3cd7,"# Introduction
**Quantitative data** is information that can be measured in real numbers. Examples include,
* Height of a person
* Speed of Tesla cars
* Runs scored by a batsman
* Wickets taken by a bowler

In this notebook, we'll explore various statistical concepts involved in **summarizing quantitative data** with the help of **Indian Premier League (IPL)** dataset.

The data consists of two CSV files for all IPL matches played from  **2008 - 2018** (11 seasons)
* **`matches.csv`** - match-by-match data
* **`deliveries.csv`** - ball-by-ball data

Let's setup `pandas` dataframes for the above files and import necessary libraries.",067dba39,0.022222222222222223
1519,20b372b6e4e276,58acfbbf,"## Acknowledgements
* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)
* [COVID-19 (Week5) Global Forecasting - EDA&ExtraTR](https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr)
* [TSE2020] RoBERTa (CNN) & Random Seed Distribution (https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution)
* Chris Deotte's post: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/142404#809872
* [Faster (2x) TF roBERTa](https://www.kaggle.com/seesee/faster-2x-tf-roberta)
* Many thanks to Chris Deotte for his TF roBERTa dataset at https://www.kaggle.com/cdeotte/tf-roberta
* https://www.kaggle.com/abhishek/roberta-inference-5-folds",ec8b0860,0.022388059701492536
1523,312135b445bd23,96fe7f4e,"This is a joint work by [Moshe Hazoom](https://www.kaggle.com/hazoom), [Sarah June Sachs](https://www.kaggle.com/sarahjune) and [Kevin Benassuly](https://www.kaggle.com/kevinbenassuly).",8ced381f,0.02247191011235955
1527,90964081c7faab,e927bbe6,"The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The csv comes with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in (Moro et al., 2014). The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).

Attribute Information:

Input variables:
#### Bank client data
1 - age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')
#### Related with the last contact of the current campaign:
8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric).
#### Other attributes
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
#### Social and economic context attributes
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)

Output variable:
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')",b423b0c3,0.022727272727272728
1529,d83e5b44d1b80d,00de848d,## Read Data,62845930,0.022727272727272728
1530,6d66ced0028dea,05c13c1b,"Советы:
0. Заполнить аккаунт н Каггл: фото + статус Contributor
1. Сделайте стабильную валидацию
2. Тестируйте 1 изменение за раз
3. Не переусердствуйте с подбором гиперпараметров модели. Делайте это 1 раз вначале и 1 раз в конце
4. Прирост качества в основном зависит от __очистки данных__ и __генерации фич__, а не от модели
5. Бывает полезно удалить ""мусорные"" фичи. Определить их можно, например, через feature importance",f50aae52,0.022727272727272728
1533,be2f4d8a6b73ca,28cb9178,![](https://www.mydr.com.au/wp-content/uploads/2019/04/heart_failure_750.jpeg),5d8ce40a,0.022727272727272728
1534,a0b321057e7402,928b8835,# **Library and data import**,5f73fb91,0.022727272727272728
1536,f269d2fbd5f1be,f0734209,# Data Extraction and Basic Exploration,1264c440,0.022727272727272728
1539,0a918602a04693,cac54a13,# EDA,c1ef0e95,0.022727272727272728
1540,da199f8fb59439,8079fab8,![](https://pmcvariety.files.wordpress.com/2019/03/netflix-logo-n-icon.png),baaa665d,0.022727272727272728
1544,ee23a565163388,c37fef10,"The symptoms of heart failure are including but not limited to,
- Chest Pain
- Fatigue
- Swelling in body parts
- Fainting
- Shortness in breath",88aacbc4,0.022900763358778626
1545,d5f78aa381f58d,51f75731,"# Dataset Features:
The Output (Positive or Negative diagnosis of Heart Disease) is determined by 13 features:
1. **age:** age of the patient
2. **sex:** 1 = male, 0 = female (binary)
3. **cp:** chest pain type (4 values) Value 0: typical angina, Value 1: atypical angina, Value 2: non-anginal pain, Value 3: asymptomatic
4. **trestbps:** resting blood pressure
5. **chol:** serum cholesterol in mg/dl
6. **fbs:** fasting blood sugar > 120 mg/dl (binary) (1 = true; 0 = false)
7. **restecg:** resting electrocardiography results (values 0, 1, 2)
8. **thalachh:** maximum heart rate achieved
9. **exng:** exercise induced angina (binary) (1 = yes, 0 = no)
10. **oldpeak:** = ST depression induced by exercise relative to rest
11. **slp:** of the peak exercise ST segment (Value 0: up sloping , Value 1: flat , Value 2: down sloping )
12. **caa:** number of major vessels (values: 0–3)
13. **thall:** maximum heart rate achieved (0 = no-data, 1 = normal, 2 = fixed defect, 3 = reversible defect)",d60f358f,0.022988505747126436
1547,ee9ddc756b2d4a,c4577732,"**Breve descrizione del progetto:**

Nel seguente lavoro ho cercato di costruire un classificatore che fosse in grado di distinguere un paziente sano da uno malato in base alla sua risonanza magnetica cerebrale (--> classificazione binaria)

Le immagini mediche che ho analizzato rappresentano delle slice del cervello dal punto vi vista assiale, alcune sono immagini di cervelli sani, altre immagini di cervelli con tumore cerebrale.


Ho utilizzato due metodi per estrarre le features su cui lavorare, e successivamente le ho utilizzate per allenare due modelli support vector machines, dei quali ho unito i risultati mediante regressione logistica.

La metrica che ho utilizzato è **sensitivity** (in base ad alcuni tentativi posso dire che basando il mio lavoro sull'ottimizzazione di tale metrica ho raggiunto risultati migliori).",e367eab3,0.022988505747126436
1548,14defffcd250f3,8156e94e,# Missing Values,3a683b94,0.022988505747126436
1549,a4f8ad33c823c5,66bbaab5,# Load Data,fcd48307,0.023076923076923078
1551,09751c520b0616,4f7fb4f8,## *1. Import train and test dataset*,a4d0c7e9,0.023076923076923078
1553,fdc3afd309b850,adbc5441,"<a id=""Methodology""></a>
# Methodology ",966bde38,0.023148148148148147
1555,c09fac3c943d51,ede3d389,# Loading data,678d076d,0.023255813953488372
1556,1660daf8867980,3b58c48d,"# Notebook II: Model-free control
In this notebook I use the same move-chess environment as in notebook 1. In this notebook I mentioned that policy evaluation calculates the state value by backing up the successor state values and the transition probabilities to those states. The problem is that these probabilities are usually unknown in real-world problems. Luckily there are control techniques that can work in these unknown environments. These techniques don't leverage any prior knowledge about the environment's dynamics, they are model-free.",42d7cffc,0.023255813953488372
1558,22bd95f4807a23,b3a99093,"## Library Load and Data Import
In this section we load our required libraries and data.",c05d356f,0.023255813953488372
1559,806ce45c8fa303,32d473e9,"## Dataset-Credit card transactions

The dataset we're going to use in this kernel is `creditcard.csv` which basically a credit card transactions in the past. Using an encoder-decorder system we will find the hidden data points and apply a linear classifier to detect the Fraud(1) or Genuine/not-fraud (0) credit card transactions. 

## Import dependent libraries",3e5c34dc,0.023255813953488372
1566,5ce12be6e7b90e,4a583c27,"`print` is a builtin function, and it can print text along with some execution -- in general `print` accepts as many arguments as we want, and separates them with spaces.",c0ab62dd,0.023391812865497075
1574,1084376bc4897c,366c7f2d,# 1. Importing exploratory libraries ,1b598487,0.023809523809523808
1575,e16860fce156b0,e2a86ca1,![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTuU5YmoWwh4KElAXYSeSZkStM7asEmsf96-g&usqp=CAU)ashoka.org,2054f1ce,0.023809523809523808
1577,31268b33de97b5,e4caf76a,# Loading the Dataset,1e6f7d14,0.023809523809523808
1578,565ad413cd802f,91f7620a,"## Exploring the Data

When you create a notebook with the ""Notebooks"" tab of a Kaggle competition, the data is automatically included in the `../input` folder. You can explore the files in the sidebar. Let us create some constants acess the data directories and CSV files.",397b074e,0.023809523809523808
1579,adb8441ad28019,3fd3cbc2,"#### If you like my work, It will be really great of you to upvote this notebook!
#### If not then you leaving a comment on what do I need to work on and improve will be really helpful!",d89de993,0.023809523809523808
1583,6a80f915608fc2,9869d04a,"### To-do:

- Make a local test set and score it rather than submitting to competition to score.
- Re-check the hyper-parameters now that we've included: new, larger feature set, and the x4 X,y expansion.
- Make the ROC plot for a subset of all the sig_ids to help diagnose where errors are coming from.
",636938eb,0.023809523809523808
1585,87e94f864d74be,df38bd21," <span style=""color:crimson;font-family:serif; font-size:28px;""> Let's get started! </span>
 
This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.",294bfe9f,0.023809523809523808
1587,a758983a68c014,91185ca4,"### Structure
1. [Import libraries](#Import-libraries)
3. [Skip-Gram example with PyTorch](#Skip-Gram-example-with-PyTorch)
6. [Summary](#Summary)",ab89f181,0.023809523809523808
1589,099311d5463909,e18e6f93,The below code will be useful in setting up TF object detection API. Part of it will be in the next notebook.Cheers! You can also refer to (https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html) for details.,71c095ce,0.023809523809523808
1590,b4ecd6e4277e3c,c828fbc9,## IMPORTS ,94d79d5f,0.023809523809523808
1591,0d59a3e0130db0,0fe9cc17,"First, import the required libraries and load the data.",285f04b2,0.023809523809523808
1593,1fac5edd4063ba,3fe1d2e5,![](http://worldkings.org/userfiles/upload/images/modules/news/2018/5/4/0_blog-hintz-eric-2017-03-21-woman-inventor-masthead.jpg)worldkings.org,04bc01e0,0.023809523809523808
1596,9ceb7278784462,af290d1e,"# <a id='2'> 2. Data</a>
## **Data Columns Means**
* Rating ------- > The user rating for the book. the rating score ranges between 0 and 5 <br>
* Reviews ------- >  The number of reviews found on this book <br>
* Book_title ------- > The name of the book<br>
* Description ------- > A short description of the book <br>
* Number_Of_Pages ------- > Number of pages in the book <br>
* Type ------- >The type of the book meaning is it a hardcover book or an ebook or a kindle book etc.<br>
* Price ------- > The average price of the book in USD where the average is calculated according the 5 web sources <br>",3768a567,0.024193548387096774
1597,8d0aebab1e5914,3a0e4d61,# `One can Understand Basic of PCA by this implementation`,084e671f,0.024390243902439025
1600,74a03887600114,296ba540,If you want to know what are these recommendation system then you must read this amazing article: https://www.bluepiit.com/blog/classifying-recommender-systems/,c0ffb2f0,0.024390243902439025
1601,8cefb86a675e5d,1e7edb21,# Importing Required Libraries.,79f9e69b,0.024390243902439025
1602,5169abdc647412,48f8c0cd,## predictions with train test split,28efc68d,0.024390243902439025
1603,786475feda0190,9e9f4d31,"## Classification of Sound.

- This kernel is an introduction to working with sound data and it's classification using Convolutional Neural Network. It's just a basic kernel
which will guide you to how to get started with sound data. The kernel will be constantly updated.

- The main aim is to create a satisfactory model and make some decent predictions.

- The Basics of Librosa isn't complete yet, so let it be please.

",e4663d97,0.024390243902439025
1604,47b2c9be5e31cb,ca75646c,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",7d4afe56,0.024390243902439025
1607,0e09587faffa8f,8632ca45,"In this notebook, we shall be analysing the data that NYC Department of Finance collects on every **parking ticket** issued in the city to deliver insights to the governing body (let's just imagine one 😋)

You can find more details about the dataset [**here**](https://www.kaggle.com/new-york-city/nyc-parking-tickets)

The notebook consists of four parts - **Environment Setup**, **Reading Data**, **Cleaning** & **Analysis**

We shall be using the first **5 Million** rows from the original dataset for our analysis

*So let's get started!*",0d563d61,0.024390243902439025
1608,62582b8036fbfe,5f1e1564,"### This is base line model. Going ahead is trying to improve this score by coming up with new model or changing the pre-processing steps. 

### Please do suggest if you have a different approach and I will try it out",6c2160db,0.024390243902439025
1609,e19e307b3fd188,c2bff7d0,# Load Data,2173955b,0.024390243902439025
1611,0b01138ad120fc,b00a23af,## Important Note: The BTC Results are poor. That happens because I'm using an simple Neural Network with an simple input data (only the values of a few previous days). The objective of this BTC work is only to see how to code an RNN(LSTM).,0b4b72e6,0.024390243902439025
1612,fd4017c1514157,d9ae933d,"
# Competition Overview",fd8f0896,0.024390243902439025
1613,979f1e99f1b309,bd5de4cc,# Gathering Data,d1bfebbf,0.02459016393442623
1616,e9b9663777db82,feaa8aa1,# Data Visualization by Qlik Sense,648e8507,0.024793388429752067
1617,2f47abddfd1928,58601252,## 1. Data Reading,ae33cc0b,0.024793388429752067
1622,254cccd5145725,b3cffa66,"<font size=""4"">This is a supervised multi-class classification machine learning problem.</font>",a49b4037,0.025
1625,fdbbd573ba31c2,8d2148b8,## About df_train & df_test,f7c28d74,0.025
1629,1011899b959f44,a55c0d13,"# What are Pandas?
Pandas is a fast, powerful, and easy to use open source data analysis and manipulation tool, built in python. Today we will be demonstrating a multitude of methods in Pandas through the analysis of a dataset showcasing the battles in the popular series, Game of Thrones. ",0b112382,0.025
1632,5ffe6aa38958a1,779b8a73,"# 2. Getting Started 

## 2.1 Import Libraries",11f5412e,0.025
1634,62487bcd70b199,99b29072,# <a id='1'>1.Data</a>,f6ae50af,0.025
1635,5626e84c4e6bf8,b29078d4,"# Aim
This is supposed to be a tutorial on **Self Organizing Maps** where we will perform clustering on Fashion MNIST using a neural network.
# Concepts covered
- Self Organizing Maps(For unsupervised deep learning)
- Bayesian Optimization
- Analysis of Self Organized Maps
- Some image processing

# Self Organizing Maps
A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
<img src=""http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/kohonen1.gif"">
This makes SOMs useful for visualization by creating low-dimensional views of high-dimensional data, akin to multidimensional scaling. The artificial neural network introduced by the Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map or network. The Kohonen net is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.
<img src=""https://www.nnwj.de/uploads/pics/1_2-kohonon-feature-map.gif"">
While it is typical to consider this type of network structure as related to feedforward networks where the nodes are visualized as being attached, this type of architecture is fundamentally different in arrangement and motivation. It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.

Source: [Wikipedia](http://https://en.wikipedia.org/wiki/Self-organizing_map)",e2ecb669,0.025
1637,4ae6a182abac64,7f67b2ea,"
 ## Libraries 📚",418676c5,0.025210084033613446
1638,c4386b8a01d66e,37b02a7c,"* ph-> pH of water
* Hardness-> Capacity of water to precipitate soap in mg/L
* Solids-> Total dissolved solids in ppm
* Chloramines-> Amount of Chloramines in ppm
* Sulfate-> Amount of Sulfates dissolved in mg/L
* Conductivity-> Electrical conductivity of water in μS/cm
* Organic_carbon-> Amount of organic carbon in ppm
* Trihalomethanes-> Amount of Trihalomethanes in μg/L
* Turbidity-> Measure of light emiting property of water in NTU (Nephelometric Turbidity Units)
* Potability-> Indicates if water is safe for human consumption",dc732bf5,0.025210084033613446
1640,5d2a3e82679cf3,6e637e4c,"# Data Description
Major League Baseball Data from the 1986 and 1987 seasons.
",9e60b1e3,0.02531645569620253
1642,9169c4e9c33c90,4c8d11e0,# Table of Contents,725bf880,0.025423728813559324
1643,1294fb4c86f993,7bb739eb,"### It also states limitation for the guns registeration system to be used as an indication of total guns sold:
>These statistics represent the number of firearm background checks initiated through the NICS They do not represent the number of firearms sold Based on varying state laws and purchase scenarios, a one-to-one correlation cannot be made between a firearm background check and a firearm sale


>A forthcoming study conducted by Harvard researchers found that roughly 40 percent of respondents had acquired their most recent firearm without going through a background check
***",4471e513,0.025423728813559324
1644,e169603b62be56,6f1135d2,importing traing data and test data and drop Id,8c311ec1,0.02564102564102564
1645,e424c111c44669,fbfdb31b,**Load Data**,d9fccfba,0.02564102564102564
1650,9eed0fae1c7958,b20e7f8b,# Data Directories,3fb1438e,0.02564102564102564
1653,34fff8ce731b03,1f3d1de1,"## Preparando o ambiente

O código abaixo adiciona a **raiz** do projeto, que contém códigos e dados necessários para o ""Hands on"".",6f9e5b2e,0.02564102564102564
1654,9b42412e75d640,e1db56fb,"# Part 1 

# Duplicates, outliers and classes",b616570a,0.02564102564102564
1655,4d91e84c564cbe,9007a423,We can put other types of things in lists:,355a43e3,0.02564102564102564
1657,50b03ce5b1a286,7756a781,![](https://media-exp3.licdn.com/dms/image/C4E1BAQEuTsJLViLkQw/company-background_10000/0/1587631889353?e=2159024400&v=beta&t=7N65emCWIoGvjrTaXUbHwLSaNEySNW0rAiEADaGOutI)br.linkedin.com,d49896a5,0.02564102564102564
1659,80ad12f326ab70,bec4ae9c,"## Objective :
Use digital learning data to analyze the impact of COVID-19 on student learning,explore the state of digital learning in 2020 and how the engagement of digital learning relates to factors such as district demographics, broadband access, and state/national level policies and events.",da404a16,0.02564102564102564
1661,c8c4705cca1ebb,4cdf483f,Sales_train csv dosyasını date formatına göre oku,6d9d7107,0.02564102564102564
1663,2ada0305b68956,8928b343,### 1. Palette = 'Accent',133e26f4,0.025714285714285714
1665,663bbc9eaf267b,bc7f6492,# Importing some libraries,32445529,0.025974025974025976
1668,4ae464582bac51,105d0eec,# LOAD DATA,ca6a52ce,0.025974025974025976
1669,75adb7945ef9bd,c8c89f4d,## 1. Basic Exploration,785c5095,0.025974025974025976
1670,722cd844dfbe8f,65f2ee4d,"<h1 style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"">Summary</h1>

1. [Exploratory data analysis (EDA)](#section_1)      
    1.1. [Submission sample & train.csv](#section_1_1)      
    1.2. [MRI train data](#section_1_2)      
    1.3. [Data cleaning](#section_1_3)      

2. [Preprocessing](#section_2)      
    2.1. [Crop and resize the images](#section_2_1)      
    2.2. [Equalization CLAHE](#section_2_2)      
    2.3. [Denoising filter](#section_2_3)      
    2.4. [Global preprocessing function](#section_2_4)      
    
3. [Development of supervised models](#section_3)      
    3.1. [Multimodal inputs CNN from scratch](#section_3_1)      
    3.2. [Define loaders for images sequences 4 MRI types](#section_3_2)      
    3.3. [Define folds](#section_3_3)      
    3.4. [Keras custom data generator](#section_3_4)      
    3.5. [Define CNN Multi-inputs model](#section_3_5)      
    
4. [Test of trained final model](#section_4)     
5. [Try another approach: Transfer Learning](#section_5)",0cedb385,0.025974025974025976
1672,241cf32abb22d8,0a5e3a0e,"# Data Preparation <a class=""anchor"" id=""2""></a> 

## Loading Dataset",47157066,0.025974025974025976
1675,a1dcd92986bc84,ca2c8a2f,"# Natural language image search with a Dual Encoder

**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>
**Date created:** 2021/01/30<br>
**Last modified:** 2021/01/30<br>
**Description:** Implementation of a dual encoder model for retrieving images that match natural language queries.",730acaaa,0.02631578947368421
1678,f35ee6e9fab592,30259cbc,Checking the Dtypes of the columns...,b15f7073,0.02631578947368421
1681,f05342aabe2b59,f9ae3e5b,"### Dairy

**(v1)**: 13-Dec-2020 Read through discussion posts, [Lilian Weng's MAB post](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html), [Wikipedia](https://en.wikipedia.org/wiki/Multi-armed_bandit), etc. The basic situation is not so tricky but the ""Simulations Platform"" is a bit mysterious... Try different code demos related to the RPS... <br>
14,15-Dec Copy/play-with Xin Cui's [""Weighted expected mean and std""](https://www.kaggle.com/xincuimath/weighted-expected-mean-and-std-estimation), (""BayXC"") vs Beta distribution. Make simple ""max candies"" model/prediction, 1292 combined scores.
Look through [Simple MAB](https://www.kaggle.com/ilialar/simple-multi-armed-bandit) notebook (nice beta plot - add a plot to BayXC stuff). <br>
Made a simple ""scan, repeat"" agent written into submission.py. Is that what I need to submit? Yes.<br>
**(v2)** 15-Dec Well, my scan-repeat starts with 0, 1, 2, etc. *and so do lots of other agents*. So subtract a small random amount from the starting all 1s so that argmax will not see them as equal (and go in order) and instead they will be in a random (fixed) scan order. <br>
The skill rating is kind of strange... For reference, submit the random agent from v2. I expect this to rate lower than my other ones:<br>
**(v3)** Setup global variables to keep track of bandit properties (pulls, rewards and their dfs, predicted mean prob with discount factor included, i.e., the prob we expect to have if it is pulled. Set all bandits to having on reward and one non-reward observation giving a mean of 0.5. Add some noise to those values to reduce sequential selection of bandits. Pull the bandit with the largest dfmean value.<br>
**(v4)** Noticed that there is a Santa 2020 submission.py that implements ucb_agent(). Compared to that my ""select largest dfmean value"" (v3) agent zooms ahead for the first half-ish of the pulls, then falls behind... THe bandit-pulled vs step number plot shows a clear change in behavior around 700 steps, my problem is in the ""end game""? Modified my agent to use only observed rewards to calculate dfmean (removed starting rewards of [1,0]), after first pass select bandit with the highest mean+std.<br>
**(v5)** Re-structure the code slightly adding a ""state"" value to help implement a strategy. Found/fixed some errors in how the reward history was filled in. In the first ""scan"" state continue pulling a bandit if a reward is obtained up to a max of 4 rewards. After that chose the bandit with the highest df-ed mean + std. <br>
**(v6)** Nice discussion posts in [Some thoughts about the rating system](https://www.kaggle.com/c/santa-2020/discussion/204741) and Aatif Fraz' notebook with [Chances of getting a candy](https://www.kaggle.com/aatiffraz/a-beginner-s-approach-to-performance-analysis) plot - add that below, along with 90th, 50th percentiles.  Clean up the structure to be more clearly state-based. The V6 agent has 3 states: i) bandits are scanned initially in random order and each bandits is pulled until they produce a 0, or 4 rewards in a row; ii) bandits with dfmean > 0.5 are pulled until there are none > 0.5; finally iii) the bandit with maximum dfmean+dfstd is pulled.<br>
**(v7)** Don't like that opponent might spy on my repeat pulls, so try to arrange things so that each pull is randomly selected from bandits with some criteria.
Go to a 5-state scheme: i) each pulled once; ii) all-1s are pulled at random until no all-1s; iii) dfmean > 0.5 pulled; iv) [0],[1,0],[0,1],etc pulled alternately w/dfmean; and v) max dfmeans+dfstds pulled.<br>
**(v8)** Use foms as flag instead of dfmeans in state 1; pull <= 2 zero rewards in state iv (more explore); add a 6-th ""endgame"" state starting at step 1200 (require mean>0.25); poach opponents bandit if played 3 times in a row.<br>
**(v9)** Checkout MAB in chapter 2 of [Reinforcement Learning](http://incompleteideas.net/book/RLbook2020.pdf). Cleaned up the states code a bit: keep all the state's action in one place (no more 'setup for next state').
Plot histograms of means and dfmeans after each state - showed where exploration was needed. Some changes for version 9: added one repeat and possible poaching in state 1; in state 3, 2/3 time do best dfmean and mix in 1/3-time pull bandits that have only 1 zero reward; state 4 slight adjusts; state 5 dfmean+dfstd but mix in 1/10-time explore bandits with mean < 0.25; endgame state 6: pull highest dfmean but only among ones with mean>0.20.<br>
**(v10)** Clean up some more, nicer histogram plots, add a 7th state to get hist plot at step 1600.  Added a ""ucb_half5percent"" agent file to do UCB ""correctly"", though the delta value is very large compared with BayXC std, so use 0.5 times delta. <br>
Some adjustment of the poaching criteria, but otherwise no intentional, significant change to the submitted agent's operation.<br>
**(v11)** Cleaning up things. Slight changes to scheme: state 2 copy opponent if I was going to do that one anyway; state 4 is 0.30 to 0.50 and <= 7; state 5 < 0.35; and state 7 >0.25. Added plots of estimated and actual thresholds for all the snapshots. <br>
**(v12)** Little stuff: added 30, 60, 90, 100 %-tile lines to snapshot plots. Added the [""Vegas Slots""](https://www.kaggle.com/sirishks/pull-vegas-slot-machines) agent as one to compare with mine. Considering [adding bandit information based on opponent's pulls](https://www.kaggle.com/c/santa-2020/discussion/203391)... added it only in my play states 2 and 3 -- still high probabilities present so OK to assign reward = 1 based on an opponent's repeat pull, might slightly over-estimate some bandit's thresholds. <br>
**(v13)** Slight changes, mostly in state 5: dfmean+0.5\*dfstd, mix w/(1/10) dfstd > 0.10 <br>
**Commit following versions running against the ""vegas slots"" agent.** <br>
**(v14)** Another tweak to state 5: dfmean+0.5\*dfstd, w/(1/6) dfstd > 0.06. <br>
**(v15)** Made a histogram of simulated player's score to show variation due to threshold randomness (at end of nb.) Combined states 1,2 and slight changes to the state 1,2 poaching and the other-reward including. Expect about the same performance. <br>
**(v16)** Had left out the ""pull other's choice if it's among ones I want to exploit"" logic that was in v14 state 2; added it to the combined states 1,2. States 1,2 prints out what happened at each of its steps; changed the Explore-every-nth value from 2 to 3.

### Submissions
Submitted agents, from newest to oldest; ratings as of most recent submission.<br>
(v16) Submission 1xxxxxxx **???.?** as v15, state 1,2 includes ""other pull"" as was in v14. <br>
(v15) Submission 1xxxxxxx **???.?** as previous v14, combined states 1 and 2.<br>
(v14) Submission 18887673 **709.5** 7 state as v13-v11 but state 5: dfmean+0.5\*dfstd, w/6 dfstd > 0.06 <br>
(v13) Submission 18876229 **678.1** 7 state as v12,v11 but state 5: dfmean+0.5\*dfstd, w/10 dfstd > 0.10 <br>
(v12) Submission 18860127 **700.0** 7 state as v11, with adding opponent assumed rewards.<br>
(v11) Submission 18846489 **667.9** 7 state: rand pull each (1 repeat); rand pull first 0; dfmean w/1-zero; dfmean+0.5dfstd, w/ 2-zeros or 30-50,7; dfmean+dfstd, w/mean<0.35; dfmean w/mean>0.20, dfmean w/mean>0.25.<br>
(v10) Submission 18843273 **658.3** 6(7) state: rand pull each (1 repeat); rand pull first 0; dfmean w/1-zero; dfmean+0.5dfstd, w/ 2-zeros or 35-55; dfmean+dfstd, w/mean<0.25; dfmean w/mean>0.20.<br>
(v10) Submission 18843269 **599.5** UCB half-delta(5%) - modified Santa 2020 UCB.<br>
(v9) Submission 18820215 **665.1** 6 state: rand pull each (1 repeat); rand pull first 0; dfmean w/1-zero; dfmean+0.5dfstd, w/2-zeros; dfmean+dfstd, w/mean<0.25; dfmean w/mean>0.20.<br>
(v8) Submission 18787619 **665.4** 6 state: pull each, pull all-1s, dfmean>0.5, pull <= 2 zeros mix w/dfmean, dfmean+dfstd, dfmean w/mean >0.25<br>
(v7) Submission 18772801 **583.9** 5 state: pull each, pull all-1s, dfmean>0.5, pull [0],[1,0],[0,1], dfmean+dfstd<br>
(v6) Submission 18758336 **631.2** 3 state: scan/pull up-to-4, dfmean>0.5, dfmean+dfstd<br>
(v5) Submission 18705910 **629.0** scan/pull up-to-4 times, dfmean+dfstd<br>
(v4) Submission 18689974 **592.4** scan, true mean, select mean+std<br>
(v3) Submission 18675001 **502.2** select largest dfmean value<br>
(v2) Submission 18668271 **302.5** random agent"" for reference<br>
(v2) Submission 18667842 **396.6** scan, repeat, shuffled order<br>
(v1) Submission 18667244 **394.2** scan, repeat, 0,1,2,3...""<br>",cfbb391f,0.02631578947368421
1682,d369f200a84c2a,d19f1275,## *What are Capsule Networks ?*,8fef4d48,0.02631578947368421
1684,d93a87fdbdb3d2,50898058,## Load Dataset and Preprocessing,30d079c3,0.02631578947368421
1686,fe7360cddc13e5,abe48358,"Gerçek hayatta müşteri davranışlarını anlamlandırabilmek,bize müşterilere olan yaklaşım stratejileri bağlamında bir içgörü sağlar. Zira bir şirketin gelecekteki değeri, uzun vadede tüm müşterilerinden elde ettiği kardır. Pazarlama,elde tutma maliyetleri düşünüldüğünde, bir firmanın tüm müşterileriyle uzun vadeli ilişkilerini sürdürmesi optimum seçenek değildir. 

Bu model,müşterilerin gelecekteki davranış yapısını ölçmek için 2 temel olasılıksal yaklaşım öne sürer.

1- Her müşterinin bir alive olma olasılığı vardır.

2- Alive olan her müşterinin ileride işlem gerçekleştirme olasılığı vardır.

Model,müşterilerin geçmiş zamana ait RFM verilerindeki dinamikleri öğrenerek müşterilerin gelecekteki işlemlerinin beklenen sayısını, ve müşterinin ""aktif"" olma olasılığını hesaplayarak, bir müşterinin yaşam boyu değerini ölçmemize yardımcı olur. 

RFM gibi klasik ve geçmişe dönük yaklaşımlardan farklı olarak davranışsal ve bayezyen bir modeldir. Ve nihayetinde her müşterinin gelecekteki karlılığını makul bir hata ile hesaplama gayesi taşır. 

En bilinenleri **Pareto/NBD** ve **BG/NBD** dir. Belirli parametreleri vardır. Parametrelerinin hesaplanmasındaki karmaşıklık ve tahmin sonuçlarının görece daha kötü olması nedeni ile Pareto/NBD yerine varsayımlarda değişiklik yapılmış hali BG/ NBD' yi kullanmak daha avantajlıdır.

BG/NBD modeli 2005 yılında bir matematikçi ve pazarlama profesörü olan Peter Fader'ın makalesinde yayımlanmıştır.

Bu notebookta modelin teorik yapısı ve örnek bir veri seti üzerinden pythondaki kullanımı anlatılacaktır.",8979e423,0.02631578947368421
1688,31b564f11ef638,5f66b2d7,"### This is a simple modeling notebook using Random Forest Regression. This model reaches the top 6.6%. If you think it's useful, please upvote ^^ ",424f9692,0.02631578947368421
1690,a5a419dc7245b0,0635146c,"<table>
  <thead>
    <tr>
      <th>Field</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> Variable</td>
      <td>Definition</td>
    </tr>
    <tr>
      <td>employee_id</td>
      <td>Unique ID for employee</td>
    </tr>
    <tr>
      <td>department</td>
      <td>Department of employee</td>
    </tr>
    <tr>
      <td>region</td>
      <td>Region of employment (unordered)</td>
    </tr>
    <tr>
      <td>education</td>
      <td>Education Level</td>
    </tr>
    <tr>
      <td>gender</td>
      <td>Gender of Employee</td>
    </tr>
     <tr>
      <td>recruitment_channel</td>
      <td>Channel of recruitment for employee</td>
    </tr>
     <tr>
      <td>no_of_trainings</td>
      <td>no of other trainings completed in previous year on soft skills, technical skills etc.</td>
    </tr>
    <tr>
     <td>age</td>
     <td>Age of Employee</td>
    </tr>
    <tr>
     <td>previous_year_rating</td>
     <td>Employee Rating for the previous year</td>
    </tr>
    <tr>
     <td>length_of_service</td>
     <td>Length of service in years</td>
    </tr>
    <tr>
     <td>KPIs_met >80%</td>
     <td>if Percent of KPIs(Key performance Indicators) >80% then 1 else 0</td>
    </tr>
    <tr>
      <td>awards_won?</td>
      <td>if awards won during previous year then 1 else 0</td>
    </tr>
      <tr>
      <td>avg_training_score</td>
      <td>Average score in current training evaluations</td>
    </tr>
      <tr>
      <td>Total expenditure</td>
      <td>General government expenditure on health as a percene of total government expenditure (%)</td>
    </tr>
      <tr>
      <td>is_promoted	(Target)</td>
      <td>Recommended for promotion</td>
    </tr>
  </tbody>
</table>",4279726e,0.02654867256637168
1691,ac1abfe1dfe815,ea972514,# Data Import & Cleaning,6529dbcb,0.02654867256637168
1695,67b7354e96113a,b72f9725,"**Key Take Aways**

1) Exploratory Data Analysis

2) Feature Engineering

3) Advanced Machine Learning Techinques

The important of all is that you will get familiar with how the data science competition works. I hope this kernal will help people to prepare themselves for the data science competitions and people can use this kernal to brush up their skills.",dca94250,0.02666666666666667
1699,7e1da639035ac5,7a8e7fe1,"# <a id='1'>1. Introduction</a>
PASSNYC is a not-for-profit organization that facilitates a collective impact that is dedicated to broadening educational opportunities for New York City's talented and underserved students. New York City is home to some of the most impressive educational institutions in the world, yet in recent years, the City’s specialized high schools - institutions with historically transformative impact on student outcomes - have seen a shift toward more homogeneous student body demographics.

PASSNYC uses public data to identify students within New York City’s under-performing school districts and, through consulting and collaboration with partners, aims to increase the diversity of students taking the Specialized High School Admissions Test (SHSAT). By focusing efforts in under-performing areas that are historically underrepresented in SHSAT registration, we will help pave the path to specialized high schools for a more diverse group of students.

",120b6c23,0.02666666666666667
1704,726833f92fb87a,6c3d22e3,"- XGBoost achives good results in terms of precision, recall and accuracy, with values around 70%.
- The most important features to determine if a new customer will accept the deposit is poutcome (>30%):<br>
    - **This means that the deposit satisfied the customers who accepted it previously, however, currently, the company is probably aiming at the 'wrong' customers** 
 <br>
- Moreover, accoring to XGBoost, other important features to predict the success of a deposit are:
    - Contact: by looking at the data it looks like the great majority of customers with unknown contact did not accept the deposit: it should be investigated why these contacts are missing.
    - Housing: customers without housing loans seems to accept the deposit",7dc5e1b6,0.026845637583892617
1705,d76896b30cebd3,66e3e2ac,"EDA of Kickstarter Projects

Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising. The company's stated mission is to ""help bring creative projects to life""

In this exploratory data analysis I will try to find out a few interesting insights.",1b4e8f34,0.02702702702702703
1707,2dda7facf3c1e0,cdc1e36d,"# Amazon Deep Composer

## Training a custom AR-CNN model 
In this Jupyter notebook, we guide you through several steps of the data science life cycle. We explain how to acquire the data that you use for this project, 
provide some exploratory data analysis (**EDA**), and show how we augment the data during training. 


### The AWS DeepComposer approach to generating music  
Autoregressive-based approaches are prone to accumulate errors during training. To help mitigate this problem, we train our AR-CNN model so that it can detect and then fix mistakes, including those made by the model itself.

We do this by treating music generation as a series of *edit events*, which can be either the addition or removal of a note. An *edit sequence* is a series of edit events. Every edit sequence can directly correspond to a piano roll.

By training our model to view the problem as edit events rather than as an entire image or just the addition of notes, we found that it can offset the accumulation of errors and generate higher quality music.

Now that you understand the basic theory behind our approach, let’s dive into the code. In the next section, we show examples of the piano roll format that we use for training the model.",45552d2b,0.02702702702702703
1711,e4525eb0c96f28,aa9f99bd,"## Gathering and Tidying the Data

We used data from Kaggle, where others have not only accumulated but built upon and updated a dataset of video game sales over time.

Video Game Sales Data: https://www.kaggle.com/ashaheedq/video-games-sales-2019

Video Game Country/Region Data: https://www.kaggle.com/andreshg/videogamescompaniesregions

The libraries used throughout this tutorial:
- Pandas: Used to organize and clean data
- Matplotlib: Used to display data
- Seaborn: Used to display data
- Numpy: Used to further calculate and display data
- SKLearn: Used to further analyze data using machine learning

In this tutorial, we will use Pandas Dataframes to store our data. Here is what each dataframe represents.
- **df** is the main pandas dataframe that will be used throughout the tutorial.
- **region_df** is a temporary dataframe used to hold the data on game country of production before it's merged into df.",2093a1f1,0.02702702702702703
1712,62037c5832129c,7ce70a34,"### Overview
- [Streamlining workflows with pipelines](#Streamlining-workflows-with-pipelines)
  - [Loading the Breast Cancer Wisconsin dataset](#Loading-the-Breast-Cancer-Wisconsin-dataset)
  - [Combining transformers and estimators in a pipeline](#Combining-transformers-and-estimators-in-a-pipeline)
- [Using k-fold cross-validation to assess model performance](#Using-k-fold-cross-validation-to-assess-model-performance)
  - [The holdout method](#The-holdout-method)
  - [K-fold cross-validation](#K-fold-cross-validation)
- [Fine-tuning machine learning models via grid search](#Fine-tuning-machine-learning-models-via-grid-search)
  - [Tuning hyperparameters via grid search](#Tuning-hyperparameters-via-grid-search)
  - [Algorithm selection with nested cross-validation](#Algorithm-selection-with-nested-cross-validation)
- [Looking at different performance evaluation metrics](#Looking-at-different-performance-evaluation-metrics)
  - [Reading a confusion matrix](#Reading-a-confusion-matrix)
  - [Optimizing the precision and recall of a classification model](#Optimizing-the-precision-and-recall-of-a-classification-model)
  - [Plotting a receiver operating characteristic](#Plotting-a-receiver-operating-characteristic)
  - [The scoring metrics for multiclass classification](#The-scoring-metrics-for-multiclass-classification)
- [Dealing with class imbalance](#Dealing-with-class-imbalance)
- [Debugging algorithms with learning and validation curves](#Debugging-algorithms-with-learning-and-validation-curves)
  - [Diagnosing bias and variance problems with learning curves](#Diagnosing-bias-and-variance-problems-with-learning-curves)
  - [Addressing overfitting and underfitting with validation curves](#Addressing-overfitting-and-underfitting-with-validation-curves)
- [Summary](#Summary)",61474350,0.02702702702702703
1717,a6c34cd514e30e,e4875289,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",bf603ddd,0.02702702702702703
1721,993966a1cb5eb1,abb40759,## Mirror Statergy Setup,5cd181e2,0.02702702702702703
1724,738bfced935b69,2c7c05bd,## Data Cleaning ,2d3c592d,0.0273972602739726
1725,91473a39b85068,6b777785,"### Multi-label Classification: 
Multi-label classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A question on Stack Overflow might be about any of C, Pointers, FileIO and/or memory-management at the same time or none of these. Go through the link and know more details about Multi Label Classification ( http://scikit-learn.org/stable/modules/multiclass.html)",6e3d91c2,0.0273972602739726
1727,1eb62c5782f2d7,0a26be36,"## 2. Area di sebelah kanan point z-score
![5_1_graph_right_of_z.png](attachment:5_1_graph_right_of_z.png)",bb69f147,0.0273972602739726
1732,166a62ebb4fc3a,f105aaf3,"# Breast Cancer Dataset Attributes Information:

1st column - ID number,
2nd column -  Diagnosis (M = malignant, B = benign),
3rd to 32nd column -  10 real-valued features are computed for each cell nucleus:

1. radius (mean of distances from center to points on the perimeter)
2. texture (standard deviation of gray-scale values)
3. perimeter
4. area
5. smoothness (local variation in radius lengths)
6. compactness (perimeter² / area — 1.0)
7. concavity (severity of concave portions of the contour)
8. concave points (number of concave portions of the contour)
9. symmetry
10. fractal dimension (“coastline approximation” — 1)

The **""mean""**, **""standard error(se)""** and **“worst”** or largest (mean of the three largest values) of these features were computed for each, resulting in 30 features. 

For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.",db48a079,0.027777777777777776
1733,c01049afb6d307,c7c2d45c,# Load and Check Data,d37d3b5d,0.027777777777777776
1735,dd3721cb49c1fd,bd6721e3,"<center><img src=""https://warehouse-camo.ingress.cmh1.psfhosted.org/a500025c0abbda3915b6ec7c0723ce36f189eed8/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6c696e6b6564696e2f677265796b6974652f6d61737465722f4c4f474f2d43382e706e67"" alt=""Greykite"" class=""center"" style = ""
  width: 60%;
  height: 60%;
  background-image: url('img_flowers.jpg');
  align: center;
  background-size: 100% 100%;""></center>
",1a53fdd9,0.027777777777777776
1736,1014e6be391084,8d4fecdb,# Explanatory Data Analysis****,46f9168f,0.027777777777777776
1737,2f0f808765fc67,714bbf30,# **Identify Unique Columns**,fd1f6494,0.027777777777777776
1740,b3e0b7e9ff6849,69142090,"<p><img style=""float: left;margin:20px 10px 5px 1px; max-width:380px"" src=""https://i.ytimg.com/vi/l_xwYM2KNX0/maxresdefault.jpg""></p>
<p> The Buy Till You Die (BTYD) class of statistical models is designed to predict the behavioral characteristics of customers. The main purpose is to model and forecast customer lifetime value. </p>
<p> Customer lifetime value (CLTV or CLV) can be defined as the present value of a customer for the company based on projected future cash flows from the customer relationship. CLTV represents the total amount of money spent on the business or products over lifetime of a customer.</p>
<p> In this notebook, I'll try to apply a BTYD model on an online retail dataset to make a 1-year CLTV prediction.</p> ",f6e4bb0d,0.027777777777777776
1743,fdc3afd309b850,42fe6777,"As said before, the aim of this project is to use WebScraping technique to extract data from the web. Cleaning and organizing the data and creating new features to help us to build a good regression model to price preditionns.

 * To clean and organize with the data we are going to use the **Pandas** and **Numpy** libraries 
 * We are going to use the **Busca CEP API** to organize the address features. With the neighborhood of the apartaments we will be able to create a new feature called Administrative Region (AR).
 * With Address, Neighborhood and AR we will be able to find the best value to fill null values of the apartments characteristics, looking for the median of the characteristics in the same address, same neighborhood or same AR  
 * After that we are going to use **Geopy** to find the geolocation of the apartaments. 
 * Using the geolocation and the **Haversine Formula** We are going to calculate a new feature called Distance to the Brasília Downtown.
 * Again we will use **Web Scraping** with **BeautifulSoup Library** in Wikipedia to find Per Capita Income (PCI) and Population of the Administrative Regions. 
 * For the Data Visualization we are going to use **Folium Library**, **Fast Marker Cluster plugin**, **Seaborn**  and **Matplotlib**.
 * Before find the best **Linear Regression**, to normalize the data we're use **numpy** and to scale we're going to use **preprocessing** 
 * To choose the best model between, **Lasso (l1)**, **Kernel Ridge (l2)**, **Elastic NNet**, **Xgb Regressor** or **Lgbm Regressor**  we are going to create a **Pipeline** with **Hyperparameter Tuning** using **Grid Search** and **K-Folds** shuffed.
 * Finally after finding our model we will plot the **PredictionError** using **Yellowbrick** library and use **Sklearn Metrics** to measure the results of our model.
 
 
 So, let's get it started! :)
",966bde38,0.027777777777777776
1745,69ac33d79f5130,48335bf2,## Data Preparation and Cleaning,9d760d2a,0.027777777777777776
1747,6dcfe6a610d86b,689ad6c3,"## PEP-8
https://www.python.org/dev/peps/pep-0008/",d05c59da,0.027777777777777776
1748,ab6da5994949a3,5001eecb,### Importing the Dataset,fae6b91d,0.027777777777777776
1755,1d73d04c3aaae8,14a9e4bb,## Quick Look at the Data,cd43d0aa,0.027777777777777776
1756,df2a7968c08ee4,0d332c1e,"Given the increasing popularity of Pytorch among Kaggle users I wanted to learn the framework. I decided to try and recreate this [MNIST Tensorflow Notebook](https://www.kaggle.com/brendanartley/mnist-keras-cnn-99-6) using Pytorch. 

Pytorch Users: Feel free to give me suggestions and let me know how I can improve my code!

-- --

Here are some Kaggle notebooks that helped me understand Pytorch so far.

- [Pytorch Tutorial for Deep Learning Lovers](https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)

- [CNN with PyTorch (0.995 Accuracy)CNN with PyTorch (0.995 Accuracy)](https://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracy/datahttps://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracy/data)

- [Shervine's Blog - Pytorch How To Generate Data](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)

- [Pytorch Dataset and Dataloader Notebook](https://www.kaggle.com/pinocookie/pytorch-dataset-and-dataloader)
",a2ba0a72,0.027777777777777776
1758,eda49464dd6d1b,2ea74092,"Build a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.

In order to predict whether the customer would be interested in Vehicle insurance, you have information about demographics (gender, age, region code type), Vehicles (Vehicle Age, Damage), Policy (Premium, sourcing channel) etc.",8421f81f,0.027972027972027972
1759,c84925c8171900,e9cae55b,"<a id=""background""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            1.1 Video Game Industry :
            </span>   
        </font>    
</h3>",e21ff7ec,0.028037383177570093
1760,06c7ba9203293f,d11aee5f,"# **Contents**

* Importing libraries and data
* A peek into the data
* Checking for closed pubs and missing values
* Checking for duplicates
* Normalizing/ correcting few columns
* EDA on categorical and numeric columns
* Predicting missing 'price' values
",1e1a2b48,0.028169014084507043
1761,9bcfa825c8b2e6,ec9c0033,Görsel: https://jnyh.medium.com/building-a-machine-learning-classifier-model-for-diabetes-4fca624daed0,220f36e4,0.028169014084507043
1766,fc8e0042411c46,9a22976a,## Data Preparation,af476c2a,0.02821316614420063
1771,b61ab8f81dc03d,019099b2,### Importing Libs,64d05394,0.028368794326241134
1774,2730840089c8eb,40036531,"One place where the Python language really shines is in the manipulation of strings.
This section will cover some of Python's built-in string methods and formatting operations.

Such string manipulation patterns come up often in the context of data science work, and is one big perk of Python in this context.

## String syntax

You've already seen plenty of strings in examples during the previous lessons, but just to recap, strings in Python can be defined using either single or double quotations. They are functionally equivalent.",34d27dac,0.02857142857142857
1776,3a6274ed72cc00,0ca0e110,"## Content:

- <a href='#1.'> 1. Importing Libraries</a>
- <a href='#2.'> 2. Loading and Checking Data</a>
- <a href='#3.'> 3. Variable Description</a>
- <a href='#4.'> 4. Data Analysis</a>
- <a href='#5.'> 5. Preprocessing</a>
- <a href='#6.'> 6. Modelling</a>",51369a2a,0.02857142857142857
1777,b3e48999ed0d00,f77fada8,## Loading datasets,fe9ada0f,0.02857142857142857
1781,675b60eaf415a6,3f594f4e,### **Download and extract Food 101 Dataset**,68c0b725,0.02857142857142857
1782,47a1b1fe51b4ad,2d51625f,"### Dependencies

  - **NumPy**
  - **Pandas**
  - **SeaBorn**
  - **SciKit-Learn**
  - **TensorFlow**  ",331ded2f,0.02857142857142857
1783,6b65d81a5743dd,f489e0d6,"Today I will explore the basketball player's performance dataset, and finish the required task by the author.

**Task Details:**
> I came up with this dataset in order to know that how long a player can play based on the previous summary stats.
> In this dataset there are 21 features describing the performance measures of each player or you can say the summary of each player.
> Your task is to predict the target variable.
> Target Variable:
> 1-Whether a player's career is equal to or greater than 5 years.
> 0-Career is shorter than 5 years
>Expected Submission
>You have to solve the task primarily using Notebooks
>Evaluation
>Use various Classification Algorithms to predict the target variable with higher accuracy score.
",4080a2d2,0.02857142857142857
1785,2b36742b49c7bc,a50119a7,"## Орчноо бэлдэх (Prepare environment)
- Repo
- Libraries
- Data

### Clone repo",c8f8a96d,0.02857142857142857
1789,867a9f977fa945,65cd90f2," # Sentence tokenization
 ",2740fcca,0.02857142857142857
1794,80f86fa2d88ff1,716e847d,Get the data for processing,f5cead1f,0.02857142857142857
1800,7454fdc444df16,154ce185,"# Preparation & peek at the data structure

## Loading packages",a7818ef5,0.02857142857142857
1801,0fa9979b5690e9,3d52153c,"Vamos explorar um conjunto de dados diferentes: uma base de vinhos; separar os dados e verificar como a mudança de parâmetros afeta o resultado. Há uma forma automatizada de fazer isso, mas isso será conteúdo do próximo encontro. Por enquanto, vamos fazer tudo manualmente para entender o processo.",c26eea94,0.02857142857142857
1803,171494b45650a2,a5dcb5b6,## ***1. Preparation***,9c8cc578,0.02857142857142857
1806,a566b5b7c374e7,1b1b33a3,#### HTML Table Format,b3dc5545,0.02877697841726619
1810,7e275c8d5ff2a0,de224c0a,T,b3afcc98,0.028985507246376812
1813,a1a31459abf078,2858f24b,"# Table of Contents - 
* [Overview of RAPIDS](#rapids)
* [Importing raw datasets](#import)
* [Exploratory Analysis](#eda)
* [Feature Engineering ](#feature)
* [Model Development & Feature Importance](#model)
* [Predictions](#preds)



### Questions answered in the EDA 
* How does the average correct answer rate vary across questions in the training dataset?
* How many questions does an average user answer over his learning journey? What is the total span of data in number of days for an average user?
* What is the distribution of number of appearances of a question on training dataset?
* Does a student's likelihood to answer a question correctly improves over time?
* What is the average size of a question bundle?
* Does watching lectures improves the chances of users answering a question correctly?
* What is the average time spent by users in reading the explanation for a prior question bundle?
* How does the correct answer rate vary across different tags? Does the count of total tags in a question have an impact on the correct answer rate for a question?
",66fc0f54,0.028985507246376812
1815,0e2a23fbe41ca9,a2968a11,"# Card Data


",64e4762c,0.028985507246376812
1816,9d9da6c439b96b,a30b4152,## Load and Describe Data,361cc7d9,0.028985507246376812
1817,b01ee6cb674fa3,5d7ad071,"# Columns rename
Rename the columns for easy typing.

space substitution for _ and capital drop",a8ffd35e,0.028985507246376812
1818,ea4e559a86d613,cb505dd7,**Importing libraries**,eff47843,0.028985507246376812
1823,98a6794067932a,f271b97f,La cellule de code ci-dessous permet d'importer les différents modules qui seront utilisés lors des nombreuses analyses effectuées dans le cadre de ce projet.,08600fe2,0.02912621359223301
1828,30fdc4a6e3c1db,a610bdf3,# 2. Reading the dataset,6111ddee,0.029239766081871343
1829,ab657da5329e3f,02143223,# TPU or GPU detection,021526f8,0.029411764705882353
1830,7f74a04ae75792,9e7d6bf7,"### How big is the dataset? (number of rows, features and total datapoints)",d01e91da,0.029411764705882353
1831,7cfd96218dd933,41f509c5,# PACKAGES AND LIBRARIES,7c34d96c,0.029411764705882353
1835,71b75664517244,366bbfd1,"As you probably know already this dataset is provide with final standing every season and managers as well.
For now we will go through the final standing first. There are 28 season in total, here is how you set up a good
Dataframe for this dataset.",fc905af5,0.029411764705882353
1836,8f50c9c16db95f,fad2da2a,"# Introduction <a id=""intro""></a>

**""Kick the ball harder, if you want to kick it further.""** This is what I learned from a lot of people saying. 

But how true is it? Or what does it exactly mean when we say ""kicking it harder""? Data from NFL games reveal to us some insight which I'd like to share with you in this notebook.

Kickoff play is the most common type in special plays from season 2018 to 2020. Even though with six different outcomes, the kicking team has basically one ultimate goal - gaining as many yards as possible after the session ends. In this notebook, I'd bring up the **kick length** of the ball as an actionable and robust metric to evaluate how good a kickoff it is. This metric has a high positive association with the number of gained yards, since the receiving team needs to return the ball from a very low yard line if the ball is kicked long distance. Apart from that, with using the multiple linear regression method, I also find two relevant variables which might affect the kick length. Those are 

1. **Foot speed** in the approach, usually seen in a long approach distance, and it is also a predictor of the last step length, which is related with how much engergy will be transmitted to the ball, according to the formula of kinetic energy.

2. The **direction of the ball being kicked to**. If the ball was kicked to the right side of the kicker, the length is usually shorter than to the center or to the left. The hypothesis behind is kicking to the right takes more energy from the body because of a larger pivoting angle. This cost of energy results in less volume brought to the ball.

",26cc763a,0.029411764705882353
1838,395ed8e0b4fd17,63f1d9f1,# <center>📊G-RESEARCH PLOTS + EDA 📊 </center>,7573ea31,0.029411764705882353
1839,55c34673c1f760,67765244,# Elberth Adrián Garro Sánchez (1-1644-0594),2663c47f,0.029411764705882353
1842,99821bc6a45be6,f177176f,# Problem Definition:,b9d59346,0.029411764705882353
1844,21bce4ec54b3fa,7d2cb432,"# Data exploration
The dataset is taken from Kaggle: https://www.kaggle.com/c/santander-customer-transaction-prediction

Load and inspect data,  check dimensions, data types, missing values, target variable",35546e30,0.029411764705882353
1845,a0a5baa6c7e12a,5a126cdb,"# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">EDA Findings in Training Set</div>

This section will be focused on the discoveries and insights we obtained from the *AutoViz*-automated EDA for the contest dataset.

**Note:** The diagrams used in the subsecctions have been automatically generated by running the code in the next chapter. The charts were generated in a matter of minutes thus exposing you to spend time on more thinking-intensive activities and drawing insights from your data.",551d41de,0.029411764705882353
1848,3d905ce4828057,e3caed07,"Our goal is to capture the buying behavior of the whole audience in a dataset and to make it possible to predict when individual characteristics come.
We will use probabilistic methods for this process.

**CLTV = (BG/NBD Model) * (Gamma Gamma Submodel)**",5b006cc3,0.029411764705882353
1849,02b7e38902069e,158150d0,"#Three Important NLP Libraries for Indian Languages

Author: MOHD SANAD ZAKI RIZVI, JANUARY 23, 2020 

Text Processing for Indian Languages using Python:

iNLTK

Indic NLP Library

Stanza - https://stanfordnlp.github.io/stanza/index.html

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.029411764705882353
1850,e4c6dd957eb5ce,24db7fe4,"## <font color=""red""> I'm near of grandmaster tier, so, if you find this kernel useful or interesting, please don't forget to upvote the kernel =)</font>",2e383665,0.029411764705882353
1851,52cfd66e9ec908,ad80a8c8,"This new Lyft competition is tasking us, the participants, to predict the motion of external cars, cyclists, pedestrians etc. to assist self-driving cars. This is a step ahead from last year's competition, where we were tasked with detecting three-dimensional objects, like stop signs, to teach AVs how to recognize these. ",c74adcdf,0.029411764705882353
1852,842547b2def18c,f358ff54,"## データを取得

PythonのPandasパッケージはデータセットの処理することに役立ちます．まず，train/testデータセットをPandas DataFrameとして取得します．また，一定の処理のために，両方のデータセットを結合します．",b8efde6d,0.029411764705882353
1854,6a80f915608fc2,bb912c44,"## <a id=""Preliminaries"">Preliminaries</a>
Back to <a href=""#Index"">Index</a> <br>",636938eb,0.02976190476190476
1855,e58e68e4eeefe5,f7a62f37,# EDA,a87662ce,0.029850746268656716
1858,a4aa36df07fd53,6a57156a,Untuk mengetahui summary dari data yang kita miliki jalankan :,d2f42b6d,0.029850746268656716
1859,21413205980558,d8d8fd92,"# Section Ⅰ Business analyse
",84197de0,0.029850746268656716
1860,20b372b6e4e276,84026476,"<a class=""anchor"" id=""0.1""></a>
## Table of Contents

1. [Import libraries](#1)
1. [Download data & FE](#2)
1. [Model tuning](#3)
   - [My upgrade of parameters](#3.1)
   - [Model training](#3.2)
1. [Submission](#4)
1. [Outlier analysis](#5)
    - [Training prediction result visualization](#5.1)
    - [WordCloud](#5.2)
    - [Subtext analysis](#5.3)
    - [Metric analysis](#5.4)
    - [PCA visualization](#5.5)
    - [Clustering](#5.6)",ec8b0860,0.029850746268656716
1861,83df814455f06c,6ae50fc2,"# **1. Introduction to Decision Tree algorithm** <a class=""anchor"" id=""1""></a>

[Table of Contents](#0.1)

A Decision Tree algorithm is one of the most popular machine learning algorithms. It uses a tree like structure and their possible combinations to solve a particular problem. It belongs to the class of supervised learning algorithms where it can be used for both classification and regression purposes. 


A decision tree is a structure that includes a root node, branches, and leaf nodes. Each internal node denotes a test on an attribute, each branch denotes the outcome of a test, and each leaf node holds a class label. The topmost node in the tree is the root node. 


We make some assumptions while implementing the Decision-Tree algorithm. These are listed below:-

1. At the beginning, the whole training set is considered as the root.
2. Feature values need to be categorical. If the values are continuous then they are discretized prior to building the model.
3. Records are distributed recursively on the basis of attribute values.
4. Order to placing attributes as root or internal node of the tree is done by using some statistical approach.


I will describe Decision Tree terminology in later section.
",c9cff71a,0.03
1862,b10bd75889dad9,b720b713,#### get all the 9th month columns to derive the target variable,ee00ceee,0.03
1863,4cd25e50c7e007,35302b0b,# Reading and Understanding the Data,ceb0c525,0.03
1864,ba4b3bd184acbb,70db51fe,***,0f5de724,0.03007518796992481
1865,4b64dc653fb7eb,65595fee,Starting by importing required libraries.,57675cc2,0.030303030303030304
1873,b241b847319d13,d2414280,**Task:** To identify and localize COVID-19 abnormalities on chest radiographs,0fb698f0,0.030303030303030304
1875,2f964d08c25d93,fb789db9,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",1f2e4468,0.030303030303030304
1877,a2444ab5d5f147,1685b68f,### Imports,10617755,0.030303030303030304
1878,ee23a565163388,37db045e,# **Objective**,88aacbc4,0.030534351145038167
1880,09751c520b0616,95e60c17," - Importing dataset and making ready for preprocessing<br>
 Traing data - train.csv<br>
Test data - test.csv",a4d0c7e9,0.03076923076923077
1882,3cb96bd8eb364b,4572bca5,"### Repository
This project source code and datasets may be found in our [gitlab](https://gitlab.com/carybe/fiot-tf) and [github](https://github.com/carybe/fiot-tf) (mirror) repositories.",3157af7e,0.03076923076923077
1887,1645979263c148,7a66dfb2,"
![](https://i.pinimg.com/564x/d8/ad/c5/d8adc5543f0f47ebb4e7da821f428937.jpg)",fa11663e,0.03076923076923077
1888,485de87c50af82,c273ee64,### Loading Data,a5bd438e,0.03076923076923077
1889,c115e287523aab,70ae2180,"# Notebooks:
* train: [[TF] PetFinder: Image [TPU][Train] 🐶](https://www.kaggle.com/awsaf49/tf-petfinder-image-tpu-train)
* infer: [[TF] PetFinder: Image [TPU][Infer] 🐶](https://www.kaggle.com/awsaf49/tf-petfinder-image-tpu-infer)
",feb1288b,0.03076923076923077
1891,d07915a6e6992e,fe507f12,"# A Thank You Note..!!!

Wherever whenever required I have given due credit to my fellow Kagglers which they deserve for their hard work.Special thanks to Yassine. I tried various ML algorithms but I found the voting method most useful based on the variables I had. ",2b912140,0.03076923076923077
1892,8ec771f5600a61,0c3b28fe," # Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",48364c1f,0.030927835051546393
1894,225b4fe5d3894a,3d3126a9,"<a id=""2""></a>
## 2. Load the Data",4b4197b3,0.030927835051546393
1896,117fc0956643d0,59e0cde1,"## Goal & Approach

Since the beginning of 2020, many articles addressing COVID-19 have been rapidly published. Due to the overflow of information, people in medical communities face difficulties getting helpful information that can answer their questions. This notebook provides an overview of information extraction to fight against the information overflow.

Our framework utilizes topic modeling to extract the relevant articles to COVID-19. Then, we use a pre-trained ALBERT model to get an excerpt from each article obtained from the previous methods that get answer questions related to COVID-19. The ALBERT model is pre-trained with two different datasets, SQuAD v1.1 and BioASQ 6b factoid QA pairs. SQuAD is a reading comprehension dataset with over 100k QA pairs. Since there is no publicly available ALBERT model pre-trained on the general QA task, we first utilize the SQuAD dataset so that our model targets explicitly the QA task. After pretraining on SQuAD v1.1, we fine-tune our model to the medical domain using BioASQ factoid QA pairs. 

The main advantage of our framework is that it can be applied to any queries related to COVID-19, as there is no additional tuning required. 
The diagram below shows the overall workflow of our approach to tackling the COVID19 task.",68cef9fd,0.03125
1897,bd0e173abb7b52,68dc591f,"# <center> Assignment #1 (demo)
## <center>  Exploratory data analysis with Pandas
",9bce3b0d,0.03125
1904,0932046e1f485d,856a75c9,**Loading the dataset**,218cc7a3,0.03125
1907,ff3a8ce61fab6a,51a87c93,"We will go through this Nootebook via following items 👇

1. TensorFlow Overview
2. Session
3. Constants
4. Variables
5. Placeholders
6. Operations
7. Graph",9afe1654,0.03125
1908,9daf8b4a46725e,5d910f86,### Importing Libraries,7d9cc411,0.03125
1909,49f2274c1dd516,2071c85c,"## Data Meta-Analysis: Exploring the data, columns, and content
### General Stats",06b0ffee,0.03125
1910,fae5023faa435f,02d077e3,"# Overview 
The stock market is very unpredictable, any geopolitical change can impact the share trend of stocks in the share market, recently we have seen how covid-19 has impacted the stock prices, which is why on financial data doing a  reliable trend analysis is very difficult. The most efficient way to solve this kind of issue is with the help of Machine learning and Deep learning. We use Recurrent Neural Networks for time series forecasting of all the banks under Bank Nifty. 
",b37c893b,0.03125
1911,51a46d0a7597f5,62320db5,**Libraries Import**,e9e25b17,0.03125
1912,68cceffe5bb8ec,c975ce1c,# Setup,dcbfcd6e,0.03125
1914,fdbbd573ba31c2,6a8d50c3,### df_train,f7c28d74,0.03125
1916,fc8e0042411c46,9c421e37,### Data Loading,af476c2a,0.03134796238244514
1917,2ada0305b68956,3b4d101d,### 2. Palette = 'Accent_r',133e26f4,0.03142857142857143
1918,ce9ed5e2d601d7,10bc04a2,"## Data Preprocessing ##

Before we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:
- **Load** the data from CSV files
- **Clean** the data to fix any errors or inconsistencies
- **Encode** the statistical data type (numeric, categorical)
- **Impute** any missing values

We'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.",f58a2f43,0.031496062992125984
1923,06ecf7a304c309,7f51778d,"### 1.2 How Autoencoder work

오토인코더의 수학적 배경에 대해 먼저 알아보겠습니다. 
가장 기본이 되는 아이디어는 '고차원 데이터에서 저차원 표현을 학습하는 것' 입니다.

예제를 통해 알아보도록 하겠습니다.  데이터를 표현할 공간과, 두 개의 변수(x1, x2)로 나타낼 수 있는 데이터를 생각해봅시다. 데이터 매니폴드는 실제 데이터가 존재하는 데이터 표현 공간 내부의 공간입니다.",714de627,0.031746031746031744
1925,f3d5d8917ce5df,c5d06629,"A model expects each row that only has variables for one prediction. So, for this particular contest, each row that's fed to a model needs to be an individual product/store/date/etc combination. This requires unpivoting the original data (via pandas' ""melt"" function) and getting it to look like this:
![image.png](attachment:image.png)

### 2. Do some text manipulation
Maybe just to be slightly more annoying, the column headings for the dates are given in the format of ""F#"" as opposed to the above historic pricing data of ""d_#"" (""forecast"" instead of ""date""), so in order to join the submission predictions to the calendar and pricing data, we'll need to change F# to d_#. Yes, this is just some basic text manipulation, but as I was hacking through this I found it really annoying that they couldn't just keep the date format consistent...

### 3. Make predictions<img src=""https://upload.wikimedia.org/wikipedia/commons/c/c1/George_%22Corpsegrinder%22_Fisher_of_Cannibal_Corpse.jpg"" align = ""right"" width = 200> 
After that, we're finally ready to make some predictions. In this notebook, we'll be using an out of the box Random Forest. 

The point of this notebook isn't to build a highly accurate model or to do the other fun/creative stuff of data science (like feature engineering, comparing differnt models, EDA, etc.); **the point of this notebook is to do the unglamorous grunt work so folks can fork it and do fun stuff yourself.**

*(see the picture of a man who knows how to grunt...)*


### 3. Re-pivot and do some text manipulation and make the submission file
The contest requires that the CSV submission file to be in the format as given (see the first picture)... so you have to re-pivot after you've made your predictions to make the final submission file. Yeah, this is more annoying string manipulaiton work...


Ok, enough of my rambling, let's get to the code...",e45112f8,0.031746031746031744
1926,4b4117cf42ef8d,29eca6e5,# Import the Data,457cd6f4,0.031746031746031744
1928,4c47839b067546,bccece4e,Добавим функции для анализа:,1f517b02,0.031914893617021274
1933,c0ddb77bf32e2b,9177b501,"# EDA & Preprocessing #
This is a data set with 218640 smaples and 23 variables. Knowing our data before starting building model can save hours of pain.

## The NAs##",a0cb45f7,0.03225806451612903
1935,9ceb7278784462,54ee8e15," # <a id='3'> 3.Exploratory Data Analysis</a>
 * Exploratory Data Analysis refers to the critical process of performing 
initial investigations on data so as to discover patterns,to spot anomalies, 
to test hypothesis and to check assumptions with
the help of summary statistics and graphical representations. <br>

* Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.<br>

* Generate questions about your data.<br>

* Search for answers by visualising, transforming, and modelling your data. <br>

* Use what you learn to refine your questions and/or generate new questions. <br>

* EDA is not a formal process with a strict set of rules.<br> 
* More than anything, EDA  is a state of mind.<br> 
* During the initial phases of EDA you should feel free to investigate every idea that occurs to you.<br> 
* Some of these ideas will pan out, and some will be dead ends.<br>
* As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others.<br>

* Let's start exploring our data",3768a567,0.03225806451612903
1937,16862cb02d73d5,a9349d6e,"A sudden spike or dip in a metric is an **anomalous** behavior and both the cases needs attention. Detection of anomaly can be solved by supervised learning algorithms if we have information on anomalous behavior before modeling, but initially without feedback its difficult to identify that points. So we model this as an **unsupervised** problem using algorithms like **Isolation Forest**,One class SVM and LSTM. Here we are identifying anomalies using isolation forest.",d7ffa1a6,0.03225806451612903
1938,0caaec057f7184,9a6949b7,## Original features,b875533e,0.03225806451612903
1942,45568f3ca94aca,47c581c7,## Read in and Explore Data,2c6ea8e4,0.03225806451612903
1944,7dec6bdea6d779,fe8a9a41,"The EDA is done in a [separate Kernel](https://www.kaggle.com/maxlenormand/first-eda-to-get-started)

This is the first iterations I am doing, simply to have a relevant predicted output. Future work will consist of improving this along multiple aspects.",18be5949,0.03225806451612903
1945,b05ee1ea1c8269,84bf6aad,# Omicron is spreading all over the world!,19e4d303,0.03225806451612903
1948,0cb456a5456cf9,a1bf60fd,# PART 1.<br>Have a feel of hotel data using python pandas<br> 简单的浏览数据,5701729c,0.03225806451612903
1950,0925f172b5eb74,b6a6d1a6,### EfficientNet requires special installation,ec34cd72,0.03225806451612903
1952,f15eac23fbcc9d,df3b5222,"This is just a simple model developed to try out fast.ai's RF approach on MNIST dataset. I know RF might not be the best model to solve a image recognition problem, but I want to see how RF can adapt to different type of problems and explore ways to improve the model.",ea46d8af,0.03225806451612903
1956,fdc3afd309b850,52aaa623,"<a id=""dex""></a>
# 4 Data Extraction",966bde38,0.032407407407407406
1960,601e18072783b4,084a7f9f,# IMDb dataset,36b2b1fa,0.03278688524590164
1962,918040fad252ec,6e30623d,**Import** Kebutuhan Plugin,966fcd8f,0.03278688524590164
1968,e9b9663777db82,2c300829,"We created some dashboards to get initial insights from Home Sales Data.Our data contains lots of categorical features, so it is needed to monitor the distribution of those features.",648e8507,0.03305785123966942
1969,864302b10e7730,39765bd9,# Importing *Matplotlib* and *Seaborn*,e9dd1d2d,0.03333333333333333
1970,6998861ff6ff01,a3895b8b,"# Get our environment set up
________

The first thing we'll need to do is load in the libraries and datasets we'll be using. For today, we'll be working with two datasets: one containing information on earthquakes that occured between 1965 and 2016, and another that contains information on landslides that occured between 2007 and 2016.

> **Important!** Make sure you run this cell yourself or the rest of your code won't work!",ea9e72cf,0.03333333333333333
1971,be616f0785c32d,ff4afb50,"

<div id=""top""></div>

# Subtyping COVID-19 Therapeutic Research Findings


#### Yuanfang Guan

#### Michigan Medicine, University of Michigan, Ann Arbor


## Summary

The goal of this exercise is to study this literature provided by the Kaggle COVID-19 challenge organizing team, and to subtype the COVID-19 therapeutic research findings. Specifically, we carried out the following four parts of analyses:

**[Part A. Drugs that have been used in clinical trials for COVID-19](#PartA).** We identified and characterized the [drugs on clinical trials](#PartAdrug) by integrating the FDA drug database and PubChem repository. We hand-curated and summarized the [reported effectiveness](#PartAeff) for each drug. We presented the mutual similarity of [chemical structures](#PartAchem) across the drugs used in clinical trials. 

We [categorized](#PartAcat) the drugs based on their molecular mechanisms, which can facilitate the discovery of related drugs of similar mechanisms and the creation of an effective cocktail treatment: 

 **[Category 1](#PartAcat1).** RNA mutagens 
 
 **[Category 2](#PartAcat2).** Protease inhibitors  
 
 **[Category 3](#PartAcat3).** Virus-entry blockers 

 **[Category 4](#PartAcat4).** Virus-release blockers
 
 **[Category 5](#PartAcat5).** Monoclonal antibodies
 

**[Part B. Drugs that have been proposed by computational works](#PartB).** We identified the computational publications for COVID-19 drugs, categorized their approaches into the following categories and listed their previous applications in other disease domains, and potential limitations. 
 
**[Category 1](#PartBcat1).** Gene-gene network-based algorithms. 

**[Category 2](#PartBcat2).** Expression-based algorithms
 
**[Category 3](#PartBcat3).** Docking simulation or protein structure-based for 
                     
   [Category 3.a](#PartBcat3.a). Small molecules 
                    
   [Category 3.b](#PartBcat3.b). Monoclonal antibodies
 

**[Part C. Drugs that have been proposed by in vitro experiments of COVID-19 invading human cells](#PartC).** We characterized the [chemical structures](#PartCchem) and analyzed the chemical similarity for this group.

For this list, other than literature mining, we carried out a [machine learning experiment](#PartCexp) to prioritize previously unexplored FDA-approved drugs (in order to circumvent ADMET evaluation) for repurposing. After hand-removing the contaminations, we identified the following top candidates for repurposing: OLUMIANT(Baricitinib) used to treat rheumatoid arthritis, BRIMONIDINE, used to treat glaucoma, EDURANT(rilpivirine) used to treat Human Immunodeficiency Virus-1 (HIV-1), MARPLAN used to treat depression, Corlanor (ivabradine) used to reduce the spontaneous pacemaker activity of the cardiac sinus node. We listed the potential [contaminations/biases](#PartClim) in this and relevant protein binding-associated approaches.

**[Part D. Epitope study for vaccines](#PartD)** We categorized vaccine studies by their approaches and discussed the background and [limitations](#PartDlim) in relationship to evolution:

**[Approach 1](#PartDcat1).** Homology-based with SARS-COV (the 2003 version of SARS), other coronavirus or Ebola.

**[Approach 2](#PartDcat2).** Immunoinformatics including docking/molecular dynamics/protein structures/antigeniticity predictions.

We hand-curated a list of [147 epitopes](#PartDepi) from these publications and their supplementary materials, grouped them by the source virus proteins, human T-cell/B-cell targets and MHC class. We merged all published epitopes into [124 consolidated groups](#PartDconsolidate) by partial sub-sequence search and [91 unique virus protein sequence regions](#PartDuniq) by BFS search algorithms. We hope the above lists will serve as the 'wisdom-of-the-crowd' reference for vaccine development.

**Summary points and future recommended research topics for Phase 2. **
    
 **Conclusion 1.** There is not a single drug for which consistent positive response has been reported. 
 
 **Conclusion 2.** There are overlaps between the drugs in clinical trials, proposed by computational analysis and proposed by in vitro experiments. However, some of the overlaps, especially those with computational analysis may come from a circularity in the methods.
 
 **Conclusion 3.** Drug candidates proposed by computation and in vitro screening could be biased towards cancer-related targeted therapy and substantially contaminated by existing literature or sometimes anecdotes. This bias/contamination may affect a significant number of computation-based drug-repurposing studies including our own work, and certainly not limited to COVID-19.
 
 **Future direction 1.** Disagreement in the reported drug response can root from differences in dosage, baseline biometrics and population groups. With more clinical trial results coming in, the next step is to carry out meta-analysis to stratify these variables.
 
 **Future direction 2.** Analyzing vaccine findings at this stage is premature as there is no clinical effectiveness study yet. It will be meaningful to make genome variation and vaccines (or maybe antibodies as well) into the same topic, therefore allowing connecting the genome variations to what fraction of the virus strains that a vaccine could cover. 
 
 **Future direction 3.** We suggest a topic on news (e.g., google news) retrieval for therapeutic development, as many (if not most) treatment responses may not first appear in manuscripts. 

Finally, we would like to take this opportunity to make one comment: Literature tends to be biased towards reporting positive results，known biology (e.g., cancer and immuno- drugs), and anecdotes, and we should take the results of this exercise and other documents critically. 
<div id=""PartA""></div>




[Go Top](#top)



## Part A Subtyping drugs currently in clinical trial
##### A.1 Methods: 
We first counted how many times each FDA drug occured in the documents provided by Kaggle:",b78e18aa,0.03333333333333333
1973,37b09262279764,cabf0c79,### Loading the data,37c4c417,0.03333333333333333
1975,061d6757dfbce0,e42593fc,"## Evaluation Metric

We will be evaluated by the metirc `Root Mean Squared Logarithmic Error`.

The RMSLE is calculated as:

$$ \epsilon = \sqrt { \frac{1}{n} \sum_{i=1}^{n}{(log(p_i + 1) - log(a_i +1)) ^ 2} }$$ 

Where:

- $ \epsilon $ is the RMSLE value (score)
- $ n $ is the total number of observations in the (public/private) data set,
- $ p_i $ is your prediction of target, and
- $ a_i $ is the actual target for $i$.
- $log(x)$ is the natural logarithm of $x$",c0c2915a,0.03333333333333333
1976,07f5853e4db8f8,f8dd50e9,# EDA,d13c2c32,0.03333333333333333
1978,8d575f495686ab,947122dc,## Importing necessary libraries,0fc16499,0.03333333333333333
1979,cf4d1c1ad1476c,bd1566c8,# TPU and [Resources](http://https://codelabs.developers.google.com/codelabs/keras-flowers-tpu/#2),768c1a59,0.03333333333333333
1980,712198370d5521,7f4a8022,"<a id=""1""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">IMPORTING LIBRARIES</p>",5882e04c,0.03333333333333333
1984,2bd6c370695ea7,834142ce,## Image model loading,cbe6aec8,0.03333333333333333
1987,541d0fa0e26b80,ecab7168,# Importing Libraries,a29e0f29,0.03333333333333333
1988,f6488772605bb5,8aeee9b0,## **Imports**,068d4697,0.03333333333333333
1990,b547f0f38f7744,4107c464,"### Add Data

[Global Wheat Detection](https://www.kaggle.com/c/global-wheat-detection)

- train.csv - the training data
- sample_submission.csv - a sample submission file in the correct format
- train.zip - training images
- test.zip - test images",b6ba66b3,0.03333333333333333
1992,63d0d9b9a8c7d2,d053c2eb,# Reading the datasets,e32e5933,0.03333333333333333
1994,bc058fe14d3d1b,e386c7be,### 0.1 Import Packages,d0273670,0.03333333333333333
1995,6a1ae8234c7653,c984d36c,This is my first trial writing a notebook in Kaggle.,2d643c72,0.03333333333333333
1996,c18267b203f28a,8d5dd32c,# Set up environment,09ca8efb,0.03333333333333333
1998,c91c137284976f,679540c6,# 1. Preparing data,c6888c0a,0.03333333333333333
2000,91eaec994e0c6f,959e6a20,"- `calendar.csv` - Contains information about the dates on which the products are sold.
- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]
- `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.
- `sell_prices.csv` - Contains information about the price of the products sold per store and date.
- `sales_train_evaluation.csv` - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)",376aef10,0.03333333333333333
2001,d6cbd7160961dc,bd561b37,"# 1. Notebook Goals

* 1) Introduce the reader to the Benford's law. We begin by explaining the theory and how it can be applied to identify data manipulation. We also present the Chi-Squared test, that will later be used to identify if some data set is statistically fraudulent according to Benford's theory.


* 2) Explain our utility script ""RKN Module - Benford Law"". This script is a useful tool for researchers using Benford's Law to detect data manipulatio and fraud.


* 3) Apply our utility script ""RKN Module - Benford Law"" on 3 different datasets.
    * 2.1) Application on a random generated dataset. (Type of analysis disaggregated)
    * 2.2) Application on a sampled data set from Brazilian cities and the town hall's expenditure during 2010. (Type of analysis disaggregated)
    * 2.3) Application on fibonacci sequence. (Type of analysis aggregated)",36d74664,0.03333333333333333
2002,726833f92fb87a,a6418bb9,"<img src=""https://i.imgur.com/8xFCrIJ.png"" width=""1000px"">",7dc5e1b6,0.03355704697986577
2006,312135b445bd23,25f2caf2,"# **Goal**
Our goal is to build an infrastructure that can serve whoever fights the novel COVID-19 virus (researches, doctors, health care workers, etc.) by finding the most useful information using state-of-the-art NLP tools and algorithms. We hope this project will be useful and that our efforts will yield fruits to make our world without the COVID-19 virus. 
",8ced381f,0.033707865168539325
2007,5f27526aa6c113,dbe5a48c,# Data Loading,a5c26ab6,0.033707865168539325
2009,04ff2af52f147b,737ac2a5,"**Understand the Dataset:**

To begin, we load in the data and then familiarize ourselves with the coarse grain aspects like datatypes, null values, etc.

Numerical:

- Training set has 891 samples
- Testing set has 418 samples
- Both *Age* and *Fare* features have distributions biased towards lower values with <1% high outliers
- 76% of passengers did not travel with parents or children
- 30% of the passengers had siblings and/or spouse aboard
- 38% of passengers survived in the training set compared to the actual survival rate of 32%

These can be seen by modifying *percentiles* in the following cell.

Categorical:

- 77% of *Cabin* values are missing, and of the ones that exist, 37% are duplicates
- There are 3 places to embark from, with 70% embarking from S=Southampton
- All but two values for *Name* are unique
- Male and Female options for *Sex*, with 64% of passengers being male
- 29% of *Ticket* values are duplicates and there is at least one ticket that shows up 11 times",d5f37be9,0.033707865168539325
2012,a077820f7ab459,2475ace3,"### Work Flow
- Create DINO attention map images and saved
- Transfer learning using DINO attention map image",05a43104,0.03389830508474576
2015,a44368590e878a,87e313e6,# Datasets,77743ba8,0.03389830508474576
2016,9169c4e9c33c90,fc070678,"[Summary](#Summary)

[Dataset Overview](#Dataset_Overview)

[Title](#Title)

[Author](#Author)

[User Rating](#User_Rating)

[Reviews](#Reviews)

[Price](#Price)

[Year](#Year)

[Genre](#Genre)",725bf880,0.03389830508474576
2018,b9bc7dc9f582e5,f160d0a4,# Importing Data,15cc4d28,0.03389830508474576
2019,a81661cc35d8d2,363cc6b5,***,3331f113,0.03389830508474576
2020,dac3c8204a2d1b,09ef4c0b,"**FEATURES:**

* Name - Name of the Book
* Author - The author of the Book
* User Rating - Amazon User Rating
* Reviews - Number of written reviews on amazon
* Price - The price of the book
* Year - The Year(s) it ranked on the bestseller
* Genre - Whether fiction or non-fiction",b0d2d0dc,0.03389830508474576
2021,1294fb4c86f993,aea3ff4a,"### Census data base
> The U.S. census data is found in a .csv file. It contains several variables at the state level. Most variables just have one data point per state (2016), but a few have data for more than one year.

***
***
### So, *The plan* here is as follows:
* Load both guns and census data files.
* Clean both database.
* Get insights from `guns.csv`
* Filter `census.csv` for 2016 data to get insights from it.
* Merge it with `guns.csv` and get the `guns_per_capita` parameter.
* Examine which parameter correlates most with the `guns_per_capita`",4471e513,0.03389830508474576
2022,f2e5e9fb9eaaf7,3bb4e930,"[back to top](#table-of-contents)
<a id=""2""></a>
# 2 Preparations
Preparing packages and data that will be used in the analysis process. Packages that will be loaded are mainly for data manipulation, data visualization and modeling. There are 2 datasets that are used in the analysis, they are train and test dataset. The main use of train dataset is to train models and use it to predict test dataset. While sample submission file is used to informed participants on the expected submission for the competition. *(to see the details, please expand)*",048e0d08,0.03389830508474576
2024,bb0905d33ae417,fb43c0d3,"# Contents
1. [Skeleton code](#Skeleton-code)
* [Import packages](#Import-packages)
* [Exploratory data analysis](#Exploratory-data-analysis)
* [Data loading and preparation](#Data-loading-and-preparation)
* [Model creation](#Model-creation)
* [Model training](#Model-training).
* [Model interpretation](#Model-interpretation)
* [Transfer learning](#Transfer-learning)
* [Generating submission](#Generating-submission)
* [Future work](#Future-work)
* [Acknowledgements](#Acknowledgements)",25fd1965,0.03389830508474576
2030,49ee86d074de69,44d060d2,"<a id = ""2""></a><br>
## Load Dataset",71ccc6d3,0.03418803418803419
2031,fdc9f4863744b1,5ea38f08,I will also set the style use of seaborn and matplotlib for my visualizations.,b4529365,0.03424657534246575
2032,738bfced935b69,eb738271,"**I had checking before every data set and i saw six problems:**
* In hyundi dataset need tax(£) column to rename to tax.
* no need to read unclean focus & unlclean cclass data sets.
* The focus & cclass datasets are 7 columns for each data sets while the remainder of datasets are 9 columns for each data sets.
* we need to drop duplicate rows and reset index after that.
* we need to remove space in model column by replace.
* The max value in year column is 2060 we need to checking and replace with right value.",2d3c592d,0.03424657534246575
2035,2c5cb484988da2,c2ef15e3,We set options to display all the columns from the dataset.,d94f9784,0.034482758620689655
2041,1750367e54f407,686a1123,Below are some version notes which were written at version 20 so I only included what I could remember.,a8e655b2,0.034482758620689655
2043,fb9296ecd0cb2a,760fb16c,## Explore all csv,aa66d98c,0.034482758620689655
2044,00d295edcd117e,d6d494b9,## 关于数据,f5810f4b,0.034482758620689655
2046,858da4bb312f67,83dff370,"### What is Cycle GAN?
Cycle GAN is the image style transfer. It translates an image from a source domain to a target domain. Zebra to Horse / Horse to Zebra is well-known examples.

Following images are from original paper.
![image.png](attachment:image.png)

For further information, please check original paper and Monet competition.
* original paper: https://arxiv.org/pdf/1703.10593.pdf
* Monet Competition: https://www.kaggle.com/c/gan-getting-started/overview

### Why Cycle GAN?
In this competition, we need to classify the images of the same plant. The only difference is the health condition.
That inspired me to use Cycle GAN as a health transfer. It might changes the color of leaves, might makes some holes on the leaves.",9cca4391,0.034482758620689655
2048,5ea840754577e3,4aede7ba,# Data Summary,9cf9b73f,0.034482758620689655
2049,cd10f3afd970b3,f337bc95,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",2db3c8e4,0.034482758620689655
2050,18a96bb5711ed9,ab71cbe6,"# Background of the dataset <br> 

> Missing Migrants Project tracks deaths of migrants, including refugees and asylum-seekers, who have gone missing along mixed migration routes worldwide. The research behind this project began with the October 2013 tragedies, when at least 368 individuals died in two shipwrecks near the Italian island of Lampedusa. Since then, Missing Migrants Project has developed into an important hub and advocacy source of information that media, researchers, and the general public access for the latest information. 
With a count surpassing 60,000 over the last two decades, IOM calls on all the world’s governments to address what it describes as “an epidemic of crime and abuse.""
Missing Migrants Project is made possible by funding by UK Aid from the Government of the United Kingdom; however, the views expressed do not necessarily reflect the Government of the United Kingdom’s official policies. [(source)](https://missingmigrants.iom.int/about)
 
 <br>

Like the content of the dataset mentions, this dataset represents minimum estimates, as many deaths during migrations are unrecorded. Thanks to **@Stefano Nocco** for sharing this dataset and raising awareness of the issues affecting refugees, asylum seekers and stateless persons.[His kernel](https://www.kaggle.com/snocco/dead-on-mediterranean-routes) has good EDA, so check it out as well. 
",e79768db,0.034482758620689655
2051,6903d3f38c6a66,d7797d92,"### **Hello Everyone!**

I am Gaurav Ahuja And This is a notebook which organizes various tips and contents of matplotlib.

**Matplotlib** is a plotting library for the Python programming language and its numerical mathematics extension **NumPy**.",6067ce5e,0.034482758620689655
2052,a1ba5ffd30dbde,1c0cf9de,### Imorting Essential Libraries,48e57546,0.034482758620689655
2053,6a1d04e8153df3,e9266e91,"# Operation :

To analyse the given the data I am using PYTHON , Becuase this language had so many machine leanring libraries which we are going to use. 
 
1. Read the data 
2. Understand the data.
3. Visualize the data
4. Conclusion on what we visualization from the data",38572b05,0.034482758620689655
2056,9535bb04ae042c,7cc9be93,# **Step.1: Make Machine Learning Model**,165b6fae,0.034482758620689655
2057,4b7039cb44a54c,8f23e550,# Get GPU Info,24e806af,0.034482758620689655
2061,a4f0a3e1316ff9,5e83b103,# Load libraries,53bf0160,0.034482758620689655
2062,ee9ddc756b2d4a,5a528931,"### Features extraction

**1. Primo metodo: efficientnetB5**

References: ""EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"" Mingxing Tan 1 Quoc V. Le 1


Efficientnet è una recente classe di modelli di reti convoluzionali; al momento ne esistono 8 tipi (dalla B0 alla B7), e presentano via via maggiore efficienza e accuratezza. 

Una possibile applicazione di queste reti convoluzionali è in diagnostica medica.

Ho utilizzato efficientnetB5 (in model.summary() è possibile vedere nel dettaglio le componenti di questa rete convoluzionale), inzializzata con i pesi di imagenet. Dunque ho utilizzato la rete ""pretrained"" per estrarre i features vectors dalle immagini, che ho poi utilizzato nella vera e propria parte di training.

Per estrarre i feature vectors ho ""tagliato"" la rete al layer 'global_average_pooling2d', uno dei layer finali, dove le informazioni piu dettagliate vengono elaborate (dunque la presenza di un oggetto estraneo nel cervello potrebbe essere messa in evidenza negli ultimi layer; avessi preso un layer iniziale la distinzione tra cervello sano e malato sarebbe stata meno evidente).

**2. Secondo metodo: non negative matrix factorization**

Ho implementato NMF di rango 1 sulle immagini per estrarre un altro tipo di features vectors.

Un'immagine può essere vista come una matrice, quindi è possibile implementare matrix factorization (con opportuna normalizzazione).

NMF è un metodo di apprendimento non supervisionato che mette in evidenza le parti delle immagini e il loro peso; questo algoritmo permette di fare 'image encoding' catturando le informazioni fondamentali dall'immagine.
",e367eab3,0.034482758620689655
2065,1cd8be6e679620,c1c7484e,"**What do we have**?

We have a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position.

There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.
Files

*     train.json - the training data
*     test.json - the test set, without any columns associated with the ground truth.
*     sample_submission.csv - a sample submission file in the correct format

**Columns**

*     `id` - An arbitrary identifier for each sample.

*     `seqscored` - (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of reactivity, deg and error* columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.

*     `seq_length` - (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of sequence. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.

*     `sequence` - (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of A, G, U, and C for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in seq_scored (note: indexed starting at 0).

*     `structure` - (1x107 string in Train and Public Test, 130 in Private Test) An array of (, ), and . characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.

*     `reactivity` - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are * reactivity values for the first 68 bases as denoted in sequence, and used to determine the likely secondary structure of the RNA sample.

*     `deg_pH10` - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).

*     `deg_Mg_pH10` - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).

*     `deg_50C` - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).

*     `deg_Mg_50C` - (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).

*     `error` - An array of floating point numbers, should have the same length as the corresponding reactivity or deg* columns, calculated errors in experimental values obtained in reactivity and deg* columns.

*     `predicted_loop_type` - (1x107 string) Describes the structural context (also referred to as 'loop type')of each character in sequence. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: S: paired ""Stem"" M: Multiloop I: Internal loop B: Bulge H: Hairpin loop E: dangling End X: eXternal loop",3ce15a43,0.034482758620689655
2066,434f930cb58aee,ea2480b5,Not lets start with the notebook. First we will use [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) extension to reload imported modules before executing the code in the cell,0e1d3554,0.034482758620689655
2068,84127ade6fde87,170ac58d,"* https://www.kaggle.com/dmisky/dlwpt-p1ch2-dog-detection
* https://www.kaggle.com/dmisky/dlwpt-p1ch2-gan-horse-zebra
* https://www.kaggle.com/dmisky/dlwpt-p1ch3-tensors
* https://www.kaggle.com/dmisky/dlwpt-p1ch4-working-with-images
* https://www.kaggle.com/dmisky/dlwpt-p1ch4-3d-images-volumetric-data
* https://www.kaggle.com/dmisky/dlwpt-p1ch4-tabular-data
* https://www.kaggle.com/dmisky/dlwpt-p1ch4-time-series",f55d05b6,0.034482758620689655
2069,20e1ba19eb9b5e,a1731ac5,"# 1. Loading the data
## 1.1 Load data",4569bfc1,0.034482758620689655
2070,401338428b2d1c,622a960a,## Importing the libraries,e4b768be,0.034482758620689655
2072,fb5c6021d127ef,b78bc8eb,"# Stocking rental bikes

![bike rentals](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg/640px-Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg)

You stock bikes for a bike rental company in Austin, ensuring stations have enough bikes for all their riders. You decide to build a model to predict how many riders will start from each station during each hour, capturing patterns in seasonality, time of day, day of the week, etc.

To get started, create a project in GCP and connect to it by running the code cell below. Make sure you have connected the kernel to your GCP account in Settings.",dd05cbd3,0.034482758620689655
2076,bb8f5d7807718b,e610a325,Load the necessary libraries,181ec286,0.034482758620689655
2081,eda49464dd6d1b,629f1a6a,"<font size=""+3"" color='#053c96'><b>This Notebook will cover - </b></font>
### 1. Exploratory Data Analysis
#### * Analysis of Each Variable
### 2. Data Modelling and Evaluation
#### * Random Forest with Sci-Kit Learn
#### * Dense Neural Network with Tensorflow/Keras
### 3. Generation of Submission CSV",8421f81f,0.03496503496503497
2084,3fb15e6e48aec2,c138f0ab,# Creating DataFrame,9d1f4358,0.03508771929824561
2085,30fdc4a6e3c1db,c283eeb0,Listing the available files,6111ddee,0.03508771929824561
2086,fe7360cddc13e5,5b6caa40,<font color='blue'>**TEORİK KISIM**,8979e423,0.03508771929824561
2087,c2a9f2fb3e1594,0d68b93e,"<a id=""ch2""></a>
# A Data Science Framework
1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.
2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are “drowning in data, yet staving for knowledge."" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming ""dirty data"" to ""clean data.""
3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn “wild” data into “manageable” data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.
4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.
5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that’s used as actionable intelligence) at worst.
6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html).
7. **Optimize and Strategize:** This is the ""bionic man"" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your “currency exchange"" rate.",53411c04,0.03508771929824561
2089,5ce12be6e7b90e,0783d9ec,## **Exercise**: `print`,c0ab62dd,0.03508771929824561
2094,e03eb63c1f725d,4398896c,"<a id=""intro""></a>
<font size=""+2"" color=""blue""><b>Introduction and Imports</b></font><br>

<font size=""+1"" color=""magenta"">
There are 2 files one which has true news and the other fake news.
</font>",e204b7e3,0.03508771929824561
2097,513ce405d7f6a3,caf37aaa,# Data loading,8461e086,0.03529411764705882
2098,869a39a3d4dea2,1ff3fd14,"* pip install numpy
* pip install scipy
* pip install matplotlib
* Mahotas to complement OpenCV
* pip install mahotas
* pip install scikit-learn
* pip install -U scikit-image",9020daf8,0.03529411764705882
2100,a5a419dc7245b0,ac2fd3ce,#### Importing Package,4279726e,0.035398230088495575
2101,c9b4e282e4e2c1,fd3da60a,"**A)Dataset InjuryRecord**

1-Reading and preprocessing the data.",f44d339f,0.035398230088495575
2104,957e035ba5b9d5,6f7d2e79,# Dataset Prep,778ab3d3,0.03546099290780142
2105,b7298d6aaff625,95aadd4f,"## Digit Recognizer 

This is my 2nd Notebook with just a few tweaks from the last notebook so I'm not gonna add more comments but just the code.

Problem Statement & Data background :- The Bacis Digit Recognization using the pouplar Dataset MNIST the AKA Hello world of Computer vision.

The Data have Handwritten Digits from 0-9.

My approach :- I'll be using Keras basics CNN model using 2 layers of Convolutions. 
Changes from the last notebook :- In the last notebook I used the Adam optimizer and got an accuracy of around 98.14 on the validation Data since I used a callback and cancelled training at 99% accuraacy on training Data. 
<br>So in this notebook I'll try to improve my accuracy by using the RMSprop optimizer and having 50 epochs with batch size 32(Default)",bdf24bf7,0.03571428571428571
2106,f4514ec092a771,64ddde3f,"## Data preparation
So let begin with creating data for training.
Just to remind, preparing data for Kaldi needs three files:
* *wav.scp*: Each line of file is followed by pattern: utterance_id path_to_audio
* *text*: Pattern of file is: utterance_id transcript
* *utt2spk*: Pattern of file is: utterance_id speaker",3739ab1e,0.03571428571428571
2108,98fd05fcc5c3e3,19c7f51e,## Importing Libraries,55fe7ece,0.03571428571428571
2114,8dd655515e7d18,60094ca4,## Data Loading,895f41cf,0.03571428571428571
2117,0dd3ac2d55efd7,4eb75197,# Text processing,e9aa2cc2,0.03571428571428571
2119,df51d4c54fbb91,62fa98c3,Let's load the data.,4226dd72,0.03571428571428571
2122,6e9b4020644836,15ec4b5d," <a id='top'></a>
<div class=""list-group"" id=""list-tab"" role=""tablist"">

# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Table Of Contents</p>
    
* [1. IMPORTING LIBRARIES](#1)
    
* [2. LOADING DATA](#2)
    
* [3. DATA EXPLORATION](#3) 
    * [3.1 DATA CLEANING](#3.1)
    * [3.2 FEATURE ENGINEERING](#3.2)
    * [3.3 FEATURE EXTRACTION](#3.3) 
   
     ",5ad41fc6,0.03571428571428571
2124,d1f92a87a0a1a5,b9013228,"# Introduction
### Recent historical data is used to predict earthquakes. However, almost predicted earthquakes are of small magnitude because the data do not include historical huge earthquakes. In order to predict future huge earthquakes, it is essential to include the historical records of old past huge earthquakes. This notebook is an attempt for it.",2cd610b2,0.03571428571428571
2128,62ae2b200f6b36,b16b4149,"Total number of bikes (count) is the sum of number of registered and casual bikes. On any random day, the registered bikes used and casual bikes used would be independent of each other. Also, it can be intutively concluded that number of registered bikes and number of casual bikes used on a random day would depend differently on the features listed in the data-set.

One such example,it maybe possible that on a working-day the number of casual bikes used would be less while number of registered bikes used would be greater. This example on an intutive level seems plausible, since people on working day are less likely to casually use bikes for some work.

So we model this using linear regression. First on any random day, we calculate number of casual bikes used based on the features in the data-set. And then we calculate the number of registered bikes used. The summation of these two predict the values of total number of bikes used.",7da4ea31,0.03571428571428571
2129,1a285e4c830f3f,ec768309,"### **Import der Module** <a id=""2""></a>
<mark>[Return to Contents](#0)
<hr>

Importing the necessary modules.",360b50e9,0.03571428571428571
2132,f13534449a3750,8b60aa6c,# Table of contents,8b7f3332,0.03571428571428571
2134,3cd78d8d6d56e4,82861348,# Get Data,9f632e94,0.03571428571428571
2135,b8849a04581d32,84a89df2,"# **NFL Big Data Bowl 2022**

#### Punt (punt) is one of the most important actions by a special team. Understanding what leads to scoring after the Punt is one of the key tasks for the coaching staff. This notebook contains an attempt to create a model predicting the likelihood of what the result of the draw a ball performance will be. ",b8a568cd,0.03571428571428571
2140,b01ee6cb674fa3,8dc376eb,# Columns info,a8ffd35e,0.036231884057971016
2148,c80939c7c626cf,6525a343,"# Data Dictionary

Survived 0= Dead, 1 = Alive
Pclass = (Ticket) 1st class, 2nd class or 3rd class 
SibSp = sibling or spouse
Parch = parents or children with passenger
Embarked = Port of loadnig passenger(S= Southampton,C = Cheroburg,Q=Queenstown)",b9ac31e2,0.0364963503649635
2149,3c2033cc99c12c,b6a2100a,## Data Exploration and Data Cleaning,dfa22a54,0.0364963503649635
2150,fd4017c1514157,dd253cfa,"Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls).

In this competition, you’ll automate the acoustic identification of birds in soundscape recordings. You'll examine an acoustic dataset to build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably.

### **Task:-**
Your challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. For each row_id/time window, you need to provide a space delimited list of the set of unique birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code nocall.",fd8f0896,0.036585365853658534
2151,9c26c5dcd46a25,a5e02940,"### <font color=""#ea1c60"" id=""section_1"">1. Analyses univariées</font>

Commençons par charger le **jeu de données nettoyées** et regardons une rapide description : ",1bbbb677,0.036585365853658534
2152,0b01138ad120fc,ef4057da,"**RNNs are Neural Networks that allows previous outputs to be used as inputs (inputs and outputs have the same shape), while having hidden states.  
RNNs are used to deal with sequential/temporal informations, since each epoch in RNN is an time step.**",0b4b72e6,0.036585365853658534
2153,b10bd75889dad9,b3a591f6,#### number of customers churned,ee00ceee,0.03666666666666667
2155,c65a65d4041018,85e3d7cb,Let's see how many people responded to the survey in different countries.,824fb229,0.03676470588235294
2158,b809d07ddd17ed,8b51d6cb,## Install MONAI,e32bf3b1,0.037037037037037035
2162,fce6f1b02867e3,bf277ddd,"# ****Here we try to learn about pandas basic operations over the early stage diabetes detection dataset****

## reading the csv dataset",3fb572c2,0.037037037037037035
2165,24e550b8226932,2fa781c1,#### Importing libraries:,0caee953,0.037037037037037035
2169,faa8e6c8ab9246,7caf3f2f,Lets make copies of train_df and test_df.,2bea1419,0.037037037037037035
2171,ac04ba639d1c93,f0e5ff2a,"# Table of Contents:

**1. [Problem Definition](#id1)** <br>
**2. [Get the Data (Collect / Obtain)](#id2)** <br>
**3. [Load the Dataset](#id3)** <br>
**4. [Data Pre-processing](#id4)** <br>
**5. [Model](#id5)** <br>
**6. [Visualization and Analysis of Results](#id6)** <br>
**7. [Submittion](#id7)** <br>
**8. [References](#ref)** <br>",748059d5,0.037037037037037035
2174,b9328fe3b0cefc,e98c2c8e,"# EDA(数据探索性分析)

This notebook is mainly used to make a overview of the data in this analysis competition. If you like this notebook, please upvote it and make more kaggler see this, and if you have any idea, please tell me, I will update here as soon as possible.(本notebook主要用于对该次分析竞赛的大量数据进行简单概览，以及普及一些关于NCAA相关的知识，毕竟对于国内的同学来说，看NCAA的人应该是少数吧，提前声明，本人只看NBA，不看NCAA，所以可能有些信息有误的地方，望大家评论区指出，感谢)；",3a35eb23,0.037037037037037035
2176,fdc3afd309b850,b9416299,"First of all, we will  WebScrapping the Viva Real website. To do that we are going to use the BeautifulSoup package. I want to thank [Mrs. Adativa](https://www.kaggle.com/aliceadativa) for this very helpful notebook published here on [Kaggle](https://www.kaggle.com/aliceadativa/web-scraping-com-python-parte-1). Also, I want to thank [Anki Kumar](https://www.kaggle.com/ankikumar) that made a [nice job](https://www.kaggle.com/ankikumar/script-for-extracting) using the Chrome Drive on Kaggle Kernel.",966bde38,0.037037037037037035
2181,c6f8ff61a5fa87,eeb0755c,"<span style=""color:blue;""><strong>ABOUT COMPETITION</strong></span>
-------------------------------------
<div class=""competition-overview__content""><div><div class=""markdown-converter__text--rendered""><img src=""https://storage.googleapis.com/kaggle-media/competitions/LANL/nik-shuliahin-585307-unsplash.jpg"" alt=""map"" width=""300"" style=""float:right;"" class=""hoverZoomLink"">
> <p>Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: <b>when</b> the event will occur, <b>where</b> it will occur, and <b>how large</b> it will be.</p>  

<span style=""color:blue;""><strong>WHAT THEY WANT:</strong></span>
> <p>In this competition, you will address <b>when</b> the earthquake will take place. Specifically, you’ll predict the time remaining before laboratory earthquakes occur from real-time seismic data. </p>

<span style=""color:blue;""><strong>CHALLANGE:</strong></span>
> <p>If this challenge is solved and the **physics are ultimately shown to scale from the laboratory to the field**, researchers will have the potential to **improve earthquake hazard assessments** that could **save lives and billions of dollars in infrastructure.**This challenge is hosted by  <a href=""https://www.lanl.gov/"" rel=""nofollow"">Los Alamos National Laboratory</a> which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.</p>

### <span style=""color:red;""><strong>SUBMISSION FORMAT</strong></span>
> * Submissions are evaluated using the [**mean absolute error**](https://en.wikipedia.org/wiki/Mean_absolute_error) between the predicted time remaining before the next lab earthquake and the act remaining time.

### <span style=""color:red;""><strong>Submission File</strong></span>

For each `seg_id` in the test set folder, you must predict `time_to_failure`, which is the remaining time before the next lab earthquake. The file should contain a header and have the following format:

    seg_id,time_to_failure
    seg_00030f,0
    seg_0012b5,0
    seg_00184e,0
    ...


<span style=""color:red;"">**GOAL OF COMPETITION**</span>
-----------------

* ***The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes.*** The *data comes from a well-known experimental set-up used to study earthquake physics.* The` acoustic_data` input signal is used to **predict the time remaining before the next laboratory earthquake (time_to_failure).**
* The ***training data** is a **single, continuous segment of experimental data.** The ***test data*** consists of a folder containing many **small segments.** The data within each **test file is continuous, but the test files do not represent a continuous segment of the experiment**; thus, the **predictions cannot be assumed to follow the same regular pattern seen in the training file.**
* For each `seg_id` in the test folder, you should predict a single `time_to_failure` corresponding to the time between the **last row of the segment and the next laboratory earthquake.**

<span style=""color:Red;"">**DATA DESCRIPTION**</span>
----
### <span style=""color:blue;"">**File descriptions**: </span>
* **train.csv** - A single, continuous training segment of experimental data.
* **test** - A folder containing many small segments of test data.
* **sample_sumbission.csv** - A sample submission file in the correct format.

### <span style=""color:blue;"">**Data fields**:</span>
* **acoustic_data** - the seismic signal [int16]
* **time_to_failure** - the time (in seconds) until the next laboratory earthquake [float64]
* **seg_id** - the test segment ids for which predictions should be made (one prediction per segment)",3eea586b,0.037037037037037035
2183,ddcdecdd6a3b6d,695e9e68,"LeNet:  在大的真实数据集上的表现并不尽如⼈意。     
1.神经网络计算复杂。  
2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  
  
机器学习的特征提取:手工定义的特征提取函数  
神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  
  
神经网络发展的限制:数据、硬件",90831448,0.037037037037037035
2188,2500c5fe8497ee,d062fe17,# Import Libraries,855355f0,0.037037037037037035
2189,1667a100fc8b42,5d2e93fe,"**The idea is:**

 - Feature reduction with PCA
 - Data transformation (log, hot encoding, nan)
 - Test different regression models

**Things found:**

- Applying log transformation increases the accuracy.
- Using PCA with 36 components makes the learning and testing much (much much) faster.
- Removing columns with more than 1000 NaNs gives better result than applying ""mean"" to them.
- There are outliers. Instead of removing them, using HuberRegressor seems to provide a good result. HuberRegressor is a model robust to outliers.",6c8cd6b6,0.037037037037037035
2191,6d29650083cbde,2ac0094c,"**2. DATA PRESENTATION**

Each row of my dataset represents the performance of a player over a specific season. All measures (except for Rating & Rank) are *per 90 minutes*.

Rank is on a League and season basis, and stems from the Rating. Rating is on a scale from 0 to 1000.

Ratingf1 and Ratingf2 link the player's performance to his Rating respectively one and two seasons later (done in Excel).

Classf1 (also done in Excel) categorizes the players from their Rating percentile: 1 is for top 10% players, 2 for top 25% players, 3 for top 50% players, 4 for top 75% players and 5 for the bottom 25%.",e65fd993,0.037037037037037035
2192,613bf7bfdcb9e3,9878b98f,# scipy.stats.mode,32beb65d,0.037037037037037035
2193,e6576e985ccc71,715b626f,#Codes by Yaroslav Isaienkov  https://www.kaggle.com/ihelon/monet-eda-and-visualization-techniques/notebook,63d2c3a5,0.037037037037037035
2195,f4b9042e693b6c,c21ca219,"## Installs & Imports

The below cell will install the PyTorch XLA package.",676cacc9,0.037037037037037035
2199,9ad9a97e628bfa,26e879eb,Nans (Null Data)찾아보기,0a7e1136,0.037037037037037035
2200,2ada0305b68956,3a5b8fbf,### 3. Palette = 'Blues',133e26f4,0.037142857142857144
2202,20b372b6e4e276,c91b16f5,"## 1. Import libraries <a class=""anchor"" id=""1""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.03731343283582089
2203,c84925c8171900,85fd07b8,"<p style=""text-indent: 25px;"">
    <span style='font-family:Georgia'>
        A video game is an electronic game that can be played on a computing device, such as a personal computer, gaming console or mobile phone. Depending on the platform, video games can be subcategorized into computer games and console games. In recent years however, the emergence of social networks, smartphones and tablets introduced new categories such as mobile and social games. Video games have come a long way since the first games emerged in the 1970s. Today’s video games offer photorealistic graphics and simulate reality to a degree which is astonishing in many cases.
        </span>
</p>",e21ff7ec,0.037383177570093455
2205,3dd4294f903768,34b2ae24,Then we will upload our data to a DataFrame.,0d89d098,0.0375
2206,254cccd5145725,4fd48fd5,<h1> Objective</h1>,a49b4037,0.0375
2208,ba4b3bd184acbb,ded30e8e,"# Creating DataFrames

### DataFrame Method
Syntax: `DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)`

#### No Data

The simplest way to create a DataFrame is to just provide the structure without any data.",0f5de724,0.03759398496240601
2209,fc8e0042411c46,d064aa6f,Data Dictionary,af476c2a,0.03761755485893417
2211,510b8303776bb6,483d246b,# Loading the training dataset,18080db8,0.03773584905660377
2212,f3c8651cb08234,53c689d6,"In this notebook we will predict car price using regresson models and compare **XGBoost** with **Random Forest** to check which will perform better 
what is done in this notebook


> * Data Preprocessing => checking for missing values 
> * Seprate Categorical variables from numericals 
> * Feature engineering and change column year to a useful column (car age)
> * Dealing with categroical variables and encode them
> * Scape from dummy variable trap 
> * Check and visualize correlation coefficient
> * Find feature importance and top important features
> * Split data into training set and test set
> * Tuning hyper parameters with Randomized Search CV to find the best parameter
> * Fit the models and evaluate

",37f86e36,0.03773584905660377
2213,4dd47072617594,37124287,![img.png](https://www.emerald.com/insight/proxy/img?link=/resource/id/urn:emeraldgroup.com:asset:id:article:10_1016_j_aci_2019_11_003/urn:emeraldgroup.com:asset:id:binary:ACI-j.aci.2019.11.003002.tif),44ff1d11,0.03773584905660377
2214,0ad8d416b89b78,a67e5464,**Importing file and brief analysis of the datasets content**,0b0562f0,0.03773584905660377
2218,614ba9f0c62677,190d687a,"<a id=""1""></a>
## Loading the Data Set
* In this part we load and visualize the data.",b8551335,0.03773584905660377
2219,f015d0147e8fbf,81177f7c,"## Characteristics of the Competition and its Dataset

This [competition](https://www.kaggle.com/c/home-credit-default-risk) ran for three months, from May 17 to August 29, 2018. The objective was to build an algorithm that could predict the likelihood that a loan applicant would eventually default on his or her loan. The training set contained various financial and personal information originally taken from the loan application profiles 307,511 previous Home Credit borrowers. The test set had 48,744 borrower records. The scoring metric was area under the ROC curve. Features were contained in seven different data tables. The largest table contained demographic information such as job type and gender, along with various numerical features that described a borrower's financial status, such as normalized credit rating scores. Each of the six supplementary data tables contained different kinds of detailed financial records. (e.g. credit card payment histories, loan payment histories recorded as recorded by the credit bureau, etc.)

As for the content of the dataset itself, several features were noticeably sparse, and it was clear to me early-on that I would need an algorithm that handled NaN entries as deftly as possible. Furthermore, some features had names and descriptions that were cryptic or vague at best. At times this made it tough to gain an intuition of how to best engineer new features. There were even two features that were incorrectly described as normalized even though they were clearly categorical (they contained word strings as entries). Finally, several features didn't always have missing values represented by np.nan. For some numerical features, the integer 365243 was equivalent to NaN. For certain categorical features, the strings 'XNA' or 'XAP' were used to denote missing entries. None of this information was included in the dataset's description, but was shared in the forum by Home Credit's liaison during the course of the competition.

These speedbumps added a certain element of challenge that helped to level the playing field. Namely, there seemed to be a larger than normal benefit to those teams taking the time to diligently explore and understand all the quirks and idiosyncrasies of the dataset. Simply having the most advanced stacking or ensembling methods would not be enough to guarantee victory in this competition.

Most importantly, however, the consensus amongst competitors was that Home Credit's team went above and beyond in curating the test set such that data leak was minimized as much as possible. Furthermore, Home Credit's representative was active and responsive on the forums throughout the duration of the competition, and was helpful in clearing up questions that competitors had regarding the dataset and its feature definitions.

At the time of competition's conclusion, 7,198 teams had submitted entries, making this the largest ever featured competition in Kaggle's history. The [top team](https://www.kaggle.com/c/home-credit-default-risk/discussion/64821) achieved a private leaderboard score of `0.80570`.",518954fb,0.03773584905660377
2221,5d2a3e82679cf3,b77ed778,"A data frame with 322 observations of major league players on the following 20 variables.

- AtBat: Number of times at bat in 1986
- Hits: Number of hits in 1986
- HmRun: Number of home runs in 1986
- Runs: Number of runs in 1986
- RBI: Number of runs batted in in 1986
- Walks: Number of walks in 1986
- Years: Number of years in the major leagues
- CAtBat: Number of times at bat during his career
- CHits: Number of hits during his career
- CHmRun: Number of home runs during his career
- CRuns: Number of runs during his career
- CRBI: Number of runs batted in during his career
- CWalks:Number of walks during his career
- League: A factor with levels A and N indicating player's league at the end of 1986
- Division: A factor with levels E and W indicating player's division at the end of 1986
- PutOuts: Number of put outs in 1986
- Assists: Number of assists in 1986
- Errors: Number of errors in 1986
- Salary: 1987 annual salary on opening day in thousands of dollars
- NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987

",9e60b1e3,0.0379746835443038
2222,55a5e31d03df9f,42e5b1bc,"We use the function of `walk_through_dir` to explore all the directories available at the `PATH`. From this we can conclude that we have a total of 483 images, these images are organized as the following:
* 78 images in the test folder.
* The remaining images are divided into 4 directories each with a theme (Star Wars, Marvel, Jurrasic World, and Harry Potter) and in each of these directories, we can find folders with numbers that range is between 0001 and 0017. Besides, Marvel and Star Wars appear to be the theme with more images.",06dce00f,0.0380952380952381
2223,04bac111ffbe9c,7dcaca54,"Let us see what columns we have and assert their data types 
*  If any datatype doesn't match ur criteria. If that is the case, try modifying it to the datatype it should be in.",82576b17,0.0380952380952381
2226,7a058705183598,669efd2d,Top 5 records of dataset,b0ead917,0.0380952380952381
2227,ee23a565163388,dc5ba7e1,The primary objective of this notebook is to visualize the trends in the dataset and to predict the heart failure of different patients. There could be many reasons behind the heart failure. We'll use Python's visualization libraries to understand the trends lying in the patient dataset and then build a model to predict the heart failure.,88aacbc4,0.03816793893129771
2230,e7237da7cbec10,eb9346ff,"1. Data read_csv
",5fcf5e3d,0.038461538461538464
2231,897ca904b74a98,2c503c23,# Exploratory Data Analysis,c5844ad4,0.038461538461538464
2233,af6556ced704f6,f9ae9649,"**This dataset contains Comic Characters od DC and Marvel.**

> The data comes from Marvel Wikia and DC Wikia. Characters were scraped on August 24. Appearance counts were scraped on September 2. The month and year of the first issue each character appeared in was pulled on October 6.",881577c0,0.038461538461538464
2234,e1a69c71c2c282,795fad7a,# Train Data,b2ca7d3a,0.038461538461538464
2238,d07915a6e6992e,6df8513e,"# Article on medium publication


I also wrote an article on medium on the same topic. You can [click this clink](https://medium.com/@rp1611/model-ensembles-for-survival-prediction-a3ecc9f7c2ae) and access the blog. Please leave comments/feedback. It would help me improve.
",2b912140,0.038461538461538464
2240,1bd6cc83c02681,d804138c,# Importing Libraries,17ff92f0,0.038461538461538464
2246,80ad12f326ab70,52f9b0d4,"## Data Overview
* engagement_data : is based on LearnPlatform’s Student Chrome Extension. The extension collects page load events of over 10K education technology products in our product library, including websites, apps, web apps, software programs, extensions, ebooks, hardwares, and services used in educational institutions. The engagement data have been aggregated at school district level, and each file represents data from one school district.
* products_info.csv : includes information about the characteristics of the top 372 products with most users in 2020.
* districts_info.csv : file includes information about the characteristics of school districts, including data from NCES and FCC.

External data :
COVID-19 US State Policy database and KFF",da404a16,0.038461538461538464
2247,71d3e4aee86e3e,d957beb7,"## 2. Dataset Imports
<a></a>
Download the dataset from the links provied [here](https://api.covid19india.org/documentation/csv/)
Remember you update this on a daily basis !",69706f0b,0.038461538461538464
2250,6f1481148352e9,bf76e656,![](https://pbs.twimg.com/media/Eh5dK_mXsAA0fuW.jpg),7cfbdb8f,0.038461538461538464
2254,d0f6276d5b628c,789544c0,### Libraries :,c64f5ce5,0.038461538461538464
2257,a4f8ad33c823c5,421a72ea,# Data pre-processing and Data cleaning,fcd48307,0.038461538461538464
2258,669ce946943d60,e6e843e3,## MCRMSELoss,0f63c4ce,0.038461538461538464
2259,8ddaa0c6c395ec,43bee507,## Environment Setup,9fccabdc,0.038461538461538464
2261,a915263bc207da,46359176,### Importing the necessary modules,b17ebcda,0.038461538461538464
2262,2facf256353117,41a78562,"# Image file format
* Image file formats provide a standardized way to store the information describing an image in a computer file.
>* Neuroimaging Informatics Technology Initiative (Nifti), 
>* Minc
>* Digital Imaging and Communications in Medicine (Dicom).

The file format describes how the image data are organized inside the image file and how the pixel data should be interpreted by a software for the correct loading and visualization",18f579be,0.038461538461538464
2263,d4c5aaa4b36810,14bfff14,"# Data Cleaning and Combining
The data for this project comes from two data sets and thus will require cleaning for various possible errors. One dataset contains the world happiness report from 2015, the other contains the economic development indicators. ",65441f28,0.038461538461538464
2272,722cd844dfbe8f,5dab5bee,"# <span style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"" id=""section_1"">Exploratory data analysis (EDA)</span>",0cedb385,0.03896103896103896
2273,2cb457b60dd246,6674fd5f,"### Create the directory structure

In these folders we will store the images that will later be fed to the Keras generators. ",339367df,0.03896103896103896
2274,90691864eb68c7,5b1444d9,# 2. loading the Dataset,3555ef9b,0.03896103896103896
2278,d0080e3a39bc5c,79c4d75e,"Hello Everyone !

This kernel consists of my work for the **Game of Deep Learning** - Compter Vision Hackathon on Analytics Vidhya in which we were supposed to classify different images of ships into 5 classes - 

1. Cargo
2. Military
3. Carrier
4. Cruise
5. Tanker


The kernel got a highest Public Leaderboard score of **0.9813**.

And the highest Cross-Validation Score attained was **0.985**.

The private Leaderboard score attained by it is **0.98007**, which implies a rank of 15th among the 450 odd submissions.

The kernel explains the different steps and decisions I took during the training of the model and the reason behind them too.",2fcde4cf,0.0392156862745098
2282,fa02c409161192,7f5a723d,## 1. First attempt (no libraries) ,e97077f7,0.0392156862745098
2283,907f08f9a2c6cf,7df0093e,### Load in Data,aa84c325,0.0392156862745098
2284,1a0bd2f72bbe36,852dc1da,## IMPORT LIBRARIES,2fa311dc,0.0392156862745098
2285,52cfd66e9ec908,d2fae707,**TIP: Use plt.imshow instead of IPython.display**,c74adcdf,0.0392156862745098
2288,917957c6c4065f,7889d0fe,"pd.read_csv 를 했을 때   
""utf-8"" codec can""t decode bytes in position 253-254: unexpected end of data 가 발생한다면  
engine=""python"" 을 명시해주면 됩니다.",55b8ed68,0.0392156862745098
2289,64169805aacf17,f4a29163,"---
# The Code 🤖
---",1f12ded0,0.0392156862745098
2291,ce9ed5e2d601d7,9f8b8e44,"### Handle Missing Values ###

Handling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `""None""` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating ""missing value"" indicators: `1` whenever a value was imputed and `0` otherwise.",f58a2f43,0.03937007874015748
2293,b01ee6cb674fa3,a984603f,## Company name,a8ffd35e,0.03985507246376811
2296,ce7abd85d777b5,fd698088,Let's go into the detail.,0a340dbb,0.04
2298,274b32da3b19a8,d8247914,"**All content from:**

[HuggingFace QA Example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=A0sAS_xYeFvc)

[ChAII - EDA & Baseline](https://www.kaggle.com/thedrcat/chaii-eda-baseline?scriptVersionId=71505522)",408f7268,0.04
2299,0687cd5c8597db,18607b88,### **Importing Libraries**,4edec76a,0.04
2307,10c5a39a87c47e,0ec9223c,## Step 1: Importing Essential Libraries<a id='step-1'></a>,09c7337a,0.04
2308,91eaec994e0c6f,f4fd2ff8,# 1. Read Data,376aef10,0.04
2311,8854f72e7e9be0,5d10dbbc,**Read Images**,2a1031b7,0.04
2315,caaa6793391520,1c36e717,"## Problem statement ##
Most common way to handle missing data is to drop them. The second most common way is to replace the missing data with the most likely value. For the categorical features it is the most frequent value. For the numerical features it is the mean. `scikit-learn` has a class available for this: [SimpleImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html). The problem with this approach is that even though it preserves mean, but it reduces the standard deviation, sometimes very significantly. To demonstrate this, let's consider a simple array, then remove half of the values and replace them with mean, and see what happens with STD:",1e79f342,0.04
2318,cee088a6840708,78a3560f,"# Fast approximate convolutions on graphs

GCN is just a layer similar to Dense, Conv2D,...layers. It has an activation function and some inputs, and output. This is the GCN formula.
![gcn.png](attachment:gcn.png)


![desc.png](attachment:desc.png) [reference](https://docs.dgl.ai/tutorials/models/1_gnn/1_gcn.html)

In the equation, there are A (adjacency matrix), squareroot of D (D to -1/2) , W and H.
1. Adjacency matrix is a binary matrix; It compares each node with all other nodes and if they are connected they score 1 else 0

![amatrix.png](attachment:amatrix.png)


2. Degree Matrix (D) is a diagonal matrix where each number refers to the number of degrees (edges; lines connected to) of each node. For example, here node(6) had only one edge while node(1) has 4 (Two other nodes plus two for itself (how many lines touch the circle around node (1) ? Four))

![degree_matrix.jpg](attachment:degree_matrix.jpg)


3. H(l) is the input which is a matrix of nodes as rows and each column is a feature e.g. if nodes represent users; features could include user age, country, etc. If there are no features we can the index number of each node as its sole feature. 

4. H(l+1) is computed from the equation above, and can be used as input again.",55463e1c,0.04
2319,6338f6b0178d13,95ee2e7c,"# About Dataset:
We’ll use a telecommunications data for predicting customer churn. This is a historical customer data where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it’s less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.

This data set provides info to help you predict behavior to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.

The data set includes information about:

Customers who left within the last month – the column is called Churn
Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies
Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges
Demographic info about customers – gender, age range, and if they have partners and dependents",ae81b18b,0.04
2321,19aae4a6ede288,b29ecf41,"My other notebooks in this competition:
- [Tabular Playground Series - June/2021: Starter - EDA + Base LightGBM](https://www.kaggle.com/jonaspalucibarbosa/tps06-21-starter-eda-base-lgbm)
- [Tabular Playground Series - June/2021: Simple Neural Network with Keras](https://www.kaggle.com/jonaspalucibarbosa/tps06-21-simple-nn-with-keras)
- [Tabular Playground Series - June/2021: Keras Neural Network with Embedding Layer](https://www.kaggle.com/jonaspalucibarbosa/tps06-21-keras-nn-with-embedding)
- [Tabular Playground Series - June/2021: LightAutoML with KNN Features](https://www.kaggle.com/jonaspalucibarbosa/tps06-21-lightautoml-w-knn-feats)
- [Tabular Playground Series - June/2021: Keras Neural Network with Skip Connections](https://www.kaggle.com/jonaspalucibarbosa/tps06-21-keras-nn-with-skip-connections)",56934674,0.04
2322,5cb7f999fd1ecb,30ac48ed,"### Used Libraries
1. NumPy (Numerical Python)
2. Pandas
3. Matplotlib
4. Seaborn
5. Plotly
6. Missingno
7. Folium",88b54f70,0.04
2324,7e1da639035ac5,6a7ee45b,# <a id='2'>2. Loading libraries and retrieving data</a>,120b6c23,0.04
2325,cb570c7b7f0501,ba9c4dd2,"<a id='wrangling'></a>
## Data Wrangling
It's consist of three steps:
<ul>
<li><a href=""#Gathering"">Gathering Date</a></li>
<li><a href=""#Assessing"">Assesing Data</a></li>
<li><a href=""#Cleaning"">Cleaning Data</a></li>
</ul>

",a200a0ec,0.04
2326,83df814455f06c,df6ebe8a,"# **2. Classification and Regression Trees (CART)** <a class=""anchor"" id=""2""></a>

[Table of Contents](#0.1)


Nowadays, Decision Tree algorithm is known by its modern name **CART** which stands for **Classification and Regression Trees**. Classification and Regression Trees or **CART** is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification and regression modeling problems.


The CART algorithm provides a foundation for other important algorithms like bagged decision trees, random forest and boosted decision trees. In this kernel, I will solve a classification problem. So, I will refer the algorithm also as Decision Tree Classification problem. 
",c9cff71a,0.04
2328,bbad077c274022,771d53a3,# Importing essentials and the dataset,3c2e3dea,0.04
2329,37e461081e47c5,3e643619,"# Data import, EDA and model preparation",b3e6549e,0.04
2330,5f32117bcd5255,1eb059a7,# OVERVIEW AND DATA ANALYSIS,85882abf,0.040268456375838924
2331,726833f92fb87a,f7bce9cd,# Bank Marketing Analysis results dashboard:,7dc5e1b6,0.040268456375838924
2332,9ceb7278784462,a8e9ef34,"* Data has only float,object and integer values.
* Variable column has not missing values.",3768a567,0.04032258064516129
2336,63b44c85e32c1f,c21f51c0,One can directly assign the sequence of data to a list x as shown.,fb9b9562,0.04054054054054054
2339,e19e307b3fd188,6a4136a1,"The features are:
* **city** - city where the property is located
* **area** - property area
* **rooms** - quantity of rooms
* **bathroom** - quantity of bathrooms
* **parking spaces** - quantity of parking spaces
* **floor** - floor
* **animal** - acept animals?
* **furniture** - furniture?
* **hoa** - Homeowners association tax
* **property tax** - IPTU / property tax
* **rent amount** - rental price
* **fire insurance** - fire insurance
* **total** - total value
",2173955b,0.04065040650406504
2341,2343dc02ffb96a,f0f4cc14,# 1. Prepare the Environment,29aa95a4,0.04081632653061224
2343,087e21401d7dfc,31ddbb63,"###### So finally we have our data in our hand. Now lets do some visualization to understand it even more finely. As we know there are only two columns in our dataset so we will se historgram & kde plot(univariate distribution) and scatter plot(bivariate distribution), What is univariate & bivariate distribution? As name suggest univariate in plotting only a single feature e.g. histogram and kde plot (kernel distribution estimation). Bivariate means plotting 2 two variables e.g. scatter plot",42000489,0.04081632653061224
2347,eb33e05704d647,126b2444,#config,cd80436d,0.04081632653061224
2351,5ce12be6e7b90e,178b0997,"Print to the screen the following sentences:  

- ""I love Python!""
- ""7 + 6 = RESULT"", replacing `RESULT` with the computation of 6+7
- ""my name is NAME"", replacing `NAME` with your name",c0ab62dd,0.04093567251461988
2353,979f1e99f1b309,45396c4e,# Assess Data,d1bfebbf,0.040983606557377046
2354,91473a39b85068,365d6de3,"### Performance Metrics
For a standard Binary of Multi-class classification problems, we can use performance metrics like Precision,Recall,F1-Score,Log-loss,AUC Curve etc..But for the present Multi-Label problem the mentioned metrics may not work well. As part of the business requirement we want high precision and recall rates for each and every predicted tag. We can use F1 Score here as it only gives good value if both the Precision and Recall are high. The F1 Score performs really well for Binary classifications. So for Multi Label Setting we can modify it into two types Micro Averaged F1 Score and Macro Averaged F1 Score

Please go through the below links to understand more about Precision, Recall, Micro and Macro averaged f1 scores.
https://medium.com/@klintcho/explaining-precision-and-recall-c770eb9c69e9",6e3d91c2,0.0410958904109589
2355,3d08ca7656dec0,8cd70d1f,# Load dataset,bd3f87e3,0.0410958904109589
2357,1eb62c5782f2d7,8dce9d05,"## 3. Area diantara 2 point z-score
![5_1_graph_between_two_z.png](attachment:5_1_graph_between_two_z.png)",bb69f147,0.0410958904109589
2364,2f47abddfd1928,85ab92ee,The first step is to visually inspect both datasets.,ae33cc0b,0.04132231404958678
2365,e9b9663777db82,029c7d12,![1.png](attachment:1.png),648e8507,0.04132231404958678
2366,69ac33d79f5130,62efe8c3,"- Load the file using Pandas
- Loak at some information about the data & columns
- Fix any missing or incorrect value",9d760d2a,0.041666666666666664
2367,593d1d3d1df05a,7e290dfe,# Read Input Image,bc682ffe,0.041666666666666664
2368,57740be713cf12,74e3dfa3,## ***1. Preparation***,ac122df5,0.041666666666666664
2369,d8d227c158d883,b3a8c931,"**This notebook predicts the probability of the occurrence of the upset, which means that the low seed rank team beats the high seed rank team, aggregating past game results**",3391b4a7,0.041666666666666664
2371,1c5aaf7bea6414,7bf2f461,# Loading Data,34d8f42d,0.041666666666666664
2377,e82462cdc998a7,9eb99925,"Forked from [MoA: Pytorch-RankGauss-PCA-NN upgrade & 3D visual](https://www.kaggle.com/vbmokin/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual) (and butchered beyond all recognition, probably).",b39bf244,0.041666666666666664
2378,166a62ebb4fc3a,54ff873b,Loading the initial libraries,db48a079,0.041666666666666664
2381,3b5903412fe741,e95b77c8,Selecting specific values of a `pandas` `DataFrame` or `Series` to work on is an implicit step in almost any data operation you'll run. Hence a solid understanding of how to slice and dice a dataset is vital.,ad231969,0.041666666666666664
2391,2a377ced98d67a,12372251,## 1. Import,262231a8,0.041666666666666664
2392,1d5daeca89f48d,594292c0,![TP.PNG](attachment:TP.PNG),48d478bc,0.041666666666666664
2396,1014e6be391084,d5fe6751,1.Checking Null Values,46f9168f,0.041666666666666664
2398,02773bdc5d3c7a,ea0c6d73,The following notebook deals with the Credit Card Fraud Detection problem. A cursory glance on the dataset reveals that the data is highly imbalanced and the accuracy score would not suffice as a performance metric. Here's a ouline of what has been done in this notebook.,86245f35,0.041666666666666664
2400,8c7e00ca3dc5a7,e0e14c09,## Data Preview,c83346e4,0.041666666666666664
2401,62487bcd70b199,85ea85bf,## <a id='1.1'>1.1. Data Overview</a>,f6ae50af,0.041666666666666664
2402,eb0854a6601407,3e011e99,"# The Data
Note that the training data is roughly 18.55 Gb in size. This is too large to load into memory directly in kaggle notebook.

Some things to note when exploring the entire dataset on a local machine:
- There are 3579 unique `investment_id`s
- There are 1211 unique `time_id`s - we are told these are not equally spaced and could be different in the test set.
- The features columns are mostly normalized with a mean value close to 0 and standard deviation of ~1.
",6d107747,0.041666666666666664
2403,fdc3afd309b850,521529f7,"<a id=""ws_vr""></a>

## 4.1 Web Scrapping - Vivareal WebSite ",966bde38,0.041666666666666664
2407,a69d41047fdd3e,a2cb012c,"# Introduction

The first test of your new data exploration skills uses data describing crime in the city of Chicago.

Before you get started, run the following cell. It sets up the automated feedback system to review your answers.",b1f28647,0.041666666666666664
2409,eda49464dd6d1b,09f73cb2,# Import Libraries and Dataset,8421f81f,0.04195804195804196
2411,4ae6a182abac64,7827af26,### 1.2 Acquire data,418676c5,0.04201680672268908
2412,f91f58d488d4af,e7cf63c2,"By using `ls()` which return an object of a special fastai class called L, which has same functionality of Python's built-in list, plus a lot more. ",5df1bbf3,0.042105263157894736
2415,631cd434fc3aa2,2cd24191,#### Read dataset,2b74febb,0.04225352112676056
2416,bddd799cdbbae8,78f7ca6f, # <a id='2'> 2. Dataset</a>,b44e3c08,0.04225352112676056
2417,9bcfa825c8b2e6,bbfd166a,"**Veri Seti Hakkında Bilgi:** ABD'deki Arizona Eyaleti'nin en büyük 5. şehri olan Phoenix şehrinde yaşayan 21
yaş ve üzerinde olan Pima Indian kadınları üzerinde yapılan diyabet araştırması için kullanılan verilerdir.
768 gözlem ve 8 sayısal bağımsız değişkenden oluşmaktadır. Hedef değişken ""outcome"" olarak belirtilmiş olup;
1 diyabet test sonucunun pozitif oluşunu,0 ise negatif oluşunu belirtmektedir.

**Değişkenler:**
Pregnancies – Hamilelik sayısı,Glucose – Glikoz değeri,SkinThickness – Cilt Kalınlığı,Insulin – 2 saatlik serum insülini (mu U/ml)
Blood Pressure – Kan Basıncı (Küçük tansiyon) (mm Hg),DiabetesPedigreeFunction– Aile öyküsüne dayalı olarak diyabet olasılığını puanlayan bir fonksiyon
Age – Yaş (yıl),Outcome– Hastalığa sahip (1) ya da değil (0)

**Amaç:** Özellikleri belirtildiğinde kişilerin diyabet hastası olup olmadıklarını tahmin edebilecek bir makine öğrenmesi
modeli geliştirmek.

**Yararlanılan kaynak**: https://bootcamp.veribilimiokulu.com/egitim/veri-bilimci-yetistirme-programi/",220f36e4,0.04225352112676056
2422,9169c4e9c33c90,e8c6fff6,"<a id=""Summary""></a>",725bf880,0.0423728813559322
2423,4c47839b067546,76d1711e,## Setup,1f517b02,0.0425531914893617
2424,b61ab8f81dc03d,3f6e6127,### Loading data,64d05394,0.0425531914893617
2425,957e035ba5b9d5,cd992b00,"The dataset contains two directories which can be compined. Also, there are some bad images (invalid) which we need to delete. While we are at it, we will re-orginize the directory structure a little bit and rename the images. This is just for us and not necessarily needed for keras. 

This is a one off and can be remove from the notebook. For the time being I will leave it in here for documentation. But shouldn't be executed again, otherwise we will just see errors.",778ab3d3,0.0425531914893617
2426,73893f0467d5e3,e57a5c79,# EDA,279787c6,0.0425531914893617
2428,f6648e47713411,c7615468,"# 1.  Thiết lập giá trị của các tham số cố định
Thiết lập giá trị của các tham số cố định có trong bài:
1. *num_classes*: tổng số lượng nhãn.
2. *img_size*: kích thước của ảnh sau quá trình resized bởi DataLoader.
3. *batch_size*: kích thước mỗi batch.
4. *device*: accelerator được sử dụng.
5. *criterion*: hàm mất mát (loss function) được sử dụng.",f4af4d1c,0.0425531914893617
2429,c7e5f658090347,28d8f1a6,## Setting up the Environment and Reading in the Dataset,43c78e7d,0.0425531914893617
2431,3f25b363afec54,f0adcb74,"# About 

Congratulations – you have been hired as Chief Data Scientist of MedCamp – a not for profit organization dedicated in making health conditions for working professionals better. MedCamp was started because the founders saw their family suffer due to bad work life balance and neglected health.

MedCamp organizes health camps in several cities with low work life balance. They reach out to working people and ask them to register for these health camps. For those who attend, MedCamp provides them facility to undergo health checks or increase awareness by visiting various stalls (depending on the format of camp).

MedCamp has conducted 65 such events over a period of 4 years and they see a high drop off between “Registration” and Number of people taking tests at the Camps. In last 4 years, they have stored data of ~110,000 registrations they have done.

One of the huge costs in arranging these camps is the amount of inventory you need to carry. If you carry more than required inventory, you incur unnecessarily high costs. On the other hand, if you carry less than required inventory for conducting these medical checks, people end up having bad experience.

 
  
## Process:
 
MedCamp employees / volunteers reach out to people and drive registrations.
During the camp, People who “ShowUp” either undergo the medical tests or visit stalls depending on the format of healthcamp.
 

### Note:

Since this is a completely voluntary activity for the working professionals, MedCamp usually has little profile information about these people.
For a few camps, there was hardware failure, so some information about date and time of registration is lost.
MedCamp runs 3 formats of these camps. The first and second format provides people with an instantaneous health score. The third format provides information about several health issues through various awareness stalls.
 

### Favorable outcome:
For the first 2 formats, a favourable outcome is defined as getting a health_score, while in the third format it is defined as visiting at least a stall.
You need to predict the chances (probability) of having a ```favourable outcome```.",bbdaae25,0.0425531914893617
2432,56785caebaa256,789a27b9,"## 1. Import libraries<a class=""anchor"" id=""1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.0425531914893617
2436,2ada0305b68956,4359933d,### 4. Palette = 'Blues_r',133e26f4,0.04285714285714286
2446,f3c6048d1058e3,4e1236b7,**Sentiment count**,1d9056b0,0.04310344827586207
2447,a566b5b7c374e7,8a9bc000,## Prepare Data Tables,b3dc5545,0.04316546762589928
2448,b10bd75889dad9,d7161601,#### Deriving the target variable,ee00ceee,0.043333333333333335
2450,a1a31459abf078,a44d94bd,"# Overview of RAPIDS <a name=""rapids""></a>

[RAPIDS](https://rapids.ai/) is a suite of open-source software libraries and APIs for executing data science pipelines entirely on GPUs. Some of the advantages are - 
* **Faster Execution Time** - RAPIDS leverages NVIDIA CUDA® under the hood to accelerate your workflows by running the entire data science training pipeline on GPUs. This reduces training time and the frequency of model deployment from days to minutes.
* **Use the Same Tools** - By hiding the complexities of working with the GPU and even the behind-the-scenes communication protocols within the data center architecture, RAPIDS creates a simple way to get data science done.

I initially started working on this problem in Pandas, but my code was quite slow and I wasn't able to handle the datasets properly. Therefore I switched to RAPIDS and since then the overall runtime of my notebook has come down to ~2 mins(excluding load time for cudf packages)


To use RAPIDS in our notebook, we need to add RAPIDS package files to our notebook and then load the package. ",66fc0f54,0.043478260869565216
2453,9d9da6c439b96b,a34c39cc,### Load data,361cc7d9,0.043478260869565216
2454,f6c1eb62cceb70,e81ab057,1. Import the data to use for predictions,90a1b790,0.043478260869565216
2455,17a24d566ffa59,c6912418,"##### Limitations of the vector space model:

We generally do not want to feed a large number of features directly into a machine learning algorithm because:
- They are expensive to store.
- They slow down computations (e.g. in algorithms like k nearest neighbors)
- Large samples are required to avoid overfitting.


**Synonymy:** the characteristic of language to have several terms that mean essentially the same thing
-  In the SAS technical support data set the terms “frozen” and “hangs” often refer to the same situation where the program has reached a point where nothing is happening and yet the user cannot continue working

**Polysemy:** is the tendency for the same term to mean different things in different contexts.
- The term “monitor” in technical support data is a good example of this. At times it refers to the computer screen, sometimes it refers to a piece of software that displays a graphical result and still other times it refers to the user “watching” or “observing” an event

**Term dependence:** refers to the tendency for certain terms to be highly correlated with one another. This problem is not unique to text but also occurs with most other sets of data as well. 
- The terms “error” and “message” are strongly correlated in the technical support collection. When one occurs, the other also tends to occur. A pair of documents, each containing these two terms, may have their similarity overrated in this case

SOURCE: 
- https://davidrosenberg.github.io/ml2015/docs/13.Lab.PCA-SVD-LDA.pdf
- ftp://ftp.sas.com/techsup/download/EMiner/TamingTextwiththeSVD.pdf",89049e56,0.043478260869565216
2456,fe118026267a88,5800d6d0,"# Intro
Welcome to the **[Learn Pandas](https://www.kaggle.com/learn/pandas)** micro-course. 

You will go through a series of hands-on exercises. If you already know some Pandas, you can try the exercises without the reference materials or tutorials. But most people find it useful to open the pages listed as `relevant resources` to help you as you go through the exercise questions

The first step in most data analytics projects is reading the data file. So you will start there.

# Relevant Resources
* **[Creating, Reading and Writing Reference](https://www.kaggle.com/residentmario/creating-reading-and-writing-reference)**
* [General Pandas Cheat Sheet](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)

# Set Up

Run the code cell below to load libraries you will need (including code to check your answers).",612efa48,0.043478260869565216
2458,b49bb7b41806a7,e6ca544e,Here we have separated the text data using sep function in read.csv command. We also have named the columns for the time being to make it easier to read.,d034d34d,0.043478260869565216
2462,ab741058d7ab79,99414612,We first read the csv data.,fcfc7f71,0.043478260869565216
2466,7e275c8d5ff2a0,e9cd6fcc,# IMPORTING VARIOUS LIBRARIES FOR COVID PREDCITION AND ANALYSIS,b3afcc98,0.043478260869565216
2470,57bad3860b0fa4,10b3c93b,# Loading In The Data,05138a5e,0.043478260869565216
2475,73ca9abcc2034e,715315de,# Import the libraries,cec3446c,0.043478260869565216
2476,7e2644d6b415bc,2b6544e8,"# If you like, please upvote",52de7ef0,0.043478260869565216
2484,2b434130adf886,14cfc9d3,# Get the data,0c4afeca,0.043478260869565216
2485,59236ba162ab7b,152ab0a4," 1. Import Libraries
 ",ace5b0ef,0.043478260869565216
2488,3c2033cc99c12c,e2bc5c28,"**Brief Intro:** *In the first part of the project, I will conduct the process of data exploration and data cleaning, which could removes major errors and inconsistencies that are inevitable when multiple sources of data are getting pulled into one dataset. The process pave the way for the prediction and anlysis.*",dfa22a54,0.043795620437956206
2490,fe7360cddc13e5,0a54a94b,# BG / NBD,8979e423,0.043859649122807015
2494,3d905ce4828057,681395a7,"The BG/NBD Model (Expected Number of Transaction) allows us to express the number of purchases as probabilistic.
Gamma Gamma Submodel (Conditional Expected Average Profit) allows us to express the average amount of snow as probabilistic.
Data must be unique on a per customer basis. Every customer;

**Recency value** (last shopping date — first shopping date)

**Tenure value** i.e. customer age (today's date — date of first purchase)

**Purchase quantity**

**Total income** is calculated.",5b006cc3,0.04411764705882353
2495,dc0b0e1cb46c6f,565d8a3f,"# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;"">💉 COVID-19 🦠🧬 World Vaccination Progress 💉</p>

The data contains the following information:  

* **Country** - this is the country for which the vaccination information is provided;     
* **Country ISO Code** - ISO code for the country;   
* **Date**- date for the data entry; for some of the dates we have only the daily vaccinations, for others, only the (cumulative) total;   
* **Total number of vaccin ations** - this is the absolute number of total immunizations in the country;  
* **Total number of people vaccinated** - a person, depending on the immunization scheme, will receive one or more (typically 2) vaccines; at a certain moment, the number of vaccination might be larger than the number of people;  
* **Total number of people fully vaccinated** - this is the number of people that received the entire set of immunization according to the immunization scheme (typically 2); at a certain moment in time, there might be a certain number of people that received one vaccine and another number (smaller) of people that received all vaccines in the scheme;  
* **Daily vaccinations (raw)** - for a certain data entry, the number of vaccination for that date/country;  
* **Daily vaccinations** - for a certain data entry, the number of vaccination for that date/country;  
* **Total vaccinations per hundred** - ratio (in percent) between vaccination number and total population up to the date in the country;  
* **Total number of people vaccinated per hundred** - ratio (in percent) between population immunized and total population up to the date in the country;  
* **Total number of people fully vaccinated per hundred** - ratio (in percent) between population fully immunized and total population up to the date in the country;   
* **Number of vaccinations per day** - number of daily vaccination for that day and country;   
* **Daily vaccinations per million** -  ratio (in ppm) between vaccination number and total population for the current date in the country;    
* **Vaccines used in the country** - total number of vaccines used in the country (up to date);    
* **Source name** - source of the information (national authority, international organization, local organization etc.);   
* **Source website** - website of the source of information;",47b17a7b,0.04411764705882353
2497,156bbcff05dcea,b03d959a,# Basic EDA,66ad1fe9,0.04411764705882353
2498,9cec5ddf8b6f49,34563eaf,## 1.b) Load Dataset,d39fc8e7,0.04411764705882353
2499,eb0ecd6bebeb15,3c8ba104,Veri çerçevemizi bulunduğumuz dizinden yükleyelim ve bir veri çerçevesi haline getirerek df değişkenine atayalım. (pd.read_csv(...csv)),d7b93a60,0.04411764705882353
2501,99821bc6a45be6,8c040fb3,"For this project we were motivated to see how machine learning could be applied to more complex datasets than have been discussed in class.
We immediately thought to look into the Coronavirus. While looking at relevent datasets we decided that an attainable goal would be to build a XRay Classified for Coronavirus, which provides both a real world scenario and the opportunity to teach ourselves about image classification. 

We have settled on the [COVID-19 Radiography Database](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database) hosted on Kaggle to build our classifier. This dataset was recently updated on March 6, 2021 and is the largest the we could find. 

While ",b9d59346,0.04411764705882353
2502,e4c6dd957eb5ce,a8dfead9,# Importing Libraries,2e383665,0.04411764705882353
2503,7f74a04ae75792,e9ce1909,### What columns do we have?,d01e91da,0.04411764705882353
2509,5be39e4e35cec7,93a35b14,"<a id = ""1""></a><br>
# Load And Check Data",14d617c9,0.044444444444444446
2511,c8bf959b9608cf,1a7aef74,"### Input to the model
Using image of rabbit as content image and splash of light as style image. You can use any other images as these two images. Here, train it for 10 iterations. Remember that we are not training any weights, but generating the candidate image using training. Therefore, not many iterations will be required. At the end of notebook, you will be able to see the results. 

In this notebook, 'candidate image' is referred as 'generated image'.",155e3672,0.044444444444444446
2515,d96e03a9e7c030,a352bc55,"In the above chart, the size of each bubble represents the number of **additional** students expected to have taken the SHSAT in 2017 at each school, with the color representing the percentage of students at or below the poverty level at each school. One possible intervention to encourage underrepresented populations to take the SHSAT is to focus on the schools that are represented by larger, red bubbles.

The below chart shows the same exact data as the chart above, except with more interactivity; when you hover a particular bubble, it shows the school name and the number of additional SHSAT testtakers expected, based on the output of the model.",d2b72ced,0.044444444444444446
2520,892be0a523578c,70ea5a6c," **1.1** By checking the mean of each attributes for each participants, I found that the total steps of Id 1927972279 is abnormally small. Then I filterd out this participant from the dataset and found that many records of he (she) are zero, which may indicate the device failed to collect the data. I define these records as **invalid records**.",b0e8d7c0,0.044444444444444446
2522,d6cbd7160961dc,95e35033,"# 2. Benford's Law
* 2.1. Benford's Law: Introduction
* 2.2. Benford's Law: Formula for the first digit
* 2.3. Benford's Law: Chi-Squared Test",36d74664,0.044444444444444446
2523,d77e6d61ad2e8b,f187286b,# Getting the Data,03fd0e96,0.044444444444444446
2524,b0c2805cd5c087,fa50fd98,Image hai.stanford.edu,0446f327,0.044444444444444446
2525,4fd4b6a80d40e3,eede14fc,"## Perceptron Review

![image.png](attachment:image.png)",f6913cc3,0.044444444444444446
2526,d58491f2896fc1,64b787dd,"
<center><img src=""https://i.pinimg.com/originals/e1/3b/cd/e13bcd0c2045b0a548d3d1663da17564.gif"">",514bfdff,0.044444444444444446
2527,e25c0f830df3f4,45672d87,# Testing Sentiment Analysis (sample),fdcf7189,0.044444444444444446
2534,312135b445bd23,40041301,"# **Project Description**
This project consists of different modules that serve together as a full pipeline in order to extract relevant, useful and accurate information from the the scholarly articles.
We believe that our solution is capable to mine information that answers the research question accurately, flexible enough in order to support future research questions and easy to understand. 
We will describe in details the different modules below:
1. **Data Preprocessing** - ETL, Keyword Extraction & Word Embeddings.
2. **Topic Modeling** - LDA model.
3. **Search Engine for Seed Sentences** - Simple but useful search engine to find seed sentences using keywords.
4. **Semantic Search for Relevant Sentences** - Find relevant answers from seed sentences from #3 using sentence embedding techniques.
5. **Answer Summarization** - Generate abstractive summary for answers using Facebook's BART model.

All the code and notebooks are availabe in the Github [repo](https://github.com/Hazoom/covid19).",8ced381f,0.0449438202247191
2537,e67925694c07d3,5c9a5c50,"Nan preprocess helper
[Martin Kotek (Competition Host): ""Value 365243 denotes infinity in DAYS variables in the datasets, therefore you can consider them NA values. Also XNA/XAP denote NA values.""](https://www.kaggle.com/c/home-credit-default-risk/discussion/57247)
",83af4c4a,0.0449438202247191
2539,dd02a9b545f742,ce839614,"# Quick planning

- UX design: Design thinking approach
- Modeling language: UML
- Programming type: Object-Oriented Programming
- Environments: pandas, scikit-learn, pmdarima
- Computer-science field: Machine Learning
- Knowledge field: trading - International Finance Management",7116cd2d,0.045454545454545456
2546,c2be02442e8cfd,9012d06a,"# **DataSet information:**

* Data Description The Haberman's survival dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's 
* Billings Hospital on the survival of patients who had undergone surgery for breast cancer.


# **Objective**

* To predict whether the patient will survive after 5 years or not based upon the patient's age, year of treatment and the number of positive lymph nodes",2d364acc,0.045454545454545456
2548,ae058c3f1439c3,335a8720,"> As a Computer Science, I am very Curious to know about Google, what are the Requirements, Educational and other Requirements, Locations available to work with Google and most Important the Job Roles in Google.
We are Going to use Countplots and Words Cloud to Carry our Study.",965da99d,0.045454545454545456
2550,b066ab2167199c,bd3569dc,"<h2 style=""color:blue"" align=""left""> 1. Import necessary Libraries </h2>",18a1753d,0.045454545454545456
2553,be2f4d8a6b73ca,17a53f55,"**<span style=""font-family:cursive;"">Machine Learning is used across many spheres around the world. The healthcare industry is no exception. Machine Learning can play an essential role in predicting presence/absence of Locomotor disorders, Heart diseases and more. Such information, if predicted well in advance, can provide important insights to doctors who can then adapt their diagnosis and treatment per patient basis.</span>**",5d8ce40a,0.045454545454545456
2554,32e04b08ff52eb,2b1738fa,Reading Data,8d5b86e0,0.045454545454545456
2561,f06fd8f5916431,7058adec,"# Objective:


The objective of this notebook is to demonstrate how make a submission by using a trained model previously stored in a private dataset. This version is optimized to be memory efficient. I have previously tried to process the images in memory, and kept running out of memory. The solution was to write the image data into a TFRecord file and then use tf.Data.Dataset to feed the TFRecord file to the model for predictions. The tf.Data.Dataset reads the data sequentially from the file and therefore no extra copy of the image is needed in memory. This was the ONLY way I found for submissions to execute succefully both in the public and private datasets.

The Keras model that is used was built and trained using TPUs as described in this notebook, the dataset was all tiles with gloms for 200 epochs:
[https://www.kaggle.com/marcosnovaes/hubmap-unet-keras-model-fit-with-tpu/](https://www.kaggle.com/marcosnovaes/hubmap-unet-keras-model-fit-with-tpu/)

I also have a GPU version that is not as powerful :
[https://www.kaggle.com/marcosnovaes/hubmap-unet-keras-model-fit-with-gpu](https://www.kaggle.com/marcosnovaes/hubmap-unet-keras-model-fit-with-gpu)

The Unet Keras model utilized is the one proposed by a [popular paper in biomedical image segmentation](https://arxiv.org/abs/1505.04597), by (Olaf Ronneberger, Philipp Fischer, Thomas Brox).

The particular implementation used is the one proposed by by [Dr. Bradley Erickson](https://github.com/slowvak), available in the: [The Magician's Corner repository](https://github.com/RSNA/MagiciansCorner/blob/master/UNetWithTensorflow.ipynb). 

The basic modification that I have made to the implementation provided by Dr. Erickson is to enable the Tensorflow distributed training strategy (tf.strategy). You will notice that the function model.fit() is used within a strategy.scope(), so that it leverages either GPU or TPU acceleration. 

In previous notebooks, I demonstrated how to read the competition data and produce a TFRecord dataset tiling the images in 512x512 tiles. This Notebook will use this dataset as input to the Keras Unet model:
--> [Link to the TFRecord Dataset used for training.](https://www.kaggle.com/marcosnovaes/hubmap-tfrecord-512)

Previous Notebooks in this competition: 

[https://www.kaggle.com/marcosnovaes/hubmap-3-unet-models-with-keras-cpu-gpu/](https://www.kaggle.com/marcosnovaes/hubmap-3-unet-models-with-keras-cpu-gpu/): Investigates three implementations of the Unet model

[https://www.kaggle.com/marcosnovaes/hubmap-read-data-and-build-tfrecords/](https://www.kaggle.com/marcosnovaes/hubmap-read-data-and-build-tfrecords/): Demonstrates how the TFRecord Dataset was built

[https://www.kaggle.com/marcosnovaes/hubmap-looking-at-tfrecords/](https://www.kaggle.com/marcosnovaes/hubmap-looking-at-tfrecords/): Explains how to read the data using the TFRecord Dataset",854fa433,0.045454545454545456
2563,5083d7a61f2426,7e245a84,"

Function to access a specific set of information, which will be used in the predictive model",541a0fec,0.045454545454545456
2564,90964081c7faab,11d3ae65,## 1) Load relevant packages,b423b0c3,0.045454545454545456
2565,e323e594ef918f,ee670aa0,# Model Training,6e829ab6,0.045454545454545456
2566,0475899eec1ffe,c45a7ecc,"Hello,
I went to be a data scientist, and 2 studies I have done by doing several kynalktan researches are as follows. The data on the Kaggle side is taken from a developed kernel first. Thanks to my friend who works, eternally. ( https://www.kaggle.com/mczielinski/bitcoin-historical-data). The data explanations are as follows. 

CSV files for select bitcoin exchanges for the time period of Jan 2012 to July 2018, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. Timestamps are in Unix time. Timestamps without any trades or activity have their data fields forward filled from the last valid time period. If a timestamp is missing, or if there are jumps, this may be because the exchange (or its API) was down, the exchange (or its API) did not exist, or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability, but obviously trust at your own risk. ",d825dc37,0.045454545454545456
2568,930cd79ca51204,9db022e1,**This Notebook made as a first try and exploration of the pandas and seaborn visualiazation features.**,5506779a,0.045454545454545456
2570,6d66ced0028dea,18edb6cc,"Дедлайны и требования:
- Соревнование заказнчивается 29 апреля
- Необходимо получить R2 > 60% на __Private__ Leaderboard для успешного завершения курса
- Необходимо получить ранг ""Contributor"" в соревнованиях на Каггле: https://www.kaggle.com/progression
- Прислать в комментариях к ДЗ ссылка на решение проекта в git / public kaggle notebook",f50aae52,0.045454545454545456
2573,59845b1e4f2bf6,a32929c1,"CTRL+ENTER - run the current cell
LEFT CLICK ON CELL/ENTER - start text editing cell
LEFT CLICK OFF CELL/ESC - stop text editing cell
b - create new code cell",6eda5cda,0.045454545454545456
2578,450fda47b03baa,933848f0,Veri çerçevemizi bulunduğumuz dizinden yükleyelim ve bir veri çerçevesi haline getirerek df değişkenine atayalım. (pd.read_csv(...csv)),62c04adb,0.045454545454545456
2582,ee23a565163388,43d89fad,# **Prerequisites**,88aacbc4,0.04580152671755725
2584,ee9ddc756b2d4a,4dc14e9a,"### Metriche

Ho esplorato alcune metriche utili in diagnostica, basandomi su un lavoro precedente fatto con alcuni miei colleghi: https://medium.com/mljcunito/daignosis-exploring-the-space-of-metrics-c6bca5d53acb

Ottimizzando la metrica sensitivity sono riuscita ad ottenere buoni (ma non ottimi) risultati.

""*Sensitivity is the probability that the model predicts positive if the patient have the disease: it is the proportion of examples classified as positive in a total of positive examples.*""

*Sensitivity = True positive / (True positive + False negative)*

Dunque il seguente lavoro è buono nel riconoscere un paziente malato, che presenta tumore.
",e367eab3,0.04597701149425287
2589,1645979263c148,4628b8f7,"## Index

1. Data Description
2. Used Python Libraries
3. Know Dataset Nature
4. Exploratory data analysis (EDA)
5. Data Preprocessing
6. Data Normalization
7. Feature Selection
8. Feature engineering
9. Model Buliding
10. Receiver Operating Characteristic Curve (ROC AUC)
11. conclusion",fa11663e,0.046153846153846156
2591,3cb96bd8eb364b,4b319801,"## Collect Data
(try to store as many data as possible to save quota)

**As we had previously collected the data and stored it in IME's network,
this step isn't needed**",3157af7e,0.046153846153846156
2593,c115e287523aab,47b04242,"# Overview:

## Augmentations:
* Random - Horizontal Flip
* Random - Brightness, Contrast, Hue, Saturation
* Coarse Dropout

<img src=""https://i.ibb.co/XC4dBJs/results-39-0.png"" alt=""results-39-0"" border=""0"">

## WandB Integration:
* You can track your training using **wandb**
* It's very easy to compare model's performance using **wandb**.

<img src=""https://i.ibb.co/KKxF8Fj/wandb-result.png"" alt=""wandb-result"" border=""0"">

## Grad-CAM:
* You can use **Grad-CAM** to interpret the results

<img src=""https://i.ibb.co/WPwNcc9/wandb-grad-cam.png"" alt=""wandb-grad-cam"" border=""0"">

## Train Vs OOF Distribution:

<img src=""https://i.ibb.co/Yy4NXvW/results-58-0.png"" alt=""results-58-0"" border=""0"">",feb1288b,0.046153846153846156
2594,d07915a6e6992e,726d5a99,"# What would be the workflow?

I will keep it simple & crisp rather than using buzz words & useless data science frameworks. Frankly speaking no one cares. 

This will help you to stay on track. So here is the workflow.

**Problem Identification**

**What data do we have?**

**Exploratory data analysis**

**Data preparation including feature engineering**

**Developing a model**

**Model evaluation**

**Conclusions**

That's all you need to solve a data science problem.",2b912140,0.046153846153846156
2595,03048e86a6d806,c0e7a439,# Know Your (Potential) Co-Workers,1285c231,0.046153846153846156
2596,09751c520b0616,2f8b2096, - Getting information about train and test datase</b>,a4d0c7e9,0.046153846153846156
2598,a8c042af6b7245,745eaff6,### Loading data,2487ac62,0.046153846153846156
2599,a4f8ad33c823c5,b221fb8b,"## Explore interesting themes¶
* Find out whether the patient has been diagnosed with Diabetes Mellitus so that appropriate and timely medication can be provided to the patients admitted into the icu wards when unconscious.
* Diabetes is a medical condition in which the blood glucose levels remain persistently higher than normal. It is becoming more common in Singapore. 
    As supported by the data provided by the [Singapore healthhub](https://www.healthhub.sg/a-z/diseases-and-conditions/626/diabetes), 440,000 Singaporeans was reported to have been diagnosed with diabetes in 2014. This figure is expected to go up to 1 million by 2050. Diabetes is now a concerning cause of disease among Singaporeans and accounts for 10% of disease burden in Singapore.
* Detect cases of class imbalance (target variable - diabetes Mellitus)
* Age group
* Different categories
* Total number of patients
* Different type of admissions
* Capcity of the icu ward
* Length of stay
* Readmitted?
* APACHE scores (how to interpret them)
* What vital signs to pick up
* Look at lab results",fcd48307,0.046153846153846156
2600,2f0f808765fc67,d3fe3738,"# **A SHORT DESCRIPTION OF THE FEATURES**

datetime - hourly date + timestamp

season - 1 = spring, 2 = summer, 3 = fall, 4 = winter

holiday - whether the day is considered a holiday

workingday - whether the day is neither a weekend nor holiday

weather -

1: Clear, Few clouds, Partly cloudy, Partly cloudy

2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist

3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds

4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog

temp - temperature in Celsius

atemp - ""feels like"" temperature in Celsius

humidity - relative humidity

windspeed - wind speed

casual - number of non-registered user rentals initiated

registered - number of registered user rentals initiated

count - number of total rentals",fd1f6494,0.046296296296296294
2601,ab6da5994949a3,c09b32af,### Checking for nulls,fae6b91d,0.046296296296296294
2604,22ba3a8149c2f1,9a01e0e4,# 1 Data,19c82be5,0.046511627906976744
2605,8539260444e6b5,642b54dd,# Reading Data,0369463f,0.046511627906976744
2606,743ae010f5e875,62b17cda,# Install packages,02c54445,0.046511627906976744
2607,2e40928927c0d4,7788fa19,**Declaring the constansts**,b6385ef2,0.046511627906976744
2610,72d393488311b6,0cfe06fe,# import DataSource,80663df0,0.046511627906976744
2612,d96642860ab3dd,2766c8d4,# 1. EDA Data Visualization,98419d48,0.046511627906976744
2615,2bd6c370695ea7,8e4a953f,## Metadata and Sentiment data,cbe6aec8,0.04666666666666667
2617,c84925c8171900,acdbccc0,"<a id=""market""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            1.2 Market Overview :
            </span>   
        </font>    
</h3>",e21ff7ec,0.04672897196261682
2620,3cc097a5859dc1,b5e117c6,# **Data Exploration**,14380d73,0.046875
2621,ff3a8ce61fab6a,b2c2bca1,# Now let's take fast overview about tensorflow.,9afe1654,0.046875
2622,c85c94076e9c3a,33d2a999,# Data Wrangling,3ea0c443,0.046875
2623,0932046e1f485d,dd72e89f,"Table of content
1. [App Wordcloud](#app_wordcloud)
2. [Missing Data Check](#missing_data_check)
3. [Data Cleaning](#data_cleaning)
4. [Numerical Data](#numerical_data)
5. [Genres & Categories](#genres)
6. [Reviews Dataset](#review_dataset)
7. [Finish line](#done)",218cc7a3,0.046875
2624,5f32117bcd5255,358ad764,### K2-18 PATH,85882abf,0.04697986577181208
2625,726833f92fb87a,c31122c9,"Main results from the data analysis:
- Customers who previously accepted the deposit tends to accept the deposit.
- Customers with job type as 'student' or 'retired' tends to accept the deposit, while blue collars tends to refuse the deposit.
- Customers without a personal loan tends to accept the deposit.
- Customers without a Housing Loan tends to accept the deposit.
- Customers tends to accept more the deposit on march, april, september and october.",7dc5e1b6,0.04697986577181208
2626,fc8e0042411c46,b36f83b8,## Duplicate Check,af476c2a,0.047021943573667714
2627,869a39a3d4dea2,fe2f9b95,Using the Captcha Images for this exercise,9020daf8,0.047058823529411764
2630,b01ee6cb674fa3,8e422ba3,## Location,a8ffd35e,0.04710144927536232
2636,e16860fce156b0,74032757,"<a id=""1.1""></a>
<h3 style=""background-color:skyblue;font-family:newtimeroman;font-size:200%;text-align:center""> Dataprepare.eda by SFU Database System Lab</h3>


License: MIT License (MIT)

Author: SFU Database System Lab  - Maintainer: Weiyuan Wu https://pypi.org/project/dataprep/",2054f1ce,0.047619047619047616
2637,7454fdc444df16,e55cc0b5,### Settings,a7818ef5,0.047619047619047616
2639,a758983a68c014,8fa775a6,"Word Embeddings are extremely important for NLP tasks. Ready-to-use solutions like Glove, FastText etc. are very useful and relatively efficient, so why wasting time on Skip-Gram, which works empirically worse and need much time to fit? The goal of this notebook is to manually implement this model for deeper understanding of Word2Vec mechanism to prepare myself and probably readers for diving deeper into more complex solutions. In the notebook I'll use PyTorch, but if you're not familiar with it, no problem, some comments for understanding the code are provided. Nevertheless, it's assumed the one is familiar with basic Neural Nets concepts. Besides, I'll not discuss here why we need Word2Vec as there are much useful info on this topic throughout internet.",ab89f181,0.047619047619047616
2643,6471597c5d2f66,47f1bd89,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",a41b4abe,0.047619047619047616
2646,c818250dd720eb,56bb615e,"# Exploratory Data Analysis


A detailed introduction to the challange and exploratory data analysis including python code can be found [here.](http://www.kaggle.com/dararc/introduction-eda/) For the sake of brevity within this notebook I present a small snapshot of the analysis I conducted. 

We see below for reference the first five rows of the CSV file provided alongside our images and masks. Our csv file contains the image id with which we can access the multi-level tiff file, the data provider, the ISUP grade, and the underlying gleason patterns. 

It is worth mentioning here how the gleason score and the isup grade are calculated. The gleason score refers to which patterns are present, 0+0 reflects no cancer present while a gleason score of 4+3 indicates gleason pattern four is the most prevalent cancer present while gleason three is also present. The ISUP grade follows directly from the given Gleason score, it is essentially a tidier scale to reflect the Gleason Score. ISUP of 0 indicates no cancer, while ISUP 5 is the most severe grading. ",68ee40de,0.047619047619047616
2647,60d500d196eb42,d5e0e437,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",2ad55f3f,0.047619047619047616
2650,e04e5204572e7e,1622838e,"#### *A lots of wool!! Certainly this one is not meant for meat!! This is the Merino sheep, wool of which is one of the most expensive in the world!!!*
![](https://upload.wikimedia.org/wikipedia/commons/a/a1/Merino_sheep.png)",6c888be9,0.047619047619047616
2653,066c5ee1ef39e6,57f6e5d2,## Training Data,0f394e1b,0.047619047619047616
2654,2b97b399158701,ab6b48d6,## Initial Setup,04bd0060,0.047619047619047616
2659,898d18d501f68d,977d4b88,overview of the DATA,d8bdea2d,0.047619047619047616
2660,0c57e3132ae184,e993e811,# Data Cleaning,f6bac298,0.047619047619047616
2663,fda19edaf5c621,6a8572e7,"It is a old saying that when people gets old, they start behaving like a child. And that is what I found from these Survey dataset. Let's pull in the data first:",5cfff76e,0.047619047619047616
2664,87e94f864d74be,88762c91,"# <span style=""font-family:serif; font-size:28px;""> 1. Quick look at the data</span>",294bfe9f,0.047619047619047616
2666,b6e698d389d0d3,8d318eef,# global variables,f02f68b5,0.047619047619047616
2667,53f302571cd4ac,4dc1f877,"## Class:
A class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods).<br>
Unlike the primitive data structures, Classes are the data structures that users define.",62c28443,0.047619047619047616
2669,b74076b2f8ba1d,1e937131,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",9ace22d4,0.047619047619047616
2671,6f4795cfdc96c7,0c70be5b,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",1f3ab82f,0.047619047619047616
2675,659f5f3ef8aa0e,f49b2872,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",3654c2d0,0.047619047619047616
2678,916ccf243827f1,0038a67a,## 1. Load the Train and Test dataset,5147f4d2,0.047619047619047616
2680,9456df44ab9308,05e647d9,"## [Code](https://www.kaggle.com/ashkhagan/pose-estimate)
The kernel shows how to use the [tf_pose_estimation](https://github.com/ildoonet/tf-pose-estimation) package in Python on a series of running videos.",a8e94298,0.047619047619047616
2682,adb8441ad28019,b0d85d1a,"# <span style=""color:seagreen;""> Importing Libraries </span>",d89de993,0.047619047619047616
2684,565ad413cd802f,7dcd62f7,The `train.csv` file contains image IDs and labels for training data. Note that this is a multi-label classification problem. Each image can have more than one type of protein.,397b074e,0.047619047619047616
2686,fdc9f4863744b1,84119b06,"Finally will import regression , metrics and other model libraries for ML from sklearn",b4529365,0.04794520547945205
2689,44f6a002ecd033,114bf3e4,## Analyzing the Data,70bbe106,0.04807692307692308
2693,ad26c020235dfc,628a36f6,"# Path
Define input path and show content files.",bf766e48,0.04838709677419355
2696,57070ad5e0f94f,60d24557,# **Getting Data**,d97edc41,0.04838709677419355
2700,98a6794067932a,c10ee387,"La cellule ci-dessous permet d'importer le fichier csv servant de base de données à ce projet. Il s'agit donc du fichier comprenant les données de ventes des quatre années mentionnées plutôt dans ce rapport. Après avoir importé le fichier csv, la cellule permet également d'afficher la base de données avec l'aide de Pandas en prenant la colonne du numéro d'identification des commandes comme index de référence.",08600fe2,0.04854368932038835
2701,2ada0305b68956,fba7a4cb,### 5. Palette = 'BrBG',133e26f4,0.04857142857142857
2705,0e09587faffa8f,8bec4dc2,## I. Environment Setup,0d563d61,0.04878048780487805
2707,fd4017c1514157,6baf5927,***,fd8f0896,0.04878048780487805
2709,786475feda0190,4e3bf4ca,### Importing the Libraries,e4663d97,0.04878048780487805
2710,dbccf99c49570f,32731b73,## Reading Data,c20fc09e,0.04878048780487805
2711,74a03887600114,83efe51b,First of all we will going to import all the required libraries,c0ffb2f0,0.04878048780487805
2713,8d70dcae7f40a3,d7a1c3c3,# **Preprocessing data**,472c71ce,0.04878048780487805
2717,e19e307b3fd188,9edc2517,# Exploratory Data Analysis (EDA),2173955b,0.04878048780487805
2719,71b75664517244,7165f6cd,Lets check how it look,fc905af5,0.049019607843137254
2720,7cfd96218dd933,06c381e3,# INVESTIGATION FOR SPECIFIC AREA,7c34d96c,0.049019607843137254
2721,842547b2def18c,bc18d2d2,"## 要約統計量による分析

Pandasは，次の問いに簡単に答えるようにデータセットを要約することに役立ちます．

> ***データセットの中で，どの特徴量が利用可能なのか?***

特徴量を直接操作/分析するために，特徴量の名前を記録しておきます．これらの特徴量の名前は，[Kaggle data page](https://www.kaggle.com/c/titanic/data)で述べられています.",b8efde6d,0.049019607843137254
2723,52cfd66e9ec908,4b66e2be,"This is apparently the **largest collection of traffic agent motion data.** The files are stored in the .zarr file format with Python, which we can easily load using the Level 5 Kit (l5kit for the pip package). Within our training ZARRs, we have the agents, the masks for agents, frames and scenes (which you might recollect from last year) and traffic light faces.

The test ZARR however is almost practically the same format, but the only exclusion is that of the data masks. for the agents. ",c74adcdf,0.049019607843137254
2725,601e18072783b4,e04b3a33,## Download the dataset files from imdb,36b2b1fa,0.04918032786885246
2728,2105f2c5132866,e535ab2e,"# Project Planning

* Understand nature of the data .info() .describe()
* Histograms and boxplots 
* Value counts 
* Missing data 
* Correlation between the metrics 
* Explore interesting themes 
* Feature engineering 
* preprocess data together or use a transformer? 
* use label for train and test   
* Scaling?
* Model Baseline 
* Model comparison with CV ",bfe8023d,0.04918032786885246
2729,0858e1bb3cbaca,027389d6,# Display,78548374,0.04918032786885246
2730,4883314a96dc34,03848cc7,# Load dataset,50d36836,0.04938271604938271
2732,e9b9663777db82,f4b602da,#### Home Properties Analysis,648e8507,0.049586776859504134
2737,8bb432d338a70b,94f28608,Carga de ficheros de datos,7aab1dfd,0.05
2739,b10bd75889dad9,9572fcef,"#### drop all the 9th month columns, since the target variable has been derived",ee00ceee,0.05
2740,1011899b959f44,484ee1ef,"# Import
First and foremost, we begin by loading and importing the Pandas method and the dataset csv file(s) and saving them as DataFrames, otherwise known as objects which represent data in tables.

In this tutorial, we save them under the names 'battles' and 'deaths.'",0b112382,0.05
2742,ff83da40bcdb19,51ba520e,"**PART 1:**
*Importing libraries, giving path and defining the parameters.*",36b5ec8c,0.05
2744,37b09262279764,93cbb1fc,### Miscellaneous,37c4c417,0.05
2750,fc3adf9d45953e,163e43c3,"#Five Reasons why Learning AI and and ML are Important in Early Education:

Algoritms

Solution-oriented

Opportunities

Big Data

Morality and Humanity

https://thestempedia.com/blog/reasons-why-learning-ai-and-ml-are-important-in-early-education/

![](https://v6f7u6f8.rocketcdn.me/wp-content/uploads/2020/09/Importance-of-AL-ML-1000x563.png)thestempedia.com",e3789b90,0.05
2753,f0faf9e7ac5abd,af68d4e5,br.pinterest.com,794edb83,0.05
2755,ad121e0531afa4,c0b96fd5,"<h1 style='color: #fc0362; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 24px'>If you liked this notebook, kindly leave an upvote ⬆️</h1>",a3492905,0.05
2756,83df814455f06c,5b0094fe,"# **3. Decision Tree algorithm terminology** <a class=""anchor"" id=""3""></a>

[Table of Contents](#0.1)


- In a Decision Tree algorithm, there is a tree like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from the root node to leaf node represent classification rules.

- We can see that there is some terminology involved in Decision Tree algorithm. The terms involved in Decision Tree algorithm are as follows:-


## **Root Node**

- It represents the entire population or sample. This further gets divided into two or more homogeneous sets.


## **Splitting**

- It is a process of dividing a node into two or more sub-nodes.


## Decision Node

- When a sub-node splits into further sub-nodes, then it is called a decision node.


## Leaf/Terminal Node

- Nodes that do not split are called Leaf or Terminal nodes.


## Pruning

- When we remove sub-nodes of a decision node, this process is called pruning. It is the opposite process of splitting.


## Branch/Sub-Tree

- A sub-section of an entire tree is called a branch or sub-tree.


## Parent and Child Node

- A node, which is divided into sub-nodes is called the parent node of sub-nodes where sub-nodes are the children of a parent node. 


The above terminology is represented clearly in the following diagram:-",c9cff71a,0.05
2757,5626e84c4e6bf8,17d797ad,"# Things we can do with Self Organizing Maps
* Visualizing high dimensional data into a low dimensional view which is usually 2D - In this case we have 784 columns because the Fashion MNIST dataset has images of dimensions 28x28 
* Clustering - According to [Wikipedia](https://en.wikipedia.org/wiki/Self-organizing_map): It has been shown that while self-organizing maps with a small number of nodes behave in a way that is similar to K-means, larger self-organizing maps rearrange data in a way that is fundamentally topological in character.
* Anomaly detection - We identify entities whose topological distance to its topological neighbors is significantly higher than all its topological neighbors amongst themselves as anomalies
* Non-linear DImensionality Reduction - For visualization, we convert high-dimensional data into low-dimensional data",e2ecb669,0.05
2764,09bac0c221388e,a6de5f11,"<img src= ""https://images.pexels.com/photos/6801648/pexels-photo-6801648.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940"" alt =""Document"" style='width: 1050px;'>",bea4aa2e,0.05
2765,254cccd5145725,0d921695,"<font size=""4"">The objective is to predict poverty on a household level i.e the Target Variable. </font>",a49b4037,0.05
2775,5ba4207c371899,56f373e7,Importing libraries,187b1451,0.05
2778,9a040a4f21091e,2b892818,"# Basic EDA
First, let's check out a few examples of non-toxic and toxic speech.",f591b57d,0.05
2781,edc19e349fe80a,d49b5af6,"## Data preparation

### 1. Load the data and take a peek",7882221a,0.05
2783,cf4d1c1ad1476c,2d81f22b,"## MXU and VPU

A TPU v2 core is made of a Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc. 
The VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16-32 bit floating point format.",768c1a59,0.05
2784,2bace980aeb34c,3b39f6aa,"In this exercise, you will use **pipelines** to improve the efficiency of your machine learning code.

# Setup

The questions below will give you feedback on your work. Run the following cell to set up the feedback system.",dc05ef6c,0.05
2787,a566b5b7c374e7,55b848a1,### Prepare Sleep Cycle table (sc_table),b3dc5545,0.050359712230215826
2791,08f845750d026a,b65242ad,The median loan fuunded amount is $450,1c54de30,0.05063291139240506
2792,5d2a3e82679cf3,f8eb30d8,"# I wanted to learn dtypes.

-  Then i saw that there are missing values in Salary variable.",9e60b1e3,0.05063291139240506
2794,0e2a23fbe41ca9,20e6644b,"## Sanity Check

- Unique ```card_id```'s in training data
- Unique ```card_id```'s in testing data
- Overlap of cards in training and testing data
- Nulls",64e4762c,0.050724637681159424
2796,b660910fcc2954,ff6876f2,## Load the data,80b74f88,0.05084745762711865
2797,a81661cc35d8d2,30c33937,"<font size=""4"">This database has been sourced from kaggle (link: https://www.kaggle.com/andrewmvd/heart-failure-clinical-data). The goal is to see whether we can predict heart failure from the features available to us. Since this effort requires a binary categorical prediction, we will use two commonly applied classification algorithms - Logistic Regression and Random Forests. We will also explore if we can improve performance by feature engineering, and compare performance across.</font>


### References

1. Ahmad, Tanvir; Munir, Assia; Bhatti, Sajjad Haider; Aftab, Muhammad; Ali Raza, Muhammad (2017): DATA_MINIMAL.. PLOS ONE. Dataset. https://doi.org/10.1371/journal.pone.0181001.s001 


2. Chicco, D., Jurman, G. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Med Inform Decis Mak 20, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5",3331f113,0.05084745762711865
2799,9169c4e9c33c90,13124dbe,# Summary,725bf880,0.05084745762711865
2800,c4bca5d86a38c3,345977d1,Muestra de la data,e23d297c,0.05084745762711865
2801,149cb8d3489224,8f351753,## Load data,116858e7,0.05084745762711865
2805,bb0905d33ae417,e47df94b,# Skeleton code,25fd1965,0.05084745762711865
2806,1294fb4c86f993,fc6fca57,"### Questions That can be asked from this data file<br>
<img src=""Images/questions.jpg"" alt=""drawing"" width=""300"" align=""right""/>
<b>
Q1: Which states have had the highest growth in gun registrations?<br>
Q2: Which states have the hightest regestered guns from 1998 to 2020?<br>  
Q3: What is the Guns Registeration Year 2020?<br>  
Q4: What is the trend of total guns registeration over time?<br>  
Q5: What is the trend of guns registeration in the state Louisiana and Texas?<br>  
Q6: What census data is most associated with high gun per capita?  <br>
Q7: What is the distribution of the total registerations of guns? <span style=""color:red"">(Resubmission Note)</span>
</b>",4471e513,0.05084745762711865
2807,dac3c8204a2d1b,00e35ae1,Load the required Libraries,b0d2d0dc,0.05084745762711865
2808,a077820f7ab459,5a2a3da3,"# Create DINO attention map images and saved
https://www.kaggle.com/stpeteishii/dandelion-image-dino-vision-transformers",05a43104,0.05084745762711865
2815,4d91e84c564cbe,eef40895,We can even make a list of lists:,355a43e3,0.05128205128205128
2816,49ee86d074de69,32d760bc,"<a id = ""3""></a><br>
## Basic Data Analysis",71ccc6d3,0.05128205128205128
2818,80ad12f326ab70,3bd817e8,#### Import libraries,da404a16,0.05128205128205128
2823,4bbe953f82d29b,777b540c,"### Курс молодого бойца

#### Загрузка данных",772301f2,0.05128205128205128
2825,897ca904b74a98,6aa59a42,## Shape Analysis,c5844ad4,0.05128205128205128
2827,0a1fcda859252c,a06e0f16,Reproducibility is a great concern when doing deep learning. There was a good discussion on `KaggleNoobs` slack regarding this. We will set a numer of things in order to make sure that the results are almost reproducible(if not fully). ,13a38774,0.05128205128205128
2831,50b03ce5b1a286,75b185f6,"<center style=""font-family:verdana;""><h1 style=""font-size:200%; padding: 20px; background: #001f3f;""><i><b style=""color:white;"">Feyn and QLattice</b></i></h1></center>

""Feyn is a Python module for interacting with the QLattice.""

""The QLattice is a machine learning technology that helps you search through an infinite list of potential mathematical models to solve your problem.""

""It's a quantum-inspired simulation where you make decisions when exploring the data, giving you a good understanding of the relationships in your data and closing the loop between scientific inquiry and data science.""

https://docs.abzu.ai/

https://docs.abzu.ai/docs/guides/getting_started/community.html",d49896a5,0.05128205128205128
2834,81712ee7510ac5,355d0d24,**Exponents**,c4685e79,0.05142857142857143
2837,225b4fe5d3894a,f0d49237,"<a id=""3""></a>
## 3. Take a Quick Look at Data Structures",4b4197b3,0.05154639175257732
2838,063a35f644e3c5,79f2db13,#### Loading Data,1c30fb0a,0.05154639175257732
2840,8ec771f5600a61,18fe131d,# OVERVIEWING OF DATA,48364c1f,0.05154639175257732
2842,1cd8be6e679620,5df0e619,"## Install Packages

---
- Reference

    1. [https://anaconda.org/bioconda/viennarna](https://anaconda.org/bioconda/viennarna)
    2. [https://github.com/ViennaRNA/forgi](https://github.com/ViennaRNA/forgi)",3ce15a43,0.05172413793103448
2845,1dd9c6aa74d289,88174a11,"# [Q1: How long could it take to climb my first 6a, 7a or 8+?]",5ef9a1be,0.05172413793103448
2848,1750367e54f407,14c4aa97,"Version notes:
* 27: Fix bug in the TTA process.
* 26: Move from using `ImageDataGenerator` to `tf.data` to load data into the model. This allows to tune the Normalization layer in the pretrained EfficientNetB3 model provided by `tf.keras`. By default, images are normalized using the `imagenet` dataset's mean and standard deviation. *- score: 0.888*
* 24: Test CosineDecay instead of ReduceLRonPlateau and increase the dropout rate within the EfficientNetB3 model. *- score: 0.885*
* 22: Replace the custom generator and data augmentation using `imgaug` with the new Keras preprocessing layers. Also increase the image size from 300x300 to 512x512. *- score: 0.880*
* 21: *- score: 0.883*
* 20: Fix bug activating the ""training mode"" by default in the custom generator, even during validation. Removed data standardization and class weights, simplify the bottleneck layers of the model, and remove the dropout from the data augmentation techniques. *- score: 0.873*
* 17: Additional data augmentation techniques. *- score: 0.821*
* 16: Tighter scan of the images at test time. *- score: 0.857*
* 15: Add data standardization. (Cannot remember if there was a bug but the score was abnormaly low) *- score: 0.692*
* 11: Train the model by randomly cropping 300x300pixel tiles from the original images *- score: 0.847*
* 9: Classification from resized images. *- score: 0.421* ",a8e655b2,0.05172413793103448
2851,00001756c60be8,7e2f170a,**Подключаем предупреждения**,945aea18,0.05172413793103448
2856,722cd844dfbe8f,6f585bb9,"First, we have to load the usefull Python libraries :",0cedb385,0.05194805194805195
2859,c13f73168789c2,2263269e,# Selecting<a id='1'></a>,16175052,0.05194805194805195
2860,663bbc9eaf267b,19b1af06,# Importing Data,32445529,0.05194805194805195
2862,4ae464582bac51,486ab8bf,# DATA DESCRIPTION,ca6a52ce,0.05194805194805195
2864,20b372b6e4e276,2b9b47fe,"## 2. Download data & FE <a class=""anchor"" id=""2""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.05223880597014925
2865,917957c6c4065f,1c1f9593,"전체 데이터는 34,567개 column은 16개 있습니다.  
description은 결측치가 있는 행이 있네요.",55b8ed68,0.05228758169934641
2866,9ceb7278784462,395756af,* Dataset comprises of 271 observations and 7 characteristics.<br>,3768a567,0.05241935483870968
2870,9e27af2600925c,a5512623,"## 2 - Overview of the Problem set ##

**Problem Statement**: You are given a dataset (""data.h5"") containing:
    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)
    - a test set of m_test images labeled as cat or non-cat
    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).

You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.

Let's get more familiar with the dataset. Load the data by running the following code.",9b556435,0.05263157894736842
2875,a1dcd92986bc84,9b0f2a51,"## Introduction

The example demonstrates how to build a dual encoder (also known as two-tower) neural network
model to search for images using natural language. The model is inspired by
the [CLIP](https://openai.com/blog/clip/)
approach, introduced by Alec Radford et al. The idea is to train a vision encoder and a text
encoder jointly to project the representation of images and their captions into the same embedding
space, such that the caption embeddings are located near the embeddings of the images they describe.

This example requires TensorFlow 2.4 or higher.
In addition, [TensorFlow Hub](https://www.tensorflow.org/hub)
and [TensorFlow Text](https://www.tensorflow.org/tutorials/tensorflow_text/intro)
are required for the BERT model, and [TensorFlow Addons](https://www.tensorflow.org/addons)
is required for the AdamW optimizer. These libraries can be installed using the
following command:

```python
pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons
```",730acaaa,0.05263157894736842
2878,29437539745aa5,0d84009f,"<h2 style=""font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"">TABLE OF CONTENTS</h2>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#imports"">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#background_information"">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#setup"">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#helper_functions"">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#inference"">4&nbsp;&nbsp;&nbsp;&nbsp;INFERENCE LOOP</a></h3>

---

<h3 style=""text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;""><a href=""#viz"">5&nbsp;&nbsp;&nbsp;&nbsp;VISUALIZATION</a></h3>",c17b490a,0.05263157894736842
2879,fe7360cddc13e5,42e88b35,"**BG** = (beta-üstel dağılımı karışımı) Tekrar satın alma davranışını modellemiştir.

**NBD** = (poisson-gama karışımı) Gelecekteki işlemlerin beklenen sayısını modellemiştir.",8979e423,0.05263157894736842
2880,d369f200a84c2a,b8f73b94,"[Capsule Network](https://arxiv.org/pdf/1710.09829.pdf) is a little-known category of neural network. 
>Briefly explaining it, capsules are small groups of neurons where each neurons in a capsule represents various properties of a particular image part.

The main interest of this type of network is to address some issues of convolutional neural networks (CNN). such as for example :
*  Relative position

    CNN are unable to identify the position of one object relative to another, they can only identify whether or not an object exists in a certain region.

![kndrck.co](https://i.imgur.com/0ZyaPt3.png)

For example, both images are identified as a face by a CNN because all the key features are present regardless their position, while the capsule network (CapsNet) will not classify the one on the left as face because the key features are not positioned correctly with each other.",8fef4d48,0.05263157894736842
2883,c2a9f2fb3e1594,c72d0fbd,"<a id=""ch3""></a>
# Step 1: Define the Problem
For this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.

......

**Project Summary:**
The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.

Practice Skills
* Binary classification
* Python and R basics

# Step 2: Gather the Data

The dataset is also given to us on a golden plater with test and train data at [Kaggle's Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)
",53411c04,0.05263157894736842
2889,1f3295ed0d4e4a,bc1d28ea,# Imports,b0aeb172,0.05263157894736842
2892,c3498779cda661,f6bbebe1,# Parte 1: Clasificación de animales con K-Means,0f531b65,0.05263157894736842
2893,5ce12be6e7b90e,015c83dd,"# Variables

A variable is a _name_ that references a an _object_ in memory.
An object has a _value_ and a _type_.

To bind an _object_ to a _variable_, we use the _assignment_ operator `=`.",c0ab62dd,0.05263157894736842
2899,0d8df2c2983694,af4f59b0,"Create a logistic regression based on the bank data provided. 

The data is based on the marketing campaign efforts of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).

Note that the first column of the dataset is the index.

Source: [Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014
",9bf7fa4e,0.05263157894736842
2904,840534f2908a9c,c46710f0,**Reading testing data**,8081c3cc,0.05263157894736842
2905,31b564f11ef638,e8e839d4,### I also shared [basic EDA notebook for everyone](https://www.kaggle.com/werooring/bike-sharing-demand-basic-eda-for-everyone),424f9692,0.05263157894736842
2906,30fdc4a6e3c1db,38010fe9,# 3. Summary Statistics,6111ddee,0.05263157894736842
2907,6b955982396c14,7307374c,"## 문제1
- 데이터셋(basic1.csv)의 'f5' 컬럼을 기준으로 상위 10개의 데이터를 구하고,
- 'f5'컬럼 10개 중 최소값으로 데이터를 대체한 후, 
- 'age'컬럼에서 80 이상인 데이터의'f5 컬럼 평균값 구하기",2b4cb71b,0.05263157894736842
2908,5c3b8925bb1e43,52e39a20,"**Wine Quality classification using Decision trees and Random forest**
",d606a719,0.05263157894736842
2910,bef2347846e476,100a442f,Let us first import the needed csv files from the dataset bwith the belowing code.,cb93bf51,0.05263157894736842
2914,d81d3830152f88,6e29e3b2,"## Motivation
Since the Golden State Warriors, led by superstar Stephen Curry, won their NBA championship in 2015, there has been a increase in 3-pt shooting in the games. We now know that 3-pt shooting team can win championship, but we might want to know how important it is to have a high team 3-pt field goal percentage in order to win games. 
",9551eac9,0.05263157894736842
2918,4c47839b067546,d02dda1e,# 2. Загрузка данных и предварительный анализ,1f517b02,0.05319148936170213
2929,cb570c7b7f0501,e5e2588a,"<a id='Gathering'></a>
## Gathering Data:
we have the csv file of data, all we have is to read it to a DataFrame",a200a0ec,0.05333333333333334
2931,ee23a565163388,f6f17ef8,"To get the most out of this notebook, make sure that you know the basics of Pandas, Plotly and machine learning algorithms. This notebook is aimed to help the beginners understand the machine learning flow, hence it is prepared simple. No advanced knowledge is required.",88aacbc4,0.05343511450381679
2933,585c280865b46e,e62a80f4,"# Load information files: genes annotations, cells info etc...",4d6056f1,0.05357142857142857
2935,6a80f915608fc2,f4a12926,#### The y_yhat_plots() routine,636938eb,0.05357142857142857
2937,f13534449a3750,b0adcc21,"Intro
[Segmentation in Image Processing](#introduction)

1.[Loading libraries and functions](#section-one)
 - [Libraries](#subsection-one-1)
 - [Functions](#subsection-two-1)
 
2.[Exploratory Data Analysis (EDA)](#section-two)
 - [Statistics about data](#subsection-one-2)
 - [Images Visualizations](#subsection-two-2)
 - [Data Sampling](#subsection-three-2) 
",8b7f3332,0.05357142857142857
2939,726833f92fb87a,3576b34a,"<img src=""https://i.imgur.com/3s4oCPA.png"" width=""1000px"">",7dc5e1b6,0.053691275167785234
2942,d07915a6e6992e,619b33e6,# Problem Identification,2b912140,0.05384615384615385
2944,a4f8ad33c823c5,17b0596a,"# What is Diabetics?

![img](https://www.news-medical.net/image.axd?picture=2019%2F8%2F%40shutterstock_1182539971.jpg)
https://www.news-medical.net/image.axd?picture=2019%2F8%2F%40shutterstock_1182539971.jpg

Diabetics is a medical condition in which the blood glucose levels remain persistently higher than normal. 

Insulin is a hormone produced by the pancreas that allows your body cells to use blood glucose (sugar) for energy. Carbohydrate is converted into glucose before it is absorbed into the bloodstream. The pancreas then releases insulin to move the glucose from the bloodstream

Types of diabetics:

Type 1 Diabetics:
* No insulin is produced due to damaged pancreatic cells
* Usually diagnosed in children or young adults although it can occur at any age
* Insulin is needed for treatment

Type 2 Diabetics:
* Insulin produced is not enough or not effective (insulin resistance)
* Occurs more frequently in people over 40 years old (overweight and physically inactive)
* More younger adults and children are developing Type 2 Diabetics
* Can be controlled with proper diet and exercise but most diabetics require oral medication

Gestational:
* Occurs in about 2 to 5 percent of all pregnancies. Women who were not diagnosed to have diabetes previously show high blood glucose levels during pregnancy
* Needs specialised obstetric care to reduce serious complications to the unborn baby.

Signs and Symptoms:
* Frequent thirst despite drinking lots of water
* Constant hunger
* Constant tiredness
* Itchy skin especially around the genital area
* Passing excessive urine during day and night
* Weight loss despite good appetite
* Poor healing of cuts and wounds

Complications:
Uncontrolled diabetes can cause the blood sugar to fluctuate between very high (hyperglycaemia) and very low (hypoglycaemia). Both situations can cause a diabetic to become very sick very quickly and even go into a coma.

Screening and diagnosis:
Detected through glucose test

* Random blood glucose is 11.1 mmol\L or higher
* Fasting blood glucose is 7.0 mmol\L or higher

link: https://www.healthhub.sg/a-z/diseases-and-conditions/102/topics_diabetes",fcd48307,0.05384615384615385
2948,ccabe7a86825ce,2587e88d,Define function to calculate MAE,d766cbf9,0.05405405405405406
2949,b7b1057764fa02,1fac38b9,"# 2. Loading the data

Since the data is held in multiple folders in the directories, it is necessary to extract it and store it in arrays. Since I will be doing this operation twice - once on the training data, and again on the evaluation data, it makes sense to write a function that helps with this exaction. This is the function given below, which takes each individual image, resizes it to a convenient size and adds it to an array. It also adds the corresponding `label code` to a `labels` array. A `label code` is simply a number that we associate with each label. For example, `A` goes to `0`, `B`:`1`, `C`:`2` and so on. This is known as label encoding and I will cover it in a little more detail in section 4.",5053a192,0.05405405405405406
2950,bdf23d2d396916,b4ad8c13,"## Exploratory Data Analysis and Visualization
",b0e45a49,0.05405405405405406
2951,62037c5832129c,3a81c67f,# Streamlining workflows with pipelines,61474350,0.05405405405405406
2954,297cbe4a23c4bf,22338cf9,# Exploring features,a843e619,0.05405405405405406
2957,63b44c85e32c1f,e4d2dba5,### Indexing,fb9b9562,0.05405405405405406
2963,2dda7facf3c1e0,0ccac0e1,#Start Here,45552d2b,0.05405405405405406
2964,2ada0305b68956,ce9bea2f,### 6. Palette = 'BrBG_r',133e26f4,0.054285714285714284
2965,b01ee6cb674fa3,9688c47d,"space.location contains the following informations, stored into a string: 
- Launch site code
- Launch site name 
- state, or geolocation (it may be not present)
- Country ",a8ffd35e,0.05434782608695652
2967,016abae0483764,d432aabe,# Loading Raw Data and converting it to a cleaned data,bc9f289b,0.05454545454545454
2969,5a8c553e21c70f,83ddf367,"## Load Data

We use **Credit Card Fraud Detection** dataset. The dataset contains transactions made by credit cards in September 2013. There are 492 frauds out of 284807 transactions.

Features from V1 to V28 are the principal components obtained with PCA. Feature **Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature **Amount** is the transaction amount. **Class** is the target variable.

We load dataset from input csv file using **Pandas**. **Pandas** extracts the data and stores it in a dataframe.",9ebd9d8f,0.05454545454545454
2970,0932046e1f485d,cd1c0000,## <a id=app_wordcloud>App Wordcloud</a>,218cc7a3,0.0546875
2972,1eb62c5782f2d7,61998439,# Contoh dengan Python,bb69f147,0.0547945205479452
2976,91473a39b85068,ea48915f,"### Mapping to an Machine Learning Problem
Since we understand the Business problem well, let’s try to pose it as a proper Machine Learning problem. The first thing to do is to aquire the data and understand it.",6e3d91c2,0.0547945205479452
2979,ce9ed5e2d601d7,9e37aa9b,## Reduce Memory usage,f58a2f43,0.05511811023622047
2982,6dcfe6a610d86b,d7a76121,## MD5 via Shell and Python,d05c59da,0.05555555555555555
2983,30c8dc87ce52ca,ad9bd9ff,READING IN THE DATA AND PROCESSING IT,805e9d67,0.05555555555555555
2987,245c89d02f3f5f,b6d7072d,"## Tensorboard και ngrok

Αντικαταστήστε το authentication token σας που παίρνετε από το [ngrok](https://ngrok.com/) στη γραμμή που υποδεικνύεται. Όταν εκτελεστεί το κελί θα σας δώσει το URL όπου μπορείτε να βλέπετε το TensorBoard. Σημειώστε ότι σε περίπτωση επανεκκίνησης του πυρήνα θα πρέπει να ξανατρέξετε το κελί. Η διεύθυνση θα είναι διαφορετική, αλλά τα προηγούμενα στατιστικά σας δεν χάνονται (μέχρι να ανακυκλωθεί ο πυρήνας).",61a1eacd,0.05555555555555555
2988,b39684e6670dd7,09143bd3,"Scale features using statistics that are robust to outliers.

This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).

Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the transform method.

Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.",83de9873,0.05555555555555555
2989,3597174a998d4d,01168f41,# 1. State of hotels,276892ed,0.05555555555555555
2991,b6c0ad74f95b8c,1eee4580,"In the notebook I explored similarities between different Football QBs using the K Nearest Neigbours technique. I took data from pro Football reference on the first three years of a QB's career from years 1980-2019 and measured similarity based on completion percentage, passing yards, TDs and interceptions. ",5de5b241,0.05555555555555555
2993,9289395e9c480f,a1a4f79c,# Prepare data,55d03d67,0.05555555555555555
2995,4d1d6dbab10b20,d7d87d50,The goal of this whole dataset is to create a model which can predict the prices of the house.,fe366345,0.05555555555555555
2996,b3e0b7e9ff6849,953c72c7,## 1. Loading the Dataset & Checking Variables,f6e4bb0d,0.05555555555555555
2998,e2b5084a3cf530,9c879350,# We will use the Auto-ViML Library to explore the Auto-NLP!,9bc44045,0.05555555555555555
2999,e2a94f078e1161,4632a0bf,"Excel was used to make the following graphs and Plots. Normally matplotlip library is used in Data Science EDA's on Kaggle but for this case since the data was quite small and complexity was minimum I decided to use Excel. I wouldn't recommend doing this for very Large or Complicated Datasets



**The following is a Analysis of Categorical Data:**




![image.png](attachment:1aaa3e57-f719-4ee6-9e3e-57b2e4f35fb8.png)
![image.png](attachment:b5e892dc-26fb-491d-8fe8-19cd7e72940e.png)
![image.png](attachment:18eeea91-b70f-4ce0-9625-42c152ec30a4.png)
![image.png](attachment:1c2a05fb-9e2b-47dd-aac2-c9e262082449.png)
![image.png](attachment:eebf6afc-f8ea-4554-86a1-57abff07b092.png)
![image.png](attachment:f07d97e1-935d-46ef-b815-947703a1c2bb.png)
![image.png](attachment:577b6061-8d0e-4ea7-8863-aa0edffad767.png)
![image.png](attachment:9c6c8b0d-693d-43ff-8456-3bbac879a2d4.png)
![image.png](attachment:6242ec7c-3241-43f6-8188-b97c4a6cb10e.png)
![image.png](attachment:23e1b3d8-60d2-4c91-a06d-1cff1beba920.png)
![image.png](attachment:09605957-f41b-4a1a-af00-9b49ea01a5f6.png)
![image.png](attachment:a50e38f1-f50d-452f-ad28-81ed11d8085c.png)
![image.png](attachment:daf01105-03de-488a-88d2-2d6945229977.png)



**The following is a ananlysis of Numerical Data:**




![image.png](attachment:52aa614e-1002-4043-9ba4-23d6d7a250c5.png)
![image.png](attachment:f3cfafe5-b451-4e82-b8f4-0afcde5cae67.png)
![image.png](attachment:c009d765-e4fc-47ef-a404-23bd8b1677d3.png)
![image.png](attachment:3d4cd398-eb79-476f-8b36-fc1241cb9a8e.png)
![image.png](attachment:af17a11a-e110-42cb-a7fc-3ae436b48839.png)
![image.png](attachment:e18b30a8-27dd-4951-a042-91b29db98b3e.png)
![image.png](attachment:2aaf8b1c-4940-43b1-983c-5d0fd45e24ca.png)
![image.png](attachment:2a67786a-3450-4d22-b61f-983f7968799a.png)
",5fc53059,0.05555555555555555
3002,5d6d539f8e7121,b30b4a21,"### 문제 : f1 컬럼 결측치 제거 후 city와 f2를 그룹핑하여 합계를 구하고, city가 경기, f2가 0인 조건에 만족하는 f1 값 구하기",79340a85,0.05555555555555555
3004,caee5b3bdf65c1,d26fa0c1,# Data Analysis,a46111dd,0.05555555555555555
3005,10b5af05d804ff,d1d45962,**The main idea is that we have not one but two train data sets to fit our models!**,4a9b1705,0.05555555555555555
3011,a4a494c667c673,aa2750da,# Getting Started,397d12f8,0.05555555555555555
3012,a2286e7c88bb76,a4584029,## Loading the CSV file with our data,be48f3fb,0.05555555555555555
3013,dd3721cb49c1fd,80ee593a,"<div style=""margin: 0px; padding: 10px; background-color: #00897b;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px;"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white; text-align:center"">Table of Contents</h1>
  </div>
  
  <div style=""margin: 0px; padding: 10px; background-color: #ffffff;
        box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
        border-radius:2px; text-align:left"">
    <h5><a href=""#0"">0. References</a></h5>
    <h5><a href=""#1"">1. Installing the Greykite package</a></h5>
    <h5><a href=""#2"">2. Re-installing the pandas</a></h5>
    <h5><a href=""#3"">3. Importing all the required libraries.</a></h5>
    <h5><a href=""#4"">4. Forecaster</a></h5>
    <h5><a href=""#5"">5. Plotting the results using the Plotly module</a></h5>
    <h5><a href=""#6"">6. Performing Grid Search for hyper-parameter optimization</a></h5>
    <h5><a href=""#7"">7. Plotting the Backtest</a></h5>
    <h5><a href=""#8"">8. Listing the different evaluation metrics</a></h5>
    <h5><a href=""#9"">9. Analyse the forecast Dataframe</a></h5>
    <h5><a href=""#10"">10. Plotting the different componenets</a></h5>
    <h5><a href=""#11"">11. Printing the summary</a></h5>
    <h5><a href=""#12"">12. Forecasting for the values for the future time periods</a></h5>
    
  </div>
</div>",1a53fdd9,0.05555555555555555
3018,f18e737fcc4b06,61a4ceda,# Load Datas,087b8637,0.05555555555555555
3019,56cc8fb47bef6a,0c49f966,"<p><u><span style=""color:#a52a2a""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Below are the steps that we are going to do:</span></span></u></p>

<p style=""margin-left: 40px;""><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">1. Read data from the dataset</span></span></p>

<p style=""margin-left: 40px;""><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">2. Remove the empty columns v3,v4,v5&nbsp;</span></span></p>

<p style=""margin-left: 40px;""><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">3. Remove the first row which contains v1, v2 which is not releveant to our dataset</span></span></p>
",652d6670,0.05555555555555555
3021,df2a7968c08ee4,462a2256,"### Set Seed

Setting a seed so that the notebook results are reproducable.",a2ba0a72,0.05555555555555555
3025,cf39cde80e66b7,ab94ef86,"[Crislânio Macêdo](https://medium.com/sapere-aude-tech) -  March, 13th, 2020

<div class=""h1"">Understanding Regression Error Metrics in Python🐍</div>

- [**Github**](https://github.com/crislanio)
- [**Linkedin**](https://www.linkedin.com/in/crislanio/)
- [**Medium**](https://medium.com/sapere-aude-tech)
- [**Quora**](https://www.quora.com/profile/Crislanio)
- [**Hackerrank**](https://www.hackerrank.com/crislanio_ufc?hr_r=1)
- [**Blog**](https://medium.com/@crislanio.ufc)
- [**Personal Page**](https://crislanio.wordpress.com/about)
- [**Twitter**](https://twitter.com/crs_macedo)
",aed4bc9b,0.05555555555555555
3026,6a05614abce6d9,f9cafd1f,## Setup,c0c9da16,0.05555555555555555
3028,c9dc8d00773da4,b8f96f67,"# Data info

https://www.kaggle.com/c/bengaliai-cv19/data

(train/test).parquet
Each parquet file contains tens of thousands of 137x236 grayscale images. The images have been provided in the parquet format for I/O and space efficiency. Each row in the parquet files contains an image_id column, and the flattened image.",d9aa2f85,0.05555555555555555
3029,fbb1f9d3818830,e107030e,"Recently I was going throught the paper : [Intriguing Properties of Contrastive Losses](https://arxiv.org/pdf/2011.02803.pdf) published by the researchers at Google Brain. The paper was mainly a follow up for the **self supervised learning (SSL)** technique [SimCLRv1](http://arxiv.org/abs/2002.05709), explaining why the contrastive loss based pretext task of augmentation & random cropping worked well. 


The authors of ""Intriguing Properties of Contrastive Losses"" mainly summarize their results into 4 points:

1. Building a generalized family of contrastive loss functions & proving that a deep non-linear projection head(**an innovation explained in SimCLRv1**) reduces the performance gap between various functions & also allows the models to make use of smaller batch sizes to learn good/generalised features (*since in contrastive losses larger batch size help in faster convergance*).
2. Contrastive loss function based models make use of instance based objective (**i.e. the models that learn on entire image summary**) but can learn on images with multiple objects & learn good quality local features.
3. The authors build custom datasets with controllable features to understand the competing feature suppression phenomenon in contrastive based learning.
4. Lastly they show how dominant, easy to learn features in an image can interefere with learning of other features present.


I have summarized this as a very raw write-up & those who are interested in performing a deep dive in this topic can refer : [Intriguing](https://contrastive-learning.github.io/intriguing/.)

",c7027f86,0.05555555555555555
3034,20e523830aab51,b9fc7b0a,# Getting Started,810b8785,0.05555555555555555
3038,c6f8ff61a5fa87,0314c8d1,"<strong><span style=""color:Red;"">*Article By Los Alamos National Laboratory*</span></strong>
<strong><span style=""color:black;"">‘Fingerprint’ of fault displacement also forecasts magnitude of rupture</span></strong>
-----------------------------------------------------------------------

LOS ALAMOS, N.M., Dec. 17, 2018—Machine-learning research published in two related papers today in _Nature Geosciences_ reports the detection of seismic signals accurately predicting the Cascadia fault’s slow slippage, a type of failure observed to precede large earthquakes in other subduction zones.

* **Los Alamos National Laboratory researchers applied machine learning to analyze Cascadia data and discovered the megathrust broadcasts a constant tremor, a fingerprint of the fault’s displacement.** More importantly, they **found a direct parallel between the loudness of the fault’s acoustic signal and its physical changes**. Cascadia’s groans, previously discounted as meaningless noise, foretold its fragility.
* **“Cascadia’s behavior was buried in the data**. Until **machine learning revealed precise patterns**, we all discarded the **continuous signal as noise, but it was full of rich information.** We discovered a **highly predictable sound pattern that indicates slippage and fault failure**,” said Los Alamos scientist Paul Johnson. “We also found a precise link between the fragility of the fault and the signal’s strength, which can help us more accurately predict a megaquake.”  
* The **new papers** were authored by ***Johnson, Bertrand Rouet-Leduc and Claudia Hulbert*** from the ***Laboratory’s Earth and Environmental Sciences Division, Christopher Ren from the Laboratory’s Intelligence and Space Research Division and collaborators at Pennsylvania State University.***
* **Machine learning crunches massive seismic data sets to find distinct patterns by learning from self-adjusting algorithms to create decision trees that select and retest a series of questions and answers.** Last year, the team simulated an earthquake in a laboratory, using steel blocks interacting with rocks and pistons, and recorded sounds that they analyzed by machine learning. They discovered that the numerous seismic signals, previously discounted as meaningless noise, pinpointed when the simulated fault would slip, a major advance towards earthquake prediction. Faster, more powerful quakes had louder signals.
* The team decided to apply their new paradigm to the real world: Cascadia. Recent research reveals that Cascadia has been active, but noted activity has been seemingly random. This team analyzed 12 years of real data from seismic stations in the region and found similar signals and results: Cascadia’s constant tremors quantify the displacement of the slowly slipping portion of the subduction zone. In the laboratory, the authors identified a similar signal that accurately predicted a broad range of fault failure. Careful monitoring in Cascadia may provide new information on the locked zone to provide an early warning system.

The papers:
-------------------

*   [Similarity of fast and slow earthquakes illuminated by machine learning](https://www.nature.com/articles/s41561-018-0272-8 ""Machine learning""), Nature Geoscience, Dec. 17, 2018
*   [Continuous chatter of the Cascadia subduction zone revealed by machine learning,](https://www.nature.com/articles/s41561-018-0274-6) Nature Geoscience, Dec. 17, 2018

**REFERENCES:**  
https://www.lanl.gov/discover/news-release-archive/2018/December/1217-machine-learning.php",3eea586b,0.05555555555555555
3043,ac04ba639d1c93,77742c53,"<a id=""id1""></a> <br> 
# **1. Problem Definition:** 

This challenge aims to predict interactions between atoms. The main task is develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant)<br>

In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.

**Data**
* **train.csv** - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates, the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (**scalar_coupling_constant**) is the scalar coupling constant that we want to be able to predict
* **test.csv** - the test set; same info as train, without the target variable
* **sample_submission.csv** - a sample submission file in the correct format
* **structures.csv** - this file contains the same information as the individual xyz structure files, but in a single file

**Additional Data**<br>
*NOTE: additional data is provided for the molecules in Train only!*
* **scalar_coupling_contributions.csv** - The scalar coupling constants in train.csv are a sum of four terms. The first column (**molecule_name**) are the name of the molecule, the second (**atom_index_0**) and third column (**atom_index_1**) are the atom indices of the atom-pair, the fourth column indicates the **type** of coupling, the fifth column (**fc**) is the Fermi Contact contribution, the sixth column (**sd**) is the Spin-dipolar contribution, the seventh column (**pso**) is the Paramagnetic spin-orbit contribution and the eighth column (**dso**) is the Diamagnetic spin-orbit contribution.

",748059d5,0.05555555555555555
3044,d6cbd7160961dc,2bd81193,## 2.1. Benford's Law: Introduction,36d74664,0.05555555555555555
3046,c84925c8171900,ced753a0,"<p style=""text-indent: 25px;"">
    <span style='font-family:Georgia'>
        The global gaming market was valued at USD 151.55 billion in 2019 and is expected to reach a value of USD 256.97 billion by 2025, registering a CAGR of 9.17% over the forecast period (2020 – 2025). Game developers across emerging economies are continually striving to enhance gamer's experience, launching, and rewriting codes for diverse console/platforms, such as PlayStation, Xbox, and Windows PC, which are incorporated into one product provided to the gamers through the cloud platform. <br>
        The world is currently undergoing a harrowing and unprecedented event: the COVID-19 pandemic. During these trying times, gaming has become a means of escapism and time filling for many.As a result, one of this year’s growth drivers is an increased interest in gaming due to COVID-19-related lockdown measures. However, the launch of the next-generation consoles toward the end of the year is also a key contributing factor.
        </span>
</p>",e21ff7ec,0.056074766355140186
3047,312135b445bd23,491c1ea2,"# **Pros/Cons**

## Pros
1. Our solution finds information to all the research questions in accuracte, consice and informative manner.
2. It's simple to understand, read and reproduce results.
3. Easy to expand to other research questions and domains.

## Cons
1. The keyword generation for each task can be improved to better use the search engine for seed sentences.
2. There is some manual work needed to pick the best seed sentences for each sub-task in each task. In most cases, we took the first 1-3 sentences that the search engine returned, but it wasn't the case for all search queries. At this point, after that we marked for each query the best results out of 10, we can better improve the search engine against gold-data set with a defined evaluation metric for information retrieval tasks (e.g. [nDCG - Normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)) . We believe this can be further improved and might be an interesting direction for future research.",8ced381f,0.056179775280898875
3049,04ff2af52f147b,7729a91c,"**Investigate Assumptions:**

It is worth noting some assumptions based on the information we have.  We know that women and children had priority when evacuating.  Consequently, we expect women to be overrepresented in *Survived*.  We also know that class always comes into play when resources are limited (especially since first class accomodations were closest to lifeboats).  By the same logic, we expect passengers in class 1 will be more likely to survive than their class 2 and 3 counterparts.  We will test these assumptions noting that we have not yet filled in all *Age* values, so our results will not be completely accurate.",d5f37be9,0.056179775280898875
3052,d8ff894670d506,ba7f85d1,**Loading the Dataset**,eb0fb7de,0.056338028169014086
3058,fc8e0042411c46,0d7b210b,"The shape after running the drop duplicate command is same as the original dataframe.

Hence we can conclude that there were zero duplicate values in the dataset.",af476c2a,0.05642633228840126
3062,f015d0147e8fbf,39c2455e,"## My Journey Toward Participating in this Competition

I began this competition in early June of 2018, in order to complete my [final project](https://github.com/jamesdellinger/machine_learning_nanodegree_capstone_project) for Udacity's Machine Learning Engineer Nanodegree.

I spent about a month exploring the idiosyncracies of the main data table's (`application_{train|test}.csv`) 120 features, experimented with various single model predictors (Naive Bayes, Logistic Regression, AdaBoost, Multi-Layer Perceptron, and LightGBM), and alternately trained each of them on the table's full feature set, on just the top 30 features according to SelectKBest, and on a featureset where the dimensionality of the main table's numerical features was reduced using PCA. I compared the performance of each of these models using a simple 80%-20% train-validation set split. 

Ultimately, my LightGBM single model that was trained on all 120 main table features performed the best, with a local CV score (ROC AUC) of `0.76092`. This translated to a public leaderboard score of `0.74111`. 

At this point, I was pleased that I had put into practice several of the techniques and algorithms I had learned while completing the Machine Learning Engineer Nanodegree. However, I was by no means satisfied that I had done all I could do to build a kernel that was *competitive* on the Home Credit dataset. This spurred me to spend the remainder of the competition learning and applying new techniques and approaches.",518954fb,0.05660377358490566
3066,a070fd03ae8ed2,4b8c7d60,## 1.2. Глобальные константы,c0ec4138,0.05660377358490566
3067,23df07a474aaae,30474af6,"# Cleaning Data

As the dataset is small, Microsoft Excel was used to clean the data.
The Following steps were taken to clean the dataset.
1. Changing Datatypes of Data Columns
2. Removing values with more than 80% Null values
3. Removing Data Columns that are Irrelevant to the analysis.
",0ea40276,0.05660377358490566
3072,56785caebaa256,2fc9ee4a,Import libraries,a792961a,0.05673758865248227
3075,0a918602a04693,3ab14982,"As, we can see the data types in the columns are of int64, float64, object. The columns 'Unnamed: 0' and 'customerID' doesn't provide any signifiant help in prediction so we can drop those columns",c1ef0e95,0.056818181818181816
3076,d1ff7e10ee0102,aaaf1816,"# 1. So... What can we expect?

In order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.

In order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:
* <b>Variable</b> - Variable name.
* <b>Type</b> - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.
* <b>Segment</b> - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').
* <b>Expectation</b> - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.
* <b>Conclusion</b> - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.
* <b>Comments</b> - Any general comments that occured to us.

While 'Type' and 'Segment' is just for possible future reference, the column 'Expectation' is important because it will help us develop a 'sixth sense'. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:

* Do we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its 'Masonry veneer type'?).
* If so, how important would this variable be? (e.g. What is the impact of having 'Excellent' material on the exterior instead of 'Poor'? And of having 'Excellent' instead of 'Good'?).
* Is this information already described in any other variable? (e.g. If 'LandContour' gives the flatness of the property, do we really need to know the 'LandSlope'?).

After this daunting exercise, we can filter the spreadsheet and look carefully to the variables with 'High' 'Expectation'. Then, we can rush into some scatter plots between those variables and 'SalePrice', filling in the 'Conclusion' column which is just the correction of our expectations.

I went through this process and concluded that the following variables can play an important role in this problem:

* OverallQual (which is a variable that I don't like because I don't know how it was computed; a funny exercise would be to predict 'OverallQual' using all the other variables available).
* YearBuilt.
* TotalBsmtSF.
* GrLivArea.

I ended up with two 'building' variables ('OverallQual' and 'YearBuilt') and two 'space' variables ('TotalBsmtSF' and 'GrLivArea'). This might be a little bit unexpected as it goes against the real estate mantra that all that matters is 'location, location and location'. It is possible that this quick data examination process was a bit harsh for categorical variables. For example, I expected the 'Neigborhood' variable to be more relevant, but after the data examination I ended up excluding it. Maybe this is related to the use of scatter plots instead of boxplots, which are more suitable for categorical variables visualization. The way we visualize data often influences our conclusions.

However, the main point of this exercise was to think a little about our data and expectactions, so I think we achieved our goal. Now it's time for 'a little less conversation, a little more action please'. Let's <b>shake it!</b>",2cc71c3c,0.056818181818181816
3077,e19e307b3fd188,1f7d3988,#### Shape,2173955b,0.056910569105691054
3084,2730840089c8eb,e4fe2220,"Double quotes are convenient if your string contains a single quote character (e.g. representing an apostrophe).

Similarly, it's easy to create a string that contains double-quotes if you wrap it in single quotes:",34d27dac,0.05714285714285714
3086,7454fdc444df16,0428086e,## Exploring the data structure,a7818ef5,0.05714285714285714
3089,3cea0f929a2035,4983863a,"Below we can see the summary of descriptive statistics, data features and their types:",04cfbade,0.05714285714285714
3090,04bac111ffbe9c,5cccd2ee,"#### There are 4 categorical columns :
*  Survived (NOM)
*  PClass (ORD)
*  Sex (NOM)
*  Embarked (NOM)

#### There are 8 numerical columns :
*  PassengerId
*  Name
*  Age
*  SibSp'
*  Parch
*  Ticket
*  Fare
*  Cabin",82576b17,0.05714285714285714
3091,7a058705183598,ba328924,Summary of a DataFrame,b0ead917,0.05714285714285714
3095,fe6750354fb64f,671de0f5,# EDA,271741f0,0.05714285714285714
3096,f50dc95483c98f,7bb5217b,"### **Reading the Dataset by converting it into a readable Dataframe using Pandas**

pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.
For the source file of pandas you can go on the [`Github link`](https://github.com/pandas-dev/pandas)",cd9e9621,0.05714285714285714
3102,3a6274ed72cc00,98b37633,## <a id='1.'> 1. Importing Libraries</a> ,51369a2a,0.05714285714285714
3107,6b65d81a5743dd,f590a23c,# 2. Import Libraries and load data,4080a2d2,0.05714285714285714
3113,14defffcd250f3,8df5bdba,"
Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like ""Cabin Known: 1 or 0""

Let's continue on by visualizing some more of the data!",3a683b94,0.05747126436781609
3115,ee9ddc756b2d4a,68c080f7,"#### Problemi riscontrati nel lavoro:

Il dataset che ho trovato è molto rumoroso, ho fatto fatica ad analizzare i dati nella loro totalità.

Ho dovuto calcolare i features vectors tenendo separate le classi, per non perdere informazioni sull'ordine, importante perchè utilizzo due modelli diversi e devo metterne insieme i risultati.

Inoltre, come per la maggior parte dei dataset medici, il numero di immagini è molto limitato. Lavorare su un dataset piccolo non permette di ottenere risultati molto accurati. Una soluzione è data-augmentation: tuttavia, è un lavoro che va fatto con cura e con qualche competenza medica (fare il flip delle immagini, ad esempio, non è possibile, in quanto il cervello è per sua natura asimmetrico; anche altri tipi di deformazioni delle immagini vanno valutate e analizzate con attenzione, per evitare di creare immagini non reali sulle quali fare training, il che porterebbe a un peggioramento del risultato finale).",e367eab3,0.05747126436781609
3126,e9b9663777db82,5d487aa8,![2.png](attachment:2.png),648e8507,0.05785123966942149
3128,598b6228760590,6fab2e42,# EDA,be30ab66,0.057971014492753624
3131,17a24d566ffa59,25d7dc56,## Dimensionality Reduction and Semantic Transformations,89049e56,0.057971014492753624
3132,ea4e559a86d613,b654d21c, # Processing,eff47843,0.057971014492753624
3134,548f961125248d,f0a4f41b,### Functions,d8c5e8b8,0.057971014492753624
3138,b01ee6cb674fa3,f63bfca7,## Datum,a8ffd35e,0.057971014492753624
3148,c80939c7c626cf,a6c208e6,Trainig data has 891 rows and 12 columns,b9ac31e2,0.058394160583941604
3149,30fdc4a6e3c1db,6f395659,### 1.1 Sales Data,6111ddee,0.05847953216374269
3153,64169805aacf17,41e73f5a,## Check the GPU available to us,1f12ded0,0.058823529411764705
3155,7f74a04ae75792,a33dd217,"### What is the type of each column?
",d01e91da,0.058823529411764705
3164,fa02c409161192,8d36c548,### Importing libraries,e97077f7,0.058823529411764705
3165,395ed8e0b4fd17,2aad04d8,# <center>IMPORTS</center> ,7573ea31,0.058823529411764705
3166,a0a5baa6c7e12a,1ff57282,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Target Class Label Distribution</div>

One of the plots auto-generated by *AutoViz* displays the bar chats to visualize the distribution of the class labels of target variable.",551d41de,0.058823529411764705
3167,6aaee7fdbc7945,8013838b,"# **Team 8**
+ Trần Đức Thịnh - 18521450

+ Ngô Phan Phúc Nguyên -18521159

+ Nguyễn Tấn Phúc - 18521258

+ Đinh Thị Mỹ Hoàn - 18520771",dae653ae,0.058823529411764705
3172,c4386b8a01d66e,9583df06,## Null Values,dc732bf5,0.058823529411764705
3173,3d905ce4828057,55a94166,"***Now, let's apply what we have told on an example and interpret the results.***",5b006cc3,0.058823529411764705
3174,ab657da5329e3f,44f0cecf,"* How does **tf.distribute.MirroredStrategy** strategy work?

    - All the variables and the model graph is replicated on the replicas.
    - Input is evenly distributed across the replicas.
    - Each replica calculates the loss and gradients for the input it received.
    - The gradients are synced across all the replicas by summing them.
    - After the sync, the same update is made to the copies of the variables on each replica.",021526f8,0.058823529411764705
3175,e93a41c03638fe,c148324c,"The distribution of classes is optimum. No class imbalance found. If found, handle accordingly.",7363527b,0.058823529411764705
3176,a871419285588a,020da98d,"# Data visualization

Notebook to visualize:
- the repartition of classes
- the 2D slices with the tumor mask
- the tumors",5e08e15f,0.058823529411764705
3178,c65a65d4041018,e40d3da8,"As expected most of the responders are from USA, India and China.

USA is the country where Data Science is the most developed, India and China have a lot of aspiring DS.

My country (Russia) has the fourth place, Kaggle is becoming more and more popular there.",824fb229,0.058823529411764705
3184,6b7c80ed7bd03d,72c61b6d,"# 1. Preparation
Before converting the given DICOM dataset, we will prepare some of the python packages and define some of the python custom functions.  
Note that, DICOM is a format for storing medical data, and NumPy is a format for storing arrays in Python.

* DICOM: https://www.dicomstandard.org/
* Numpy: https://numpy.org/",7bba27db,0.058823529411764705
3185,7cfd96218dd933,33003c26,### DATA READING & CONTROL,7c34d96c,0.058823529411764705
3186,6cac6d4743088f,69fa3fb2,"## Questão 1
**Enunciado:**  Este notebook está associado ao *Kaggle Dataset* chamado ""Exercício 1"". Este *Kaggle Dataset* possui dois arquivos em formato CSV (anv.csv e BR_eleitorado_2016_municipio ). Escolha um dos datasets disponíveis e já conhecidos, a seu critério. Uma vez definido o csv, escolha no mínimo 7 e no máximo 12 variáveis (colunas) que você avalia como sendo relevantes. Para cada uma das suas variáveis escolhidas, forneça:


### Questão 1 - Item A - Classificação das variáveis

Classifique todas as variáveis escolhidas, e construa um dataframe com sua resposta.
Exemplo:",55fb02fa,0.058823529411764705
3188,513ce405d7f6a3,3958b294,# Data Analysis,8461e086,0.058823529411764705
3189,917957c6c4065f,21050094," ![image.png](attachment:image.png)  
description에 결측치가 있는 사례 중 하나로, descriptipn이 없는지 유튜브에서 확인해봤습니다.  
실제로 description에 내용이 없습니다. ",55b8ed68,0.058823529411764705
3190,99821bc6a45be6,450ba110,# Data Exploration,b9d59346,0.058823529411764705
3191,4ae6a182abac64,023806ec,* **Variables** ,418676c5,0.058823529411764705
3192,869a39a3d4dea2,b8ed0003,"## Setting up things here <a id=""setup""></a>",9020daf8,0.058823529411764705
3194,871901bb4aae21,ab059c58,# Import libraries and files,79ab34a9,0.058823529411764705
3196,55c34673c1f760,10c01e60,# Imports,2663c47f,0.058823529411764705
3203,52cfd66e9ec908,26332b59,# Get started with the data,c74adcdf,0.058823529411764705
3207,1294fb4c86f993,6c1f2fcf,"<a id='wrangling'></a>
## Data Wrangling<img src=""Images/wrangling.jpg"" align=""right"" width=""500"" height=""500"" />





",4471e513,0.059322033898305086
3208,9169c4e9c33c90,facd0e3c,Check for any immediate correlations between variables,725bf880,0.059322033898305086
3212,fc8e0042411c46,294fe1b7,## Data Inspection,af476c2a,0.05956112852664577
3216,ba655a261cc09e,204db2a8,## EDA,48cc549a,0.05970149253731343
3218,20b372b6e4e276,2d1a2d90,Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972,ec8b0860,0.05970149253731343
3221,b86bda7afe3ac3,83f1ff0c,Load data,16197934,0.05982905982905983
3222,2ada0305b68956,fec50a7c,### 7. Palette = 'BuGn',133e26f4,0.06
3223,b10bd75889dad9,1caa099d,## Data Cleaning,ee00ceee,0.06
3228,2bd6c370695ea7,c1788baa,## Load data,cbe6aec8,0.06
3231,83df814455f06c,1ca4a7d2,"### Decision-Tree terminology

![Decision-Tree terminology](https://gdcoder.com/content/images/2019/05/Screen-Shot-2019-05-18-at-03.40.41.png)",c9cff71a,0.06
3233,ba4b3bd184acbb,3b22762a,"#### List of Lists

When providing a list of lists, each internal list is interpreted as a row in the DataFrame.

It is important to note that all of the lists must be the same length for this to work.",0f5de724,0.06015037593984962
3237,f3c6048d1058e3,5db03d80,We can see that the dataset is perfectly balanced.,1d9056b0,0.0603448275862069
3238,5f32117bcd5255,d12dea96,### READING FITS FILE,85882abf,0.06040268456375839
3241,efd44ce2c08541,f4cf9811,# Load libraries,ebc2d00c,0.06060606060606061
3244,7c89a32e3562ca,0807fa0c,# Data Loading,32dd8913,0.06060606060606061
3245,f166950fa915f8,02aa2e1a,### Settings,a7f6ca5e,0.06060606060606061
3249,a2444ab5d5f147,adcfab9b,### Loading Data,10617755,0.06060606060606061
3250,adf419444a59df,2c207077,"## 1. Creating the Model 
We will create the original yolov1 model from scartch so if you are not familiar with the model you can check this image.
![Screenshot 2021-08-22 210816.png](attachment:05157939-7be4-4965-9327-5142a37c461e.png)",3a275e7f,0.06060606060606061
3252,b42180a6a5b42f,927d0405,"

## Metodologia
A metodologia aplicada irá verificar se há diferença entre as datas da notificação dos óbitos com a efetiva data do seu acontecimento. Pois os dados do site de campanha (brazil_covid19.csv), na sua coluna ""new_deaths"", apresenta os dados de óbitos por data de notificação, o que pode vir a ser diferente dos números de mortes de acordo com a data do óbito. 

Para isto, faremos uso de um outro dataset, este disponibilizado pelos cartórios espalhados pelo Brasil, (https://data.brasil.io/dataset/covid19/_meta/list.html), o qual trás em sua coluna ""new_deaths_covid19"" a quantidade de óbitos ocorrida na respectiva data, conforme documentação:

    new_deaths_covid19: Quantidade de óbitos em decorrência de suspeita ou confirmação de covid19 para o estado state ocorridos na data date (em 2020).


# Fontes <a id='fonte'></a>
* Raphael Fontes (https://www.kaggle.com/unanimad/corona-virus-brazil)

* Ministério da Saúde (https://covid.saude.gov.br/)

* Secretarias Estaduais de Saúde (https://brasil.io/dataset/covid19/caso_full/?page=2)",987cea5f,0.06060606060606061
3253,b241b847319d13,2a18d16e,# **Tensorflow Records**,0fb698f0,0.06060606060606061
3254,63b44c85e32c1f,fb0addec,"In python, Indexing starts from 0. Thus now the list x, which has two elements will have apple at 0 index and orange at 1 index.",fb9b9562,0.060810810810810814
3256,fd4017c1514157,fe94f827,# Data Description,fd8f0896,0.06097560975609756
3257,0b01138ad120fc,8ecb5fb9,"**As I made in the ANN Notebook, first I'll find some RNN work to take as a base, so I can learn in the process and apply it form myself later.**",0b4b72e6,0.06097560975609756
3259,ee23a565163388,4dbed80f,# **Dataset**,88aacbc4,0.061068702290076333
3260,396bc36edb95d3,74a055c5,#### Treating Bad Data,965e4f8f,0.06111111111111111
3261,ffd1df95ca5289,6ac39d82,All the percentiles are -1 and mean also seems to be less compare to max values ,db00c338,0.061224489795918366
3263,12f4d16fc21645,f00baaed,<h1 style='color:blue'>Import dataset</h1>,c7752038,0.061224489795918366
3266,f35bf4df70d310,4474c179,## 1. Loading the datasets,10bb859a,0.061224489795918366
3267,087e21401d7dfc,678a659f,"##### lets start few visualisation.
##### We are plotting histogram for height. We can see there are few params. Lets understand them.
<ul>
    <li>
kind: this mean what kind of plot you want e,g, hist, box and other kind. So in our case its hist for histogram. </li>
<li>title: The title of the plot. You can consider it as a name for your plot \n</li>
<li>color: you can choose any color. In my case 'c' stands for cyan</li>
    </ul>",42000489,0.061224489795918366
3268,5a04b504932f52,ab2afd24,# Read Data,39b55fdc,0.061224489795918366
3271,6cade0b6a41ba2,20ef6dff,# 2. Data Sourcing,e6110293,0.06140350877192982
3273,fe7360cddc13e5,bcd402ef,# Modelin Varsayımları,8979e423,0.06140350877192982
3276,f2f2db16a2f86c,347549da,"Here **median_house_value** is the dependent attribute and **longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households and median_income** are independent attributes.

**ocean_proximity** needs to be encoded to train the model which will be dealt with later.",ffc6a115,0.06153846153846154
3277,c115e287523aab,e6d3b1ee,"# Content:
* Install Libraries.
* Import Libraries.
* Libraries Version Check
* Wandb
* Configuration.
* Set Seed for Reproducibility.
* TPU Configs.
* GCS Path for TPU.
* Meta Data.
* Train-Test Distrubution
* EDA
    * Train.
    * Test.
* Data Split.
* Data Augmentation.
* Data Pipeline.
* Visualization.
* Loss Function.
* Build Model.
* Learning-Rate Scheduler.
* Grad-CAM Helper
* Wandb Logger
* Train Model
* Calculate OOF Scorej
* Pawpularity Distrubtion of Train & OOF



",feb1288b,0.06153846153846154
3281,03048e86a6d806,f67b67fd,"### Gender Representation

First, let's explore the gender composition of data-related employees.",1285c231,0.06153846153846154
3282,d07915a6e6992e,22671dfe," 
![Prob%20Ident.png](attachment:Prob%20Ident.png)
**Best Practice -** The most important part of any project is correct problem identification. Before you jump to ""How to do this"" part like typical Data Scientists, understand ""What/Why"" part.  
Understand the problem first and draft a rough strategy on a piece of paper to start with. Write down things like what are you expected to do & what data you might need or let's say what all algorithms you plan to use. 

Now the <a href=""https://www.kaggle.com/c/titanic/""> Titanic challenge</a>  hosted by Kaggle is a competition in which the goal is to predict the survival or the death of a given passenger based on a set of variables describing  age, sex, or passenger's class on the boat.

![](http://www.tyro.com/content/uploads/2016/04/blog-twenty-one-business-icebergs-sink-business-280416.jpg)

So it is a classification problem and you are expected to predict Survived as 1 and Died as 0.",2b912140,0.06153846153846154
3285,1645979263c148,69fcfed8,"## Data Description:

We use the following representation to collect the dataset

1. age - age
2. bp - blood pressure
3. sg - specific gravity
4. al - albumin
5. su - sugar
6. rbc - red blood cells
7. pc - pus cell
8. pcc - pus cell clumps
9. ba - bacteria
10. bgr - blood glucose random
11. bu - blood urea
12. sc - serum creatinine
13. sod - sodium
14. pot - potassium
15. hemo - hemoglobin
16. pcv - packed cell volume
17. wc - white blood cell count
18. rc - red blood cell count
19. htn - hypertension
20. dm - diabetes mellitus
21. cad - coronary artery disease
22. appet - appetite
23. pe - pedal edema
24. ane - anemia
25. class - class",fa11663e,0.06153846153846154
3287,738bfced935b69,1934283b,I concatenation the data set in two group first with 9 columns and the second with 7 columns: ,2d3c592d,0.06164383561643835
3290,faa8e6c8ab9246,1cee64e3,"To know the information of the dataset, we use info() function. It gives information about variables, number of non-null count, datatype of each columns and memory usage.",2bea1419,0.06172839506172839
3299,f5ca8fb6a465f3,51b59686,# Link to the [Training Notebook](https://www.kaggle.com/basu369victor/chest-x-ray-abnormality-detection-with-yolo-v3),56c45a1b,0.0625
3300,0635781991a885,f9a7aea9,**The Purpose of this Notebook is to use the data directly and train on it and do prediction. Usually if we have a mix of Categorical and Continuous Features we need to convert it to ONE HOT Encoding. But Boosting frameworks now provide a way to directly use Categorical Features. **,13ab3e33,0.0625
3302,254cccd5145725,e7317c4b,"<font size=""4"">The core Data Fields are as follows:
* Id - a unique identifier for each row.<br>
* Target - the target is an ordinal variable indicating groups of income levels.<br>
    * 1 = extreme poverty <br>
    * 2 = moderate poverty <br>
    * 3 = vulnerable households <br> 
    * 4 = non vulnerable households <br>
* idhogar - this is a unique identifier for each household. This can be used to create household-wide features, etc. All rows in a given household will have a matching value for this identifier.<br>
* parentesco1 - indicates if this person is the head of the household.<br>
* This data contains 142 total columns.<br>
    </font>",a49b4037,0.0625
3303,ff3a8ce61fab6a,37fd8a1b,"
# 1. tensorflow Overview
<br>

tensorflow is consist of two words ➡ tensor ➕ flow.<br><br>
**Tensor :** refer to a matrix of numbers consist of (nxn) dimensions.<br>
**Flow :** refer to graph that contain computational operations on tensors.<br>

<div>
    <img src='https://miro.medium.com/max/1594/1*EHugUlJHdFnUecICP8DM9Q.png' style=""float:left"">
</div>
<div style=""clear:both""></div>
<br><br>
As we said that **graph** contain all computational operations on tensors for this graph consist of two elements.<br><br>

**Edges :** refere to data flow or tensors. <br>
**Nodes :** refere to computational operations.<br><br>

<div>
    <img src='https://static.packt-cdn.com/products/9781786468574/graphics/image_01_006.jpg' width= 400 style=""float:left"">
</div>
<div style=""clear:both""></div>

<br>

There is important term called **Rank** that refere to dimensions of data into tensors. let's see 👇<br>

* Rank 0 : refer to tensor that contain singel value.
* Rank 1 : refer to tensor that contain a vector (1x1) dimension
* Rank 2 : refer to tensor that contain a 2D matrix
* Rank 3 : refer to tensor that contain a 3D matrix
* Rank n : refer to tensor that contain a nD matrix

<div>
    <img src='https://miro.medium.com/max/1000/0*jGB1CGQ9HdeUwlgB' style=""float:left"">
</div>
<div style=""clear:both""></div>

<hr>


# 2.  Session

<p>A session allows to execute graphs or part of graphs.<br>
It allocates resources for that and holds the actual values of intermediate results and variables.</p>

<div>
    <img src='https://www.easy-tensorflow.com/files/1_1.gif' style=""float:left"">
</div>
<div style=""clear:both""></div>

**So taht we can say that Session is a framework that combine edges and nodes.**
<br>

There is other image that show session content.

<div>
    <img src='https://miro.medium.com/max/2994/1*vPb9E0Yd1QUAD0oFmAgaOw.png' width= 500 style=""float:left"">
</div>

<div style=""clear:both""></div><hr>

",9afe1654,0.0625
3305,3dd4294f903768,4fc324ab,Let's take a look at the 2 first rows of our dataset:,0d89d098,0.0625
3308,5e02999ca74e7e,ea9cfa99,### **Setup Libraries**,b69da28e,0.0625
3309,204a60bace6fdb,7f9c9c5d,## Libraries,5cb3de8f,0.0625
3312,6a49325ea305e2,800f506a,# Load Libraries,119fdcf9,0.0625
3313,fae5023faa435f,1e7b9a3b,"# Time Series Forecasting
Time series data is recorded at regular time intervals, and the order of these data points is important. Therefore, any predictive model based on time series data will have time as an independent variable. The output of a model would be the predicted value or classification at a specific time. We use stock price data over a period of three years as a time series to predict the future price of those shares. 
",b37c893b,0.0625
3317,95656e8d666b16,208da08f,"Trying to predict the 'Species' based off the Weight, Length1, Length2, Length3, Height, and Width",65e88599,0.0625
3319,117fc0956643d0,3b6286cf,LDA+ALBERT.png![image.png](attachment:image.png),68cef9fd,0.0625
3322,fdbbd573ba31c2,2f5a56ed,### df_test,f7c28d74,0.0625
3324,3b5903412fe741,a9393f67,"## Naive accessors

Native Python objects provide many good ways of indexing data. `pandas` carries all of these over, which helps make it easy to start with.

Consider this `DataFrame`:",ad231969,0.0625
3326,96c4c0e36b8ec0,11864f3b,**Importing data**,4dd6de8c,0.0625
3328,64a336ac34d95c,99e8786d,"# Data Loading and Data Analysis
",be73a990,0.0625
3331,e82462cdc998a7,94b614f9,"## 1. Import libraries<a class=""anchor"" id=""1""></a>",b39bf244,0.0625
3333,2a724fb7835cdc,b8fe5d11,"For the examples which occur in both sets, we can directly use the labels from train set as our prediction. (* Is it cheating ??? *)",c38ac61d,0.0625
3336,4375764a7aa56c,1286cf2f,"# RANZCR Complete Modular PyTorch trainer 👾

This is the training pipeline that I've used in Cassava Leaf Disease Classification. Feel free to fork it, and make your changes!",6519c78d,0.0625
3338,c85c94076e9c3a,95e8ebf5,"### Let us now explore the dataset :
",3ea0c443,0.0625
3339,64a3009a7e3dcd,04126a6d,**CHEST X-RAY CLASSIFICATION**,b25f54ed,0.0625
3343,6f05f4ea9addbf,6273e034,"# understanding our dataset

The first step towards data analysis is to understand the data in hand. Its columns, max, min, count, null values, etc...",dfb04c84,0.0625
3344,5ffe6aa38958a1,c43c4a7d,"## 2.2 Import Data

",11f5412e,0.0625
3348,ff029d7b52ae1d,d45677bf,## Install Auto_ViML,c987f868,0.0625
3351,69130a37583a06,3fbcba2a,"### Data Overview :
",65a4de1c,0.0625
3352,bd0e173abb7b52,bb720f64,"**In this task you should use Pandas to answer a few questions about the [Adult](https://archive.ics.uci.edu/ml/datasets/Adult) dataset. (You don't have to download the data – it's already here). Choose the answers in the [web-form](https://docs.google.com/forms/d/1uY7MpI2trKx6FLWZte0uVh3ULV4Cm_tDud0VDFGCOKg). This is a demo version of an assignment, so by submitting the form, you'll see a link to the solution .ipynb file.**",9bce3b0d,0.0625
3355,9d561aa4a298f3,911c040a,"# Analysis preparation

## Load packages and data",f56bdd1c,0.0625
3362,169177b6e9edea,fba2acb7,"<p> Foram escolhidas os seguintes parâmetros para a construção dos modelos iniciais:
   <p> 'Pclass','Sex','Age','SibSp','Parch','Fare', 'Embarked'",ca42152f,0.06315789473684211
3364,08f845750d026a,8bf9ab58,### Which countries have the largest loan amount?,1c54de30,0.06329113924050633
3366,b10bd75889dad9,7bd01182,### Missing value treatment,ee00ceee,0.06333333333333334
3367,06ecf7a304c309,778490bc,"이 데이터를 표현하기 위해서는, 2개의 차원을 사용합니다. (X축과 Y축) 
하지만 다음과 같은 정보가 있다면, 이 데이터를 하나의 차원, 1D로 축소할 수 있습니다. 

- 기준점 : A
- 수평선과의 각도 : L

위에 정보만 가지고 있다면 직선 위의 점 B는 거리 D로 표현가능합니다.",714de627,0.06349206349206349
3369,4b4117cf42ef8d,c98b4f7c,# Data Overview,457cd6f4,0.06349206349206349
3375,04e6b0d3c70f46,8845992f,### Detect Hardware and accordingly set strategy,56344f77,0.06382978723404255
3376,73893f0467d5e3,d44c8ef1,## For mean_radius,279787c6,0.06382978723404255
3377,3f25b363afec54,ded9dd54,"### Data Description 

```train.zip``` contains 6 different csv files apart from the data dictionary as described below:

Health_Camp_Detail.csv – File containing Health_Camp_Id, Camp_Start_Date, Camp_End_Date and Category details of each camp.

```Train.csv``` – File containing registration details for all the test camps. This includes Patient_ID, Health_Camp_ID, Registration_Date and a few anonymized variables as on registration date.

```Patient_Profile.csv``` – This file contains Patient profile details like Patient_ID, Online_Follower, Social media details, Income, Education, Age, First_Interaction_Date, City_Type and Employer_Category

```First_Health_Camp_Attended.csv``` – This file contains details about people who attended health camp of first format. This includes Donation (amount) & Health_Score of the person.

```Second_Health_Camp_Attended.csv``` - This file contains details about people who attended health camp of second format. This includes Health_Score of the person.

```Third_Health_Camp_Attended.csv``` - This file contains details about people who attended health camp of third format. This includes Number_of_stall_visited & Last_Stall_Visited_Number.



### Test Data

```Test.csv``` – File containing registration details for all the camps done after 1st April 2006. This includes Patient_ID, Health_Camp_ID, Registration_Date and a few anonymized variables as on registration date. Participant should make predictions for these patient camp combinations



### sample_submission.csv

```Patient_ID```: Unique Identifier for each patient. This ID is not sequential in nature and can not be used in modeling

```Health_Camp_ID```: Unique Identifier for each camp. This ID is not sequential in nature and can not be used in modeling

```Outcome```: Predicted probability for having a favourable outcome depending on the format",bbdaae25,0.06382978723404255
3378,5f674175839b32,ad97b6df,Data preparation and Cleaning,53a2e343,0.06382978723404255
3379,b61ab8f81dc03d,256b780b,"<a id=""understand_data""></a>
# Understand the data
It is very important you understand well the data before starting any machine learning process if you want to getting better results.",64d05394,0.06382978723404255
3381,f6648e47713411,8970ab42,# 2. Chuẩn bị dữ liệu,f4af4d1c,0.06382978723404255
3385,d4c5aaa4b36810,502b90ee,"The development data can be read from csv files but I will read it from the sqlite database to demonstrate my SQL knowledge.  

The data base has various development indicators stored in the indicators table along with country codes,year and indicator code. The countries in the happiness data may have different names from those in indicator database so it makes sense to first find what countries in the happiness data set do not appear in the indicators data set. ",65441f28,0.0641025641025641
3387,bd380b97b5c894,a656e998,## patient_id,66f2562a,0.06422018348623854
3388,5ce12be6e7b90e,c8bb553d,"Once a variable has been declared, we can use its name to get its value:",c0ab62dd,0.06432748538011696
3392,7dec6bdea6d779,38621db2,"Updates to do to improve performances:
- take into account different image sizes
- perform data augmentation (but not all images can be augmented the same way. Ex: portrait cannot  be flipped vertically. Some abstract objects / representations could).",18be5949,0.06451612903225806
3393,0d9a2067267ba1,524f2979,### Data Analysis,abc194fb,0.06451612903225806
3401,9ceb7278784462,82c7d839,* We don't have duplicated data ,3768a567,0.06451612903225806
3407,c0ddb77bf32e2b,d06b41dd,"
If we sort the counts of NA in each columns, we can get a graph below. We can see there is a notable shrink from 'THC' to 'WS_HR', droping from over 0.5 to below 0.5 , after that, the NA ratio of each column becoms much more stable. Based on some empirical approachs, it's fine to just impute, fill NAs and use the columns after 'WS_HR'. But we will have a look of columns before that just in case we miss some useful information.

My strategy here is dealing with the column with most and least NAs first, and come back to decide to how to deal with the columns with about 0.5 NA ratio.",a0cb45f7,0.06451612903225806
3414,b05ee1ea1c8269,a1c57967,"On 26 November 2021, WHO designated the variant B.1.1.529 a variant of concern, named Omicron, on the advice of WHO’s Technical Advisory Group on Virus Evolution (TAG-VE).  This decision was based on the evidence presented to the TAG-VE that Omicron has several mutations that may have an impact on how it behaves, for example, on how easily it spreads or the severity of illness it causes. Here is a summary of what is currently known.  <br/>
https://www.who.int/news/item/28-11-2021-update-on-omicron",19e4d303,0.06451612903225806
3417,2f0f808765fc67,950cc09e,*ALL THE VARIABLES OR FEATURES ARE NUMERIC AND THE TARGET VARIABLE THAT WE HAVE TO PREDICT IS THE count VARIABLE. HENCE THIS IS A TYPICAL EXAMPLE OF A **REGRESSION PROBLEM** AS THE count VARIABLE IS CONTINUOUS VARIED.*,fd1f6494,0.06481481481481481
3419,ab6da5994949a3,32aa63b5,### Description of Dataset,fae6b91d,0.06481481481481481
3420,75adb7945ef9bd,217e655d,There are 52 duplicated rows. The duplicates will be removed.,785c5095,0.06493506493506493
3422,c13f73168789c2,0350533d,"## 1. Select rows and columns using labels<a id='2'></a>
You can select rows and columns in a Pandas DataFrame by using their corresponding labels.",16175052,0.06493506493506493
3425,90691864eb68c7,16438749,# 3. Exploratory Data Analysis,3555ef9b,0.06493506493506493
3426,2cb457b60dd246,5315fbc7,### Create Train and Val Sets,339367df,0.06493506493506493
3433,b01ee6cb674fa3,f83656b4,"Datetime of the launch, in the followin form
 - weekday
 - month
 - day number
 - year 
 - launch time",a8ffd35e,0.06521739130434782
3434,0e2a23fbe41ca9,9aa281f8,There are no duplicate ```card_id```'s in the train set.,64e4762c,0.06521739130434782
3436,9b5de3823ad5ab,1f229094,## Data analysis,33e48774,0.06521739130434782
3439,917957c6c4065f,fe9d90c9,#### video_id ,55b8ed68,0.06535947712418301
3440,c84925c8171900,73018cc9,"<a id=""datadesc""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            1.3 Data Description
            </span>   
        </font>    
</h3>",e21ff7ec,0.06542056074766354
3443,bcd7e398c4d0ec,864960cc,"##  Missing data review
There are only Pet's name of mising data. But we don't need Pet's name for feature extraction.",77a143f6,0.06557377049180328
3445,918040fad252ec,feffbd93,Deklarasi **PATH** gambar,966fcd8f,0.06557377049180328
3446,2105f2c5132866,0c763d99,"# For numeric data
* Made histograms to understand distributions
* Corrplot
* Pivot table comparing survival rate across numeric variables

# For Categorical Data
* Made bar charts to understand balance of classes
* Made pivot tables to understand relationship with survival",bfe8023d,0.06557377049180328
3447,0858e1bb3cbaca,053f4d39,"To see the dataset on python, we can use 

**.head(*n*)**

to display the top *n* rows of data, default *n*=5",78548374,0.06557377049180328
3449,3c2033cc99c12c,d88a5651,### Nan value processing ,dfa22a54,0.06569343065693431
3450,2ada0305b68956,b9f5c3f6,### 8. Palette = 'BuGn_r',133e26f4,0.06571428571428571
3451,52ee792e228d54,a21a9b35,### We can see from above that there are no null values in any of the features 😱. Another peculiarity of the dataset is all the columns are numerical(data is already encoded).,5096094e,0.06578947368421052
3453,5f4ae633cfd090,bee03cb5,"# 1. Creating a baseline model

 Since this is a classification task, the target value will be 'Winner'.",a30a16e2,0.06593406593406594
3454,43e60eb1362f5c,3d04aca2,The Data Contains 31 columns and 1048575 Rows,87934234,0.0660377358490566
3455,510b8303776bb6,078a63a6,# Data Preprocessing:,18080db8,0.0660377358490566
3456,2f47abddfd1928,7ee3c4fe,"To be able to analyse the data as a whole and also to deal with NaN values I join both datasets.

We need to record any identification to be able to separate later both datasets, in this case PassengerId will work.",ae33cc0b,0.06611570247933884
3457,e9b9663777db82,3ba72427,#### Price&m2 Analysis,648e8507,0.06611570247933884
3458,c65a65d4041018,38d43bae,"#### Dividing responders in groups by countries

I have decided to compare countries based on the information of responders living there. While all countries are unique, it would be difficult to analyze each and every country. As a result I have decided to take USA and India separately, because they have the largest number of responders, Russia, because I live there and other countries will be grouped in one category.",824fb229,0.0661764705882353
3460,c91c137284976f,7a9bba66,#### 1.1. Loading data,c6888c0a,0.06666666666666667
3469,3597174a998d4d,a2344ccd,"In this part, I'll analyze the state of resort hotel and city hotel from 3 ways:
* the total number of booking. 
* the sum of all lodging transactions. It can be calculated by the defination of adr variable.
* the ratio of booking's cancelation.",276892ed,0.06666666666666667
3473,c18267b203f28a,4baf3bd0,# Detect TPU,09ca8efb,0.06666666666666667
3474,6e472c6c591c7d,80bba9ed,"# Introduction

You've built up your SQL skills enough that the remaining hands-on exercises will use different datasets than you see in the explanations. If you need to get to know a new dataset, you can run a couple of **SELECT** queries to extract and review the data you need. 

The next exercises are also more challenging than what you've done so far. Don't worry, you are ready for it!

Run the code in the following cell to get everything set up:",65532a3d,0.06666666666666667
3476,bc058fe14d3d1b,3e0e1eb8,### 0.2 Loading Data,d0273670,0.06666666666666667
3478,d58491f2896fc1,0bb9d0f1,<h2> Genetik Algoritmada Temel Kavramlar </h2> ,514bfdff,0.06666666666666667
3480,4fd4b6a80d40e3,0d009f34,"### Expression 1

![image.png](attachment:image.png)",f6913cc3,0.06666666666666667
3482,63d0d9b9a8c7d2,373e0c07,# Understanding about The Dataset,e32e5933,0.06666666666666667
3487,cf4d1c1ad1476c,160cfd73,"![](https://lh3.googleusercontent.com/pacQdCJFoCq5ME7h2FfKCTmd6HwoEnq38PzZZFpAIfuSs5kvL05luyNJo4BWQxHXBy2ij006yo_JPk2UGiZhuskcQDxX7xIqzEAZt0lLC9Kb6QQfR0_8aajJLRffpST4fPWGhsag)
## Mixed precision floating point and bfloat16

The MXU computes matrix multiplications using bfloat16 inputs and float32 outputs. Intermediate accumulations are performed in float32 precision.

Neural network training is typically resistant to the noise introduced by a reduced floating point precision. There are cases where noise even helps the optimizer converge. 16-bit floating point precision has traditionally been used to accelerate computations but float16 and float32 formats have very different ranges. Reducing the precision from float32 to float16 usually results in over and underflows. Solutions exist but additional work is typically required to make float16 work.

That is why Google introduced the bfloat16 format in TPUs. bfloat16 is a truncated float32 with exactly the same exponent bits and range as float32. This, added to the fact that TPUs compute matrix multiplications in mixed precision with bfloat16 inputs but float32 outputs, means that, typically, no code changes are necessary to benefit from the performance gains of reduced precision.

    The use of bfloat16/float32 mixed precision is the default on TPUs. No code changes are necessary in your Tensorflow code to enable it

## Systolic arrays

CPUs are made to run pretty much any calculation. Therefore, CPU store values in registers and a program sends a set of instructions to the Arithmetic Logic Unit to read a given register, perform an operation and register the output into the right register. This comes at some cost in terms of power and chip area.

For an MXU, matrix multiplication reuses both inputs many times,
Under the hood: XLA

Tensorflow programs define computation graphs. The TPU does not directly run Python code, it runs the computation graph defined by your Tensorflow program. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the Tensorflow graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. The compilation happens automatically as work is sent to the TPU. You do not have to include XLA in your build chain explicitly.
## Using TPUs in Keras

TPUs are supported through the Keras API as of Tensorflow 2.1. Keras support works on TPUs and TPU pods.

Don't worry TPU is also supported in Pytorch, check out @abhishek, 4X Kaggle grandmaster's video on training BERT's in TPU

Do check out System Architecture of TPU gives more detials of TPU configurations and various versions of TPU

This video explains in detail about main differences between TPUv2 and TPUv3",768c1a59,0.06666666666666667
3498,9276fa5cc2fef6,59258292,"## Pause And Think! 

This is one of the stategies I adopt. I avoid looking into the head frame directly and instead focus on the competition problem. One of the reasons doing so is keeping my mind in a calm state where I could imagine what features I could think of or what insicere comments would read like?

![Ask](http://www.gregorypouy.com/wp-content/uploads/2015/12/what-how-why.jpg)



Few pointers I have is;
1. Are they toxic with targetted words on race/region/religion? 
2. Do they contain obscene words ? Are these questions long or short?
3. And, dividing them into clusters will help my model predict - what cluster of insincerity does an insincere question lies in .... etc

So, let's check the head frame now..
",24aa6a52,0.06666666666666667
3499,e0e19e91579432,eac96e3e,![title](https://images.deepai.org/glossary-terms/05c646fe1676490aa0b8cab0732a02b2/hyperparams.png),0c8a0755,0.06666666666666667
3504,b547f0f38f7744,c8680967,"### Read Data

Read the `train.csv` data:",b6ba66b3,0.06666666666666667
3510,f89f8540df580e,5427ef08,# Imports & Model downloading,83579ee7,0.06666666666666667
3511,4bada947d597ac,96e50d05,# Making training dataframe,eab5094a,0.06666666666666667
3512,e25c0f830df3f4,885008f2,# Importing comments data,fdcf7189,0.06666666666666667
3513,04bac111ffbe9c,fab9a7dd,Let's check for null values and data types.,82576b17,0.06666666666666667
3514,7454fdc444df16,75375563,We start by exploring the file structure to gain an understanding of how everything is organised,a7818ef5,0.06666666666666667
3517,7e1da639035ac5,9901417d,# <a id='3'>3. Data Impressions</a>,120b6c23,0.06666666666666667
3520,188731d7fa0604,172e0999,## 사용자 코딩,7cc543d3,0.06666666666666667
3521,5b92c712910a11,86e7230f,# Reading a zip file with help of Pandas,e1d17100,0.06666666666666667
3523,d905cde3391d2b,af8753e1,Let's inspect the `matches` data before stepping into the concepts,067dba39,0.06666666666666667
3524,07f5853e4db8f8,b3668059,"# District dataset 

District information data

| Name | Description |
| :--- | :----------- |
| district_id | The unique identifier of the school district |
| state | The state where the district resides in |
| locale | NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See [Locale Boundaries User's Manual](https://eric.ed.gov/?id=ED577162) for more information. |
| pct_black/hispanic | Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data |
| pct_free/reduced | Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data |
| county_connections_ratio | `ratio` (residential fixed high-speed connections over 200 kbps in at least one direction/households) based on the county level data from FCC From 477 (December 2018 version). See [FCC data](https://www.fcc.gov/form-477-county-data-internet-access-services) for more information. |
| pp_total_raw | Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district. |

",d13c2c32,0.06666666666666667
3527,6f8bd70d7430f6,88f3a54c,"Hello everyone, this is EDA a notebook of vaccination data, will be updated frequently",8bb65eb0,0.06666666666666667
3529,91eaec994e0c6f,3c1e812f,# 2. Exploratory Data Analysis,376aef10,0.06666666666666667
3531,70c1c70437ce86,a4cd7316,## EDA,e94552a4,0.06666666666666667
3534,712198370d5521,396b6882,"<a id=""2""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">LOADING DATA</p>",5882e04c,0.06666666666666667
3542,4ae6a182abac64,bce2b4a9,"-`PassengerId` is the unique id of the row and it doesn't have any effect on target

- `Name` 

- `Sex` 

- `Age`

-`Survived` is the target variable we are trying to predict (0 or 1):
     
      1 = Survived
      0 = Not Survived
-`Pclass` (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3):
      
      1 = Upper Class
      2 = Middle Class
      3 = Lower Class

-`SibSp` is the total number of the passengers' siblings and spouse.

-`Parch` is the total number of the passengers' parents and children.

-`Ticket` is the ticket number of the passenger.

-`Fare` is the passenger fare.

-`Cabin` is the cabin number of the passenger.

-`Embarked` is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):
      
     C = Cherbourg
     Q = Queenstown
     S = Southampton",418676c5,0.06722689075630252
3548,312135b445bd23,7902d19b,"# **Data Preprocessing**

## Article Filtering
In order to be focusing on relevant articles only, we filtered out articles that are not related specificaly to COVID-19 by using [@ajrwhite](https://www.kaggle.com/ajrwhite)'s list of keywords (thanks!) and took only articles that contains at least one word from the following list:",8ced381f,0.06741573033707865
3551,e4525eb0c96f28,0eea43e2,"#### Below we merge region_df into df.

We drop the extra created ""Developer_y"" column, and rename ""Developer_x"" back to ""Developer"".

Also, we decided to drop the regional sales columns: NA_Sales, PAL_Sales, JP_Sales, and Other_Sales. The reason for doing this is because the provided data is too inconsistent with how it stores sales data. We concluded that the data had too many conflicts when it came to storing data in terms of regional sales.



Therefore, regional sales data as a whole will not be evaluated in this tutorial. More information on the details of how sales data is stored in the dataset is addressed in the ""Data Cleaning/Exploratory Data Analysis"" section.",2093a1f1,0.06756756756756757
3555,62037c5832129c,790e1fbb,"* Important things during modeling is to make sure you follow the same steps for your training set and testing dataset. 
* We might be scaling the data, applying some transformation or do dimensionality reduction on training set. 
* Sci-kit learn Pipeline class is a handy tool that will help us create a Pipeline with multiple steps and allows us to follow the same steps on both the training and testing dataset.",61474350,0.06756756756756757
3561,a81661cc35d8d2,18b87a32,***,3331f113,0.06779661016949153
3564,1294fb4c86f993,381b0162,### General Properties,4471e513,0.06779661016949153
3568,a44368590e878a,0734c4e7,"## Patient

**Columns**

1. **patient_id** the ID of the patient
2. **global_num** the number given by KCDC
3. **sex** the sex of the patient
4. **birth_year** the birth year of the patient
5. **age** the age of the patient
6. **country** the country of the patient
7. **province** the province of the patient
8. **city** the city of the patient
9. **disease** TRUE: underlying disease / FALSE: no disease
10. **infection_case** the case of infection
11. **infection_order** the order of infection
12. **infected_by** the ID of who infected the patient
13. **contact_number** the number of contacts with people
14. **symptom_onset_date** the date of symptom onset
15. **confirmed_date** the date of being confirmed
16. **released_date** the date of being released
17. **deceased_date** the date of being deceased
18. **state** isolated / released / deceased",77743ba8,0.06779661016949153
3569,bb0905d33ae417,cf306829,"Following from the initial idea of showing the simplicity of using the `fastai` library, below is a code snippet containing 27 lines of code using default settings for a base model generation. Of these 27 lines, 10 lines are used to generate the submission file for the required format.
```
from fastai import *
from fastai.vision import *

data = ImageDataBunch.from_csv(path, folder = 'train', csv_labels = ""train_labels.csv"",
                               test = 'test',suffix="".tif"", size = 36, ds_tfms = get_transforms())
data.path = pathlib.Path('.')
data.normalize(imagenet_stats)

learn = create_cnn(data,resnet50,pretrained = True,metrics = accuracy)
learn.fit_one_cycle(5)

learn.unfreeze()
learn.lr_find()
learn.recorder.plot()
learn.fit_one_cycle(3,max_lr = slice(1e-6,3e-4))

interp = ClassificationInterpretation.from_learner(learn)
interp.plot_top_losses(9)
interp.plot_confusion_matrix()
preds,y = learn.TTA()
acc = accuracy(preds, y)
print('The validation accuracy is {} %.'.format(acc * 100))

def generateSubmission(learner):
    submissions = pd.read_csv('../input/sample_submission.csv')
    id_list = list(submissions.id)
    preds,y = learner.TTA(ds_type=DatasetType.Test)
    pred_list = list(preds[:,1])
    pred_dict = dict((key, value.item()) for (key, value) in zip(learner.data.test_ds.items,pred_list))
    pred_ordered = [pred_dict[Path('../input/test/' + id + '.tif')] for id in id_list]
    submissions = pd.DataFrame({'id':id_list,'label':pred_ordered})
    submissions.to_csv(""submission_{}.csv"".format(pred_score),index = False)
 
 generateSubmission(learn)
```",25fd1965,0.06779661016949153
3570,f2e5e9fb9eaaf7,dda8b250,"[back to top](#table-of-contents)
<a id=""3""></a>
# 3 Dataset Overview
The intend of the overview is to get a feel of the data and its structure in train, test and submission file. An overview on train and test datasets will include a quick analysis on missing values and basic statistics, while sample submission will be loaded to see the expected submission.

<a id=""3.1""></a>
## 3.1 Train dataset
As stated before, train dataset is mainly used to train predictive model as there is an available target variable in this set. This dataset is also used to explore more on the data itself including find a relation between each predictors and the target variable.

**Observations:**
- `claim` column is the target variable which is only available in the `train` dataset.
- There are `120` columns: `118` features, `1` target variable `claim` and `1` column of `id`.
- `train` dataset contain `957,919` observation with `1,820,782` missing values which need to be treated carefully.


### 3.1.1 Quick view
Below is the first 5 rows of train dataset:",048e0d08,0.06779661016949153
3573,98a6794067932a,90228221,"Finalement, cette dernière cellule de code pour la section d'initialisation du document permet de créer une copie de la base de données maîtresse afin d'être certain de ne pas modifier celle-ci de manière permanente en cas d'erreur. La base de données copiée se nommant my_data1 sera donc utilisée pour la suite afin d'effectuer les analyses. ",08600fe2,0.06796116504854369
3575,18ce858f90966d,b8374f6f,# **Data Exploration**,09e9caed,0.06818181818181818
3577,d1ff7e10ee0102,82b712c7,"# 2. First things first: analysing 'SalePrice'

'SalePrice' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)

Using the women analogy, let's build a little story, the story of 'How we met 'SalePrice''.

*Everything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That's a sign that she's there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:*

*'Hi, I'm Kaggly! And you? 'SalePrice'? What a beautiful name! You know 'SalePrice', could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I'd like to apply it to us!'*",2cc71c3c,0.06818181818181818
3578,a35cdce61f4059,00f18e98,* **EXPLORING AND CLEANING DATA**,acc8eab6,0.06818181818181818
3583,da199f8fb59439,39c17485,"### **About NETFLIX**
> ****Netflix has been leading the way for digital content since 1997****


Netflix is the world's leading streaming entertainment service with 193 million paid memberships in over 190 countries enjoying TV series, documentaries and feature films across a wide variety of genres and languages. Members can watch as much as they want, anytime, anywhere, on any internet-connected screen. Members can play, pause and resume watching, all without commercials or commitments.",baaa665d,0.06818181818181818
3586,be2f4d8a6b73ca,d6e6c246,"<div style=""color:white;
           display:fill;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana;
           letter-spacing:0.5px"">

<p style=""padding: 25px; color:white; text-align:center""><b>Importing Libraries</b></p>
</div>",5d8ce40a,0.06818181818181818
3588,6d66ced0028dea,9952dcfa,"Переобучение

1. Большая разница между R2 на train и кросс-валидации
2. Большая разница между R2 на кросс-валидации и Public Leaderbord",f50aae52,0.06818181818181818
3594,1eb62c5782f2d7,c6cfa43e,## 1. Area di sebelah kiri point z-score,bb69f147,0.0684931506849315
3600,9ceb7278784462,25b27551,"# <a id='4'> 3.1 Missing Value</a>
* Missing data, also known as missing values, is where some of the observations in a data set are blank <br>
* The concept of missing values is important to understand in order to successfully manage data.  If the missing values are not handled properly by the researcher, then he/she may end up drawing an inaccurate inference about the data.  Due to improper handling, the result obtained by the researcher will differ from ones where the missing values are present.<br>

## **The Missing Data Mechanisms**
### **Missing Completely at Random(MCAR)**
* Means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others.

### **Missing at Random (MAR)**
* Means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.

* Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual’s observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR.

### **Missing Not at Random(MNAR)**

* Means there is a relationship between the propensity of a value to be missing and its values. This is a case where the people with the lowest education are missing on education or the sickest people are most likely to drop out of the study.

* MNAR is called “non-ignorable” because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are.
",3768a567,0.06854838709677419
3605,52cfd66e9ec908,76446403,"Wait! Before we directly get into the fancy visualization and all it entails, why not watch a short YouTube video to enlighten us on the subject of operating an autonomous vehicle?",c74adcdf,0.06862745098039216
3606,842547b2def18c,2c22833f,"> ***どの特徴量がカテゴリカル変数か?***

これらの変数はサンプル全体を似たようなサンプル集合に分類します．
カテゴリカル変数において，それぞれの特徴量は，名義・順序・比例・間隔のうちどの尺度に基づいているでしょうか？
これは我々が可視化において適切なプロットを選択することに役立ちます．

- カテゴリカル変数: Survived, Sex, and Embarked. Ordinal: Pclass.

> ***どの特徴量が数値変数か?***

どの特徴量が数値的でしょうか?数値変数の値はサンプル間で異なります．数値変数において，それぞれの特徴量は離散・連続・時系列のうちどれに基づいているでしょうか？
これは我々が可視化において適切なプロットを選択することに役立ちます．

- 連続変数: Age, Fare. Discrete: SibSp, Parch.",b8efde6d,0.06862745098039216
3608,ee23a565163388,cf5bc321,"The dataset we have consist of different attributes of the heart patients as follows.
- Age (Age of the patient)
- Creatinine (Level of the CPK enzyme in the blood (mcg/L))
- Aneamia (Decrease of red blood cells or hemoglobin)
- Diabetes (If the patient has diabetes)
- Ejection Fraction (Percentage of blood leaving the heart at each contraction (percentage))
- Hypertension (If the patient has hypertension)
- Platelets (Platelets in the blood (kiloplatelets/mL))
- Serum Creatinine (Level of serum creatinine in the blood (mg/dL))
- Serum Sodium (Level of serum sodium in the blood (mEq/L))
- Sex (Gender of the patient)
- Smoking (If the patient smokes or not (boolean))
- Time (Follow-up period (days))
- Death Event (If the patient deceased during the follow-up period)",88aacbc4,0.06870229007633588
3610,b01ee6cb674fa3,62d848ec,## Detail,a8ffd35e,0.06884057971014493
3612,858da4bb312f67,e50aa31e,## Import Libraries,9cca4391,0.06896551724137931
3613,6903d3f38c6a66,69f3a2bf,"#will put table of content later
",6067ce5e,0.06896551724137931
3616,c18c37441caa8d,1e03eed1,> **Preprocessiong - Working with data **,ca98414d,0.06896551724137931
3617,9535bb04ae042c,8c59106c,## i) Calling Necessary Dependencies,165b6fae,0.06896551724137931
3618,6b54e39f86bdb5,9a4f843f,"## Loading the data

I start by loading the data onto the notebook by using Pandas as the format is CSV.",198084bc,0.06896551724137931
3622,d5f78aa381f58d,5f94b769,# Data Exploration,d60f358f,0.06896551724137931
3623,eb800c50fcfbb2,3116abbe,"If the dissimilarity is high it means, that particular geographical region is flooded. Low value of dissimilarity means, that region is less affected by flood.

This approach to figure out flooded regions can be applied to all other natural disaster as well, since the main idea here is to measure the changes of a particular region before and during the disaster.",e7173f4d,0.06896551724137931
3637,434f930cb58aee,56b6133c,Now we will import some of the common useful libraries along with tensorflow.,0e1d3554,0.06896551724137931
3641,d42518f6cb0995,04b39fc9,"### Training data is in the competition dataset as usual
It's larger than will fit in memory with default settings, so we'll specify more efficient datatypes and only load a subset of the data for now.",26913a9b,0.06896551724137931
3644,6a1d04e8153df3,3458b842,"**1. Read the data**

I am using pandas to read the Habermans survival data set.
",38572b05,0.06896551724137931
3645,f3c6048d1058e3,8cea58b4,### Change Target variable,1d9056b0,0.06896551724137931
3646,84127ade6fde87,1e87931a,# Representing text,f55d05b6,0.06896551724137931
3651,00d295edcd117e,7eef4de5,"torch创建了一个torchvision，它包含了一些处理基本图像数据集的方法。这些数据集
包括Imagenet、CIFAR10、MNIST等。除了数据加载以外，torchvision还包含了图像转换器，
torchvision.datasets和torchvision.utils，torch.DataLoader。",f5810f4b,0.06896551724137931
3655,09751c520b0616,07dc80a1, - Concat train and test dataset for futher preprocessing togather ,a4d0c7e9,0.06923076923076923
3656,be9597c72542a2,a0385bb0,"**Explore each dataset - columns, counts, basic stats**",6f29c6d8,0.06923076923076923
3657,d07915a6e6992e,84581c07,# What data do we have?,2b912140,0.06923076923076923
3660,166a62ebb4fc3a,5ceece39,Let us load the data set,db48a079,0.06944444444444445
3661,1014e6be391084,d3989187,Data desciption,46f9168f,0.06944444444444445
3664,593d1d3d1df05a,1737291a,# Convert Image to Grayscale,bc682ffe,0.06944444444444445
3671,806ce45c8fa303,11816156,## Loading Dataset,3e5c34dc,0.06976744186046512
3679,83df814455f06c,0a54b28f,"# **4. Decision Tree algorithm intuition** <a class=""anchor"" id=""4""></a>

[Table of Contents](#0.1)

The Decision-Tree algorithm is one of the most frequently and widely used supervised machine learning algorithms that can be used for both classification and regression tasks. The intuition behind the Decision-Tree algorithm is very simple to understand.


The Decision Tree algorithm intuition is as follows:-


1.	For each attribute in the dataset, the Decision-Tree algorithm forms a node. The most important attribute is placed at the root node. 

2.	For evaluating the task in hand, we start at the root node and we work our way down the tree by following the corresponding node that meets our condition or decision.

3.	This process continues until a leaf node is reached. It contains the prediction or the outcome of the Decision Tree.
",c9cff71a,0.07
3683,c3498779cda661,c93405b3,# Importar Datos,0f531b65,0.07017543859649122
3684,3fb15e6e48aec2,f977a436,"# EDA
* Merge data to process (seperate before fitting) df's last passengerId = 891",9d1f4358,0.07017543859649122
3688,fe7360cddc13e5,5d5cea9e,Model belirli varsayımlar altında çalışır ve bu varsayımlardaki dağılımlara göre hesaplamalar gerçekleştirilir.,8979e423,0.07017543859649122
3692,9f3710be6aea65,c4694bbf,"Pclass: socio-economic status 1st = Upper 2nd=Middle 3rd=Lower 

SibSp: The dataset defines family relations # of siblings / spouses aboard the Titanic

Parch: # of parents / children aboard the Titanic


Spouse = husband, wife (mistresses and fiancés were ignored)

parch: The dataset defines family relations in this way...
Some children travelled only with a nanny, therefore parch=0 for them.


survival	Survival	0 = No, 1 = Yes
pclass	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd

embarked	Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton

Cabin: cabin number",ae9bda88,0.07017543859649122
3694,c2a9f2fb3e1594,3eafc75c,"<a id=""ch4""></a>
# Step 3: Prepare Data for Consumption
Since step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.

## 3.1 Import Libraries
The following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. ",53411c04,0.07017543859649122
3696,0932046e1f485d,9a3e8f84,Generation of the mask used for the word cloud,218cc7a3,0.0703125
3706,dbd96dd275dc60,7a892208,"### Parsing Dates
When we work with timeseries data, we want to enrich the time and Date coponent
as much a possible

We can do that by telling pandas which of our columns has dates in it using 'pars date' parameter",1ed493a8,0.0707070707070707
3710,ce9ed5e2d601d7,b4a0d938,"# Feature Engineering
These features are borrowed from https://www.kaggle.com/gulshanmishra/tps-dec-21-tensorflow-nn-feature-engineering
Do read dataset description from https://www.kaggle.com/c/forest-cover-type-prediction/data
The log transform is a powerful tool for dealing with positive numbers with a heavy-
tailed  distribution.  (A  heavy-tailed  distribution  places  more  probability  mass  in  the
tail range than a Gaussian distribution.) It compresses the long tail in the high end of
the  distribution  into  a  shorter  tail,  and  expands  the  low  end  into  a  longer  head.",f58a2f43,0.07086614173228346
3712,b61ab8f81dc03d,159be11d,"<a id=""data_dictionary""></a>
## Data Dictionary
![image.png](attachment:image.png)",64d05394,0.07092198581560284
3713,56785caebaa256,34194ae5,"## 2. Download data<a class=""anchor"" id=""2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.07092198581560284
3718,a758983a68c014,765e2e1d,### Import libraries,ab89f181,0.07142857142857142
3720,f13534449a3750,96b9f27e,"<a id=""section-one""></a>
## Loading libraries and functions

In this section we will load the libraries and the function that will be used in the notebook.",8b7f3332,0.07142857142857142
3721,087e21401d7dfc,86c79270,# Visualization,42000489,0.07142857142857142
3728,20c9a2456e494a,30715a60,"# Introduction
In this notebook, I used the CNN model from Keras to train it over the CIFAR10 dataset.",3e487f55,0.07142857142857142
3729,6a80f915608fc2,96a09312,"## <a id=""DataProcessing"">Reading and Processing csv Data Files</a>
Back to <a href=""#Index"">Index</a>",636938eb,0.07142857142857142
3730,8dd655515e7d18,93ec3668,## Data Exploration,895f41cf,0.07142857142857142
3731,1fac5edd4063ba,85934b8f,![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRzpEvWCs3JK5Hw0679KOzNx2JBu9V_IAlq5w&usqp=CAU)slideshare.net,04bc01e0,0.07142857142857142
3737,92e9fc3a0ff5c0,a6ae2ebe,## **Reading CSV Files**,d53da425,0.07142857142857142
3738,e78e7edae89049,107d3114,# Data Analysis,9cef1d94,0.07142857142857142
3743,e8750f24c66614,77d69ae6,"this notebook is copy and edit of DmitryS who have very good approch please upvote him i public this notebook because it shows little more good accuracy if you like please upvote below mentioned link
https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2",bdebc727,0.07142857142857142
3745,7686f42e1f28d2,7c53c871,"# Loading the Data
We will not use `PP_recipies.csv` and `PP_users.csv` but instead will vectorize the data ourselves. The index columns of this dataset is kind of confusing since there are two columns (`u` and `user_id`) for user ids and two columns (`i` and `recipe_id`) for recipe ids.",6c128859,0.07142857142857142
3751,e16860fce156b0,6068d16a,"#<font color=""#EC7063"">About dataprepare.eda, by Slavvy Coelho and Ruchita Rozario
  </font>
  
Authors: Slavvy Coelho, Ruchita Rozario.  April 14, 2020.

Mentor: Dr. Jiannan Wang, Director, SFU’s Professional Master’s Programs (Big Data and Cybersecurity and Visual Computing)

Dataprepare is an initiative by SFU Data Science Research Group to speed up Data Science. Dataprep.eda attempts to simplify the entire EDA process with very minimal lines of code.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.07142857142857142
3755,1c381451c17150,62c4e249,"# Changing Database into Text
The first thing that needs to be done is change the Monty Python database into one nice long string of all the scripts. Originally, I tried putting both the directions and dialogue into the text. However, since there are so many directions this ends up making a script that is mostly stuff like ""cut to a picture of a man in the street."" or ""cut to stock video of a train"" ect. We will just focus on the dialogue to make something more interesting to read. ",e79b530f,0.07142857142857142
3757,67efe818cb2372,e4782f35,"**Prepare the training data**
1. Read the csv file into a ndarray.
2. Separate the labels from the vectors of pixel data.
3. Normalize pixel values from {0,255} to {0,1}
4.  Reshape the input vectors from rows (elements) of 784 values to tensors 28x28 values with 1 channel (greyscale)
5. Recode the labels to a 10 element vector (corresponding to the 10 output neurons we will have in the output layer - one hot encoding) 
6. Separate (randomly) the training tensors into training and validation sets (9:1 split)
",f28a2a34,0.07142857142857142
3758,6e9b4020644836,abd30f0e," <a id=""1""></a>
# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Importing Libraries</p>",5ad41fc6,0.07142857142857142
3760,c54ea4523bd49c,0367c438,Here I write the function for loading and preprocessing the image data. There's only one target category in this dataset so I show the first 8 images. ,097ccba2,0.07142857142857142
3764,1084376bc4897c,700bf1b5,# 2. Take a look at the data set,1b598487,0.07142857142857142
3765,38b79494ac749e,7e3ef14d,## Settings,39162a40,0.07142857142857142
3768,31268b33de97b5,4920d5bb,# Some insights of Data,1e6f7d14,0.07142857142857142
3769,b4ecd6e4277e3c,e2637b9e,### Basic Parameters,94d79d5f,0.07142857142857142
3771,400bbcc496138f,47c8a507,"
## Load Data",191b86b8,0.07142857142857142
3773,b7298d6aaff625,f2a148fe,### Importing Important Libraries ,bdf24bf7,0.07142857142857142
3775,b8849a04581d32,3da5b9bd,"<center>
<img src=""https://i.gifer.com/8Zr9.gif"" alt=""drawing""/>
<img src=""https://static.www.nfl.com/image/upload/v1554321393/league/nvfr7ogywskqrfaiu38m.svg"" alt=""drawing"" width=""320""/>
</center>",b8a568cd,0.07142857142857142
3776,f4514ec092a771,4798f614,### Training and validation data,3739ab1e,0.07142857142857142
3779,2ada0305b68956,1d1c3dbc,### 9. Palette = 'BuPu',133e26f4,0.07142857142857142
3781,be357c1e2c975d,6acbdbf3,### Task 1: Import Libraries,2486a061,0.07142857142857142
3789,b211c8c107f56d,2b03ebd2,"# Contents
1. [Start h2o and load the data](#step1)
2. [Define a grid and train](#step2)
3. [Best model](#step3)
4. [Submission](#step4)
",805b90f3,0.07142857142857142
3790,565ad413cd802f,9b556412,"Similarly, `submission.csv` contains image IDs for test data. However, since the goal of this competition is to make predictions for the test set, the `submission.csv` files contains **dummy labels**, which you need to replace with your predictions and sumbit to the competition on the ""Submission"" tab. In other words, we don't have the labels for the test set.",397b074e,0.07142857142857142
3791,2d40f383473fa4,1e92ce64,# 3. Exploratory Data Analysis (EDA),1da1eff0,0.07142857142857142
3792,ffdb3fe29f4755,36737a72,"# Part 0
**Data reading**",019b2e69,0.07142857142857142
3796,fc8e0042411c46,d97d8c62,## Data Cleaning,af476c2a,0.07210031347962383
3797,8ec771f5600a61,dccdde1b,# EDA,48364c1f,0.07216494845360824
3799,2a123b4e8f9433,8e36d6c1,# Convert alpha data to numeric data,0a082218,0.07216494845360824
3802,4daf6153275cbf,2aff7755,### Data Preparation,51db1961,0.07228915662650602
3805,8336d84cf3ff6b,364b55a7,# Explore the Values of various independent columns ,b96b58a0,0.07246376811594203
3807,9d9da6c439b96b,19941434,### Describe data,361cc7d9,0.07246376811594203
3809,17a24d566ffa59,9b8b31a6,"### Principal Component Analysis (PCA)

The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.

- Principal Component Analysis (PCA) is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.
- Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components. 
- PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance
- The first principal component accounts for as much of the variability in the data as possible, and each succeeding component accounts for as much of the remaining variability as possible.

SOURCE: 
- http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html
- ftp://statgen.ncsu.edu/pub/thorne/molevoclass/AtchleyOct19.pdf",89049e56,0.07246376811594203
3812,7e275c8d5ff2a0,ff1145f4,# LOADING THE DATA,b3afcc98,0.07246376811594203
3813,7e89d387feb9f5,51e31b65,"## Инсайт!!!
Из анализа количества уникальных значений можно сделать следующие выводы:
- Поле Restaurant_id является чем угодно, но только не уникальным идентификатором ресторана. Ориентироваться на эти данные не стоит. 
- На роль уникальных идентификаторов скорее могут претендовать в равной степени как URL_TA, так и ID_TA. Но и тут наблюдаются дубликаты. Посмотрим на них внимательней: ",989e3a1b,0.07246376811594203
3822,c80939c7c626cf,06f2119e,Test data has 418 rows and 11 columns,b9ac31e2,0.072992700729927
3823,e19e307b3fd188,ff4be805,#### Basic info,2173955b,0.07317073170731707
3826,fd4017c1514157,cb87d9a9,"
#### **train_short_audio -**
The bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org.

#### **train_soundscapes -** 
Audio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ogg format. The test set also has soundscapes from the two recording locations represented here.

#### **test_soundscapes -**
When you submit a notebook, the test_soundscapes directory will be populated with approximately 80 recordings to be used for scoring. These will be roughly 10 minutes long and in ogg audio format. The file names include the date the recording was taken, which can be especially useful for identifying migratory birds.

This folder also contains text files with the name and approximate coordinates of the recording location plus a csv with the set of dates the test set soundscapes were recorded.

#### **test.csv -** Only the first three rows are available for download; the full test.csv is in the hidden test set.

**row_id:** ID code for the row.

**site:** Site ID.

**seconds:** the second ending the time window

**audio_id:** ID code for the audio file.

#### **train_metadata.csv -** A wide range of metadata is provided for the training data. The most directly relevant fields are:

**primary_label:** a code for the bird species. You can review detailed information about the bird codes by appending the code to https://ebird.org/species/, such as https://ebird.org/species/amecro for the American Crow.

**recodist:** the user who provided the recording.

**latitude & longitude:** coordinates for where the recording was taken. Some bird species may have local call 'dialects,' so you may want to seek geographic diversity in your training data.

**date:** while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.

**filename:** the name of the associated audio file.

#### **train_soundscape_labels.csv -**

**row_id:** ID code for the row.

**site:** Site ID.

**seconds:** the second ending the time window

**audio_id:** ID code for the audio file.

**birds:** space delimited list of any bird songs present in the 5 second window. The label nocall means that no call occurred.

#### **sample_submission.csv -** A properly formed sample submission file. Only the first three rows are public, the remainder will be provided to your notebook as part of the hidden test set.

**row_id**

**birds:** space delimited list of any bird songs present in the 5 second window. If there are no bird calls, use the label nocall.
",fd8f0896,0.07317073170731707
3829,514d8de15cb7ef,5f6c33cf,"<p> For information related to the basics of dataset and the data analysis part of it refer to this link  
    <a href = ""https://www.kaggle.com/bhaargavi/wine-classification-analysis-of-data""> <h3>Link to Wine Classification Analysis of Data </h3> </a>  </p> 
<p> For information related to the word cloud formations and various basic concepts of NLP  refer to this link  
   <a href = ""https://www.kaggle.com/bhaargavi/wine-review-classification-making-word-clouds ""> <h3>Link to Wine Review Classification -- Making Word Clouds</h3> </a> </p>    ",cfe111b2,0.07317073170731707
3833,0b01138ad120fc,1f31c05d,"**NOTES:**  
   - **I'll use [This Work](https://www.datatechnotes.com/2018/12/rnn-example-with-keras-simplernn-in.html) from datatechnotes to learn how to code an RNN**
       - **All merits of this first work must be given to the author.**
   - **After this, to apply what I've learned, I'll try to make on my own a RNN predict the next dinner. This is an example that I saw when I was learning about RNNs.**
   - **Last, I'll try to make on my own a _BTC Price Predict_ applying LSTMs.**",0b4b72e6,0.07317073170731707
3835,47b2c9be5e31cb,dc9f2541,"There are 10 csv files in the current version of the dataset:
",7d4afe56,0.07317073170731707
3837,8cefb86a675e5d,3ea0c067,# Reading data from the Database into Pandas.,79f9e69b,0.07317073170731707
3840,91eaec994e0c6f,c646841b,"<i>`sales_train_validation` Dataset is our train data set: [D1 - D1913].</i> <br>
<i>`sales_train_evalutaion` Dataset is data used to evaluate our models, it contains [D1914 - D1941].</i>",376aef10,0.07333333333333333
3843,3d905ce4828057,6098ba01,# **CLTV Estimation with BGNBD & GG and Sending Results to Remote Server**,5b006cc3,0.07352941176470588
3844,7f74a04ae75792,9669fc7e,"### Why columns such as `Cust_Last_Purchase` are `object` while they should be `float64`?

#### Notice Some features have Dollar AND Comma. Remove the dollar sign and comma from these features

Can use `df.col=df.col.str.replace('OldSign', 'NewSign')`",d01e91da,0.07352941176470588
3846,e4c6dd957eb5ce,1854b6e8,## Util Functions,2e383665,0.07352941176470588
3848,eb0ecd6bebeb15,7923fdfd,Veri çerçevesinin ilk 5 gözlemini görüntüleyelim.,d7b93a60,0.07352941176470588
3852,9cec5ddf8b6f49,205157cb,#### Some Functions the we would use,d39fc8e7,0.07352941176470588
3854,840534f2908a9c,2bfcc308,**Compare training data and testing data**,8081c3cc,0.07368421052631578
3855,169177b6e9edea,a4e2a3b4,"<p>Separar o conjunto de dados em treino e validação para avaliarmos os modelos.<p>
<p>Treinar no conjunto de treinamento e avaliar no conjunto de validação usando 'accuracy'<p>",ca42152f,0.07368421052631578
3857,726833f92fb87a,979c23f9,## Custom functions definition,7dc5e1b6,0.0738255033557047
3859,c968dbd8d49ae6,250a2890,# **First create the functions to mapp all the categorical variables**,dfb2684d,0.07407407407407407
3861,ac04ba639d1c93,007e50ad,"<a id=""id2""></a> <br> 
# **2. Get the Data (Collect / Obtain):** ",748059d5,0.07407407407407407
3863,db5a369894fef6,594e755b,"# matplotlib
Time series data is easily plottled using `matplotlib`
- `time_series_covid_19_confirmed.csv`, `time_series_covid_19_deaths.csv` and `time_series_covid_19_recovered.csv`. 
- Also calculate the active cases with the formula:

`[Active] = [Confirmed] - [Recovered] - [Deaths`]



To visualize worldwide data over time using `matplotlib`:
1. Sum up counts over all countries 
- Calculate the active cases
- Plot confirmed, recovered, deaths and active cases over time [on same figure]
- Modify labels, title, legend",065aaf61,0.07407407407407407
3865,ddcdecdd6a3b6d,b0a995bd,"### AlexNet
首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。   
**特征：**
1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. 用Dropout来控制全连接层的模型复杂度。
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。
![1576144713%281%29.png](attachment:1576144713%281%29.png)

",90831448,0.07407407407407407
3866,1667a100fc8b42,68740ccc,"## What is PCA?
The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. The same is done by transforming the variables to a new set of variables, which are known as the principal components (or simply, the PCs) and are orthogonal, ordered such that the retention of variation present in the original variables decreases as we move down in the order.
<img src='https://s3.amazonaws.com/files.dezyre.com/images/Tutorials/Principal+Component+Analysis.jpg'>
<b>Dimensionality</b>: It is the number of random variables in a dataset or simply the number of features, or rather more simply, the number of columns present in your dataset.<br>
<b>Correlation</b>: It shows how strongly two variable are related to each other. The value ranges from -1 to +1. Positive indicates that when one variable increases, the other increases as well, while negative indicates the other decreases on increasing the former. And the modulus value of indicates the strength of relation.<br>
<b>Orthogonal</b>: Uncorrelated to each other, i.e., correlation between any pair of variables is 0.<br>",6c8cd6b6,0.07407407407407407
3875,b9328fe3b0cefc,32e45689,"## What is NCAA March Madness(什么是NCAA疯狂三月)

First of all, NCAA is the National University Sports Association of the United States. It is an association of hundreds of universities and colleges in the United States. Its main activities are various sports leagues held every year, among which the basketball leagues in the first half of the year and rugby leagues in the second half of the year are the most concerned. March Madness is actually the NCAA tournament. It was founded in 1939 and is one of the largest national sports events in the United States. Because most of the NCAA tournament match are held in March, so we call it March Madness.(首先，NCAA是美国的全国大学体育协会，它是由美国千百所大学院校所参与结盟的一个协会。其主要活动是每年举办的各种体育项目联赛，其中最受关注的是上半年的篮球联赛和下半年的橄榄球联赛。而疯狂三月其实是通称的NCAA锦标赛，即美国大学体育总会(NCAA)一级联赛男篮锦标赛，成立于1939年，是美国最大的全国性体育赛事之一。因为NCAA锦标赛的大部分比赛都在3月进行，俗称疯狂三月)

In short, March Madness is the time when NCAA Basketball tournament in March every year.(简单说，疯狂三月就是每年三月份，NCAA篮球锦标赛集中进行比赛的这段时间)

### Tournament Rule(赛制规则)

1. The first is the ""First Four"", which consists of the four lowest ranked champion teams in 32 division areas and the four lowest ranked teams in 36 teams selected by NCAA. The winning four teams and the other 60 teams make up the top 64 teams.(首先进行的是“最先四场（First Four）”，由32个分赛区里排名最低的四支冠军队伍和NCAA选拔的36支队伍中排名最低的四支捉对厮杀，胜出的四队和另外的60支队伍组成64强)
2. 64->32->16->8->4->2->Champion

In addition, it should be noted that the tournament is **one-round**, so compared with the seven games in the NBA playoffs, it is more likely to have black horses, which also brings More surprises.(另外要注意，比赛是**一轮制**，因此相比较NBA季后赛的7场，更加容易出现黑马，这也带来了更大的观赏性)",3a35eb23,0.07407407407407407
3878,4883314a96dc34,d204a230,## Preliminary exploration,50d36836,0.07407407407407407
3879,233cb23d9e01b9,aafab025,## Image samples,ffa56c19,0.07407407407407407
3880,55ce731a138ca7,773d07f9,"# Image Resize 
Source: https://www.kaggle.com/mlubbilgee/baseline",4996250b,0.07407407407407407
3883,46778b77b8d195,8a727da0,# EDA,ec849695,0.07407407407407407
3884,613bf7bfdcb9e3,fab34f34,"scipy.stats.mode(a, axis=0, nan_policy='propagate')[source]



#### Return an array of the modal (most common) value in the passed array.


### If there is more than one such value, only the smallest is returned. 

The bin-count for the modal bins is also returned.",32beb65d,0.07407407407407407
3892,0c452d3a0b9339,47a296ef,# Token Dictionary,5d857385,0.07407407407407407
3896,efbcfe95cd7fde,6a24b06c,Read Data¶,54be281a,0.07407407407407407
3898,8106640e2f9c7e,8287ea96,## Загрузка данных,faea9b8e,0.07407407407407407
3903,81712ee7510ac5,1b0ae264,**MOD FUNCtion**,c4685e79,0.07428571428571429
3904,63b44c85e32c1f,faa38873,"Indexing can also be done in reverse order. That is the last element can be accessed first. Here, indexing starts from -1. Thus index value -1 will be orange and index -2 will be apple.",fb9b9562,0.07432432432432433
3905,e9b9663777db82,269d36ab,![3.png](attachment:3.png),648e8507,0.0743801652892562
3912,1a222fee3089d2,af213bf4,# Dataset cleaning,59ab8894,0.07462686567164178
3915,a4aa36df07fd53,3ebbe424,Untuk melihat seperti apa data sesungguhnya silahkan run:,d2f42b6d,0.07462686567164178
3916,c84925c8171900,fd605bc1,"<p style=""text-indent: 25px;"">
    <span style='font-family:Georgia'>
        This dataset contains a list of video games with sales greater than 100,000 copies. It was generated by a scrape of <a href= ""https://www.vgchartz.com/gamedb/"">vgchartz.com </a>
                </span>
</p>
<span style='font-family:Georgia'>
    <ul>
        <li><b>Rank</b> - Ranking of overall sales</li>
        <li><b>Name</b> - The games name</li>
        <li><b>Platform</b> - Platform of the games release (i.e. PC,PS4, etc.)</li>
        <li><b>Year</b> - Year of the game's release</li>
        <li><b>Genre</b> - Genre of the game</li>
        <li><b>Publisher</b> - Publisher of the game</li>
        <li><b>NA_Sales</b> - Sales in North America (in millions)</li>
        <li><b>EU_Sales</b> - Sales in Europe (in millions)</li>
        <li><b>JP_Sales</b> - Sales in Japan (in millions)</li>
        <li><b>Other_Sales</b> - Sales in the rest of the world (in millions)</li>
        <li><b>Global_Sales</b> - Total worldwide sales.</li>
    </ul>
</span>
    
",e21ff7ec,0.07476635514018691
3917,9f0ccf5b9e8f03,abc4aecf,"Kawasaki disease is a rare acute paediatric vasculitis, with coronary artery aneurysms as its main complication. The diagnosis is based on the presence of persistent fever, exanthema, lymphadenopathy, conjunctival injection, and changes to the mucosae and extremities.",66691203,0.075
3918,5626e84c4e6bf8,1a34ca27,"# About the dataset: Fashion MNIST
## Context
Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.

The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. ""If it doesn't work on MNIST, it won't work at all"", they said. ""Well, if it does work on MNIST, it may still fail on others."" Zalando seeks to replace the original MNIST dataset.

## Content
Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image. To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. 

## Labels
Labels
Each training and test example is assigned to one of the following labels:
* 0 T-shirt/top
* 1 Trouser
* 2 Pullover
* 3 Dress
* 4 Coat
* 5 Sandal
* 6 Shirt
* 7 Sneaker
* 8 Bag
* 9 Ankle boot 
",e2ecb669,0.075
3920,254cccd5145725,daf61879,"<font size=""4"">As how the norm goes we will be training our data on the train dataset and test our model against the test dataset. The Kernel is divided into three major parts</font>",a49b4037,0.075
3923,62487bcd70b199,aa0df33c,"## Inferences:
1. The distribution of target column is not uniform. 
   The dataset is imbalanced and we cannot rely only on accuracy score of models to evaluate performance.
   We will use f1 score to measure different models performance
2. Experience Column has -ve values and to be corrected
3. Highly skewed Income and Mortgage columns need to check for outliers
4. No Missing values in any columns",f6ae50af,0.075
3931,37b09262279764,c711c8cf,There are <b>891</b> rows and <b>12</b> columns in the data,37c4c417,0.075
3933,9a040a4f21091e,e95675af,**Non-Toxic Text Examples:**,f591b57d,0.075
3934,6e28c4f557f736,e3486ecf,"### Fixing data source error
Ref: https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data
There are several tweets in the dataset are incorrectly labeled. In fact, they are not disaster related by they are labeled so. This will actually harm the model performance. ",021fdf75,0.075
3935,ba4b3bd184acbb,7d4222b6,"#### Dictionary of Lists

If the lists we have represent a column rather than a row, we can use a dictionary of the lists where the key for each list becomes the column label.

This method also requires the lists to be the same length.",0f5de724,0.07518796992481203
3937,0caaec057f7184,4a43e627,"## Data cleaning 
check duplicated, uniqness, null",b875533e,0.07526881720430108
3938,738bfced935b69,b1b59bca,## Data with Nine Columns,2d3c592d,0.07534246575342465
3940,0ad8d416b89b78,3317daa3,"# Exploritory Analysis

**Pandas Profiling** was implemented to provide quick and efficient initial exploratory analysis of the dataset. The Profiling tool shows initial attribute distributions, identifies outliers and highlights correlations between data that can be further explored in the EDA process. 

Pandas Profiling is a quite new and exciting package for EDA utilising Pandas, with consistent continued version releases it is a tool that is only going to improve. To find out more visit the [Pandas Profiling GitHub:](https://github.com/pandas-profiling/pandas-profiling)",0b0562f0,0.07547169811320754
3944,07bfec3562f9b3,4ae14bd8,"# Method #1.    
Implementation of scale space technique for word segmentation as proposed by R. Manmatha and N. Srimal. Even though the paper is from 1999, the method still achieves good results, is fast, and is easy to implement. The algorithm takes an image of a line as input and outputs the segmented words.

Scale space technique for word segmentation proposed by R. Manmatha: http://ciir.cs.umass.edu/pubfiles/mm-27.pdf",327e7d5b,0.07547169811320754
3945,510b8303776bb6,660a7979,## Checking and removal of null values,18080db8,0.07547169811320754
3949,f015d0147e8fbf,74f66601,"## Credit Where Credit is Due

I believe that apprenticeship and discipleship are some of the most effective ways to learn, and I have certainly benefited from being an indirect ""apprentice"" of some very generous world-class Kagglers. They have initiated and participated in extended threads in the forums, where they patiently described the ins and outs of their approaches and implementations. They have also shared kernels that contain real, usable implementations of some of the most important techniques. These kernels are far more useful than the run-of-the-mill blog posts that tend to introduce and wax poetic about the philosophy behind some advanced technique (such as stacking), yet leave the uninitiated woefully unprepared for the nitty-gritty and the subtle, yet crucial, details of the technique's actual implementation. 

I am particularly indebted to:

* [Silogram](https://www.kaggle.com/psilogram), whose extensive comments and advice on this [thread](https://www.kaggle.com/c/home-credit-default-risk/discussion/58332#348689) helped me to learn about mean rank prediction blending, a heuristic for deciding the number of training rounds, the usefulness of LightGBM's built-in CV, and most importantly, underscored for me the importance of trusting my CV and not giving in to the temptation to overfit to the public leaderboard.


* [olivier](https://www.kaggle.com/ogrellier), whose three kernels showed me how to implement [target encoding](https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features/notebook), the different ways I could create [aggregate](https://www.kaggle.com/ogrellier/home-credit-hyperopt-optimization) features from the various data tables, as well as the code for using Seaborn to plot LightGBM [feature importance](https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm/code).


* [Laurae](https://www.kaggle.com/laurae2), whose [masterpiece of a website](https://sites.google.com/view/lauraepp/parameters) taught me more than I could have dreamed about LightGBM, its parameters, and how to tune them.


* [neptune-ml](https://neptune.ml/), whose [open solution](https://github.com/neptune-ml/open-solution-home-credit/blob/solution-5/notebooks/eda-application.ipynb) showed me several features that I could engineer and aggregate from the main data table's featureset.",518954fb,0.07547169811320754
3951,4ae6a182abac64,e72ad12f,### 1.3 Descriptive Statistics,418676c5,0.07563025210084033
3955,5d2a3e82679cf3,d9470caa,# 2) DESCRIBE AND TRY TO FILL NA VALUES,9e60b1e3,0.0759493670886076
3957,30fdc4a6e3c1db,7269cb61,"The unique identifier seems to be a concatenation of item_id and store_id. There are 3049 unique items and 10 unique stores (Total number of rows = 30490)

The columns d_1 to d_1913 gives the sales of the given item in that store on the nth day for 1913 days",6111ddee,0.07602339181286549
3958,b01ee6cb674fa3,159e7bf6,"Detail contains:
- rocket name
- mission name",a8ffd35e,0.07608695652173914
3959,bbaa07ad21cf4e,83942ddf,"## 3. EDA <a class=""anchor"" id=""4""></a>",3ab6b254,0.0761904761904762
3963,7a058705183598,aff86a47,Descriptive statistics of DataFrame,b0ead917,0.0761904761904762
3966,ee23a565163388,0c14a82a,# **Table of Contents**,88aacbc4,0.07633587786259542
3974,c115e287523aab,90896829,# Install Libraries,feb1288b,0.07692307692307693
3977,95efc1ad1d3e26,3f2cbec4,"# 1. Vanilla Autoencoder

## 1.1. Data preparation",79de1120,0.07692307692307693
3979,44f6a002ecd033,c2528838,"Will need to apply log transformations on the test columns ApplicantIncome, CoapplicantIncome and LoanAmount.",70bbe106,0.07692307692307693
3982,5f4ae633cfd090,dfdb3fa7,"***Encoding categorical variables***

I'll be encoding the categorical columns prior to doing anything so it'll be easier for me to split the data while avoiding data leakage.",a30a16e2,0.07692307692307693
3984,1645979263c148,3e14ec1f,## Used Python Libraries,fa11663e,0.07692307692307693
3990,d07915a6e6992e,51479bde,"
![Data.jpg](attachment:Data.jpg)


Let's import necessary libraries & bring in the datasets in Python environment first. Once we have the datasets in Python environment we can slice & dice the data to understand what we have and what is missing.",2b912140,0.07692307692307693
3994,8e9d63e1f6319e,a5c81ea9,"[Starter notebook](https://www.kaggle.com/devanshu125/starter-notebook) have said, that y coordinate is `-1 <= y <= 1`. That's correct mathemically, but practically it may break all your strategies, cause y approximately is `-0.436 <= y <= 0.436`. The proof is below.",4743b346,0.07692307692307693
3995,2facf256353117,d0100a7e,"# Basic concepts common to all image file formats

",18f579be,0.07692307692307693
3997,d0f6276d5b628c,aa0342c0,"Now we are going to read the files and then visualize it for the first time, this might help us to understand that how we can wrangle the data.",c64f5ce5,0.07692307692307693
4002,33398ae40da63d,296c9ba6,"First, let's read into our data from the csv files",7bd02b88,0.07692307692307693
4004,99bf357eaf61f1,56fde16e,# Exploratory Data Analysis,9d92fafe,0.07692307692307693
4010,9eed0fae1c7958,f324ed6f,# Install TF nighlty,3fb1438e,0.07692307692307693
4017,8ac70416723897,e9c2100c,Loading in the data,d32fd8f6,0.07692307692307693
4024,50b03ce5b1a286,1c4c4204,#Code by Casper Wilstrup https://www.kaggle.com/wilstrup/use-qlattice-to-predict-rainy-days-in-australia/notebook,d49896a5,0.07692307692307693
4025,a8c042af6b7245,e6aa1a0a,"### Data at first sight

Here is an excerpt of the data description for the competition:

* Features that belong to similar groupings are tagged as such in the feature names (ind, reg, car, calc)
* Feature names include the postfix bin to indicate binary features and cat to indicate categorical features
* Features without these designations are either continious or ordinal
* Values of -1 indicate that the feature was missing from the observation
* The target columns signifies whether or not a claim was filled for that policy holder",2487ac62,0.07692307692307693
4029,71d3e4aee86e3e,652760fc,## 3. Data Preprocessing,69706f0b,0.07692307692307693
4033,d78988cb5a1b02,fb483566,**Lets make a dataset to create a model for binary classification**,233f3a92,0.07692307692307693
4038,4d91e84c564cbe,e540b33d,A list can contain a mix of different types of variables:,355a43e3,0.07692307692307693
4039,eda49464dd6d1b,ce654cb0,"* The training data is 381,109 rows long, and the test dataset is 127,037.  
* The training dataset has a ""response"" column, but the test dataset does not.
* ""Vintage"" refers to how many days the customer has been with the company",8421f81f,0.07692307692307693
4040,4f69b7bb1ca287,8baf4132,## Importing Relevant Libraries,34abc6a1,0.07692307692307693
4042,163ceeb80d6923,9ec1b92e,## Load Train Data and EDA,4adfbb90,0.07692307692307693
4046,020c28a360b0cd,06c3da28,Samuel Montoya,2ba397f0,0.07692307692307693
4051,2ada0305b68956,77447acb,### 10. Palette = 'BuPu_r',133e26f4,0.07714285714285714
4054,a2176d4653ef60,a96c3050,# Data Exploration,ac908675,0.07766990291262135
4059,3597174a998d4d,196550f2,"From the figures, it can be concluded that:
* Overall, the City Hotel's total number of booking is higher than the Resort Hotel's, and its inverted U shape is more obvious. What's more, it has increased compared with the same period last year.
* The Resort Hotel's sum of all lodging transactions is obviously a inverted V shaped line. I guess there is a big price difference between busy and idle time in the Resort Hotel, and the price in the City Hotel is stable.
* The City Hotel's ratio of booking's cancelation is higher than the Resort Hotel's. Also, it has increased compared with the same period last year.
",276892ed,0.07777777777777778
4061,892be0a523578c,0b97710b," **1.2** Next, I want to see how many invalid records exist for each individual. It turns out that half of the participants have invalid records, 4 of whom have **more than 10** invalid records",b0e8d7c0,0.07777777777777778
4063,722cd844dfbe8f,9992e409,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_1_1"">Submission sample & train.csv</span>

We will already look at the exemple **submission file** to see exactly what we need to predict in the end.",0cedb385,0.07792207792207792
4064,c13f73168789c2,a71813c1,"### 1.1 To select a single column<a id='3'></a>
Syntax 1 : `df.loc[:, 'column_name']`

Syntax 2 : `df['column_name']`

Syntax 3 : `df.column_name`",16175052,0.07792207792207792
4069,241cf32abb22d8,6c784f19,"The dataset contains 395 observations. The numeric target feature ""G3"" has been renamed as ""target"" and transformed into a binary categorical feature with two levels ""pass"" and ""fail"".",47157066,0.07792207792207792
4070,663bbc9eaf267b,8e348cfb,# Exploring Data,32445529,0.07792207792207792
4071,b61ab8f81dc03d,67a39768,"<a id=""variable_notes""></a>
## Variable Notes
pclass: A proxy for socio-economic status (SES)
1st = Upper
2nd = Middle
3rd = Lower

age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5

sibsp: The dataset defines family relations in this way...
Sibling = brother, sister, stepbrother, stepsister
Spouse = husband, wife (mistresses and fiancés were ignored)

parch: The dataset defines family relations in this way...
Parent = mother, father
Child = daughter, son, stepdaughter, stepson
Some children travelled only with a nanny, therefore parch=0 for them.",64d05394,0.07801418439716312
4073,56785caebaa256,71d3c1f5,#### Thanks to [COVID-19 in Ukraine: daily data](https://www.kaggle.com/vbmokin/covid19-in-ukraine-daily-data),a792961a,0.07801418439716312
4077,ff3a8ce61fab6a,e3591beb,"# As most programming language like **C++, C#, Java, etc** we must define variable type. So let's know variable types in tensorflow.<br>",9afe1654,0.078125
4082,1a0bd2f72bbe36,1835ce96,## IMPORT DATA-SET:,2fa311dc,0.0784313725490196
4085,d0080e3a39bc5c,80eb7d92,"**DATA PREPROCESSING**

**Preparing the Dataset for training.**",2fcde4cf,0.0784313725490196
4088,917957c6c4065f,e3e55596,video_id가 동영상의 고유값인지 확인해보겠습니다.,55b8ed68,0.0784313725490196
4094,e67925694c07d3,520cbf24,# **EDA and Cleaning**,83af4c4a,0.07865168539325842
4096,04ff2af52f147b,bad1f581,We can see above that there is a very significant disparity in survival rates between men and women.  Next we can check survival rate differences between adults and children.,d5f37be9,0.07865168539325842
4100,31b564f11ef638,e3ca407e,"- [Bike Sharing Demand Competition](https://www.kaggle.com/c/bike-sharing-demand)

- [Modeling Reference Notebook](https://www.kaggle.com/viveksrinivasan/eda-ensemble-model-top-10-percentile)",424f9692,0.07894736842105263
4101,f05342aabe2b59,0d0949e0,### Kludgy maximum candies calculation,cfbb391f,0.07894736842105263
4104,f35ee6e9fab592,7cc69b1c,Replacing unacceptable date formats with NaNs for quick analysis purposes,b15f7073,0.07894736842105263
4109,d369f200a84c2a,6ad9c609,"An advantage of CapsNet is that capsules represent the relationships between parts of a entire object by using dynamic routing to weight the connections between one layer of capsules and the next and by creating strong connections between spatially related object parts.

The output of each capsule is a vector, this vector has a magnitude and orientation.

   >Magnitude: Indicate whether this particular part of the image is present or not. Basically, we can sum it up as the probability of the part's existence (it should be between 0 and 1).

   >Oriantation : It changes if any of the properties of that particular image have changed.",8fef4d48,0.07894736842105263
4112,a1dcd92986bc84,ad51a72c,## Setup,730acaaa,0.07894736842105263
4113,fe7360cddc13e5,a377991e,"<font color='red'> **1- Aktifken,bir müşteri tarafından t uzunluktaki bir dönemde yapılan işlemlerin sayısı, alfa ile birlikte bir Poisson dağılımı izler.**",8979e423,0.07894736842105263
4122,73d8e56bc709b1,4c41d837,# 2. Data Cleaning,78ec3cce,0.07954545454545454
4126,ac1abfe1dfe815,2ef35e3a,"**`airline_sentiment_confidence` is left skewed, 75% of the values are bigger than 0.69, and the mean < mode**  
This means the confidince in the sentiment is high.",6529dbcb,0.07964601769911504
4127,b01ee6cb674fa3,d75045dc,## Status Rocket,a8ffd35e,0.07971014492753623
4128,0e2a23fbe41ca9,7c671610,There are no duplicate ```card_id```'s in the test set.,64e4762c,0.07971014492753623
4134,19aae4a6ede288,06da49a3,## Importing Libraries and Datasets,56934674,0.08
4138,21c1e34efd71b8,c520fbd2,Importing all the necessary lib and the best estimated logestic Regression Model that was derived in Part 1.,23b2cdd6,0.08
4139,b10bd75889dad9,3141df9f,### Lets Analyse Numeric and Categorical variables separately,ee00ceee,0.08
4146,519e936017c30a,c47aec06,"Nuestro primer paso ha sido cargar la base de datos y almacenarla en nuestro Data frame ""videojuegos"" ,para poder hacer uso de él cuando la necesitemos. A continuación, observamos cuantos valores nulos contiene este ejemplo y los limpiamos para poder obtener un mejor estudio. Finalizado este proceso, imprimimos los cinco primeros valores y los cinco finales para observar como se estructura este conjuto de datos.

Mediante un análisis rápido, se puede apreciar que los videojuegos están ordenados de mayor a menor, en cuanto a ventas globales obtenidas. Siendo el primero ""Wii Sports"" por un valor de 82,74 millones de dólares.

Para poder visualizar de forma más intuitiva como evolucionan dichos ingresos, vamos a ordenar los datos en base al año de su puesta en venta. ",dc34915d,0.08
4147,10c5a39a87c47e,d806dd2a,"## Step 2: Loading Data<a id='step-2'></a>
In this step we will first create separate directory to store Parasitized and Normal images (Uninfected)",09c7337a,0.08
4155,0687cd5c8597db,e8adc30a,### **Reading Training and Testing CSV Files**,4edec76a,0.08
4157,83df814455f06c,30457034,"# **5. Attribute selection measures** <a class=""anchor"" id=""5""></a>

[Table of Contents](#0.1)


The primary challenge in the Decision Tree implementation is to identify the attributes which we consider as the root node and each level. This process is known as the **attributes selection**. There are different attributes selection measure to identify the attribute which can be considered as the root node at each level.


There are 2 popular attribute selection measures. They are as follows:-


- **Information gain**

- **Gini index**


While using **Information gain** as a criterion, we assume attributes to be categorical and for **Gini index** attributes are assumed to be continuous. These attribute selection measures are described below.
",c9cff71a,0.08
4163,9395559895004f,55d72697,## Extract Dataset from ZIP,b5a0494b,0.08
4164,cee088a6840708,bb0491ab,"# Step 2 -> Decide which library to use 

I will use the dgl library (similar to keras) with tensorflow as backend

dgl is responsible for the  message passing  part and everything else is conducted by tensorflow",55463e1c,0.08
4165,cb570c7b7f0501,7d6c39d8,"<a id='Assessing'></a>
## Assessing Data",a200a0ec,0.08
4167,3c2033cc99c12c,56d03052,"As is shown above the columns of V22 and V23 have Nan values. Since the data of Credit Fraud Detection is highly imbalanced, so we should fisrt have a brief view of whether directly dropping the Nan value will have a great influence on the Fraud data, and we found that the Nan value dosen't have any impact on the Fraud class data, so I choose drop the whol row of Nan value.",dfa22a54,0.08029197080291971
4173,5f32117bcd5255,07ac048e,### INFORMATIONS,85882abf,0.08053691275167785
4175,57070ad5e0f94f,e4aff5d9,# **Analysing Overall Data**,d97edc41,0.08064516129032258
4179,ad26c020235dfc,66366b87,"# Waterbodies
Write the waterbodies into a list. ",bf766e48,0.08064516129032258
4181,c65a65d4041018,10588a68,### How long did it take to answer the survey,824fb229,0.08088235294117647
4187,bbb3f4b76a4559,f27d6197,"### Very simple EDA
Titanic dataset is quite famous and there are a lot of great EDA. So I don't spend time to EDA. Just use pandas-profiling.",75185823,0.08108108108108109
4193,2dda7facf3c1e0,7a19b348,"## Installing dependencies
First, let's install and import all of the Python packages that we will use in this tutorial.",45552d2b,0.08108108108108109
4196,62037c5832129c,34a6357c,## Loading the Breast Cancer Wisconsin dataset,61474350,0.08108108108108109
4199,a6c34cd514e30e,965dd99f,"There are 6 csv files in the current version of the dataset:
",bf603ddd,0.08108108108108109
4203,d96642860ab3dd,0249e7ba,"### At first find the relationship between two variables i.e Bivariate Analysis, predictor [ feature ] and target variables [ outcome ].",98419d48,0.08139534883720931
4206,2343dc02ffb96a,990dbd8e,# Read test and train data which is already split in this data.,29aa95a4,0.08163265306122448
4207,eb33e05704d647,6319f258,"Parser the data from annotation file
",cd80436d,0.08163265306122448
4209,f1e162ddd14f11,d848a1ed,"Since by looking at the data we can see that the selling price is our dependent value and fuel_type, seller_type, transmission are our categorical values",cdb2e771,0.08163265306122448
4213,e69a496109e7d8,b3dbb522,It has **four features**,1c640591,0.08163265306122448
4217,5ce12be6e7b90e,31779a99,"# Types

These are the basic Python data types:

| Type | Description | Range | Use |
|--------|-----------|-------|--------|
| `int`  | Integers | -oo to oo | counting, indexing |
| `float` | Decimal fractions | limited precision, depends on machine | calculations |
| `complex` | Complex numbers | just two floats | complex calculations  |
| `str` | Strings | unicode | text, categories |
| `bool` | Booleans | `True` and `False` | boolean logic |

We can determine a variable's type using the `type` function.",c0ab62dd,0.08187134502923976
4224,20b372b6e4e276,72eb306e,"## 3. Model tuning <a class=""anchor"" id=""3""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.08208955223880597
4225,fdc9f4863744b1,1a20d7af,"I will further import my data and start the cleaning process. I grabbed the data ""nyc-rolling-sales"" from Kaggle. It is in csv format and easy to import.",b4529365,0.0821917808219178
4231,e93a41c03638fe,748f6ed3,"Here, we observe keyword and location variables contain null values. Since, these are not important columns we will be dropping them along with id:",7363527b,0.08235294117647059
4233,869a39a3d4dea2,4f495400,"Reading the image using the open cv and plotting the image to visualize it in notebook.
<br/>*Note: OpenCV represents RGB images as multi-dimensional NumPy arrays however in reverse order like BGR*",9020daf8,0.08235294117647059
4234,2a123b4e8f9433,04fb8cc8,Looking at the unique values for esrb_rating,0a082218,0.08247422680412371
4239,e9b9663777db82,2edb638c,# Libraries,648e8507,0.08264462809917356
4240,2f47abddfd1928,951285a8,With .info() method we can identify each feature type and also if there are NaN values.,ae33cc0b,0.08264462809917356
4242,2ada0305b68956,b1b05988,### 11. Palette = 'CMRmap',133e26f4,0.08285714285714285
4243,b3e0b7e9ff6849,91d132c0,"
### ****Content of Dataset****

This Online Retail II data set contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift-ware. Many customers of the company are wholesalers.",f6e4bb0d,0.08333333333333333
4246,df7b0c10ec94fd,0b0e283b, **SETUP**,ec1a6e07,0.08333333333333333
4247,7ba63a2d9abb58,af1c0f76,"This data is retrieved from an API which gives information about total confirmed, deaths and recovered cases by country. The data is updated everyday. 
<p>API Documentation: <a href=""https://documenter.getpostman.com/view/10808728/SzS8rjbc?version=latest"">https://documenter.getpostman.com/view/10808728/SzS8rjbc?version=latest</a>",821a261f,0.08333333333333333
4250,dd3721cb49c1fd,4c1fd2d6,"<a id='0'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px;"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white; text-align:center"">0. References 📓</h1>
  </div>
  
  <div style=""margin: 0px; padding: 10px; background-color: #ffffff;
        box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
        border-radius:2px; text-align:left"">
    <h5><a href=https://linkedin.github.io/greykite/docs/0.1.0/html/index.html>1. Greykite Documentation </a></h5>
  </div>
</div>",1a53fdd9,0.08333333333333333
4258,ab6da5994949a3,558e036e,"### Class column is response and rest columns are predictors.
### Seprating Predictors and Response",fae6b91d,0.08333333333333333
4261,396bc36edb95d3,260274dc,From the above we can see that we have one observation in Duration column with value -1 which cannot be the case. Duration of a stay cannot be in negative numbers. Hence let us impute the bad data with mean value 72.14 days.,965e4f8f,0.08333333333333333
4264,a827e04a19562c,a3433344,"### Introduction

We will implement the PageRank algorithm from scratch and explore it on famous social networks available in Networkx.
There are few preprocessing steps:
1. Converting the graph to a directed graph (if the graph is undirected, change it to bi-directed graph)
2. Relabelling nodes as integers (encoding string names to unique node IDs)
3. Remove isolated nodes (if there are nodes which are not connected to other nodes howsover, remove them from graph)",ba301c25,0.08333333333333333
4268,62487bcd70b199,8db9901a,## <a id='1.2'>1.2. Profile Report of data for better understanding</a>,f6ae50af,0.08333333333333333
4269,2f0f808765fc67,1f9af27e,"# **A. Data Preprocessing**
check Null values",fd1f6494,0.08333333333333333
4274,95656e8d666b16,79160455,First let's check for null values,65e88599,0.08333333333333333
4276,10b5af05d804ff,1dd4893a,## An example:,4a9b1705,0.08333333333333333
4278,f6488772605bb5,94a5dc76,## Exploring the data,068d4697,0.08333333333333333
4281,8447633e1d256c,99258e04,## 1. Loading Dataset,60d593ce,0.08333333333333333
4283,1d5daeca89f48d,0ec9ec04,"# Word Embeddings

To represent words as vector (Count Vectoizer to Word2Vec)",48d478bc,0.08333333333333333
4286,923e97b05be00b,e0833dc8,"## 1. Defining a Problem

Great! Let's start by thinking of something we can look for in a chest xray.

Once we think of something, let's type it in below, in between the quotes <kbd>""</kbd><kbd>""</kbd>, and run the cell. I'll put in ""atelectasis"" by default, but let's see if we can think of something else.",3a4a22dd,0.08333333333333333
4289,cf39cde80e66b7,9a9fd6a6,"<a class=""anchor"" id=""top""></a>
<a id='dsf4'></a>
# <div class=""h2"">  Table of contents</div>
1. [Imports](#IMPORT)
2. [Regression metrics summary ](#M)
   -  <a href='#m1'>MAE</a>
   -  <a href='#m2'>MSE</a>
   -  <a href='#m3'>RMSE</a>
   -  <a href='#m4'>MAPE</a>
   -  <a href='#m5'>RMLSE</a>
   -  <a href='#m6'>R-Square</a>   
   -  <a href='#m6'>Ajusted R-Square</a>
   -  <a href='#m7'>Residual Sum of Squares (RSS)</a>
   
  <hr>",aed4bc9b,0.08333333333333333
4290,cf46cd6f7c55c0,f51250ab,"# What is Pseudo Labeling?
Pseudo labeling is the process of adding confident predicted test data to your training data. Pseudo labeling is a 5 step process. (1) Build a model using training data. (2) Predict labels for an unseen test dataset. (3) Add confident predicted test observations to our training data (4) Build a new model using combined data. And (5) use your new model to predict the test data and submit to Kaggle. Here is a pictorial explanation using sythetic 2D data. 
  
## Step 1 - Build first model
Given 50 training observations (25 target=1 yellow points, 25 target=0 blue points) build a model using QDA. Notice how QDA calculates the two multivariate Gaussian distributions that the target=1 and target=0 were drawn from. QDA's approximation is represented as ellipses of 1, 2, 3 standard deviations for each distribution.

![image](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/p16419.png)

## Step 2 - Predict test data
Using our model (ellipses), predict the target of 50 unknown data points. The bottom picture shows the decisions made by our classifier.

![image](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/p26419.png)

![image](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/p36419.png)

## Step 3 and 4 - Add pseudo label data and build second model
Add all predictions with `Pr(y=1|x)>0.99` and `Pr(y=0|x)>0.99` to our training data. Then train a new model using the combined 90 points with QDA. The red ellipses show QDA's new approximation of the two Gaussian distributions. This time QDA has found better ellipses then before.

![image](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/2020/p46419.png)

## Step 5 - Predict test data
Finally use our more accurate QDA ellipses to predict test (a second time) and submit to Kaggle.",191b86b8,0.08333333333333333
4297,2a377ced98d67a,7f84d28e,## 2. Read Data,262231a8,0.08333333333333333
4299,b3681fd423741d,9d0452b1,# 1. Merek mobil apa saja yang tersedia dan ada berapa banyak mobil untuk tiap merek tersebut?,1aec06ce,0.08333333333333333
4300,87e94f864d74be,75be8f89,"The dataset has 7787 rows and 12 columns:
* show_id: unique id of each show (not much of a use for us in this notebook)
* type: The category of a show, can be either a Movie or a TV Show
* title: Name of the show
* director: Name of the director(s) of the show
* cast: Name of actors and other cast of the show
* country: Name of countries the show is available to watch on Netflix
* date_added: Date when the show was added on Netflix
* release_year: Release year of the show
* rating: Show rating on netflix
* duration: Time duration of the show
* listed_in: Genre of the show
* description: Some text describing the show",294bfe9f,0.08333333333333333
4302,d46508f983e086,98cbbd9e,**Data Importing**,454138b8,0.08333333333333333
4303,64c5c66fd713b1,e4155a3e,"**** 

# Code for loading Johns Hopkins Github data and visualizing daily progress",5814b2b3,0.08333333333333333
4309,56a583a039b57c,c6e22286,### Group the data over time,c0526ea5,0.08333333333333333
4311,02773bdc5d3c7a,e81be86b,![fraud1.jpg](attachment:fraud1.jpg),86245f35,0.08333333333333333
4317,98ea617d18c9cc,207e8672,# Visaulizing some images from dataset,e6316d11,0.08333333333333333
4321,7650e0ac081e94,771bcc18,Please UPVOTE !!,0081cee5,0.08333333333333333
4325,7c7a7db391c517,9a47e1d2,"## 1.1 About the Data
Context
This dataset deals with air pollution measurement information in Seoul, South Korea.
Seoul Metropolitan Government provides many public data, including air pollution information, through the 'Open Data Plaza'
I made a structured dataset by collecting and adjusting various air pollution related datasets provided by the Seoul Metropolitan Government

Content
This data provides average values for six pollutants (SO2, NO2, CO, O3, PM10, PM2.5).

Data were measured every hour between 2017 and 2019.
Data were measured for 25 districts in Seoul.
This dataset is divided into four files.
Measurement info: Air pollution measurement information

1 hour average measurement is provided after calibration
Instrument status:
0: Normal, 1: Need for calibration, 2: Abnormal
4: Power cut off, 8: Under repair, 9: abnormal data
Measurement item info: Information on air pollution measurement items

Measurement station info: Information on air pollution instrument stations

Measurement summary: A condensed dataset based on the above three data.

Acknowledgements
Data is provided from here.

https://data.seoul.go.kr/dataList/OA-15526/S/1/datasetView.do
https://data.seoul.go.kr/dataList/OA-15516/S/1/datasetView.do
https://data.seoul.go.kr/dataList/OA-15515/S/1/datasetView.do",f53450dc,0.08333333333333333
4332,69ac33d79f5130,4efdaadb,#### Number of columns in the datasets.,9d760d2a,0.08333333333333333
4333,eb0854a6601407,9fe63827,"# Reading the Parquet Version
Reading in csvs can be slow. Instead read from the parquet version here:
- https://www.kaggle.com/robikscube/ubiquant-parquet",6d107747,0.08333333333333333
4336,ea2763c0f6c6a0,3f13ffd8,# First look at data,e5812ac1,0.08333333333333333
4343,eda49464dd6d1b,86e05525,"## Check for missing values
* No null values",8421f81f,0.08391608391608392
4344,ee23a565163388,e701f8df,"- Importing Libraries and Dataset
- Quick Inspection of Dataset
- Exploratory Data Analysis
- Feature Engineering
- Building Predictive Model",88aacbc4,0.08396946564885496
4345,c4386b8a01d66e,06ea70e8,## Handling Null Values,dc732bf5,0.08403361344537816
4346,4ae6a182abac64,d0032974,* So first we need to know **which variables are available in the dataset** ?,418676c5,0.08403361344537816
4347,c84925c8171900,820f713d,"<a id=""python""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            2.1 Import Python Libraries:
            </span>   
        </font>    
</h3>",e21ff7ec,0.08411214953271028
4348,169177b6e9edea,61a8de86,"<ul><b>Ideias para melhorar nosso código:</b>
    <li>Organizar os diferentes modelos e acurácias em um DF para ser mais fácil compará-los</li>
    <li>Categorizar as idades</li>
    <li>Usar os pronomes de tratatamento </li>
    <li>Número de pessoas por família </li>
    <li>Adicionar a cabine </li>
    <li></li>
    </ul>
    
     http://www.balmoralsoftware.com/titanic/titanic.htmsite>",ca42152f,0.08421052631578947
4350,f91f58d488d4af,7b81ae25,"Let's take a look at one now. we'll use *Image* class from the *Python Imaging Library*(PIL), which is widely used for Opening, Manipulating and viewing Images. 
",5df1bbf3,0.08421052631578947
4354,d8ff894670d506,2da4093b,**Describing the Dataset**,eb0fb7de,0.08450704225352113
4356,bddd799cdbbae8,f82e15d2, # <a id='3'> 3.Data Preprocessing </a>,b44e3c08,0.08450704225352113
4358,631cd434fc3aa2,7335b1d8,"## Data preprocessing
### Target Variable
First, let's start by doing some analysis on the target variable _SalesPrice_.",2b74febb,0.08450704225352113
4364,9ceb7278784462,54a8db86, * We dont have missing data ,3768a567,0.0846774193548387
4365,b660910fcc2954,5db2b720,## Glimpse the data,80b74f88,0.0847457627118644
4366,b9bc7dc9f582e5,a560b7a3,# Handling Categorical variables,15cc4d28,0.0847457627118644
4367,149cb8d3489224,07fc1310,"# Data exploration


## Glimpse the data",116858e7,0.0847457627118644
4370,bb0905d33ae417,d89761cc,# Import packages,25fd1965,0.0847457627118644
4372,1294fb4c86f993,3b44d123,### `Gun Data`,4471e513,0.0847457627118644
4374,a81661cc35d8d2,847c7bd0,# Setting up,3331f113,0.0847457627118644
4383,b61ab8f81dc03d,aae252c3,"<a id=""graphs""></a>
# Graphs
 The graphs will help you to visualize data which makes it easy to rapidly scan information in order to understand. ",64d05394,0.0851063829787234
4388,3f25b363afec54,4f3fee88,"## Evaluation Metric
The evaluation metric for this hackathon is ```ROC AUC Score```.",bbdaae25,0.0851063829787234
4389,957e035ba5b9d5,19d075ed,Looks like that we have duplicates in dataset and musemart.,778ab3d3,0.0851063829787234
4391,56785caebaa256,85e9c456,#### Thanks to https://api-covid19.rnbo.gov.ua/,a792961a,0.0851063829787234
4392,0b01138ad120fc,77b74f4a,## Imports and Setup,0b4b72e6,0.08536585365853659
4393,fd4017c1514157,bf8c7a9c,***,fd8f0896,0.08536585365853659
4394,9c26c5dcd46a25,3d417227,"Suite au nettoyage effectué dans le Notebook précédent, les valeurs médianes, écarts-type et valeus max semblent être cohérentes compte tenu du volume de données. 

#### <font color=""#114b98"" id=""section_1_1"">1.1. Analyse des dates de création et modification de produits</font> ",1bbbb677,0.08536585365853659
4399,04bac111ffbe9c,0a03ff4b,"##### QUALITY AND COMPLETENESS ISSUES
1. Handle missing values
2. De-label Pclass to make the data easier to interpret.
3. Sib(SIBLING) and Sp(SPOUSE) in one column.
4. Parch in one column (Par = PARENTS ; ch = CHILDREN)
5. Decide whether or not to keep Cabin.
6. Merging Sibsp and Parch into one column called 'Fam' meaning family.
7. Separate out title from name
8. Encode the 'Sex' and 'Embarked' columns
9. Drop the 'Ticket' column
10. Drop Name after feature engineering
11. If not significant, drop PassengerId

##### TIDINESS ISSUES
1. Sib(SIBLING) and Sp(SPOUSE) in one column.
2. Parch in one column (Par = PARENTS ; ch = CHILDREN)
3. Pclass in un-interpretable format
4. Merging Sibsp and Parch into one column called 'Fam' meaning family.
5. Separate out title from name and then drop it
6. Drop the 'Ticket' column
7. Drop Name after feature engineering
8. If not significant, drop PassengerId",82576b17,0.08571428571428572
4403,171494b45650a2,26c58ce9,## ***2. Data Understanding***,9c8cc578,0.08571428571428572
4404,9c044fa3072552,d9fea754,"# Ask & Answer Questions
- Are there more accidents in warmer or colder areas?
- When (as in the time of accident) are the accidents more frequent?
- Which days of the week (or even month) have the most accident?
- What is the trend of these accidents over the year? (decreasing/increasing)",1362842e,0.08571428571428572
4413,ca73f3d2e25b47,fc98ffb2,Combine trainData and testData,4cd11efe,0.08571428571428572
4415,47a1b1fe51b4ad,3a255513,"### Data Pre-processing

  - **Data Cleaning**
  - **Feature Engineering**
  - **Feature Normalization**",331ded2f,0.08571428571428572
4420,2730840089c8eb,d1a3fd1b,"If we try to put a single quote character inside a single-quoted string, Python gets confused:",34d27dac,0.08571428571428572
4421,55a5e31d03df9f,8364b863,"As we can see train and test data have a path and a class id, this class id can be matched with the metadata to know what is the Minifigure name.

There are 37 different Legos Minifigures, also we noticed something interesting is that the number of test images in the CSV is different from the folder (difference by 2), so maybe there are some images that aren't used or are corrupted. For the sake of simplicity, we will stick with the paths given by the CSV's.

We will merge the metadata with the train and test CSV, this will help us to have the Minifigure name and visualize some random images with that name.",06dce00f,0.08571428571428572
4427,7454fdc444df16,a5b78db3,"We have a total number of about 280 sub-folders, let's take a peak into the folder and try and understand what those sub-folders are.",a7818ef5,0.08571428571428572
4430,6b65d81a5743dd,5a8091eb,* 2.1 Import libraries needed,4080a2d2,0.08571428571428572
4433,f3c6048d1058e3,7f7378fa,"#### Let's start with feature extraction.
<a id = 3></a>
<h1><font color = MidnightBlue>Feature Engineering</font></h1>
<hr style=""width:100%;height:1.2px;border-width:0;background-color:silver"">

### Indirect features:

- count of sentences
- count of words
- count of unique words
- -count of letters
- count of punctuations
- count of uppercase words/letters
- count of stop words
- Avg length of each word",1d9056b0,0.08620689655172414
4436,00001756c60be8,77e56113,"**Устанавливаем значения, чтобы везде был одинаковый шрифт и размер**",945aea18,0.08620689655172414
4441,1750367e54f407,f89befec,# Prepare the training and validation data generators,a8e655b2,0.08620689655172414
4442,ef6d1e959a873e,1b4d5ba8,"Its generally a good idea to combine both train and test data sets into one, perform feature engineering and then divide them later again. This saves the trouble of performing the same steps twice on test and train. Lets combine them into a dataframe ‘data’ with a ‘source’ column specifying where each observation belongs.",f11a1f43,0.08620689655172414
4444,1cd8be6e679620,75a91106,## Library Imports,3ce15a43,0.08620689655172414
4445,a1ba5ffd30dbde,e132acdd,"#### Matrix column entries (attributes):
- name - ASCII subject name and recording number
- MDVP:Fo(Hz) - Average vocal fundamental frequency
- MDVP:Fhi(Hz) - Maximum vocal fundamental frequency
- MDVP:Flo(Hz) - Minimum vocal fundamental frequency
- MDVP:Jitter(%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP - Several measures of variation in fundamental frequency
- MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude
- NHR, HNR - Two measures of the ratio of noise to tonal components in the voice
- status - The health status of the subject (one) - Parkinson's, (zero) - healthy
- RPDE, D2 - Two nonlinear dynamical complexity measures
- DFA - Signal fractal scaling exponent
- spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation",48e57546,0.08620689655172414
4446,84127ade6fde87,e2f6992a,"Deep learning has taken the field of natural language processing (NLP) by storm, par-
ticularly using models that repeatedly consume a combination of new input and previ-
ous model output. These models are called recurrent neural networks (RNNs), and they
have been applied with great success to text categorization, text generation, and auto-
mated translation systems. More recently, a class of networks called transformers with a
more flexible way to incorporate past information has made a big splash. Previous
NLP workloads were characterized by sophisticated multistage pipelines that included
rules encoding the grammar of a language. 5 Now, state-of-the-art work trains networks
end to end on large corpora starting from scratch, letting those rules emerge from the
data. For the last several years, the most-used automated translation systems available
as services on the internet have been based on deep learning.",f55d05b6,0.08620689655172414
4454,91eaec994e0c6f,c77d4804,## 2.1 General Information,376aef10,0.08666666666666667
4461,2b434130adf886,dac30bd2,split the data into training and validation sets,0c4afeca,0.08695652173913043
4464,77f958b3f41a70,21f63008,"# Prepare data

Gather title and abstract from COVID19 articles and titles from news upto a maximum number of characters.
",2ad9bb69,0.08695652173913043
4466,71c3c1eab0377d,af460fe0,"Columns Age , Cabin and Embarked has missing Values in training data",52b4e360,0.08695652173913043
4468,548f961125248d,3ef880a1,"## Explore data <a class=""anchor"" id=""second""></a>",d8c5e8b8,0.08695652173913043
4471,7e89d387feb9f5,b1e12831,"#### Эти дубликаты только вносят шум, так что от них лучше избавиться.
## Оказалось наоборот!!! Не удаляем эти дубликаты и точность модели повышается!!! Дружно кекаем!!!",989e3a1b,0.08695652173913043
4474,b01ee6cb674fa3,ebfe9cb5,"## Rocket_cost

Cost of the mission, in U$ milion",a8ffd35e,0.08695652173913043
4475,90ead00a8ee283,174091db,"# Checking Answers

You can check your answers in each of the exercises that follow using the  `check_qN` function provided in the code cell above (replacing `N` with the number of the exercise). For example here's how you would check an incorrect answer to exercise 1:",612efa48,0.08695652173913043
4476,a6b9837940ee38,e5a6b9a3,"* The first column is the label, and we need to extract it.",52d2acc7,0.08695652173913043
4479,73ca9abcc2034e,448c4f58,# Read the dataframe,cec3446c,0.08695652173913043
4492,33d736abb432d0,0560f626,"## 定义辅助函数
### re是正则表达式相关的库
### re.sub(str1, str2, line) 即对line将str1替换为str2",d64052a2,0.08695652173913043
4498,98a6794067932a,ac803c6b,"**2.2 Nettoyage de la base de données**

Pour cette section, nous allons procéder à plusieurs manipulations afin d'évaluer les données se trouvant dans le fichier auquel nous avons eu accès. De cette manière, nous allons pouvoir observer si les données étaient adéquates afin de procéder à nos analyses ou si des modifications devront être apportées à celles-ci.",08600fe2,0.08737864077669903
4499,3dd4294f903768,ca452c70,"We can see that there are text columns, categorical columns, and numerical columns.
Let's take a deeper look at our columns properties.",0d89d098,0.0875
4502,254cccd5145725,8d2d0f1f,# Part I : Exploratory Data Analysis,a49b4037,0.0875
4504,c80939c7c626cf,e0c7aa6b,# There are some missing fields in test data,b9ac31e2,0.08759124087591241
4506,e03eb63c1f725d,8cbd8d44,"<a id=""data""></a>
<font size=""+2"" color=""blue""><b>Cleaning data</b></font><br>",e204b7e3,0.08771929824561403
4509,30fdc4a6e3c1db,65933f82,Let's look at the unique states in the sales dataset,6111ddee,0.08771929824561403
4513,54004b32784b68,de8479b7,# EDA,27213ca9,0.08771929824561403
4515,9e27af2600925c,16483458,"We added ""_orig"" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).

Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. ",9b556435,0.08771929824561403
4520,63b44c85e32c1f,26d9f95e,"As you might have already guessed, x[0] = x[-2], x[1] = x[-1]. This concept can be extended towards lists with more many elements.",fb9b9562,0.08783783783783784
4530,8f50c9c16db95f,2dc95fbc,"# Kickoff as a special team play <a id=""kickoff""></a>

**Special teams** are units that are on the field during kicking plays. While many players who appear on offensive or defensive squads also play similar roles on special teams (offensive linemen to block or defensive players to tackle), there are some specialist roles that are unique to the kicking game([1](https://en.wikipedia.org/wiki/American_football_positions#Special_teams)).

In a special play, it usually has 4 play types - those are 
* Kickoff
* Punt
* Extra Point
* Field Goal

**Kickoff** is the most commont one in terms of number. By definition in NFL official website, It is a kick that puts the ball in play at the start of each half, at the start of overtime, after each try, and after a successful field goal([2](https://operations.nfl.com/learn-the-game/nfl-basics/terms-glossary/)). Because of its popularity, I'm interested to dig it further so that we can know how to evaluate the performance of a kicking team.",26cc763a,0.08823529411764706
4532,156bbcff05dcea,4b70827a,All the datatypes have been inferred correctly.,66ad1fe9,0.08823529411764706
4534,3d905ce4828057,00a80f20,"# **BUSINESS PROBLEM**
**An e-commerce site wants to make a forward-looking projection for customer actions according to the CLTV values of its customers. Is it possible to identify the customers who can bring the most revenue within 1-month or 6-month time periods with the given dataset?**
",5b006cc3,0.08823529411764706
4536,1d1598b6fa2aa7,d84ac152,"#### Difference between ```express``` and ```graph_objects```

**Plotly Express** is the easy-to-use, high-level interface to **Plotly**, which operates on a variety of types of data and produces easy-to-style figures. 

If **Plotly Express** does not provide a good starting point, it is possible to use the more generic, for example ```go.Scatter``` class from ```plotly.graph_objects```. ```go.Scatter``` have more opportunities for styling and alignments plots.",e066accf,0.08823529411764706
4537,71b75664517244,50abec26,"Cool, there is 566 row with 12 features we can explore here.",fc905af5,0.08823529411764706
4538,842547b2def18c,8282b74f,"> ***どの特徴量が混合データ型なのか？***

同じ特徴量の中の数値型/アルファベット+数値型のデータ．目標を修正するための候補がある．

- Ticketは数値型&アルファベット型の混合です．Cabinはアルファベット型です．

> ***どの特徴量が欠損値/誤植を含んでいるの？***

これは，大きいデータセットに対してレビューを行うことは大変ですが，より小さいデータセットからの少量のサンプルの場合は，レビューを行うことで「どの特徴量が修正を必要とするのか」が即座にわかります．

- 代理名/省略名のために使われるタイトル，丸括弧，クオーツを含む名前を記述するための方法がいくつかあるため，Nameは欠損値/誤植を含んでいます．",b8efde6d,0.08823529411764706
4540,52cfd66e9ec908,0c55950a,"Seems like this car casually handles all the normal challenges a driver faces, and that too with remarkable accuracy. Over here, we are tasked with somethign to faciliate this sort of thing - **predicting the motion of extraneous vehicles and based on that, predicting the motion path of an AV.**  To predict the motion of these extraneous factors, there are many approaches which I shall discuss later, but for now let's dive in.

Here's a brief FAQ section about the dataset and all it entails:

**What is the structure of the dataset?**<br>
The dataset is structured as follows:
```
aerial_map
scenes
semantic_map
```

where each scene contains roughly a minute or so of information about the motion of several extraneous vehicles and the corresponding movement of the AV.

Under scenes, we have:
```
sample.zarr
test.zarr
train.zarr
validate.zarr
```

Now this ZARR format is a little bit interesting, as I am willing to fathom a guess most of the participants have never worked with these. Fear not, for they are very much interoperable with NumPy and the Lyft Level 5 Kit also gives us easy ways to handle the processing of the data. Of course, there also might be a few ways to use a Pandas DataFrame in the process, which leads up a road for LightGBM.

The train.zarr contains the agents, the mask for the agents, the frames, the scenes and the traffic light faces, which I'll go into more depth later.",c74adcdf,0.08823529411764706
4545,a0a5baa6c7e12a,8a0cae40,"<img src=""https://raw.githubusercontent.com/gvyshnya/tab-dec-21/main/AutoViz_Plots/Cover_Type/Dist_Plots_target.png"">",551d41de,0.08823529411764706
4548,c9b4e282e4e2c1,276fcedc,"Let's check if there is any NaN value.

",f44d339f,0.08849557522123894
4551,2ada0305b68956,aef5cbaf,### 12. Palette = 'CMRmap_r',133e26f4,0.08857142857142856
4554,9ceb7278784462,e4857ce9,"# <a id='5'>3.2 Correlation Matrix </a>

* A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables.<br>
* A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.<br>

* **There are three broad reasons for computing a correlation matrix** <br>

* To summarize a large amount of data where the goal is to see patterns. <br>

* To input into other analyses. For example, people commonly use correlation matrixes as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise. <br>

* As a diagnostic when checking other analyses. For example, with linear regression, a high amount of correlations suggests that the linear regression estimates will be unreliable.<br>",3768a567,0.08870967741935484
4555,d58491f2896fc1,81f2ae9d,"
<center><img src=""https://i.hizliresim.com/fmck43j.png"">",514bfdff,0.08888888888888889
4559,6fad63bfd45ef9,72e49065,# Helper functions,b3c6f1d6,0.08888888888888889
4560,c8bf959b9608cf,09a8a2b8,"#### Defining the shape of the generated image. Let's make height as 400, and width according to the width:height ratio of the content image. ",155e3672,0.08888888888888889
4562,d96e03a9e7c030,2a7c6d09,"# Now, let's take a look at how those visualizations were produced
## Step 1: Create the dataset
First we will have to create the functions that scrape data from both the New York Times and NY Department of Education. Kaggle doesn't currently support enable internet access within its kernels, but this code is accessible through [my github project found here](http://github.com/b-o-l-l-a/kaggle-passnyc). To explore this data set within this kernel, I have uploaded it into the `../input` folder.",d2b72ced,0.08888888888888889
4566,4fd4b6a80d40e3,7ed20ef5,"## A Perceptron Specifying Bias

![image.png](attachment:image.png)",f6913cc3,0.08888888888888889
4567,3597174a998d4d,f2287c54,"**For hotels, the increase of total number of booking and sum of all lodging transactions is good, but the high ratio of booking's cancelation is the problem.**

Next, I'll analyze variables related to the cancelation.",276892ed,0.08888888888888889
4568,42e0005bed28aa,3fc70205,> <h3> Extracting Cataract & Normal information from the Dataset </h3>,5616d451,0.08888888888888889
4571,49ac6594c8f5cf,f261074d,**Correlation Heatmap**,6f19f28a,0.08888888888888889
4573,b0c2805cd5c087,0c578b22,Image payforsuccess.org,0446f327,0.08888888888888889
4575,fdc9f4863744b1,05602a3f,## Import the Data,b4529365,0.08904109589041095
4577,6a80f915608fc2,71c0ae83,"### Read the files and do basic feature defining and processing
Much of the feature processing/adjusting is done in this one code cell to prevent it getting out of sync.",636938eb,0.08928571428571429
4580,f13534449a3750,cf9c3eca,"<a id=""subsection-one-1""></a>
## Libraries
Importing all the libraries used in the notebook ",8b7f3332,0.08928571428571429
4581,585c280865b46e,f34567dc,# Load count matrix and convert it to sparse matrix csr_matrix,4d6056f1,0.08928571428571429
4582,3cd78d8d6d56e4,91b77a12,## Train / Val Split,9f632e94,0.08928571428571429
4584,e19e307b3fd188,173bf217,#### Basic description,2173955b,0.08943089430894309
4588,21413205980558,81ece217,"# A total of 11162 rows and 17 columns of data are defined as follows:
# 共有11162行和17列数据，定义如下：",84197de0,0.08955223880597014
4589,20b372b6e4e276,8774b05c,"## 3.1. My upgrade of parameters <a class=""anchor"" id=""3.1""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.08955223880597014
4591,80ad12f326ab70,ca10a90f,#### Data overview and Preprocessing,da404a16,0.08974358974358974
4599,312135b445bd23,1e2527b9,"## ETL
For each article we did the following:
1. Parsed its full text using [scispacy](https://allenai.github.io/scispacy/) and split it into sentence. The sentence segmentation part was done using Microsoft's [BlingFire](https://github.com/microsoft/BlingFire) library since we've noticed that scispacy had difficulties to split some text into sentences and kept a very long text. The Code for sentence segmentation is available in [blingfire_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/blingfire_sentence_splitter.py) and [common_sentence_splitter.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/common_sentence_splitter.py).
2. Cleaned the text by normalizing non ASCII characters, fixing contractions, removing URLs, removing punctuations, removing stop-words, etc. The cleaning code is available in [cleaning.py](https://github.com/Hazoom/covid19/blob/master/src/nlp/cleaning.py).
3. Transformed the sentence to contains meaningful bi-grams and tri-grams. Detailed explanation below.
4. Created a metadata CSV file such that each row contains a sentence, its cleaned version, the section it came from (abstract, body) and the article metadata it came from. The code is available in [preprocess.py](https://github.com/Hazoom/covid19/blob/master/src/preprocessing/preprocess.py).",8ced381f,0.0898876404494382
4600,83df814455f06c,c6d06dc5,"## **5.1 Information gain** <a class=""anchor"" id=""5.1""></a>

[Table of Contents](#0.1)


By using information gain as a criterion, we try to estimate the information contained by each attribute. To understand the concept of Information Gain, we need to know another concept called **Entropy**. 

## **Entropy**

Entropy measures the impurity in the given dataset. In Physics and Mathematics, entropy is referred to as the randomness or uncertainty of a random variable X. In information theory, it refers to the impurity in a group of examples. **Information gain** is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. 

Entropy is represented by the following formula:-


",c9cff71a,0.09
4604,ba4b3bd184acbb,854b2ceb,"#### List of Dictionaries

For more less structured data, a list of dictionaries may be more appropriate. 

Each dictionary represents a row and each key in the dictionary represents a column.

The dictionaries may have any number of keys and they are not required to match.",0f5de724,0.09022556390977443
4609,4ae464582bac51,db909ff0,## Data Dimesion,ca6a52ce,0.09090909090909091
4612,f06fd8f5916431,9d1387f0,"# Setup

1) You need to add your own saved model to a dataset and include it to this notebook. I am not making by model public so as not to make it too easy for everyone, but you can use the above referenced notebook to train your own model.
2) Edit the code to use the name of your own trained model
3) make sure internet is turned off
",854fa433,0.09090909090909091
4614,2cb457b60dd246,04072675,### Create a stratified val set,339367df,0.09090909090909091
4615,45cf7099ebd023,c9c6e9fe,"This notebook is basic on the amazing notebook of @gaborfodor: [Summary - Covid19 Global Forecasting Challenges](https://www.kaggle.com/gaborfodor/summary-covid19-global-forecasting-challenges)

**My upgrade**: I show all ranking of all 665 participants (not Top20 only) with PrivateLeaderboardRank > 0.",0d292462,0.09090909090909091
4618,3a15bac33f2a74,fc060980,**Adding and Extracting new Features**,25204b71,0.09090909090909091
4620,eb3aeb8ad87ea0,878c16fe,### **Helper Functions to plot data and decision boundary**,2301252e,0.09090909090909091
4623,f166950fa915f8,a63a5e66,### Read Dataset,a7f6ca5e,0.09090909090909091
4626,d83e5b44d1b80d,41a5cad4,## Country participated in Kaggle Survey,62845930,0.09090909090909091
4627,1691c9f2c2f656,b43b82f5,# Install Libraries,70433e76,0.09090909090909091
4628,90964081c7faab,71bc845a,## 2) Load Data,b423b0c3,0.09090909090909091
4633,0cb9adc158b705,7d49ece7,"We are setting the seed for reproducibility. 

Next, we will initialize some path (& other) variables (for use throughout the notebook)

PS: I will be using my version for the dataset. I have resized all the images so that its much faster to load them into the RAM. You can find the dataset [here](https://www.kaggle.com/ankursingh12/resized-plant2021). ",3abf056e,0.09090909090909091
4635,241cf32abb22d8,53d10fab,## Checking for Missing Values,47157066,0.09090909090909091
4637,32e04b08ff52eb,a3857835,Organizing Data,8d5b86e0,0.09090909090909091
4640,90691864eb68c7,b4bc22ef,"The section code in above, You can find some information. 
There are 19 features in the House Price data. 
It has information about 506 Houses. 
2 features are integer type
3 features are object type
14 features are float type. 
 
Some features contain missing value.",3555ef9b,0.09090909090909091
4642,adf419444a59df,3b496bb9,First we will define a basic CNNBlock for our architecture.We will be using leaky ReLU as activation function. We will add a batch norm to our model although original yolo didnt have but it will help training and accuracy so we will include it.,3a275e7f,0.09090909090909091
4649,b241b847319d13,f11342d8,"A TFRecord file stores your data as a sequence of binary strings. Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk. However, pure performance isn’t the only advantage of the TFRecord file format. It is optimized for use with Tensorflow in multiple ways. One use being, it makes it easy to combine multiple datasets and integrates seamlessly with the data import and preprocessing functionality provided by the library. Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed. ",0fb698f0,0.09090909090909091
4650,132fa9714f2046,dfd58feb,** Import matplotlib.pyplot as plt and set %matplotlib inline if you are using the jupyter notebook. What command do you use if you aren't using the jupyter notebook?**,3bb1775f,0.09090909090909091
4654,2f964d08c25d93,86ab8786,"There are 2 project folders, `final_project` and `review` with 4 csv files in each the current version of the dataset.

There are some learning resources in `slides` and sample notebooks in `notebooks` folder as well.
",1f2e4468,0.09090909090909091
4657,a0b321057e7402,bd7fd93a,# **Exploratory data analysis**,5f73fb91,0.09090909090909091
4658,b82610a9364f75,aad45344,# helper function to load GNSS logs as dataframe,22c70e69,0.09090909090909091
4668,0475899eec1ffe,61ec4371,"Then we load our data set
",d825dc37,0.09090909090909091
4670,5a8c553e21c70f,5949f5b9,Time feature is dropped.,9ebd9d8f,0.09090909090909091
4671,4b64dc653fb7eb,f53a7485,Reading training and test data from CSV file and saving as Pandas' Dataframe,57675cc2,0.09090909090909091
4672,b42180a6a5b42f,88b2a0c4,"# Análise <a id='analise'></a>
Primeiro iremos validar os dados do síte do Ministério da Saúde, os quais apresentam óbitos por data de notificação.

Posteriormente, iremos conferir se há diferença entre a data de notificação e a data efetiva do óbito.
Caso haja, iremos alocar os óbitos das datas de notificação, para a data efetiva do óbito, fazendo assim com que os dados apresentem uma realidade mais aproximada do cenário epidemiológico 
",987cea5f,0.09090909090909091
4678,1466e61d45b718,64fabf77,"## Exploratory Analysis
To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)",b062d92b,0.09090909090909091
4680,dd02a9b545f742,376f4f64,"# Quick Idea

- Get dataset
- Get trading models
- Get a trading model tuner
- Get a trading model optimizer
- Get a returns optimizer
- Get a submission file",7116cd2d,0.09090909090909091
4684,ae058c3f1439c3,eae4d950,"<img src=""https://www.techdotmatrix.com/wp-content/uploads/2017/11/Grow-your-technical-skills-with-Google.png"" width=""800px"">
",965da99d,0.09090909090909091
4687,347c7b0f48c53f,c4e107c4,"Text summarization is a subdomain of Natural Language Processing (NLP) that deals with extracting summaries from huge chunks of texts. There are two main types of techniques used for text summarization: NLP-based techniques and deep learning-based techniques. In this article, we will see a simple NLP-based technique for text summarization. We will not use any machine learning library in this article. Rather we will simply use Python's NLTK library for summarizing Wikipedia articles.",c58305ba,0.09090909090909091
4688,450fda47b03baa,6fa10638,"Veri çerçevesinin ilk 5 gözlemini görüntüleyelim.

",62c04adb,0.09090909090909091
4691,917957c6c4065f,6ed76e02,데이터의 절반 정도가 중복값이네요. 확인해보겠습니다.,55b8ed68,0.0915032679738562
4692,ee23a565163388,52824d9c,# **Importing Libraries and Dataset**,88aacbc4,0.0916030534351145
4694,37b09262279764,4f60a4f7,"There are <b>null</b> values in the columns <b>Age</b>, <b>Cabin</b> and <b>Embarked</b><br>
There are <b>seven numerical</b> columns<br>
There are <b>five categorical</b> columns<br>",37c4c417,0.09166666666666666
4695,bd380b97b5c894,209b91e7,## sex,66f2562a,0.09174311926605505
4701,52ee792e228d54,6493c1f5,"### Hmm.. Distribution looks okay except the Experience attribute, for which minimum it is -3. Can anybody be negatively experienced 🤔",5096094e,0.09210526315789473
4709,f2f2db16a2f86c,48c68319,### **EDA**,ffc6a115,0.09230769230769231
4713,3cb96bd8eb364b,bd95e7cc,"## Data Explore
",3157af7e,0.09230769230769231
4715,a8c042af6b7245,4e3df435,"We indeed see the following 
* binary variables
* categorical varibales of which the category values are integers
* other variables with integer or float values
* variables with -1 represening missing values
* the target variable and an ID variable",2487ac62,0.09230769230769231
4716,03048e86a6d806,cac8f60c,"Seen from the plot above, most of the respondents are men, but let's see which countries have the highest proportion of women as the respondents.",1285c231,0.09230769230769231
4717,c4386b8a01d66e,c6840c46,### PH,dc732bf5,0.09243697478991597
4719,9ad9a97e628bfa,0cac1ff9,"Null Value가 Cabin의 경우 약 77프로로 상당한 양의 데이터가 유실 (혹은 없음) 되었음을 알 수 있고, age의 경우 약 20프로임. Embarked의 경우 0.22프로로 미미한 수준임을 알 수 있으며 나머지 column엔 모든 데이터가 존재. ",0a7e1136,0.09259259259259259
4720,c6f8ff61a5fa87,211f6bee,"## Outline
---
* [**1.Read Data**](#1.Read-Data)
* [**2.Simple Exploration**](#2.Simple-Exploration)
* [**3.Feature Engineering**](#3.Feature-Engineering)
* [**4.Sanity Check**](#4.Sanity-Check)
* [**5.Data Transformation**](#5.Data-Transformation)
* [**6.Model Training**](#6.Model-Training)
	* [**1.nuSVR**](#1.nuSVR)
	* [**2.SVR**](#2.SVR)
	* [**3.BayesianRidge**](#3.BayesianRidge)
	* [**4.LightGBM Regression**](#4.LightGBM-Regression)
	* [**5.CatBoost Regression**](#5.CatBoost-Regression)
* [**7.Prediction**](#7.Prediction)
* [**8.Stacking using lightgbm**](#8.Stacking-using-lightgbm)
* [**9.Total Analysis of all model**](#9.Total-Analysis-of-all-model)
* [**10.Best Model**](#10.Best-Model)",3eea586b,0.09259259259259259
4722,ac04ba639d1c93,194fb086,## All imports used in this kernel,748059d5,0.09259259259259259
4729,063a35f644e3c5,e2c1432d,### number of samples in each year,1c30fb0a,0.09278350515463918
4730,8ec771f5600a61,07788c1d,# Majority of people not survived,48364c1f,0.09278350515463918
4733,d96642860ab3dd,71faf57d,### 1.1 Survived Feature,98419d48,0.09302325581395349
4742,743ae010f5e875,845e4a2d,# Import,02c54445,0.09302325581395349
4748,67b7354e96113a,f2a261cc,**Finding the missing nature of the data**,dca94250,0.09333333333333334
4750,7e1da639035ac5,effdf2af,# <a id='4'>4. Data preparation</a>,120b6c23,0.09333333333333334
4753,2bd6c370695ea7,28b1207a,## Aggregate sentiment data and metadata,cbe6aec8,0.09333333333333334
4758,5ce12be6e7b90e,94947560,"## `int`
The **`int`** type is for integers:",c0ab62dd,0.0935672514619883
4761,ff3a8ce61fab6a,c7b5fc79,"
Let's start with Constant type.

# 3. Constants

We can defiend our constants using **constant** keyword.",9afe1654,0.09375
4763,fdbbd573ba31c2,6fce7d6f,## Missing Values & Correlation,f7c28d74,0.09375
4766,51a46d0a7597f5,4e3e5a3d,**Data Look & Feel**,e9e25b17,0.09375
4767,117fc0956643d0,62c407cd,"## Steps

0. [Install All Required Libraries](#step0)
1. [Filter for Only Question-Related Articles Using LDA](#step1)
2. [Load pretrained ALBERT model](#step2)
3. [Extract Excerpt From Abstract Using Fine-Tuned ALBERT](#step3)
4. [Get Top 10 relevant articles](#step4)
5. [Run over all 10 COVID-19 questions](#step5)
6. [Plot Confidence Score for Top 10 relevant articles](#step6)
",68cef9fd,0.09375
4768,0932046e1f485d,0fbd363d,## <a id=missing_data_check>Missing Data Check</a>,218cc7a3,0.09375
4771,bd0e173abb7b52,4a97f821,"Unique values of all features (for more information, please see the links above):
- `age`: continuous.
- `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
- `fnlwgt`: continuous.
- `education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
- `education-num`: continuous.
- `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
- `occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
- `relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
- `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
- `sex`: Female, Male.
- `capital-gain`: continuous.
- `capital-loss`: continuous.
- `hours-per-week`: continuous.
- `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.   
- `salary`: >50K,<=50K",9bce3b0d,0.09375
4773,c5fef7cc592736,27f450bf,## Data Preparation,d21dc2c1,0.09375
4774,9daf8b4a46725e,99a519c5,### Getting Data,7d9cc411,0.09375
4779,726833f92fb87a,316e5ad7,"**Dataset information:**
#### bank client data:
1 - age (numeric)<br>
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br>
5 - default: has credit in default? (categorical: 'no','yes','unknown')<br>
6 - balance: bank balance <br>
7 - housing: has housing loan? (categorical: 'no','yes','unknown')<br>
8 - loan: has personal loan? (categorical: 'no','yes','unknown')<br>
#### Info related with the last contact of the current campaign:
9 - contact: contact communication type (categorical: 'cellular','telephone')<br>
10 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>
11 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>
12 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>
#### other attributes:
13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>
14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric;-1  means client was not previously contacted)<br>
15 - previous: number of contacts performed before this campaign and for this client (numeric)<br>
16 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>

Output variable (desired target):<br>
17 - deposit - has the client subscribed a term deposit? (binary: 'yes','no')<br>

",7dc5e1b6,0.09395973154362416
4780,5f32117bcd5255,70e7eb32,### GETTING COMMENTS FUNCTIONS,85882abf,0.09395973154362416
4789,b01ee6cb674fa3,3ff4ab91,"datatype is an object, and it is better to work with a float",a8ffd35e,0.09420289855072464
4790,0e2a23fbe41ca9,767fd01c,There's no overlapping cards in the training and testing datasets.,64e4762c,0.09420289855072464
4791,2ada0305b68956,053766b1,### 13. Palette = 'Dark2',133e26f4,0.09428571428571429
4792,510b8303776bb6,0ab1a86d,As we can see there are a lot of null values so we need to replace these null values.,18080db8,0.09433962264150944
4801,07bfec3562f9b3,5620e9e4,## Text #1,327e7d5b,0.09433962264150944
4806,e4525eb0c96f28,539e4f30,"#### Cleaning sales data

Before any modification of the sales data, the dataset stored in the **df** dataframe stored data in two ways. 
- If the data ONLY had the total sales number, then the value was stored in the **Total_Shipped** column. In rows with a value in Total_Shipped, all other sales columns are NaN.
- If the data had regional sales data, then the values were stored in NA_Sales, JP_Sales, PAL_Sales, Other_Sales, and the total of the 4 columns are stored in Global_Sales. In rows with regional sales data, Total_Shipped is NaN.

The data had no rows where Total_Shipped and Global_Sales both held non-NaN values - either one held a value with the other NaN, or both were NaN.

After removing the inconsistent regional sales columns, we are left with two redundant columns (Total_Shipped and Global_Sales). Since they serve the same purpose, we decided to store sales values from the two columns into one new column, called **Total_Sales**. After storing the values in Total_Sales, we removed the redundant Total_Shipped and Global_Sales columns.

New column:
- Total_Sales - Stores the total sales of ""Total_Shipped"" and ""Global_Sales"" in one column.",2093a1f1,0.0945945945945946
4809,169177b6e9edea,d479839b,<h2><b>Feature Engeneering,ca42152f,0.09473684210526316
4811,840534f2908a9c,71275e35,"*We can see that testing data lack one column : 'fare_amount', which is what we are going to predict.*",8081c3cc,0.09473684210526316
4817,565ad413cd802f,936d3361,The image files are named `<image-id>.png` and can be found in the respective `train` and `test` folders.,397b074e,0.09523809523809523
4818,04bac111ffbe9c,72416a2e,##### VISUALIZING THE DATA BEFORE PROCESSING THE DATA,82576b17,0.09523809523809523
4820,adb8441ad28019,9975370e,"<div class=""alert alert-success"">
    <h1 align='center'>Loading up the data</h1>
</div>",d89de993,0.09523809523809523
4822,76c8afe761adc1,88356db7,# Setup,8258944b,0.09523809523809523
4823,2473d004f92592,7253b47b,**Import SMS Data**,18d3b6ee,0.09523809523809523
4831,2d40f383473fa4,fe3070db,"Here's the magic of Pandas Profiling, make a good EDA just using the function `ProfileReport`, it give variable distribution, check of null values, variable types, correlation matrix and other useful things.<br>
Applying it on Training data.",1da1eff0,0.09523809523809523
4838,55339ceb40d5e9,dadf82cb,"# Have a Bird Eye View
* read articles https://www.visual-design.net/post/semi-automated-exploratory-data-analysis-process-in-python for a comprehensive explanation
* info() and describe ()",d0f687dd,0.09523809523809523
4843,bbaa07ad21cf4e,c6b5aec1,### Class distribution,3ab6b254,0.09523809523809523
4846,7a058705183598,9bd4765b,Basic Visualization,b0ead917,0.09523809523809523
4848,9456df44ab9308,534ab11a,"## Repro [Github](https://www.github.com/ildoonet/tf-pose-estimation)
Install tf_pose and pycocotools",a8e94298,0.09523809523809523
4849,87e94f864d74be,e7c82f6f,"# <span style=""font-family:serif; font-size:28px;""> 2. Visualize missing values </span>
",294bfe9f,0.09523809523809523
4855,e04e5204572e7e,4fd3d039,### *Necessary Imports*,6c888be9,0.09523809523809523
4857,06ecf7a304c309,fb78ca48,"하지만 여기서 핵심 질문은 '어떤 논리나 규칙에 의해 B가 A와 L을 통해 표현될 수 있는가' 입니다.
또 다른 말로 B, A, L간의 방정식이 어떻게 되는지를 알아야합니다.

고정된 방정식이 없어도, 비지도 학습에 의해 가능한 최상의 방정식을 구할 수 있습니다.
간단히 말해 학습 과정은 A와 L의 형태로 B를 변환하는 규칙/방정식으로 정의 할 수 있습니다. 
오토인코딩 관점에서 이 과정을 이해해봅시다.

hidden layer가 없는 오토인코더를 가정해봅시다.
x1, x2는 낮은 표현 d로 바뀌고, 이 d는 후에 다시 x1, x2로 투영됩니다.(변환됩니다.)

![simple autoencoding](https://i.imgur.com/lfq4eEy.png)

**Step1 : 잠재 공간에 포인트 표현하기**

A와 B를 좌표로 나타낸다면 다음과 같이 할 수 있습니다.

- Point A : $(X1_A, X2_A)$
- Point B : $(X1_B, X2_B)$

이제 이를 잠재 공간에서는 다음과 같이 표현할 수 있습니다.

- A : $(X1_A, X2_A) \rightarrow (0, 0)$
- B : $(X1_B, X2_B) \rightarrow (u1_B, u2_B)$

$ (u1_B, u2_B)$는 다음과 같이 기준점과의 거리로 표현할 수 있습니다.

$u1_B = X1_B - X1_A$
$u2_B = X2_B - X2_A$

**Step 2 : 거리 d와 각도 L로 포인트 표현하기**

이제 $u1_B, u2_B$를 거리 d와 각도 L로 표현할 수 있습니다.
각도 L만큼 기존 축으로 회전한다면 L은 0이 될 것입니다. i.e 

$(d, L) \rightarrow (d, 0)$ (after rotation)

이제 이 데이터가 출력이 되고, 낮은 차원에서 표현한 입력 데이터입니다.

모든 계층의 가중치와 편향이 있는 신경망의 기본 방정식을 생각하면 다음과 같은 과정이 인코딩이라고 생각할 수 있습니다.

= $((d, 0) = W \cdot (u1_B, u2_B)$

==> (encoding)

W는 hidden layer의 가중치 행렬입니다. 이후 디코딩 과정은 인코딩 과정을 반대로 생각하면 됩니다.

=> $(u1_B, u2_B) =  Inverse(W) \cdot (d, 0)$

==> (decoding)


### Different Rules for Different data

모든 유형의 데이터가 똑같은 규칙이 적용되는 것은 아닙니다. 예시로 앞에 데이터에서는 1차원 선형 데이터 매니폴드에 투영하면서 각도 L을 없앴습니다. 하지만 정확하게 투영 불가능한 데이터에 대해서는 어떨까요?

예시로 다음 데이터를 살펴봅시다.


",714de627,0.09523809523809523
4866,7f74a04ae75792,23723887,### Check data for duplicate rows,d01e91da,0.09558823529411764
4873,738bfced935b69,84ce818f,drop duplicated rows and reset index of data,2d3c592d,0.0958904109589041
4874,91473a39b85068,0d12452a,### Loading Data ,6e3d91c2,0.0958904109589041
4875,1eb62c5782f2d7,e28cbe6e,###  A. z-score = 1.23,bb69f147,0.0958904109589041
4877,6f1481148352e9,6f624594,**Incorrect data in the date**,7cfbdb8f,0.09615384615384616
4885,44f6a002ecd033,e71d8306,"We have some missing data in LoanAmount, Loan_Amount_Term and Credit_History that will have to get cleaned up. ",70bbe106,0.09615384615384616
4888,fe7360cddc13e5,28bdd76b,"Alfa, sabit bir zaman aralığında müşterinin yaptığı ortalama işlem sayısıdır.

Ortalama 4.3 alışveriş yaptıysa poisson dağılımında alfa = 4.3 tür.

Poisson dağılımı, alfayı kullanarak müşterinin t zamanda x kez satın alma olasılığını verir bize. ",8979e423,0.09649122807017543
4890,6cade0b6a41ba2,57bf1be2,# 3. Data Cleaning,e6110293,0.09649122807017543
4892,b90ef792fd07c2,7adc309f,# Exploratory Data Analysis,5e2762eb,0.0967741935483871
4898,098fedfcd07456,3e2819ff,# Importing Dataset in train and test .,052ece26,0.0967741935483871
4899,16862cb02d73d5,3b614a08,"The data here is for a **use case**(eg revenue,traffic etc ) is at a day level with 12 metrics.We have to identify first if there is an anomaly at an use case level.Then for better actionalbility we **drill down** to individual metrics and identify anomalies in them. 
",d7ffa1a6,0.0967741935483871
4902,921fff7d3040db,9a5f2c45,# 2. Load emails,5f36ced9,0.0967741935483871
4906,b05ee1ea1c8269,989b768e,"# Variants of Concern synonyms
https://covariants.org/

* omicron: B.1.1.529
* alpha: B.1.1.7 
* beta: B.1.351 
* gammma: P.1 
* delta: B.1.617.2 
* kappa: B.1.617.1 
* epsilon: B.1.427, B.1.429
* eta: B.1.525 
* iota: B.1.526
* lambda: C.37
* mu: B.1.621",19e4d303,0.0967741935483871
4912,56e58d53ac9c57,11dea220,let us get some information about our dataset first,90e2ab8e,0.0967741935483871
4920,98a6794067932a,61243fc9,"Pour la cellule se trouvant ci-dessous, elle sert à identifier le type de données se trouvant dans chacune des colonnes de la base de données. Il est possible de constater que la majorité des données sont sous forme ""object"". Ce type de données pourrait causer un problème éventuellement lors de nos analyses effectuées à partir des colonnes ""Order date"" et ""Ship date"". Les données se trouvant dans ces colonnes devraient plutôt être présentées sous forme de ""Datetime"" afin que nous puissions extraire des délais par la suite. De plus, les données se trouvant dans la colonne ""Postal code"" pourraient porter problème éventuellement étant donné qu'elles sont représentées comme des ""Float"". Les codes postaux américains sont constitués d'une suite de cinq chiffres et ne devraient jamais être considérés comme des nombres à virgule.",08600fe2,0.0970873786407767
4923,81712ee7510ac5,b5ace777,**Variable Assignment**,c4685e79,0.09714285714285714
4924,fc8e0042411c46,01b90fd0,"Now we will take care of null values in each column one by one.
",af476c2a,0.09717868338557993
4927,fdc3afd309b850,f5b15df9,"So, we ended with a data frame that has 3114 rows and 51 columns.

",966bde38,0.09722222222222222
4929,593d1d3d1df05a,a369f5b5,# Maximize Contrast,bc682ffe,0.09722222222222222
4937,514d8de15cb7ef,4b1af3f7,"<p> Now we need to divide the points and price of the wines in specific groups so that we can classify the data easily. So here we classify the points in 4 groups -- Average, Good, Great and Perfect and the prices in the six categories 0-10, 10-20, 20-30, 30-50, 50-100, Above 100. </p>",cfe111b2,0.0975609756097561
4938,dbccf99c49570f,e6c7f6ed,## Analysis,c20fc09e,0.0975609756097561
4942,0e09587faffa8f,167f0cd9,## II. Reading Data,0d563d61,0.0975609756097561
4948,74a03887600114,a7745491,We will going to take two dataset first one is movie dataset in which we will have movie names and the second dataset will have ratings and userID,c0ffb2f0,0.0975609756097561
4950,fd4017c1514157,4349354a,"> ### **If you find this notebook useful, do give me an upvote.👍**",fd8f0896,0.0975609756097561
4954,52cfd66e9ec908,a38511e4,"<img src=""https://self-driving.lyft.com/wp-content/uploads/2020/06/dataset-steps-longimg.png""></img>",c74adcdf,0.09803921568627451
4955,4fa553c2b837d4,599b580e,# Building the first model,c65a23e9,0.09803921568627451
4957,71b75664517244,1f17823c,"## Best Team

As always let's start with the best team, who won the first place on every season.",fc905af5,0.09803921568627451
4958,907f08f9a2c6cf,d992fa06,### EDA,aa84c325,0.09803921568627451
4960,d0080e3a39bc5c,79896b00,Lets have a look at the distribution of classes in the dataset.,2fcde4cf,0.09803921568627451
4961,64169805aacf17,97676a47,# Installing Neural Painters and required dependencies,1f12ded0,0.09803921568627451
4962,fa02c409161192,66fae7f2,## 1.1 Defining the NN,e97077f7,0.09803921568627451
4964,842547b2def18c,e2640ff1,train_df.tail(),b8efde6d,0.09803921568627451
4966,629f2918807a9b,62ff80c3,"### Observations::

1. There are 3509 columns in 'Book Name' column,and there are records having more than 1 book ordered as well, We aslo need to seperate them as another record
2. Order Status have 3 unique values:  'Completed', 'Returned', 'Canceled'.
3. We have 99 cities in Pakistan, Which means guftugu delivered books across pakistan as well.
4. Order Date is in DD:MM:YYYY format ""'1/20/2021 17:43'"".",be56dc84,0.09803921568627451
4968,bcd7e398c4d0ec,d6f07735,"## Label
Categorical speed of adoption. Lower is faster. ",77a143f6,0.09836065573770492
4969,918040fad252ec,7e035d72,Membuat variable,966fcd8f,0.09836065573770492
4971,0858e1bb3cbaca,6317e819,"We can also use 

**.tail(*n*)**

to display the bottom *n* rows of data, default *n* = 5",78548374,0.09836065573770492
4973,601e18072783b4,810fe787,## Preprocessing on IMDb dataset,36b2b1fa,0.09836065573770492
4977,bddd799cdbbae8,0c05e146,### Cleaning the data,b44e3c08,0.09859154929577464
4978,9bcfa825c8b2e6,996dc1fa,VERİYE GENEL BAKIŞ,220f36e4,0.09859154929577464
4980,4883314a96dc34,3a982999,Merge the climbing and weather datasets into 1 dataset.,50d36836,0.09876543209876543
4981,faa8e6c8ab9246,2cba526f,describe() function gives statistical details of the dataset.,2bea1419,0.09876543209876543
4983,e9b9663777db82,abfa32ac,# Data gathering and Primary visualization,648e8507,0.09917355371900827
4984,2f47abddfd1928,9ba57ffd,"We can see that the next variables have NaN values that we will need some action:
- Age = 263 NaN values.
- Fare = 1 NaN value.
- Cabin = 1014 NaN values.
- Embarked = 2 NaN values.",ae33cc0b,0.09917355371900827
4989,30fdc4a6e3c1db,dfdb4a99,"We have information about 3 states of U.S. (California, Texas and Wisconsin)",6111ddee,0.09941520467836257
4992,fdbbd573ba31c2,862c6e82,### df_train,f7c28d74,0.1
4994,83df814455f06c,3dc8bbb7,"![Entropy](http://www.learnbymarketing.com/wp-content/uploads/2016/02/entropy-formula.png)



Here, **c** is the number of classes and **pi** is the probability associated with the ith class. ",c9cff71a,0.1
4995,a78d363403fce2,655ecb65,"# FastAI exploration

This kernel will make a small patch to fastai to enable it to read dicom files, and will then use the standard guidelines from the fastai course to explore this dataset.",0f824cb6,0.1
4996,2ada0305b68956,82b807d3,### 14. Palette = 'Dark2_r',133e26f4,0.1
4998,2ef36514eab722,ef300f6a,"Пробовал использовать аугментации из библиотеки albumentations, но их реализация преобразования точек у меня работала долго. Так что векторизовал нужные функции, стало пошустрее.  
До кучи добавил еще вот этот код горизонтального флипа: https://www.kaggle.com/sergemsu/landmark-preserve-horizontal-flip",759739cb,0.1
4999,bc058fe14d3d1b,64d593f4,"ID-테스트 세트 내에서 (Shop, Item) 튜플을 나타내는 Id\
shop_id-상점의 고유 식별자\
item_id-상품의 고유 식별자\
item_category_id-항목 카테고리의 고유 식별자\
item_cnt_day-판매 된 제품 수입니다. 이 측정 값의 월별 금액을 예측하고 있습니다.\
item_price-상품의 현재 가격\
date-dd / mm / yyyy 형식의 날짜\
date_block_num-편의를 위해 사용되는 연속 월 번호입니다. 2013 년 1 월은 0, 2013 년 2 월은 1, ..., 2015 년 10 월은 33입니다.\
item_name-항목 이름\
shop_name-상점 이름\
item_category_name-항목 카테고리 이름",d0273670,0.1
5003,8bb432d338a70b,a692b878,Preparación de los datos no completos (por la media),7aab1dfd,0.1
5004,864302b10e7730,94a31b36,# Loading the data and fetching informations,e9dd1d2d,0.1
5005,6998861ff6ff01,ed3de79f,"Now we're ready to look at some dates! (If you like, you can take this opportunity to take a look at some of the data.)",ea9e72cf,0.1
5006,892be0a523578c,9de0cdf4," **1.3** In addition, it is possible that they dont'wear the devices every day. By counting the records for each participant after filtering the invalid ones, I found that **only 14 participants had full records**, and 26 participants had at least 20 records.",b0e8d7c0,0.1
5011,675b60eaf415a6,539b27a3,* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**,68c0b725,0.1
5018,8d575f495686ab,0770901e,## Analyze data,0fc16499,0.1
5020,09bac0c221388e,419367f7,# What is Document Summarization?,bea4aa2e,0.1
5022,0d58c434c7db1e,bdca7d16,Dataframe has 8553 rows and 10 columns,517e01d3,0.1
5025,5ba4207c371899,2c027f53,Data extraction,187b1451,0.1
5026,f7436bc492474c,502bf7e2,"## Looking at the data

The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.",328fd235,0.1
5027,1011899b959f44,6761ce3f,"# Display
Oftentimes when working with datasets, it can be overwhelming. There can be what seems like an abundance of information to work with and you may want to selectively view bits and pieces of the dataset at a time - rather than the entire thing at once.

You can use the .head() function to display the top number of rows and the .tail() function for the bottom rows in a dataset. If you would like to specify how many rows, use the parenthesis () to fill in number of your choice. Otherwise, a default of 5 rows will be displayed. 

1. Examine the datasets and display the top 7 rows and the bottom 5 rows.",0b112382,0.1
5029,36c35f0a9f70f7,8e0b67ed,### Cleaning and preprocessing of data for training,67358bc7,0.1
5033,b547f0f38f7744,fc0208d9,"- image_id - the unique image ID
- width, height - the width and height of the images
- bbox - a bounding box, formatted as a Python-style list of [xmin, ymin, width, height]
- etc.",b6ba66b3,0.1
5034,62487bcd70b199,2635ea8e,# <a id='2'>2. Data Manipulation</a>,f6ae50af,0.1
5035,be616f0785c32d,00a43ac1,"Next, we counted the number of publications each drug appeared.",b78e18aa,0.1
5036,c18267b203f28a,f3a9166e,# Set up variables,09ca8efb,0.1
5039,38b79494ac749e,c7b07fa8,## Part 1: Underfitting vs. overfitting,39162a40,0.1
5040,a4f8ad33c823c5,0b63e0ae,"## Studying the Target Variable - diabetes_mellitus

From the below plot, it can be observed that class imbalance is present as there is only about 22% of patients detected with diabetes in the dataset",fcd48307,0.1
5049,b10bd75889dad9,28cc9992,"#### Lets drop columns having only 1 unique value, as it does not provide any data variance",ee00ceee,0.1
5053,1005ca950e8a81,369d6e73,"**Pour comprendre ce qu'est le ""clustering"":** Le clustering est une méthode d'analyse statistique utilisée pour organiser des données brutes en silos homogènes.  A l'intérieur de chaque grappe, les données sont regroupées selon une caractéristique commune. L'outil d'ordonnancement est un algorithme qui mesure la proximité entre chaque élément à partir de critères définis.

Pour établir l'équilibre, il minimise l'inertie à l'intérieur des classes et maximise celle entre les sous-groupes afin de bien les différencier. L'objectif peut être de hiérarchiser ou de répartir les données. En français, on emploie couramment le terme de regroupement ou l'expression partitionnement de données.",52570331,0.1
5057,8696921d9adc93,9667d6e2,"**Data Preparation**
1. Load Data",b8908b23,0.1
5069,f8bcb6d96fd560,5903533c,Lets make a quick look to our dataset,390bb0f0,0.1
5071,7dd46c750653eb,acb0a77d,# **Birth Vs Death In Moscow**,c2644713,0.1
5074,254cccd5145725,903d189f,# Importing Libraries,a49b4037,0.1
5076,c6df1e14d46188,7886ba0d,"This is based on paper named ""**XBNet : An Extremely Boosted Neural Network**""

Link : https://arxiv.org/abs/2106.05239",e43fb728,0.1
5080,bfe6c7096b1ad0,b6dd90d5,## Оценка распредения целевой переменной в разрезе других признаков,fffd95e0,0.1
5083,5ffe6aa38958a1,fc6f3d3c,What did we just import? ,11f5412e,0.1
5084,8cd6656a65e6e7,922346bc,"<h1 id=""dataset"" style=""color:blue; background:white; border:0.5px dotted cyan;""> 
    <center>Dataset
        <a class=""anchor-link"" href=""#dataset"" target=""_self"">¶</a>
    </center>
</h1>",c8e1697a,0.1
5090,9ceb7278784462,7ece6f02,* Some books have no reviewers ,3768a567,0.10080645161290322
5092,4ae6a182abac64,54ed1f25,"* **Categorical Features** : Survived,Embarked and Sex 
 Ordinal: Pclass.",418676c5,0.10084033613445378
5096,04ff2af52f147b,05414f00,"There is not as large of a disparity in this case, but it is still significant.  Finally we take a look at the survival rates by passenger class.",d5f37be9,0.10112359550561797
5097,e67925694c07d3,069fee25,so in this problem we have 307511 training data with 122 variables and  120 features (after we remove SK_ID_CURR and TARGET),83af4c4a,0.10112359550561797
5099,312135b445bd23,cad43baf,"The parsing method with scispacy (for demonstration purposes, it doesn't include our custom sentence segmantation):",8ced381f,0.10112359550561797
5100,6a80f915608fc2,8fc843b1,"### Check for any NaNs in the Train,Test data",636938eb,0.10119047619047619
5103,63b44c85e32c1f,5ce3488a,"Here we have declared two lists x and y each containing its own data. Now, these two lists can again be put into another list say z which will have it's data as two lists. This list inside a list is called as nested lists and is how an array would be declared which we will see later.",fb9b9562,0.10135135135135136
5106,e3fb4c6300cb56,648e5d9d,"### 201 values are not known or missing, shown in the second row above",8ebbdf89,0.10144927536231885
5107,b01ee6cb674fa3,73cc3878,"---
From the rocket costs, there is a value which gave me initially an annoyance, due to this value being quite large, unusual for rocket costs. 
5,000.0 or 5 billion dollars was it. Quite expensive for a rocket!
Initially I thougth it was a typing mistake or anything else, then I decided to investigate this",a8ffd35e,0.10144927536231885
5108,17a24d566ffa59,b42a2ed1,![](https://s3.amazonaws.com/nlp.practicum/pca.png),89049e56,0.10144927536231885
5110,598b6228760590,76332225,"- Port of Embarkation
- C = Cherbourg, Q = Queenstown, S = Southampton. 
- People who boarded in Southampton had a higher survival rate, and it can be seen that most of them also boarded in Southampton.",be30ab66,0.10144927536231885
5112,a1a31459abf078,500db2fd,"# Importing raw datasets <a name=""import""></a>


In this competition we've been given access to student data log from a learning app. Student activity includes watching lectures on different topics and then answering questions. Each question & lecture has some metadata associated to it, providing more details around the learning journey of a student. 

There are three primary datasets that are available in this competition. Below is the description provided to us - 

**train.csv**

* ```row_id```: (int64) ID code for the row.

* ```timestamp```: (int64) the time in milliseconds between this user interaction and the first event completion from that user.

* ```user_id```: (int32) ID code for the user.

* ```content_id```: (int16) ID code for the user interaction

* ```content_type_id```: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.

* ```task_container_id```: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.

* ```user_answer```: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.

* ```answered_correctly```: (int8) if the user responded correctly. Read -1 as null, for lectures.

* ```prior_question_elapsed_time```: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.

* ```prior_question_had_explanation```: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.

**questions.csv**: metadata for the questions posed to users.

* ```question_id```: foreign key for the train/test content_id column, when the content type is question (0).

* ```bundle_id```: code for which questions are served together.

* ```correct_answer```: the answer to the question. Can be compared with the train user_answer column to check if the user was right.

* ```part```: the relevant section of the TOEIC test.

* ```tags```: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.

**lectures.csv**: metadata for the lectures watched by users as they progress in their education.

* ```lecture_id```: foreign key for the train/test content_id column, when the content type is lecture (1).

* ```part```: top level category code for the lecture.

* ```tag```: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.

* ```type_of```: brief description of the core purpose of the lecture",66fc0f54,0.10144927536231885
5113,7e275c8d5ff2a0,765b11f4,# DATA OF INDIA (COVID),b3afcc98,0.10144927536231885
5117,a81661cc35d8d2,e15ce81b,"<font size=""3"">Data Processing and Visualization Modules</font>

1. Numpy
2. Pandas
3. Seaborn
4. Matplotlib

<font size=""3"">Machine Learning Modules</font>

1. Model Selection
2. Logistic Regression
3. Random Forest Classifier
4. Robust Scaler
5. Confusion Matrix
6. Classification Report",3331f113,0.1016949152542373
5119,c4bca5d86a38c3,35fb6382,Agrupando cabinas segun letra con la que empieza y haciendo grafica mostrando porcentaje de si se salvan los pasajeros o no,e23d297c,0.1016949152542373
5120,b9bc7dc9f582e5,10ce51a4,"All the text variables are categorical, hence we will transform them into numerical using Label Encoding.",15cc4d28,0.1016949152542373
5122,dac3c8204a2d1b,9ad70e47,Checking first 5 and last 5 records from the datasets,b0d2d0dc,0.1016949152542373
5126,1294fb4c86f993,b7c99706,* We need to convert `month` column into `datetime` format,4471e513,0.1016949152542373
5131,fdc3afd309b850,a32bb390,"<a id=""Dc""></a>
# 5 Data Check ",966bde38,0.10185185185185185
5132,2f0f808765fc67,1bd11d85,no missing value.,fd1f6494,0.10185185185185185
5133,ab6da5994949a3,4601fefa,"### Encoding categorical data
### Label encoding",fae6b91d,0.10185185185185185
5134,0e7ac281a19feb,519273db,## EDA,5b84d10f,0.10204081632653061
5135,ffd1df95ca5289,10855fa8,"-1 is the missing values, so we can consider removing it when describing. As many value are missing we cannot remove it ",db00c338,0.10204081632653061
5138,e69a496109e7d8,7e82404b,# *Checking for Null values*,1c640591,0.10204081632653061
5140,f35bf4df70d310,f380d192,### Splitting the data into train and test set,10bb859a,0.10204081632653061
5144,c80939c7c626cf,7b67c62a,# There are some missing fields in test data,b9ac31e2,0.10218978102189781
5145,3c2033cc99c12c,e3e05b10,"### Classify two classes into two catogories and see their distribution 
",dfa22a54,0.10218978102189781
5148,d1ff7e10ee0102,115cc884,"*'Very well... It seems that your minimum price is larger than zero. Excellent! You don't have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don't know... like, you in the beach... or maybe a selfie in the gym?'*",2cc71c3c,0.10227272727272728
5149,ce9ed5e2d601d7,4c81a64f,"# Load Data #

And now we can call the data loader and get the processed data splits:",f58a2f43,0.10236220472440945
5152,4d91e84c564cbe,ab9c96aa,"## Indexing

You can access individual list elements with square brackets.

Which planet is closest to the sun? Python uses *zero-based* indexing, so the first element has index 0.",355a43e3,0.10256410256410256
5162,34fff8ce731b03,d65b4cd9,"## Leitura dos dados

O trecho de código abaixo cria uma variável *work_dir*, que irá apontar para o caminho no sistema de arquivos onde estão os dados de entrada e onde a saída será escrita. Como os dados de entrada estão no formato Parquet, o Pandas irá utilizar o motor de leitura Pyarrow para conseguir ler este formato de dados e aumentar a performance de leitura e transformações no DataFrame.",6f9e5b2e,0.10256410256410256
5166,49ee86d074de69,230bfc32,"<a id = ""4""></a><br>
## Feature Engineering",71ccc6d3,0.10256410256410256
5167,0a1fcda859252c,da0ff849,The dataset is divided into three sets: 1) train set    2) validation set    and 3) test set.  Let's grab the dataset   ,13a38774,0.10256410256410256
5168,e169603b62be56,70762718,Exploring and cleanig the data,8c311ec1,0.10256410256410256
5171,c84925c8171900,55793228,"<a id=""warning""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            2.2 Supress Warnings:
            </span>   
        </font>    
</h3>",e21ff7ec,0.102803738317757
5174,9cec5ddf8b6f49,c826faaf,# 2. Summarize Data,d39fc8e7,0.10294117647058823
5175,c65a65d4041018,2191a8da,"Well... it seems that a lot of people closed the survey almost immediately after opening it, thus giving little info. And some people spent days on the survey! I suppose they opened the tab and forgot about it. Let's have a look at the surveys which took less than 3 hours.",824fb229,0.10294117647058823
5176,156bbcff05dcea,4a577fee,# Visualisations,66ad1fe9,0.10294117647058823
5179,eb0ecd6bebeb15,a30c3e6b,Veri çerçevesinin kaç öznitelik ve kaç gözlemden oluştuğunu görüntüleyelim.,d7b93a60,0.10294117647058823
5180,e4c6dd957eb5ce,20f98c5c,## Importing the dataset that we will work at. ,2e383665,0.10294117647058823
5182,dc0b0e1cb46c6f,710ea3f1,"<a id='1'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;"">1. Data cleaning ⚙️</p>

We are going to fix some of the missings so the EDA becomes easier and clear",47b17a7b,0.10294117647058823
5183,3d905ce4828057,b4171354,"# **Dataset Story**

**The dataset named Online Retail II includes the sales of a UK-based online store between 01/12/2009 - 09/12/2011.
The product catalog of this company includes souvenirs. They can also be considered as promotional items. There is also information that most of its customers are wholesalers.**",5b006cc3,0.10294117647058823
5184,8ec771f5600a61,2049d27c,## here we are checkin how many male and female are survived,48364c1f,0.10309278350515463
5191,84127ade6fde87,68881c86,"Our goal in this section is to turn text into something a neural network can pro-
cess: a tensor of numbers, just like our previous cases. If we can do that and later
choose the right architecture for our text-processing job, we’ll be in the position of
doing NLP with PyTorch. We see right away how powerful this all is: we can achieve state-of-the-art performance on a number of tasks in different domains with the same
PyTorch tools; we just need to cast our problem in the right form. The first part of this
job is reshaping the data.",f55d05b6,0.10344827586206896
5193,eb800c50fcfbb2,e6f75cf4,# Imoprt libraries,e7173f4d,0.10344827586206896
5197,cd10f3afd970b3,50e94bf9,"There are 5 csv files in the current version of the dataset:
",2db3c8e4,0.10344827586206896
5204,a4f0a3e1316ff9,31a84616,# Load Data,53bf0160,0.10344827586206896
5207,5fc2f23dfbeeb1,c34aead9,### Import Test & Train Data,f37b4110,0.10344827586206896
5208,00d295edcd117e,3ff9e5f6,CIFAR10数据集末，为2*32*32，即3个颜色通道，32*32像素。,f5810f4b,0.10344827586206896
5210,434f930cb58aee,fa5d2320,Initializing path to various data directories using pathlib module.,0e1d3554,0.10344827586206896
5216,4b7039cb44a54c,2698e232,# Import,24e806af,0.10344827586206896
5221,25ed87d1f0cb06,934aebec,# Visualization,7ca68782,0.10344827586206896
5222,a1ba5ffd30dbde,35879c2d,## Inspecting Data and Exploratory Data Analysis,48e57546,0.10344827586206896
5227,bb8f5d7807718b,08e5d171,"# 1. Span Selector

[Span Selector](https://matplotlib.org/3.1.1/gallery/widgets/span_selector.html) is a mouse widget in matplotlib.Span Selector returns the maximum and minimum values of a selected region in a graph, through the mouse selection.",181ec286,0.10344827586206896
5229,401338428b2d1c,cff775f6,## Importing the dataset,e4b768be,0.10344827586206896
5232,510b8303776bb6,5b7714fe,### Defining what to replace with.,18080db8,0.10377358490566038
5236,722cd844dfbe8f,b8e83ae6,"All of the subjects in this dataset appear to have a brain tumor. MGMT_Class = 0 refers to people who do not have the MGMT promoter methylation. MGMT_Class = 1 appears to be someone who has the MGMT promoter methylation. 

It is a competition which gives **the probability of it in `MGMT_value`** feature. `BraTS21ID` is the patient's identification.

Now, let's have a look to **train.csv** file :",0cedb385,0.1038961038961039
5245,a3ae04b78e45b5,57216b12,**POWERTY RATE OF EACH STATE**,4195da8b,0.10416666666666667
5247,3b5903412fe741,d9f8d37b,"In Python we can access the property of an object by accessing it as an attribute. A `book` object, for example, might have a `title` property, which we can access by calling `book.title`. Columns in a `pandas` `DataFrame` work in much the same way. 

Hence to access the `country` property of our `reviews` we can use:",ad231969,0.10416666666666667
5250,7325ce9461a814,708af039,"# 1.Data Cleaning
# 1.1.Dealing with Missing Values",c73a7825,0.10416666666666667
5254,71c3c1eab0377d,b1548ea9,"Columns Age, Cabin, Fare have Missing Values in Test Set.",52b4e360,0.10434782608695652
5255,21413205980558,43a36f21,"# 1.1 Basic attributes（基本属性）
> age：年龄<p>
    > job：工作岗位<p>
       > marital：婚姻状态<p>
         > education：教育程度<p>
            > defalut：是否违约<p>
               > balance：盈余<p>
                 > housing：是否有住房贷款<p>
                    > loan：是否有个人贷款<p>
# 1.2 Business contact（业务联系）
> contact：联系方式<p> 
    >day:上一个联系日 <p>
        > month：上一个联系月<p>
            > duration：通话时间，秒<p>
# 1.3 Marketing activities（营销活动）
> campain：上一次营销活动和此客户联系的次数<p>
    >pdays：自上一次营销活动联系后，至今的天数<p>
        >previous：上一次营销活动之前和客户累计联系过的次数<p>
            >poutcome：上一次营销的结果
# 1.4 target data(目标数据)
>deposit：客户是否有定期存款？",84197de0,0.1044776119402985
5256,e58e68e4eeefe5,b8be9cd6,"- There is nothing to conclude from discrete features correlation matrix.
- From the correlation matrix for continuous features, time is inversely correlated to death. Thus patients with less follow up time are prone to heart failure.",a87662ce,0.1044776119402985
5257,20b372b6e4e276,1b829a00,## Previous successful commits,ec8b0860,0.1044776119402985
5261,917957c6c4065f,fd6d6816,"동영상이 여러 일자에 인기동영상이 될 경우, 일자별로 데이터가 생겼습니다.  
이번 분석에서는, 해당 기간동안 처음으로 인기동영상이 되었을 때의 데이터로만 분석을 진행하겠습니다.  
중복된 video_id를 제거.  ",55b8ed68,0.10457516339869281
5267,7454fdc444df16,5ab12332,Each subfolder seems to be the ID of the corresponding patient,a7818ef5,0.10476190476190476
5268,7a058705183598,6972c549,1. Scatter plot,b0ead917,0.10476190476190476
5270,eda49464dd6d1b,006eced6,"### The data in Region_Code and Policy_Sales_Channel is in floats, but the actual values are integers, so they will be converted to int.  This is overall good practice and will make it easier to convert them to dummy variables later.",8421f81f,0.1048951048951049
5273,d81d3830152f88,a8c9e130,## Experiment Setup,9551eac9,0.10526315789473684
5276,bef2347846e476,db575c9e,Here is an example image of the data from the Google play store applications.Whenever you want to download an application you visit this part to get feedbacks from other peoples and see ratings.,cb93bf51,0.10526315789473684
5278,b568d0238ff53f,084041af,---------------- medals_total_df ----------------,ba30ccf4,0.10526315789473684
5279,d369f200a84c2a,1d5592d5,"Let's look at the example presented in the [Cezanne Camacho's blog](https://cezannec.github.io/Capsule_Networks/).
In the following image, capsules will detect a cat’s face. As shown in the image the capsule consists of neurals with properties like the position,orientation,width etc. Then we get a vector output with magnitude 0.9 which means we have 90% confidence that this is a cat face.

![](https://cezannec.github.io/assets/capsules/cat_face_1.png)

If we have changed one or more of these properties, like in the example the orientation of the cat’s face. The CapsNet still detects the cat’s face with 90% confidence(with magnitude 0.9) but there will be a change in the orientation( $\theta$ )to indicate a change in the properties.

![](https://cezannec.github.io/assets/capsules/cat_face_2.png)",8fef4d48,0.10526315789473684
5281,840534f2908a9c,887562ad,**Check for Missing Values**,8081c3cc,0.10526315789473684
5282,c950cff74e51ac,924a1225,"From the analysis above, we conclude that there's still missing values for several variables and the last_review column is still classified as object, not date.
The host id is better to be classified as object than int",d59bf323,0.10526315789473684
5285,99afe9f3af6dbc,24175240,"**Tf-idf and document similarity**

Kemudian mendefinisikan tf-idf vectorizer parameter dan meng-convert description menjadi matrix tf-idf. Vectorizer menggunakan stopwords untuk menghilangkan kata seperti 'a' atau 'the'.",cdec9b3a,0.10526315789473684
5286,c2a9f2fb3e1594,dc4f441d,"## 3.11 Load Data Modelling Libraries

We will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load.",53411c04,0.10526315789473684
5287,6cade0b6a41ba2,29ff410c,## 3.1. ID Column,e6110293,0.10526315789473684
5290,f91f58d488d4af,e78a596b,"In computers, everything is represented as number. To view the numbers that make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor*.",5df1bbf3,0.10526315789473684
5293,ba4b3bd184acbb,63492969,"### Reading Data

Though more often than not, data comes from external sources.

There are a few different input methods to create DataFrames from specific types of sources

#### Flat Files

Flat files include text or csv files that use a delimiter to seperate the columns.

Commonly used methods are `read_table` which assumes a tab delimiter while `read_csv` assumes a comma.

There are a plethora of optional parameters to explore when working with different datasets that can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_table.html).

*Note the `.head(n=5)` method can be called on DataFrames to only show the first n rows.*",0f5de724,0.10526315789473684
5294,30fdc4a6e3c1db,6690fa5f,Lets look at the number of rows for each state,6111ddee,0.10526315789473684
5295,826ccb616bd2a8,3428d08d,"### Read CSV and probe data
No need for data quality checks & remediation as population of 200 and no null values",4d7df2ec,0.10526315789473684
5297,31b564f11ef638,728efded,### Load Data,424f9692,0.10526315789473684
5301,caa0ce2715bf34,ec7da745,"### Observations
1. We have 8950 rows and 18 columns
2. We have float64(14), int64(3), object(1)
3. MINIMUM_PAYMENTS field has null values

## Descriptive Stats",78a5dc51,0.10526315789473684
5302,26b93b6f4dc148,67a1aaf0,### Importing Datasets,6f667d22,0.10526315789473684
5303,5ce12be6e7b90e,457c4bf6,"## `float`

**`float`** is for decimal point numbers, and is usually implemented using a double in C:",c0ab62dd,0.10526315789473684
5304,89afa0f49378c7,1e7eb926,# LSTM,32dbe10b,0.10526315789473684
5308,54004b32784b68,f5a13ff6,# 1- Data Analysis,27213ca9,0.10526315789473684
5309,29437539745aa5,bfc40ac4,"<a style=""text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;"" id=""imports"">0&nbsp;&nbsp;IMPORTS</a>",c17b490a,0.10526315789473684
5310,f14f6708035916,ca0ac07b,"### Load & Clean Data
- delimiter is "";"" instead of "","". 
- Date column is [0]. 
- Index is also [0]. 
- thousands=',' removes commas.",ca22d04b,0.10526315789473684
5311,c3498779cda661,c5c248f3,# Análisis de Datos,0f531b65,0.10526315789473684
5315,757fa8de4edc4c,2b210f88,Import files and see structure,87211008,0.10526315789473684
5320,396bc36edb95d3,ccc2b061,Let us segregate the data into numerical and categorical dataframes,965e4f8f,0.10555555555555556
5321,e19e307b3fd188,2591d714,#### NULL values,2173955b,0.10569105691056911
5322,2ada0305b68956,63fe26d1,### 15. Palette = 'GnBu',133e26f4,0.10571428571428572
5324,99bf357eaf61f1,c35a2026,"#### 1.Temporal Variables(Eg: Datetime Variables)
",9d92fafe,0.10576923076923077
5330,f166950fa915f8,9ee3d079,"### Dataset details
* **target**: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)
* **ids**: The id of the tweet ( 2087)
* **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)
* **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.
* **user**: the user that tweeted (robotickilldozr)
* **text**: the text of the tweet (Lyx is cool)",a7f6ca5e,0.10606060606060606
5331,ac1abfe1dfe815,1e7a894d,----,6529dbcb,0.10619469026548672
5332,c9b4e282e4e2c1,7e8f5661,"As we could see in info(), the only attribute with NaN values is PlayKey. PlayKey uniquely identifies a player's plays within a game (in sequential order). So it doesn't make sense should we apply such a tecnique like using the mean of the other values of the column to fill these empty values. 

",f44d339f,0.10619469026548672
5337,04e6b0d3c70f46,ba00b80d,### Load Audio File,56344f77,0.10638297872340426
5340,3f25b363afec54,5482506b,"## Let's Begin with the Hackathon

As the competition progresses and I apply multiple methodologies and based on those we will learn together. 

Without violating the competitive spirit, will not directly publish my solution but will provide you some hints if you implement them in your notebook might get better results.

So keep your eye on given hints.",bbdaae25,0.10638297872340426
5341,f6648e47713411,5960700d,There are 12 different labels.,f4af4d1c,0.10638297872340426
5342,957e035ba5b9d5,d400d78a,"# Rename files to to something more simple
for sub_dir in src_dirs_1:
    for d in src_dirs_2:
        src_dir = src_dirs_0[0] + '/' + sub_dir + '/' + d
        files = os.listdir(src_dir)

        print('-' * 50)
        print('Renaming files in ' + src_dir + '\n')

        for i, file in enumerate(files):
            new_name = 'image.' + str(i) + '.jpg'
            os.rename(os.path.join(src_dir, file), os.path.join(src_dir, new_name))
#             print(os.path.join(src_dir, new_name))",778ab3d3,0.10638297872340426
5344,b61ab8f81dc03d,fe999a5e,"<a id=""survived_sex""></a>
### %Survived (Sex)",64d05394,0.10638297872340426
5348,2bd6c370695ea7,b6ab3483,## Merge processed DataFrames with base train/test DataFrame,cbe6aec8,0.10666666666666667
5354,cb570c7b7f0501,eed34818,"The data set provides us with 14 columns:

1-Patient_Id: Identification of a patient.

2-Appointment_ID: Identification of each appointment

3-Gender: Male or Female . Female is the greater proportion, woman takes way more care of they health in comparison to man.

4-Scheduled_Date: The day someone called or registered the appointment, this is before appointment of course.

5-Appiontment_Date: The day of the actuall appointment, when they have to visit the doctor.

6-Age: How old is the patient.

07-Neighbourhood: Where the appointment takes place.

8-Scholarship: True of False and it's kinda of health care program.
    
09-Hyper_tension:True or False.

10 Diabetes: True or False.

11-Alcoholism: True or False.

12-Handicap: True or False.

13-SMS_received:1 or more messages sent to the patient.

14-No-show: True or False. it's True when the patient didn't attend.
",a200a0ec,0.10666666666666667
5370,62ae2b200f6b36,bbefbacd,"In the above data-set, we notice a column for week-end is missing. Apart from holidays and working-days, a third parameter for week-end is added to the data-frame.",7da4ea31,0.10714285714285714
5373,912bb73358069c,83a825e2,"Firstly, let's reproduce Keras output by Pytorch. Because the implement of GRU in Pytorch is different to Keras, so I writed a GRU by Pytorch.",0cf9db82,0.10714285714285714
5374,b290039151fb39,a252774a,## Module Param Scheduler,1836a79c,0.10714285714285714
5375,e78e7edae89049,9571af69,### Autocorrelation plot,9cef1d94,0.10714285714285714
5377,9c33d1955302bf,ae7fb008,# >  **Standardise**,0d9cfc89,0.10714285714285714
5382,98fd05fcc5c3e3,5874c860,## Loading Dataset,55fe7ece,0.10714285714285714
5385,1a285e4c830f3f,bacf76f0,### **Datenimport**,360b50e9,0.10714285714285714
5389,b8849a04581d32,78dd0dbf,## Primary data processing. Choosing the purpose and features for a model,b8a568cd,0.10714285714285714
5393,726833f92fb87a,a62e6b1a,We will first create a copy called 'df' of the original 'df1' dataset where we will perform data cleaning.,7dc5e1b6,0.10738255033557047
5394,5f32117bcd5255,654bf871,### GETTING KEYWORDS,85882abf,0.10738255033557047
5396,2f47abddfd1928,584f0c00,"Now we can identify and describe each feature:

- Categorical features:
    - Pclass: Ordinal feature that shows the class of the ticket. 1: 1st Class, 2: 2nd Class, 3: 3rd Class
    - Sex: Categorical feature that identifies the genre.
    - Embarked: Categorical feature that identifies the port in which the passenger embarked.
    - Survived: Categorical feature that shows whether the passenger survived or not.

- Numerical features:
    - Age: Continuous numerical feature of the age of the passenger.
    - Fare: Continuous numerical feature of the cost of the ticket.
    - SibSp: Discrete numerical feature showing the amount of siblings and spouse in the boat.
    - Parch: Discrete numerical feature showing the amount of parents and children in the boat.

- Other features:
    - Name: Name of the passenger, we will use this feature to derive others features more useful.
    - Ticket: Ticket number of the passenger, we might use this feature to derive passengers that bought the ticket together.",ae33cc0b,0.10743801652892562
5405,f2f2db16a2f86c,f0b484e0,**Data Cleaning**,ffc6a115,0.1076923076923077
5406,d07915a6e6992e,1d75e989,"# Exploratory data analysis
![analysis.png](attachment:analysis.png)",2b912140,0.1076923076923077
5407,c115e287523aab,e3765dfa,# Import Libraries,feb1288b,0.1076923076923077
5408,09751c520b0616,6e18f9a3,## *2. Data Preprocessing*,a4d0c7e9,0.1076923076923077
5409,1645979263c148,783c93a2,## Know Dataset Nature,fa11663e,0.1076923076923077
5410,52cfd66e9ec908,03118c1f,"Now, as we can see here, we have a sensor input (in the form of last year's sensory data) and they had to detect traffic agents in last year's competition. Here, we follow the next two steps of the process: predicting the agent motion and mapping out a path for the autonomous vehicle. The sensor is a LIDAR sensor, which basically gives us a rough perspective of the motion on the road. The sensor then feeds the data back to Lyft, who then collects the data from multiple sensors/cars all over Palo Alto, collates the data and gives it to us.",c74adcdf,0.10784313725490197
5414,842547b2def18c,8bc1c30d,"> ***どの特徴量が，空白(blank)/無効な値(null) or 空値(empty)なのか？***

これらは修正が必要となります．

- Cabin > Age > Embarked の順で，trainデータセットの特徴量は多くのnull値を含んでいます．
- Cabin > Age の順で，testデータセットの特徴量は欠損値を含んでいます．

> ***多様な特徴量のためのデータ型は何か？***

目標変数を変換する際に役立ちます．

- trainデータセットのうち，7つの特徴量はint型またはfloat型です．testデータセットでは６つです．
- 5つの特徴量はstring型(object型)です．",b8efde6d,0.10784313725490197
5415,a566b5b7c374e7,1187b7cf,### Prepare OURA table (oura_table),b3dc5545,0.1079136690647482
5418,8276973853faa1,777ddb7e,# Highlighting Max and Min Values in the table,88da542b,0.10810810810810811
5424,b7b1057764fa02,18a24d65,"Now that we have our helper function, we are ready to use it to extract data. This exaction takes a long time due to the large dataset. Even though I am using GPU services here, it does not help the time as these functions are not able to take advantage of the GPU.

An alternate method for dealing with such large datasets is to use generators - for the images, and the model. Generators are much faster than this manual, almost brute-force method, but they do not allow a similar easy access to the data. Which method to use is a judgement call made on the basis of time and space constraints. Since I have no such meaningful constraints here, I have used the longer method.",5053a192,0.10810810810810811
5426,27d5291d6365ba,ebddf9ef,"The dataset contains 12043 transactions for 100 customers who have one bank account each. Trasactional period is from 01/08/2018 - 31/10/2018 (92 days duration). The data entries are unique and have consistent formats for analysis. For each record/row, information is complete for majority of columns. Some columns contain missing data (blank or NA cells), which is likely due to the nature of transaction. (i.e. merchants are not involved for InterBank transfers or Salary payments) It is also noticed that there is only 91 unique dates in the dataset, suggesting the transaction records for one day are missing (turned out to be 2018-08-16).",96b30229,0.10810810810810811
5427,ac9b48d531bad9,d409eda2,"# Preprocessing of Data

Dropping columns: 'Sunshine','Evaporation','Cloud3pm','Cloud9am', 'RISK_MM', 'Date', 'Location'",95965e35,0.10810810810810811
5434,37b09262279764,05b7cfa8,<b>Statistics</b> about the numerical columns present in the data,37c4c417,0.10833333333333334
5436,4daf6153275cbf,9a8aeca0,Now the dataframe looks better,51db1961,0.10843373493975904
5440,9b5de3823ad5ab,3b8e0997,"As we can see, the only usefull information in the training dataframe are the labels (`sirna`). We can't get to the images using this dataframe, so we'll have to use the information in the images'path to get to their labels. What identifies a label in this dataframe is the experiment, plate and well, so we'll use this information (which, according to the documentation is also in the image path) to label the images.",33e48774,0.10869565217391304
5445,72d528df923403,c9abe32d,"# Downcasting

Downcasting is pretty importat for optimizing memory usage. I take an reference from https://www.kaggle.com/anshuls235/m5-forecasting-eda-feature-engineering. Please check that notebook for more details.",d51c8e8e,0.10869565217391304
5446,b01ee6cb674fa3,16125ebf,"Looking at the results, the cost made sense, since it was a soviet space project happening by late 80's, 
time of the renewed cold war (Reagan era)


The 1916 shows the Energyia/Buran soviet space shuttle, expected to be expensive, since it was a new technology being developed by the soviets. The Energiya is the rocket and Buran is the shuttle. 


The entry 2000 is the Energiya rocket with the Polyus Space Station. A quick search into wikipedia shows that this space station was a laser space station to counter the US Star Wars project (Strategic Defense Initiative)


This concludes that the price of the launches are quite within expected range, 5 billion dollars",a8ffd35e,0.10869565217391304
5452,6191c1f476437a,a2b57c75,import random,9dba3159,0.10909090909090909
5453,f0fab078f8533b,3ebaf3ae,# 2. Preparing data for analysis,bdb5ea32,0.10909090909090909
5455,c4386b8a01d66e,95921cb5,Filling the missing values by mean,dc732bf5,0.1092436974789916
5456,4ae6a182abac64,270d0bd3,"* **Numerical Features** : Continous: Age, Fare. Discrete: SibSp, Parch.",418676c5,0.1092436974789916
5461,c80939c7c626cf,30f0b65a,# Total count of Null values in train data is done below,b9ac31e2,0.10948905109489052
5465,fdc9f4863744b1,75323fd9,"When I initially look at the data set, I see ""Unnamed o"" column as a possible index coming from the csv and ""Easement"" variable that doesnt have any value in the rows. I will drop these columns.",b4529365,0.1095890410958904
5470,9c26c5dcd46a25,ad95c8ad,On remarque que les ajouts dans la base OpenFoodFacts se sont accélérés à partir de 2016. Les modifications de produits quant à elles se sont intensifiées nettement à partir de 2019 avec un pic pour le moment en 2020.,1bbbb677,0.10975609756097561
5471,fd4017c1514157,6b7f7658,# **Let's Understand the Data**,fd8f0896,0.10975609756097561
5472,0b01138ad120fc,f7d65018,**Generating sequential dataset**,0b4b72e6,0.10975609756097561
5474,83df814455f06c,8b791cd7,"The ID3 (Iterative Dichotomiser) Decision Tree algorithm uses entropy to calculate information gain. So, by calculating decrease in **entropy measure** of each attribute we can calculate their information gain. The attribute with the highest information gain is chosen as the splitting attribute at the node.",c9cff71a,0.11
5486,fbb1f9d3818830,281293cc,"# Inspiration


Now coming to the main point, why am I writing this notebook when all code-repository links are widely available & developers ( **way more qualified/experienced that me** ) have replicated the SimCLRv1 pipeline results. While going through the paper : ""**Intriguing Properties of Contrastive Losses**"", I was fascinated by one of the techniques used by the authors to understand the quality of features learnt by their model (that was trained using the **SSL(self supervised learning)**). This was a crucial step because knowing what your model learns using SSL & comparing it with its supervised counterpart will allow the authors to know whether their research is going in the right direction or not. Also knowing what kind of features your model has learnt is a good indicator in getting a rough estimate whether the model is focusing on the right aspects(i.e. features) of the image or not.

Feature visualisation in CNN models is a task that is well researched. Techniques such as gradCams & filter visualisation have proven quite useful when it comes to finding what the model has learned ( i.e. to what part of the image the model is paying attention while making a prediciton). For beginners there is an excellent notebook by Aakash Nain, [What does a CNN see](https://www.kaggle.com/aakashnain/what-does-a-cnn-see) explaining these techniques hands-on with code. 

The problem in SSL is slighlty different because there are no explicit class labels available to us & as far as I know the gradCam technique is highly label/class dependent ( there could be a way to use this in SSL setting but I have no clue, **suggestions are highly welcomed**) & the filter visualisation method only tells us the basic contours/features learned by each filter. Therefore the authors of the paper : ""Intriguing Properties of Contrastive Losses"" make use of K-means clustering to see how well the model has performed its learning (**how well it groups the local regions of an image**). 

I could not find a code for this task therefore I embarked upon this journey to understand & write it myself. Maybe this could serve useful to **you**.

# Idea

Thanks to [Marc Felix](https://stackoverflow.com/questions/69632019/how-to-use-k-means-clustering-to-visualise-learnt-features-of-a-cnn-model/69632937#69632937)

Once a model learns its feature/filter weights, we pass an image for inference. As the image is passed through each operation/filter in a feature extractor CNN, the original image starts to downsample & after every stage we get an activation/feature map with some dimensions as (H',W',C'). The reduced : H' & W' dimmension correspond to the spatial location on the activation map & the C' dimension is the list of features the model extracted from the image ( features which the model feels important for the final task at hand).The K-means algorithm is applied to these activation maps in order to see how these extracted features correspond to the original image. This works by flattening the spatial dimension of activation maps from (H',W',C') to (H'x W',C') [3D to 2D]. This step is done because, if we see the activation feature map, all the C' maps have the same spatial dimension but each of the C' map has extracted some different feature from the image based on the objective on which it was trained.

For e.g. in case of a dogs vs cats classifier ( **just an assumption** ) the filters for a trained CNN model could extract whiskers, ears, face, eyes, nose etc. as features, therefore an intermediate activation map in the particular CNN could have :

1. 1st activation map containing ears as extracted features
2. 2nd activation map containing whiskers as extracted features 
3. 3rd activation map containing eyes as extracted features 

& so on. At every spatial location we have C'(value varies from layer to layer) features being extracted by every layer of the model.


Now coming back to our problem, we need to reshape our activation maps for using the K-means algorithm. Each location on the feature map has C'-features, this can be seen as (C' = 3 in this case):

![Screenshot 2021-10-23 at 11.34.53 AM.png](attachment:1481459c-cdad-4c55-ab0d-fd41ad95edfe.png)

The goal of K-means is to form clusters on the right. The major driving force behind this idea is that : **If good/generalised representations are learnt by the model we should be able to group the regions of similar objects in the same cluster**. This can be simplified as (based on previous dogs Vs cat classifier example): if a filter's goal is to find all the whiskers in an image then all the pixel regions having cat whiskers present in them will have the whisker activation map being maximally activated (having a high value in them) as compared to other locations having no whisker in them. Then on the C' dimensional axes ( **I cannot draw beyond 3D**) those pixels postions (**whisker ones**) will come under the same cluster since their activation vector will close by in the C' dimensional space. 

If good features are learnt by the filters then we can scale these activation maps to overlay on original image & find out whether proper regions of same objects in the image are being grouped together or not.



*Enough Talk let's code*",c7027f86,0.1111111111111111
5491,f998cece696659,568056c2,"<a id=""1""></a><br>
# Loading and Checking Data",7964297e,0.1111111111111111
5493,3597174a998d4d,983db869,# 2. Analysis of different variables,276892ed,0.1111111111111111
5496,aa7db7b023d0a2,f35ccf41,"**<span style=""color:#DC143C;"">Columns Names:</span>**

aut = Was an autopsy performed?  Yes: 1, No: 2, Unknown: 90

autdt = Date of Autopsy

autobt =  Has a copy of the autopsy report been obtained?

autpmi = Post Mortem Interval (hours)

auttyp = Type of autopsy performed (Limited or complete)

diedcaus = Cause of death.

dieddt = Date of death.

icd10cm = ICD-10 CM Code for cause of death.

source= Data Source, medical records, original collection, patient reported, unknown, other.

sourcesp = Specify other data source

https://www.kaggle.com/alsgroup/end-als",ec912af3,0.1111111111111111
5500,613bf7bfdcb9e3,62515b2c,"Parameters
aarray_like
n-dimensional array of which to find mode(s).

axisint or None, optional
Axis along which to operate. 
Default is 0. 

#### If None, compute over the whole array a.


### nan_policy{‘propagate’, ‘raise’, ‘omit’}, optional

Defines how to handle when input contains nan. 
The following options are available (default is ‘propagate’):

‘propagate’: returns nan

‘raise’: throws an error

‘omit’: performs the calculations ignoring nan values",32beb65d,0.1111111111111111
5512,268a610bbc64b4,6e2b44d5,# Preprocessing dates,8a16f301,0.1111111111111111
5515,08b5b53e94599f,34cdcce6,"Ok, let's import the data frame!",31609f4d,0.1111111111111111
5517,c01049afb6d307,38bda719,## Some Regulations,d37d3b5d,0.1111111111111111
5522,3ac432b2cac29c,b261b741,"# Exercises

## Step 1: Split Your Data
Use the `train_test_split` function to split up your data.

Give it the argument `random_state=1` so the `check` functions know what to expect when verifying your code.

Recall, your features are loaded in the DataFrame **X** and your target is loaded in **y**.
",a358669e,0.1111111111111111
5527,2500c5fe8497ee,9a7f8c7e,# Import Dataset And Preprocessing,855355f0,0.1111111111111111
5529,24e550b8226932,faa13de0,# Overview:,0caee953,0.1111111111111111
5535,a2286e7c88bb76,99c40a6c,## exploratory data analysis,be48f3fb,0.1111111111111111
5536,4883314a96dc34,2c42f920,# Preprocess Data (1),50d36836,0.1111111111111111
5538,cb4ad8ed4cb300,dd496e86,# 2. Answers,7c0f3236,0.1111111111111111
5539,1014e6be391084,1824f984,Removing duplicates,46f9168f,0.1111111111111111
5542,b9328fe3b0cefc,3b636e96,"## Data overview and visualization(数据简单概览及可视化)

### Load lib and set path(加载相关库，设置路径)",3a35eb23,0.1111111111111111
5546,49ac6594c8f5cf,ad583d70,"**Salary distribution**

Salary distribution seems somewhat multimodal because one of the mode gets occupied by the mode of placed and rest by the zero salaried non placed candidates.",6f19f28a,0.1111111111111111
5549,b39684e6670dd7,d7c8fe3e,# import libraries,83de9873,0.1111111111111111
5552,d58491f2896fc1,a5537ec3,"* **Gen** : Yapısında probleme ait en küçük bilgiyi taşıyan birime gen denir. Genellikle her bir gen *bit* olarak ifade edilmektedir.
* **
* **Kromozom** : Kromozom, nesil veya popülasyonda bulunan bir aday çözümü temsil eder. Genlerden oluşmaktadır ve bir diğer ismi de genotip olarak bilinmektedir. 
* **
* **Popülasyon** : Olası çözüm bilgilerini içermekte ve kromozomların bir araya gelmesiyle oluşmaktadır. Buradaki birey sayısı problemden probleme değişiklik gösterebilmektedir. Popülasyon ne kadar büyürse problemin çözülme süresi de yine bir o kadar büyüme gösterecektir. Birey sayısı gereğinden fazla olduğu takdirde çözüm süresi gereksiz bir şekilde büyürken eğer birey sayısı az olursa bizi optimal değerlere ulaştıramayabilmektedir. Edward Grefenstette, genetik algoritmalar için ideal popülasyon büyüklüğünün 10 ile 160 arasında olması gerektiğini savunmaktadır.
",514bfdff,0.1111111111111111
5553,49ee86d074de69,8dbf6c3d,"<a id = ""5""></a><br>
## Remove Constant Features",71ccc6d3,0.1111111111111111
5554,b3e0b7e9ff6849,a81608fb,"### ****Definiton of Variables****
1. **Invoice:** Invoice number, unique identifier variable for each transaction. Refund invoice numbers starts with ""C""
2. **StockCode:** Unique product code
3. **Description:** Product name
4. **Quantity:** The number of product in the invoice
5. **InvoiceDate:** Date and time of the purchase
6. **Price:** Unit price of a product (in terms of Sterlin)
7. **CustomerID:** Unique customer identifier
8. **Country:** Residential country of customers",f6e4bb0d,0.1111111111111111
5557,d128317750d689,ef222ee3,"Before we go on, let's take a peek at the data we're dealing with:",d87f7428,0.1111111111111111
5558,46945b5ec34ad3,c4c8bd58,The two test and train datasets have been read. Lets directly start with some exploratory data analysis of the same.,1e18cc2e,0.1111111111111111
5563,4392956f62c040,5cfbef15,## DATA,c3ed519d,0.1111111111111111
5565,2f0f808765fc67,561375e6,# **# Data Exploration**,fd1f6494,0.1111111111111111
5568,10b5af05d804ff,9e8fbb01,You have some data like this:,4a9b1705,0.1111111111111111
5570,e2a94f078e1161,ae4a804a,"Now I will test out various Hypothesis

Hypothesis: Students in Liberal Fields have more Discussions than their Science(IT included) counterparts

Testing : First separate the two said groups (since the number of students in science is higher I will randomly pick out 100 from each) and find the sum of discussions of each. repeat this experiment 20 times.",5fc53059,0.1111111111111111
5571,df2a7968c08ee4,f936d8bb,"### Loading Data

Reading the csv files into pandas. Usually image data is not stored using a CSV file, but since MNIST images are so small we are able to do so.",a2ba0a72,0.1111111111111111
5576,cf39cde80e66b7,74952029,"# <div class=""h1"">Imports </div>
<a id=""IMPORT""></a>
[Back to Table of Contents](#top)

[The End](#theend)

We are using a stack: ``numpy``, ``pandas``, ``sklearn``, ``matplotlib``.",aed4bc9b,0.1111111111111111
5580,fdc3afd309b850,cc0064fa,"<a id=""pnv""></a>

## 5.1 Null values",966bde38,0.1111111111111111
5581,f4b9042e693b6c,176e2a5d,"The below cell will install the [timm]() library, which is what we will use to define our models and get pretrained weights.",676cacc9,0.1111111111111111
5582,2975a21f5ce2ae,a68bdc25,### Data Preprocessing,4d6a6aa0,0.1111111111111111
5589,30c8dc87ce52ca,ce69d587,USING THE SUBSET OF COLUMNS BASED ON THE CORRELATION MATRIX,805e9d67,0.1111111111111111
5595,2b39f4ff896f97,70972b21,Function to convert images to array,3ddfe182,0.1111111111111111
5596,dd3721cb49c1fd,c7e6b812,"<a id='1'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px;
            text-align:center"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white"">1. Installing the Greykite package 📗</h1>
  </div>
</div>",1a53fdd9,0.1111111111111111
5605,1667a100fc8b42,1b0edb92,## Importing Libraries,6c8cd6b6,0.1111111111111111
5606,d905cde3391d2b,4c8aac7d,"# Measuring center

First step often learnt in [descriptive statistics](https://en.wikipedia.org/wiki/Descriptive_statistics) is to measure the center of given data. There are various ways to measure the center. We'll go through some of them.

Let's get the data ready for our experiments. 
* **`win_by_runs`** columns represents the margin in which a team has won against the opponent, if the team batting first has won.
* i.e. If **`team1`** scores **200** runs and **`team2`** scores **150** runs, **`team1`** won the match by **50 runs** - If **`team1`** bats first

Hence, we have to exclude all instances of **`win_by_wickets`** cases, i.e. **`win_by_runs = 0`**",067dba39,0.1111111111111111
5609,1d73d04c3aaae8,b82ad8e1,"## Background

An obvious first thing we need is a team identification table. The team identifiers will not be recognized by any modern day NFL fan. The first two seasons were from a professional league called the American Professional Football Association, which became the National Football Leaguge (NFL) in 1922. It grew from the Ohio League and the New Your Pro Football League.

The very first game between Rock Island Independents (RII) and the St. Paul Ideals (STP) was won by Rock Island 48-0.   

In the first season, the league title was won by the Akron Pros, who were undefeated (with several ties). 

In the modern NFL, only two teams survive, the Chicago Cardinals (now Arizona Cardinals) and the Decatur Staleys, now the Chicago Bears. The Muncie Flyers had a short season, with a record of 0-1. They did play a few other games that year against local teams; however these games did not count in the APFA standings.",cd43d0aa,0.1111111111111111
5610,4fd4b6a80d40e3,a02d09a1,"## Expression 2

![image.png](attachment:image.png)",f6913cc3,0.1111111111111111
5616,2ada0305b68956,3f6287b0,### 16. Palette = 'GnBu_r',133e26f4,0.11142857142857143
5619,20b372b6e4e276,d4da19c9,"### Commit 3 (with original parameters)

* Dropout_new = 0.1
* n_split = 5
* lr = 3e-5

LB = 0.711",ec8b0860,0.11194029850746269
5620,f3c6048d1058e3,b370fceb,### Visualization- Understand your data better,1d9056b0,0.11206896551724138
5623,b01ee6cb674fa3,cff2a28c,## Status Mission,a8ffd35e,0.11231884057971014
5631,254cccd5145725,dac85f68,"<font size=""4"">This is where the actual fun begins. We start off by importing all the libraries that we will need later on. We will be using Numpy and pandas for data analysis and matplotlib (Matlab for python), seaborn for data visualisation.</font>",a49b4037,0.1125
5634,631cd434fc3aa2,c6624461,"We can see that the target variable:
* deviate from the normal distribution
* have appreciable positive skewness
* show peakedness

As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.",2b74febb,0.11267605633802817
5640,c0ddb77bf32e2b,6c0ef685,"Let's have a look of UVB first.

Accoding to [wikipedia](https://en.wikipedia.org/wiki/Ultraviolet)

> Ultraviolet (UV) is electromagnetic radiation with a wavelength from 10 nm to 400 nm, shorter than that of visible light but longer than X-rays. UV radiation is present in sunlight constituting about 10% of the total light output of the Sun. It is also produced by electric arcs and specialized lights, such as mercury-vapor lamps, tanning lamps, and black lights. Although long-wavelength ultraviolet is not considered an ionizing radiation because its photons lack the energy to ionize atoms, it can cause chemical reactions and causes many substances to glow or fluoresce. Consequently, the chemical and biological effects of UV are greater than simple heating effects, and many practical applications of UV radiation derive from its interactions with organic molecules.


UVB seems to be a potential variable that will have influence on PM 2.5. But somehow it has too much NAs. Let's see what happend to this columns.",a0cb45f7,0.11290322580645161
5641,ad26c020235dfc,0cdeac26,"# Functions
We define some helper functions for ploting, feature engineering and calculations.",bf766e48,0.11290322580645161
5644,57070ad5e0f94f,580b84bb,# **Analysing Example Output**,d97edc41,0.11290322580645161
5645,71c3c1eab0377d,e5250027,### Data Pre-Processing,52b4e360,0.11304347826086956
5646,6a80f915608fc2,f7a9d730,"## <a id=""TargetSummary"">Looking at the Targets</a>
Back to <a href=""#Index"">Index</a>

There are 206 target values given for each of the 23814 (21948 treatment) rows.<br>
The MoA target values are all binary: 0 or 1; think of these as ""not active"" and ""active"".",636938eb,0.1130952380952381
5647,43e60eb1362f5c,c08dae55,From the above table it is clear that data is not properly organised and date is given seperated and many columns have unnecessary data not useful for visualization for which it is required that we clean the data and take only those columns which is of our use.,87934234,0.11320754716981132
5649,f3c8651cb08234,4b2a1a20,# Checking for Missing values,37f86e36,0.11320754716981132
5650,4dd47072617594,3160f7e9,# 1. Text Preprocessing,44ff1d11,0.11320754716981132
5653,a070fd03ae8ed2,a8895e16,# 2. Вспомогательные функции,c0ec4138,0.11320754716981132
5655,f015d0147e8fbf,7263fe83,## I. Preprocessing and Feature Engineering,518954fb,0.11320754716981132
5661,225b4fe5d3894a,e2ec9dee,"<a id=""4""></a>
## 4. Create a Test Set",4b4197b3,0.1134020618556701
5662,2a123b4e8f9433,d3f108b4,# Drop redundant columns,0a082218,0.1134020618556701
5663,063a35f644e3c5,4870527b,"### number of columns in each dataset
",1c30fb0a,0.1134020618556701
5664,957e035ba5b9d5,c18d9328,"# How many images do we have
for sub_dir in src_dirs_1:
    for d in src_dirs_2:
        src_dir = src_dirs_0[0] + '/' + sub_dir + '/' + d
        files = os.listdir(src_dir)
        print(""Number of images in {}: {}"".format(src_dir, len(files)))",778ab3d3,0.11347517730496454
5665,56785caebaa256,ad488fd6,### We will select only the last wave: from the August,a792961a,0.11347517730496454
5668,a35cdce61f4059,509da96c,"As it can be seen our data columns do not have any name so we can't see what numbers really mean here just by looking at them.
The amount column isn't normalized, so we will normalize it to be in same range as other values",acc8eab6,0.11363636363636363
5681,6d66ced0028dea,ba3940f7,# 1. Чтение данных,f50aae52,0.11363636363636363
5686,08f845750d026a,4cc6f376,The country with the largest loan amount is Philippines with $55342225.0,1c54de30,0.11392405063291139
5687,fe7360cddc13e5,fa10f800,Ortalama 4 işlem yapan bir müşterinin t zamanda 9 ve 0 alışveriş yapma olasılığı görece düşüktür.,8979e423,0.11403508771929824
5692,3a6274ed72cc00,6c9102bf,## <a id='2.'> 2. Loading and Checking Data</a> ,51369a2a,0.11428571428571428
5693,5d5c9480b5a0a3,927207fd,"Age, Cabin and Embarked seems to have rows with null values in them. We might need to populate those rows later on.",04d82e2d,0.11428571428571428
5700,867a9f977fa945,70a5683f,# ** Removing punctuation**,2740fcca,0.11428571428571428
5701,2730840089c8eb,e083ae1e,"We can fix this by ""escaping"" the single quote with a backslash. ",34d27dac,0.11428571428571428
5702,38b79494ac749e,d8ba37e3,### Generate samples,39162a40,0.11428571428571428
5706,9c044fa3072552,ef16c003,### Missing Values,1362842e,0.11428571428571428
5715,f50dc95483c98f,34d0d4ed,### **Now Some Data Visualizations work**,cd9e9621,0.11428571428571428
5723,2b36742b49c7bc,c900579c,### Load Data,c8f8a96d,0.11428571428571428
5724,55a5e31d03df9f,0a1ce273,"## <a name=""randomviz"">Visualizing random images</a>",06dce00f,0.11428571428571428
5725,ee23a565163388,f0604bb1,# **Quick Inspection of Dataset**,88aacbc4,0.11450381679389313
5732,63b44c85e32c1f,d4ea840c,"Indexing in nested lists can be quite confusing if you do not understand how indexing works in python. So let us break it down and then arrive at a conclusion.

Let us access the data 'apple' in the above nested list.
First, at index 0 there is a list ['apple','orange'] and at index 1 there is another list ['carrot','potato']. Hence z[0] should give us the first list which contains 'apple'.",fb9b9562,0.11486486486486487
5739,ac1abfe1dfe815,2f81d339,## Missing Data,6529dbcb,0.11504424778761062
5746,80ad12f326ab70,76543e9c,* ### Districts data,da404a16,0.11538461538461539
5756,d4c5aaa4b36810,9b2eb21d,"Some of these countries are not entirely recognized such as **'Palestinian Territories**, **'Somaliland Region'**, **'North Cyprus'** and **'Taiwan'** but do go by some alternative names like **Palestine**, the **Turkish Republic of Northern Cyprus** and the **Republic of China**. **'Hong Kong'** does not tend to go by any other names but is now technically part of China so has likely not been included separately because of that. **'Congo (Kinshasa)'** appears to represent the **Demorcratic Republic of the Congo** while **'Congo (Brazzaville)'** represents the **Republic of the Congo**. The **'Ivory Coast'** may be go by it's French name **Côte d'Ivoire**. Kyrgyzstan's offical name is **Kyrgyz Republic**. Laos's official name is **Lao People's Democratic Republic**. Slovakia's official name is the **Slovak Republic**.",65441f28,0.11538461538461539
5757,a915263bc207da,fbc61293,"### Read the files
##### Reshape the dataframes and merge ",b17ebcda,0.11538461538461539
5759,2facf256353117,be9c5c70,"# What are the differences between enhancing and nonenhancing lesions in MRI?
",18f579be,0.11538461538461539
5762,09751c520b0616,794a21b8,###  Handling missing values,a4d0c7e9,0.11538461538461539
5766,d07915a6e6992e,ee431fb6,"

One important aspect of machine learning is to ensure that the variables show almost the same trend across train & test data. If not, it would lead to overfitting because model is representing a relationship which is not applicable in the test dataset. 

I will give you one example here. As we do variable analysis, try to replicate (wherever applicable) the code for test data and see if there is any major difference in data distribution. 

**Example** - Let's start with finding the number of missing values. If you compare the output you will see that missing value percentages do not vary much across train & test datasets.

Use the groupby/univariate/bivariate analysis method to compare the distribution across Train & Test data",2b912140,0.11538461538461539
5767,af6556ced704f6,8437853f,***Read data***,881577c0,0.11538461538461539
5768,21122355e39af4,9e02be62,"QUESTION: Labelings that assign all classes members to the same clusters are: complete, but not homogeneous:",88b95e2b,0.11538461538461539
5769,d0f6276d5b628c,e6bd414d,We can see that every single subscriber haven't given review or rating for every sigle movie but for some single ones. This helps us understand the consumer/subscribers taste of content.,c64f5ce5,0.11538461538461539
5773,897ca904b74a98,7b70bfb6,## Deep Analysis,c5844ad4,0.11538461538461539
5774,44f6a002ecd033,25b3504b,Working with what looks like about three times as many Males than Females in the train dataset.,70bbe106,0.11538461538461539
5777,2f47abddfd1928,b2a48260,Now we can use the method .describe() to get some statistical values for each feature.,ae33cc0b,0.11570247933884298
5784,0e2a23fbe41ca9,529d1811,"Training dataset has no nulls but testing dataset has one row where the ```first_active_month``` column is Null. Impute testing data specifically?  
We are done with our sanity check now. Let's explore the table.

## Exploration

### 1. Target Variable

Target column is a numerical loyalty score.

- Negative means the card holder is not loyal?
- And, positive means the opposite?",64e4762c,0.11594202898550725
5789,9d9da6c439b96b,f15f2196,## Numerical Data,361cc7d9,0.11594202898550725
5790,a1a31459abf078,194145db,"### Reading data using cudf

Similar to pandas, we use the ```read_csv``` function to in cudf package to read the input data provided to us. 

This is the description for cudf on RAPIDS website - Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data. cuDF provides a pandas-like API, so users can use it to easily accelerate their workflows without going into the details of CUDA programming.",66fc0f54,0.11594202898550725
5798,d96642860ab3dd,4caa6e02,### 1.2 Sex Feature,98419d48,0.11627906976744186
5802,22bd95f4807a23,a1992ff5,* The data has 23486 rows and 11 columns.,c05d356f,0.11627906976744186
5804,2e40928927c0d4,28a2e915,**Loading the dataframes**,b6385ef2,0.11627906976744186
5807,fdc9f4863744b1,bafb4311,## Data Cleaning,b4529365,0.11643835616438356
5811,98a6794067932a,54c5f1a5,"La prochaine cellule sert à transformer le type des données se trouvant dans les colonnes ""Order date"" et ""Ship date"" afin qu'elles deviennent en type ""Datetime"" et non ""Object"". La première étape est d'importer le module datetime. Ensuite, la deuxième étape consiste à prendre les données se trouvant dans les colonnes ""Order date"" et ""Ship date"" et de les convertir en type ""Datetime"". Après avoir effectué ce changement, nous nous sommes rendu compte que les mois et les jours étaient inversés nous avons donc procédé à la correction des dates en inversant leurs valeurs lors de la troisième étape. La quatrième étape était de convertir à nouveau les dates en type ""Datetime"" étant donné que la correction effectuée lors de l'étape 3 avait remis les données en type ""Object"". Finalement la dernière étape était de sortir les informations par rapport aux données des différentes colonnes afin d'évaluer si les ajustements avaient été faits avec succès.",08600fe2,0.11650485436893204
5813,63d0d9b9a8c7d2,d7408334,"**Taking the dataset except the ""Cabin"" column**",e32e5933,0.11666666666666667
5816,b547f0f38f7744,fd24e421,"
Split`bbox` to `x` `y` `w` `h`:",b6ba66b3,0.11666666666666667
5819,37b09262279764,d9db7d86,"Before moving on to <b>EDA</b>, ley's clean the data.",37c4c417,0.11666666666666667
5824,62487bcd70b199,21bb26c8,# <a id='3'>3. Statistical Tests</a>,f6ae50af,0.11666666666666667
5827,3c2033cc99c12c,197f215d,#### Randomly select a few columns and draw a distribution plot of two classes ,dfa22a54,0.11678832116788321
5829,4ae464582bac51,33325616,## DataTypes,ca6a52ce,0.11688311688311688
5834,241cf32abb22d8,c76e2e7f,## Summary Statistics,47157066,0.11688311688311688
5836,9ceb7278784462,64ca8123,"* Some books have no reviewers but have rating .
* We have **inconsistent data**
* I will fill it with the **average value** instead of inconsistent data
",3768a567,0.11693548387096774
5837,5ce12be6e7b90e,bfe2df66,"## `str`

**`str`** is for strings, used for both characters and text. 
Will deal with strings later.",c0ab62dd,0.11695906432748537
5839,4c47839b067546,1184d09f,"В данных 2020 г есть 4 признака ('hidden', 'model', 'start_date', 'Комплектация') , которых нет в тестовой выборке
И наоборот в тестовой есть 11 признаков, которых нет в train
## Проанализируем все эти признаки с целью достижения унификации:
",1f517b02,0.11702127659574468
5842,2ada0305b68956,ed9b825b,### 17. Palette = 'Greens',133e26f4,0.11714285714285715
5844,b779c3ce7b671a,56421ddb,"# Maths

Using some [orbital maths](https://pages.vassar.edu/magnes/2016/12/09/orbital-motion-of-our-solar-system/) and differential equations, we can define update rules for both $x$ and $y$ components of a planet.

$$\frac{M_pv^2}{r} = \frac{GM_sM_p}{r^2}$$

$$V_{x,i} = V_{x, i-1} - \frac{GM_s(-X_i)}{r_i^3} dt$$

$$X_i = X_{i-1} + V_{x, i} dt$$

$$V_{y,i} = V_{y, i-1} - \frac{GM_s(-Y_i)}{r_i^3} dt$$

$$Y_i = Y_{i-1} + V_{y, i} dt$$

As we can see, there are a few attributes for each planet: $x$ position and $y$ position ($\text{AU}$). Using $G = 4\pi^2 \ \text{AU}^2\text{yr}^{-2}M_\odot^{-1}$, we can translate these calculations into functions.

",ca778770,0.11764705882352941
5845,869a39a3d4dea2,ff1f7218,Write the image back to disk,9020daf8,0.11764705882352941
5846,c3dfa835621ac4,2853f5e3,"# Interactive Tool

In order to experiment rules more easily, an interactive UI is created in this notebook which eventually looks like this. It's base on the work of [Simple and Interactive visualization of tasks](https://www.kaggle.com/bsridatta/simple-and-interactive-visualization-of-tasks#Interactive-visualization), plus the rules editing and animation. To use this please go to section [Interactive CA Tool](#Interactive-CA-Tool)(maybe you need to run the notebook before able to see it), simply select the data sample, type your rules in the format defined above in the ""rules_txt"" input box, and click ""Run interact"" to see how the rule works.

![image.png](attachment:image.png)

If you want a quick look about the animation, please refer to [this section](#Visualization-&-Animation) for an animation example.",0126bdad,0.11764705882352941
5847,52cfd66e9ec908,846892ad,"We may now import the lyft level 5 kit, and all that comes with it. We have quite a lot of imports and installations required...",c74adcdf,0.11764705882352941
5851,55c34673c1f760,8951b49d,# Load Data,2663c47f,0.11764705882352941
5859,e93a41c03638fe,f84ea20e,"Generally, we employ the following steps while preprocessing texts:
<ol>
    <li>Tokenising the string</li>
    <li>Converting characters to lowercase</li>
    <li>Removing stop words and punctuations</li>
    <li>Stemming or lemmatization</li>
</ol>",7363527b,0.11764705882352941
5860,ab657da5329e3f,1ed8497e,"# Competition data access
TPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use `!ls /kaggle/input/` to list attached datasets.",021526f8,0.11764705882352941
5861,917957c6c4065f,c928b854,#### category_id & category,55b8ed68,0.11764705882352941
5862,32ddc45133f77b,c7df76ab,**Processing data using Tokenizer**,3c0d6831,0.11764705882352941
5864,1d1598b6fa2aa7,07d976a4,## Basic Charts,e066accf,0.11764705882352941
5868,4ae6a182abac64,e9bca0eb,* **What are the data types for various features?**,418676c5,0.11764705882352941
5870,0e662a463309e7,d806db8b,Tokenization,84c57e51,0.11764705882352941
5871,a0a5baa6c7e12a,24f0da71,"Based on the charts above, we find that 
- the target class labels are seriously imbalanced, with labels *4* and *5* to be neglactibely small relative to other class labels (it may justify exclusing the observations with such class labels from the training set, to improve the ML model accuracy)
- it will be required to use one of the industrial techniques to handle inbalanced target class problem iin ML modelling experiments down the road (see below)",551d41de,0.11764705882352941
5877,50fcff6c0425fa,ce03bd57,"# Ideas with the data:
1. analyze the trending videos of every year.
2. analyze what is the sweat spot timing to upload a video.",8d355348,0.11764705882352941
5883,9cec5ddf8b6f49,63898cec,## 2.a) Descriptive statistics,d39fc8e7,0.11764705882352941
5887,1a0bd2f72bbe36,5f7ee002,## let's get the basic info. about the data set:,2fa311dc,0.11764705882352941
5888,36d0d4cb9c7993,8cc03e51,## Generating Data,34b93e27,0.11764705882352941
5893,7cfd96218dd933,66954dc0,"#### **ATTENTION**

* PLEASE CHECK OUR WORK ON THIS LINK FOR DATA INFORMATION AND REVIEW.
* THE VALUES ARE RELIABLE, BECAUSE THE SOURCE WE GET THE DATA IS THE SAME.


* https://www.kaggle.com/brsdincer/turkey-recent-wildfire-analysis-investigation",7c34d96c,0.11764705882352941
5894,523123dad03177,cd56d584,"COOL, COOL, COOL!",48a5e4e6,0.11764705882352941
5895,4fa553c2b837d4,e38eb460,"1.  Choose **predicting label y**.
2. Choose the **features x**.",c65a23e9,0.11764705882352941
5896,3d905ce4828057,5f7f15b1,"# **VARIABLES**
**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter ‘c’, it indicates a cancellation.

**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.

**Description**: Product (item) name. Nominal.

**Quantity**: The quantities of each product (item) per transaction. Numeric.

**InvoiceDate**: Invoice Date and time. Numeric, the day and time when each transaction was generated.

**UnitPrice**: Unit price. Numeric, Product price per unit in sterling.

**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.

**Country**: Country name. Nominal, the name of the country where each customer resides.",5b006cc3,0.11764705882352941
5897,130af16914b3f8,6a232019,# Access the database through sqlite3 package,4b36d3e3,0.11764705882352941
5899,ce9ed5e2d601d7,9f502579,## Pseudolabeling,f58a2f43,0.11811023622047244
5900,0caaec057f7184,e166cf6f,# EDA on training data,b875533e,0.11827956989247312
5901,52ee792e228d54,f6c28b2d,### We have 52 rows having negative expeience (which is 1% of the total sample). We'll deal with them later.,5096094e,0.11842105263157894
5902,9169c4e9c33c90,ed01f323,Bivariate distribution for User Rating and Reviews,725bf880,0.11864406779661017
5904,1294fb4c86f993,c9a83063,* There are many `NaN` values in the data file. We need to fill them with zeros but first we need to verify that `totals` column really equal the summation of each row.,4471e513,0.11864406779661017
5906,ed8009f482b380,a51248a4,"As we can see from the .info() command, the bmi columns are missing about 100 data. But in the heatmap we see that the missing values are actually not too significant, so we're just gonna drop the entire missing values.",e99941fa,0.11864406779661017
5910,f2e5e9fb9eaaf7,42081459,"### 3.1.2 Basic statistics
Below is the basic statistics for each variables which contain information on `count`, `mean`, `standard deviation`, `minimum`, `1st quartile`, `median`, `3rd quartile` and `maximum`.",048e0d08,0.11864406779661017
5914,bb0905d33ae417,f157ac2a,# Exploratory data analysis,25fd1965,0.11864406779661017
5917,eda49464dd6d1b,ab6eaadb,# Exploratory Data Analysis,8421f81f,0.11888111888111888
5920,a758983a68c014,8682c9f0,### Skip-Gram example with PyTorch,ab89f181,0.11904761904761904
5932,2b97b399158701,fcda7a73,"Out of 4 models used for ensembling, two of the models use LR_EXP_DECAY of 0.8 and two of the models use 0.75.",04bd0060,0.11904761904761904
5936,565ad413cd802f,bb519f28,Let's load the `train.csv` file into a Pandas dataframe,397b074e,0.11904761904761904
5937,b4ecd6e4277e3c,e365b380,"### Ensure determinism in the results

A common headache in this competition is the lack of determinism in the results due to cudnn. The following Kernel has a solution in Pytorch.

See https://www.kaggle.com/hengzheng/pytorch-starter. ",94d79d5f,0.11904761904761904
5938,87e94f864d74be,60ae329c,"
> **Using this matrix you can very quickly find the pattern of missingness in the dataset.
From the above visualisation we can observe that ""director"" has a peculiar pattern that stands out.""cast"" and ""country"" have a similar pattern of missing values while column ""date_added"" shows a different pattern.**
",294bfe9f,0.11904761904761904
5939,fc8e0042411c46,a13ae71d,"This are few field where human psychology, consumer behavior & business understanding overpowers the statistic interpretation of the data ",af476c2a,0.11912225705329153
5940,bd380b97b5c894,9a4aac19,## Age,66f2562a,0.11926605504587157
5942,20b372b6e4e276,7ba3efda,"### Commit 5

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5

LB = 0.713",ec8b0860,0.11940298507462686
5944,a4aa36df07fd53,40dc4557,"## Simplfikasi Data 

Untuk mempermudah proses analisa sebagian besar data kita drop saja agar pelatihan bisa lebih simple dan dapat dengan mudah dipahami.",d2f42b6d,0.11940298507462686
5946,21413205980558,73cefa04,# Section Ⅱ Business analyse,84197de0,0.11940298507462686
5952,5cb7f999fd1ecb,d6834348,"### Content:
1. Missingno - Missing Data
2. Data Cleaning
3. Seaborn - Bar Plot
4. Plotly - Donut Chart
5. Plotly - 2D Histogram
6. Plotly - Map Box
7. Folium - Map",88b54f70,0.12
5954,10c5a39a87c47e,e0a10ccf,"## Step 3: EDA -> Checking sample images<a id='step-3'></a>
In this step we will check the actual sample images of both infected and normal blood sample",09c7337a,0.12
5959,bbad077c274022,e08f9e00,### Let's have a look at the data we have,3c2e3dea,0.12
5960,7e1da639035ac5,f99c35f1,# <a id='5'>5. Data cleaning</a>,120b6c23,0.12
5962,639e8aae4e046e,fbbf8122,**Data cleaning and feature engineering**,77deb4cb,0.12
5967,d6ddbe57f59cf7,59e46f05,# My random dataset is ready with some categorical value as well as time series info,504a3cda,0.12
5972,8854f72e7e9be0,fba77b8f,**Some Insights**,2a1031b7,0.12
5978,83df814455f06c,479eb0eb,"## **5.2 Gini index** <a class=""anchor"" id=""5.2""></a>

[Table of Contents](#0.1)


Another attribute selection measure that **CART (Categorical and Regression Trees)** uses is the **Gini index**. It uses the Gini method to create split points. 


Gini index can be represented with the following diagram:-",c9cff71a,0.12
5982,50d4ddf1953997,4c865437,"The heatmap presents a large number of missing directors. 
From printing the head of the dataframe, the missing values are identified to belong to the TV shows.

For the next step, the popular countries are visualised in terms of number of TV shows and Movies.",90bdddd6,0.12
5983,916a9275d9326e,4fd88b67,# データセットの確認,cbe4b24b,0.12
5991,4daf6153275cbf,9d4c13f6,### Data Cleaning Missing Values Imputation,51db1961,0.12048192771084337
5992,835a7b4e660d23,1bcc949c,"### We have to change columns'  types but when I try change I get alot error so I tidy column step by step.
Check this : https://www.kaggle.com/sabasiddiqi/google-play-store-apps-data-cleaning
",53bc7a6e,0.12048192771084337
5994,957e035ba5b9d5,db830cf6,"# Create test set directory
os.mkdir('dataset/test/')
         
for d in src_dirs_2:
    os.mkdir('dataset/test/' + d)",778ab3d3,0.12056737588652482
5995,b61ab8f81dc03d,a6e80505,"<a id=""age_sex""></a>
### Age/Sex (Survived/Not survived)",64d05394,0.12056737588652482
5997,d42518f6cb0995,eb58ec1f,Answered correctly¶,26913a9b,0.1206896551724138
6001,84127ade6fde87,9f9baa38,"Nadkarni et al., “Natural language processing: an introduction,” JAMIA, http://mng.bz/8pJP. See also
https://en.wikipedia.org/wiki/Natural-language_processing.",f55d05b6,0.1206896551724138
6002,1cd8be6e679620,f30d1409,## Data Import,3ce15a43,0.1206896551724138
6003,00001756c60be8,1ae087ab,**Задаем функцию для подсчета метрик**,945aea18,0.1206896551724138
6009,f3c6048d1058e3,1a28c53e,"#### 1) Understanding Sentiment across Word count
- This visualization shows us that count of words in positive and negative reviews have same pattern",1d9056b0,0.1206896551724138
6011,5f32117bcd5255,a84c73e8,# FULL HEADER READING,85882abf,0.12080536912751678
6012,5f4ae633cfd090,e3fd5453,"Now that this is done, I can split the dataset into training and test set.",a30a16e2,0.12087912087912088
6014,efd44ce2c08541,807a6dc3,# Load CSV,ebc2d00c,0.12121212121212122
6017,7c89a32e3562ca,26b36c58,# EDA,32dd8913,0.12121212121212122
6021,b241b847319d13,5407a0b0,# **Loading Packages**,0fb698f0,0.12121212121212122
6022,b42180a6a5b42f,cecb1a87,## 1) Análise das mortes pela data da notificação ,987cea5f,0.12121212121212122
6027,c84925c8171900,7efc7e9f,"<a id=""import""></a>
<h2>   
      <font color = blue >
            <span style='font-family:Georgia'>
            3. Reading & Understanding the data
            </span>   
        </font>    
</h2>",e21ff7ec,0.12149532710280374
6028,e4525eb0c96f28,973de38c,"#### Dropping NaN values for columns

Our next move in cleaning was to drop NaN values for independent variables used in analyzing Sales. For each column we edit, we will explain why dropping the NaN values would work better than common single imputation strategies. The two most common imputations we could have done were either Mean Imputation, which involves calculating the mean value of all values in that column and storing it in all empty spots, or Hot Deck Imputation, where you copy the last observed value forward. 

- First, we dropped NaN rows for year. Mean imputation would not help in determining data for missing values. It would not make sense to determine missing years from any average column value. Hot Deck wouldn't make sense either as a games year cannot be determined from surrounding data observations.
- We dropped NaN rows for ESRB_Rating because it does not make sense to find mean imputation for a non-numerical value, and also hot deck would not determine a specific games ESRB Rating either.
- We dropped Country NaN rows for the same reasons that we dropped for ESRB Rating.
- We dropped Total_Sales NaN rows. Mean imputation would make sense, but upon examining sales over the years and seeing how there weren't that many obvious year to year trends, we decided it wouldn't work. Also Hot Deck wouldn't be a good indicator either because of the way the data was originally organized - Organized by sales, with ALL NaN values at the bottom of the data. About half of the original data was NaN, so therefore any bad assumptions made on the data would drastically skew the way the data is displayed. Therefore we concluded that we could remove all the rows with no Total_Sales values and we could still show accurate values.",2093a1f1,0.12162162162162163
6030,62037c5832129c,03c6d0f7,"### Summary of the steps
* When you review the column definitions of the dataset you will see the first column is the unique ID of the sample and the second column is corresponding diagnosis of the sample (M=malignant, B=Benign). 
* **The goal of our model is to predict the diagnosis of the sample**. 
* We are going to encode the **diagnosis (M, B) into 1,0**
* **Split the dataset into training and testing set** - Very important step. If you don't do this your model will have good results when you are testing it but when you deploy your model it will perform really bad.
* Apply **Standard Scalar** on all the features so the data is standardized before we feed it to **linear classifier - Logistic Regression**
* Doing **dimensionality reduction** using Principal Component Analysis (PCA) - **Feature extraction technique**
* Running **Logistic Regression model** - fit so our algorithm can learn from the training set.
* Predict on the test set - Evaluating how well your model is able to predict the output
* Using **Accuracy** as our metric",61474350,0.12162162162162163
6031,27d5291d6365ba,fd0be174,# Transaction Volume over the dates.,96b30229,0.12162162162162163
6033,71c3c1eab0377d,75373a71,##### Column: Name : Feature Engineer Column 'Name',52b4e360,0.12173913043478261
6040,786475feda0190,6dd1fbaf,### Basics of Librosa.,e4663d97,0.12195121951219512
6043,4246295d91a6c1,52c773c7,Read data,9138104a,0.12195121951219512
6044,fd4017c1514157,d89bcfab,"## ***Train Metadata***
* It specifies the audible species for each recording.
* Consists of information like Primary Label, Secondary Label, Type, Location, Time & Date, Rating etc.",fd8f0896,0.12195121951219512
6045,47b2c9be5e31cb,77f8b20d,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",7d4afe56,0.12195121951219512
6047,e19e307b3fd188,da39c13b,"As it is a Kaggle dataset, it is normal to have no null data.",2173955b,0.12195121951219512
6051,892be0a523578c,9dff9b50,"**1.4** According to these findings, I think the invalid records in this dataset should not be included in the analysis, which will cause bias to the outcome. Regarding to not half of the participants have full records, to make this analysis more meaningful, I will try including participants with at least 20 records.  
               ",b0e8d7c0,0.12222222222222222
6053,d6cbd7160961dc,2a0bbe0b,---,36d74664,0.12222222222222222
6055,3597174a998d4d,2f36c6ea,## 2.1 Variables about hotels,276892ed,0.12222222222222222
6056,b0c2805cd5c087,9ed521c0,Image startup.farm,0446f327,0.12222222222222222
6059,4c47839b067546,1d0d615d,"### model_info	model_name 
### 'model'
",1f517b02,0.12234042553191489
6061,2343dc02ffb96a,223e8e5f,# Check for null values.,29aa95a4,0.12244897959183673
6064,f1e162ddd14f11,b1c51413,now check for missing values if any,cdb2e771,0.12244897959183673
6069,eb33e05704d647,93be26a9,Define ROI Pooling Convolutional Layer,cd80436d,0.12244897959183673
6071,510b8303776bb6,a5ad66c6,"Max: It is for all the categorical data where we are replacing the null with most common occuring data in that column.

Zero: We are replacing by zero because the value of all fields related to basement and garage have similar null counts which indicates that it is plausibe that in these houses there is no basement/garage so we should replace it by zero.

Median: It is for all the rest of the fields where the data is numeric and has some sort of order to it but also is either categorical or has some outliers.(Although we could have used mean here, the mean is affected by alot by outliers (For decinding this, the box plots have been drawn later.))

NA: It is for those data fields where NA is a separate category as mentioned in the data_description file and hence we have to consider it as an individual field.

Mean: It is for numeric fields that do not have many outliers. 

P.S: We have only defined replacements for the fields with missing values in either test or train sets.",18080db8,0.12264150943396226
6077,9e27af2600925c,2dd31f03,"Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. 

**Exercise:** Find the values for:
    - m_train (number of training examples)
    - m_test (number of test examples)
    - num_px (= height = width of a training image)
Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`.",9b556435,0.12280701754385964
6078,fe7360cddc13e5,8cf90438,------------------------------------------------------------------------------------------------------------------------,8979e423,0.12280701754385964
6079,5ce12be6e7b90e,100eacf4,"## `bool`

Lastly, **`bool`** is for boolean variables that are either `True` or `False`:",c0ab62dd,0.12280701754385964
6081,9f3710be6aea65,8ef656c6,We will eliminate the irrelevant features and the one that contains too many nan values,ae9bda88,0.12280701754385964
6082,30fdc4a6e3c1db,02e905c2,"40% of the rows are about California, 30% rows are about Wisconsin and Texas",6111ddee,0.12280701754385964
6086,2ada0305b68956,b4d526a5,### 18. Palette = 'Greens_r',133e26f4,0.12285714285714286
6087,979f1e99f1b309,f7ada32e,# EDA,d1bfebbf,0.12295081967213115
6088,09751c520b0616,fb2efd87,#### (i) Dealing with most null value feature in dataset,a4d0c7e9,0.12307692307692308
6090,be9597c72542a2,a049f789,# **What is the mean value of daily yield?**,6f29c6d8,0.12307692307692308
6102,b01ee6cb674fa3,2763d37d,"# Ideas

Split of the info contained in the columns into new ones, as described 

*  colun 'location'
        code 
        name
        region
        country
*  coluna 'datum'
        yearano
        month
        weekday
        launch_time
*  detail 
        Rocket name
        mission name",a8ffd35e,0.12318840579710146
6105,91473a39b85068,75a5b8ff,"## Data Overview

Refer: https://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction/data

All of the data is in 2 files: Train and Test.

Train.csv contains 4 columns: Id,Title,Body,Tags.

Test.csv contains the same columns but without the Tags, which you are to predict.

Size of Train.csv - 6.75GB

Size of Test.csv - 2GB

Number of rows in Train.csv = 6034195

The questions are randomized and contains a mix of verbose text sites as well as sites related to math and programming. The number of questions from each site may vary, and no filtering has been performed on the questions (such as closed questions).",6e3d91c2,0.1232876712328767
6111,4883314a96dc34,e89167d4,"**Ways data can be preprocessed:**
* Remove missing values
* Remove outliers
* Remove duplicates
* Feature selection
* Feature engineering
* Feature scaling
    * **Numerical** data = standardize or normalize
    * **Categorical** data = one-hot encoding or dummify
* Group data into clusters
    * Cluster by an attribute (e.g. age, price)
    * Cluster using k-means",50d36836,0.12345679012345678
6114,e67925694c07d3,2f146021,### TARGET DISTRIBUTION,83af4c4a,0.12359550561797752
6115,04ff2af52f147b,5eb3343d,"Again, we see that there is a significant difference in survival rates based on this feature.  Our assumptions were well-founded.  We will proceed with these results in mind as we process the data and make predictions.",d5f37be9,0.12359550561797752
6118,8ec771f5600a61,03c15792,"### We can clearly observe from graph that females are given priority while rescuing as majority of the females survived.
### whereas majority of Males died.
### Still we are not clear about the people who Survived as of what age group people survived or what Class of People Survived

",48364c1f,0.12371134020618557
6119,2a123b4e8f9433,3368753d,"After viewing the data in excel, there weren't any duplicate columns, but we can drop redundant columns: id, title, console
",0a082218,0.12371134020618557
6120,bbaa07ad21cf4e,12f46210,### Common words in text,3ab6b254,0.12380952380952381
6126,ac1abfe1dfe815,824a0a75,"**`negativereason_gold`, `airline_sentiment_gold`, and `tweet_coord`, are missing more than 99% of thier rows, so they can't be useful and should be dropped.**  
`Time zone` , `tweet_location`, and `tweet_coord` can be processed and merged into one column using `Geopy`**  
But right now I'm focusing on the other columns.",6529dbcb,0.12389380530973451
6130,c80939c7c626cf,6c3ae7d8,# Similarly count of null values in test data is done below,b9ac31e2,0.12408759124087591
6132,917957c6c4065f,bf0b5fb9,"category_id와 json 파일을 사용하여 텍스트로 된 category 열을 생성합니다.  
코드 참고 : https://www.kaggle.com/yontodd/us-youtube-eda-when-how-often-which-category#Cleaning-and-preparing-the-data",55b8ed68,0.12418300653594772
6134,3cc097a5859dc1,5aa4f607,# **View our target variable RainTomorrow**,14380d73,0.125
6135,68cceffe5bb8ec,711c5130,# Load dataset,dcbfcd6e,0.125
6138,f87a77cc6f9891,1daca3e5,# **Support Vector Machines**,69b52681,0.125
6140,6e28c4f557f736,4019a113,### EDA,021fdf75,0.125
6141,69130a37583a06,82c94a60,"TransactionID	:	Transaction index		Numerical

isFraud	:	•	Logic of labeling: "" reported chargeback on the card as fraud transactions and transactions posterior to it with either user account, email address or billing address directly linked to these attributes as fraud too.

			 •	If none of above is reported and found beyond 120 days, then legit transaction (isFraud=0).
             
			 •	Mislabelling (e.g. due to unreported cases) can be considered to be “unusual cases and negligible portion""""	:	Categorical
TransactionDT	:	
                   
*	time index
* timedelta from a given reference datetime (not an actual timestamp)
*	test set transactions occurred at a later timeframe (relative to train set)""""	:	Numerical
                     
TransactionAmt	:	""•	Logic of labeling: reported chargeback on the card as fraud transactions and transactions posterior to it with either user account, email address or billing address directly linked to these attributes as fraud too.
•	If none of above is reported and found beyond 120 days, then legit transaction (isFraud=0).
•	Mislabelling (e.g. due to unreported cases) can be considered to be “unusual cases and negligible portion” ""	:	Numerical

ProductCD	:	""•	product code, the product for each transaction
•	5 unique categories""	:	Categorical

card1	:	payment card information, such as card type, card category, issue bank, country, etc.	:	Categorical

card2	:	payment card information, such as card type, card category, issue bank, country, etc.	:	Categorical

card3	:	payment card information, such as card type, card category, issue bank, country, etc.	:	Categorical

card4	:	Issuer (e.g. amex, visa)	:	Categorical

card5	:	payment card information, such as card type, card category, issue bank, country, etc.	:	Categorical

card6	:	type of card (e.g. debit, credit)	:	Categorical

addr1	:	purchaser billing region	:	Categorical

addr2	:	purchaser country	:	Categorical

P_emaildomain	:	purchaser email domain	:	Categorical
R_email_domain	:	recipient email domain	:	Categorical

M1 - M9	:	Numerical	:	Categorical

dist1	:	distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc. 	:	Categorical

dist2	:	distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc. 	:	Categorical

C1 - C14	:	counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.	:	Numerical

D1 - D15	:	timedelta, such as days between previous transaction, etc.	:	Numerical

V1 - V339	:	Vesta engineered rich features, including ranking, counting, and other entity relations.	:	Numerical
",65a4de1c,0.125
6144,a1c7a94fc12ad8,9126bc07,work in progress,c67e3237,0.125
6146,37b09262279764,ae2654ab,### Cleaning the data,37c4c417,0.125
6151,9a040a4f21091e,2ebf1a0e,**Toxic Text Examples:**,f591b57d,0.125
6161,5e02999ca74e7e,39d9b0f5,# **Load Dataset**,b69da28e,0.125
6162,593d1d3d1df05a,f1c2b62c,# Adaptive Thresholding,bc682ffe,0.125
6163,2c8119a4061997,1a266186,# Data,1836a79c,0.125
6168,7f74a04ae75792,9cdc238f,### Remove duplicate rows,d01e91da,0.125
6170,c65a65d4041018,e41f6af5,"For all countries situation is similar: those who seriously took the survey, spent ~15-30 minutes on it.",824fb229,0.125
6173,0a918602a04693,18ddab45,There is presence of unbalanced dataset in categorical varibale Churn.,c1ef0e95,0.125
6177,28a1ff0f223da9,4c6edabe,"# **Task-1**
Which area of interest/expertise is in abundance in Pakistan and where we need more people?",c945b27d,0.125
6182,117fc0956643d0,be975d1b,"<div id=""step0""></div>",68cef9fd,0.125
6184,3cd78d8d6d56e4,801b4ba1,"### Label Value Count
Visualizing the label distribution of the full train dataset.",9f632e94,0.125
6188,69ac33d79f5130,499731b9,"#### Description of the entire datasets like mean, variance, standard deviation etc.",9d760d2a,0.125
6190,fae5023faa435f,56ace47b,"# Data Loading

We use ticker method to retrieve the data from Yahoo Finance website. 
In another approach we use the existing dataset that contains stock prices for over a period of 6 months for all the banks under Bank Nifty. ",b37c893b,0.125
6191,1d5daeca89f48d,09d3b78f,## Why do we need word embeddings,48d478bc,0.125
6196,8c7e00ca3dc5a7,d2c93e9e,## Primary Exploration of the Data,c83346e4,0.125
6198,4375764a7aa56c,7aef480f,"<p>If you like this notebook, please give it an <span style=""font-size:24px;color:red"">Upvote!</span></p>",6519c78d,0.125
6212,1005ca950e8a81,db942d57,"**Les fonctions metrics:** They have two parameters namely labels_true, which is ground truth class labels, and labels_pred, which are clusters label to evaluate.",52570331,0.125
6213,ffc9490c4f6c38,5ee93f29,## feature histgram,ae7bbbb3,0.125
6215,13c7672da1b571,611d2ff7,"First, analyze general facts about the data (if there are missing values and what types of data occur).",002d3ec0,0.125
6216,df51d4c54fbb91,9795dfe3,We slice the dataframes to define the features and the labels,4226dd72,0.125
6221,a69d41047fdd3e,f7e09f00,Use the next code cell to fetch the dataset.,b1f28647,0.125
6222,d1ff7e10ee0102,e39adc29,"*'Ah! I see you that you use seaborn makeup when you're going out... That's so elegant! I also see that you:*

* *<b>Deviate from the normal distribution.</b>*
* *<b>Have appreciable positive skewness.</b>*
* *<b>Show peakedness.</b>*

*This is getting interesting! 'SalePrice', could you give me your body measures?'*",2cc71c3c,0.125
6223,2c3a6969252dc0,c6b6c3ec,"Data looks fine, no missing value and the the data types are right.",d30f10ce,0.125
6228,95656e8d666b16,f9643baa,No null values; Let's check for duplicates,65e88599,0.125
6229,9ec2fb131cf677,bb3d912e,"# Ted Talks Speaker Statistics

- Most Duration Author Viewed Author is  **Douglas Adams - Average 5.2k**
- Most View Count Author is  **Amy Cuddy - Average View 43 Million**",211ea6bd,0.125
6230,e82462cdc998a7,7e849ea9,"## 2. Global parameters <a class=""anchor"" id=""2""></a>",b39bf244,0.125
6233,c5fef7cc592736,8d8ab21e,"The first step is to get our data. Data is organized in 2 files, `train.csv` and `test.csv`. In both files each row is an image containing a digit and each column contains the 8bit value for the pixel. Additionally `train.csv` has a column indicating the label of each image",d21dc2c1,0.125
6235,5ffe6aa38958a1,38402096,"Why do we have one less column in test?

",11f5412e,0.125
6239,62487bcd70b199,160638a9,##  <a id='3.1'>3.1. Distribution check on Numerical Features</a>,f6ae50af,0.125
6240,f13534449a3750,ac7f49c0,"<a id=""subsection-one-2""></a>
## Functions",8b7f3332,0.125
6241,57740be713cf12,47521b8c,## ***2. Data Preparation***,ac122df5,0.125
6243,2a377ced98d67a,dafcaf8b,### 2.1. Show input file stats,262231a8,0.125
6247,08e3444f9eddcf,a639030c,# Load Data,1d9d4f73,0.125
6248,02773bdc5d3c7a,c12d4843,"# Outline of the notebook

1. Import the required libraries 
2. Import the data into a dataframe and preview the target column
3. Remove the time column as time reveals the time after the first transaction and logically should not impact the target column
4. Create an empty dataframe for storing the performance metrics for multiple algorithms
5. Split the data into training set and testing set
6. Deal with problem of class imbalance with the following methods:
   a) By doing nothing and training the algorithm with the data as it is
   b) By oversampling the minority class in the training dataset
   c) By undersampling the majority class in the training dataset
   d) By applying SMOTE(Synthetic Minority Oversampling TEchnique) on the training dataset
7. The accuracy, precision, recall and f1 score were compared for all the above methods for 2 algorithms:
   a) Logistic Regression
   b) Decision Tree",86245f35,0.125
6265,169177b6e9edea,b2569681,"<ul><b>Pronouns:</b>
    <li> Mr : 0</li>
    <li> Miss : 1</li>
    <li> Mrs : 2</li>
    <li> Master : 3</li>
    <li> Others : 4</li>
</ul>",ca42152f,0.12631578947368421
6266,840534f2908a9c,05cd27b5,"*The test data is very clean, with no null value or zero value*",8081c3cc,0.12631578947368421
6272,08f845750d026a,6b8d1c09,### Which countries have the smallest loan amount?,1c54de30,0.12658227848101267
6273,5d2a3e82679cf3,a0f8598a,"- In real life, we know that there are huge differences btw baseball players' salaries.
- Therefore, i won't evaulate the salaries which are more than 1500 as outliers.  ",9e60b1e3,0.12658227848101267
6283,b01ee6cb674fa3,11e62136,"### Location

it may contain 2, 3 or 4 types of info, all set into one string
- Launch site code
- Launch site name 
- state, or geolocation (it may be not present)
- Country ",a8ffd35e,0.12681159420289856
6284,20b372b6e4e276,047335bc,"### Commit 6

* Dropout_new = 0.15
* n_split = 7
* lr = 3e-5

LB = 0.709",ec8b0860,0.12686567164179105
6285,f3d5d8917ce5df,4bbf208a,"# Melt!<img src=""https://www.wikihow.com/images/thumb/4/47/Make-a-Crab-Melt-Sandwich-Final.jpg/aid2170143-v4-728px-Make-a-Crab-Melt-Sandwich-Final.jpg.webp"" width=200 align = right valign = ""top"" hspace=""20"">
The pandas melt method unpivots column values and puts it in rows... i.e. melting creates a row for every day for an item/store combination. Doing this puts the data in a flat format that in a few cells will allow us us to merge the sales training data with the calendar and pricing data sets.",e45112f8,0.12698412698412698
6287,06ecf7a304c309,ef063bf3,"이런 유형의 데이터에서 문제점은 투영을 함과 동시에 데이터에 대한 손실을 복구할 수 없다는 것입니다.
아무리 많은 이동과 회전으로도 원본 데이터는 복구할 수 없습니다.

그렇다면 신경망은 이 문제를 어떻게 해결할까요?
deep neural network는 선형 데이터를 만들기 위해 공간을 구부릴 수 있습니다. 그렇기에 오토인코더는 이런 hidden layer의 기능을 활용하여 저차원 표현을 학습할 수 있습니다.

![bend the space](https://i.imgur.com/gKCOdiL.png)

이제 케라스를 이용하여 이미지에 오토인코더를 적용해봅시다.",714de627,0.12698412698412698
6295,5a8c553e21c70f,58004896,"Features and corresponding labels are assigned to **data_X** and **data_Y**, respectively. Using **Pandas** **info** function, we inspect column data types and number of non-null values in **data_X** and **data_Y**.",9ebd9d8f,0.12727272727272726
6297,71b75664517244,d6ad6910,"Quite shocking here, i'm not a fans of premier league and i know Manchester United has a great history. Now i see it with data, this is really amazing how MU won almost half of all season on premier league.",fc905af5,0.12745098039215685
6298,842547b2def18c,6f6510be,"> ***サンプルにわたって，数値型の特徴量の分布は何か？***

実際の問題領域のtrainデータセットはどれくらい(母集団を)""表現""しているのか？
This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.

- trainデータセットのサンプルサイズは891．これはタイタニック号に乗船していた実際の人数の40%に相当する．
- Survivedは，0 or 1のカテゴリカル変数．
- trainデータセットのサンプルの約38%は生存していました．これは実際の生存率である32%を表現している．
- ほとんどの乗客 (> 75%) は両親や子供と共に旅行していなかった．
- 30%近い乗客は，兄弟/配偶者と共に乗船していた．
- \$512よりも高い運賃を払っている乗客 (< 1%) の中では，運賃が有意に変化していた．
- 65歳-88歳の高齢者 (< 1%) がいる．",b8efde6d,0.12745098039215685
6300,52cfd66e9ec908,4924ea91,"**UPDATE: Finally got GPU to work by manually installing everything, adding utility scripts did not help at all. Took a painfully long time to get myself to realize that everything needs to be done in the kernel or things will break.**",c74adcdf,0.12745098039215685
6301,726833f92fb87a,88d87171,There no missing values and there are a lot of categorical features in the data. They will be analyzed first and then encoded before the ML training.,7dc5e1b6,0.12751677852348994
6302,5f32117bcd5255,060e8d7b,"### ATTENTION
* RUN FOR CHECKING",85882abf,0.12751677852348994
6303,c7e5f658090347,fa729977, <a id='Exploratory_Data_Analysis'></a> ,43c78e7d,0.1276595744680851
6306,4c47839b067546,32eb9411,"Как видно из таблиц, train.model содержит ту же информацию, что и в test.model_name, поэтому просто переименуем признак.
А признак model_info дублирует model_name, удалим test.model_info.",1f517b02,0.1276595744680851
6307,957e035ba5b9d5,d6c50fbe,"# Rename directories
shutil.move('dataset/training_set/', 'dataset/train')
shutil.move('dataset/validation_set/', 'dataset/validation')",778ab3d3,0.1276595744680851
6314,ba4b3bd184acbb,a7e7c45d,"#### HTML

Another practical datasource is an HTML file.

The `read_html` method will extract a list of tables from within an HTML source like a URL.

The tables found within the webpage are returned as a list.",0f5de724,0.12781954887218044
6318,9eed0fae1c7958,5830e774,# import TF & check the version ,3fb1438e,0.1282051282051282
6319,4d91e84c564cbe,292d9542,What's the next closest planet?,355a43e3,0.1282051282051282
6329,e424c111c44669,dd72189a,**Balance of dataset**,d9fccfba,0.1282051282051282
6333,9b42412e75d640,9782e1d5,"Classes 3,4 and 8 account to only 5% of data. In this case data oversampling would not be approrpiate as majority of wines on the market are greaded as 5,6,7. Smaller number of wines are of very low or very high quality. 

I will define 2 new categories for our ""quality"" column. If quality > 5 then category will be ""good"" or 1, otherwise ""average"" or 0.",b616570a,0.1282051282051282
6335,897ca904b74a98,3f25ce70,### Remove useless columns,c5844ad4,0.1282051282051282
6336,63b44c85e32c1f,eb8c3784,"Now observe that z1 is not at all a nested list thus to access 'apple', z1 should be indexed at 0.",fb9b9562,0.12837837837837837
6339,38b79494ac749e,36c78197,Let's pick a target function $ f(x) = 2\cdot x + 10\cdot sin(x) $ and generate some noisy samples to learn from.,39162a40,0.12857142857142856
6341,675b60eaf415a6,20681d24,### **Understand dataset structure and files**,68c0b725,0.12857142857142856
6346,2ada0305b68956,b2c093ab,### 19. Palette = 'Greys',133e26f4,0.12857142857142856
6347,fe6750354fb64f,bf2edb11,# Visualization,271741f0,0.12857142857142856
6349,30fdc4a6e3c1db,8a0ad218,Total sales from each of the state,6111ddee,0.1286549707602339
6360,0d9a2067267ba1,5b498b8f,"### Target
Let's plot target distribution",abc194fb,0.12903225806451613
6361,f15eac23fbcc9d,dd4616c3,"First, let's import all the libs that we might need. Notice that according to Jeremy of Fast.ai, this 'import *' is not your standard Software Engineering best practice, but it's somehow makes sense for interative and interactive data science experimenting. Just so when you want to try something different, you don't have to go back to the very beginning of the notebook and import the libs you need. It makes the flow of thoughts and experiments smoother. ",ea46d8af,0.12903225806451613
6362,b59b5aaeedb1fb,ef913026,> #### features with null values,1ad63faf,0.12903225806451613
6364,0cb456a5456cf9,2ae714d5,# **Q1** <br>**a.Which kind of hotel is more popular?** <br>哪一种酒店更受欢迎？<br> **b.Which kind of hotel have higher cancel rate?**<br> 哪一种酒店有更高的退订率？<br>,5701729c,0.12903225806451613
6370,535da6591a3246,acf9afda,"# Basic EDA
Here we see that gender, ssc_p, hsc_p, degree_p, workex, specialization and mba_p are the more relevant features",9ef8d0c7,0.12903225806451613
6374,7dec6bdea6d779,f1b23216,"In order to compare the performances of models, it is important to seed everything. This means random and numpy, but also tensorflow and Keras. The following code was taken from [this article](https://towardsdatascience.com/properly-setting-the-random-seed-in-machine-learning-experiments-7da298d1320b) from Cecelia Shao. The article is worth a read for a more in depth look of the effects of correctly seeding.",18be5949,0.12903225806451613
6381,ac04ba639d1c93,3f1f611a,## All function used in this kernel,748059d5,0.12962962962962962
6389,233cb23d9e01b9,d0886fdf,# EDA,ffa56c19,0.12962962962962962
6391,c13f73168789c2,68b61ea0,"### 1.2 To select multiple columns<a id='4'></a>
Syntax 1 : `df.loc[:, ['column1', 'column2', ...]]`

Syntax 2 : `df[['column1', 'column2', ...]]`",16175052,0.12987012987012986
6393,75adb7945ef9bd,6344e38d,Text is all non-null. Only a small percentage of tweets have no keyword. Location has much more null values.,785c5095,0.12987012987012986
6395,722cd844dfbe8f,b8d49d77,"We can see that the data structure is the same as for the submission file, knowing that here **MGMT_value is indeed equal to 0 or 1 and no longer a probability**.

Let's look at the distribution of the values of this variable in the train set :",0cedb385,0.12987012987012986
6399,83df814455f06c,7d17f397,"## **Gini index**

![Gini index](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRzYHkcmZKKp2sJN1HpHvw-NgqbD9EnapnbXozXRgajrSGvEnYy&s)


Here, again **c** is the number of classes and **pi** is the probability associated with the ith class.",c9cff71a,0.13
6402,e19e307b3fd188,8e7ede49,### Rent amount (R$) - Analysis,2173955b,0.13008130081300814
6405,73ca9abcc2034e,a1369d34,# Check the head of the dataframe,cec3446c,0.13043478260869565
6410,71c3c1eab0377d,a81e0cf3,Regular Expression,52b4e360,0.13043478260869565
6416,f6c1eb62cceb70,ffe8d1da,2. Explore the Data,90a1b790,0.13043478260869565
6422,9b5de3823ad5ab,7236d79b,"### Getting the data we need

Since the directory structure don't give us much beyond labeling the images as train and test, we'll start by getting the path to all images using `glob` and saving it to a Pandas dataframe.",33e48774,0.13043478260869565
6425,ea4e559a86d613,256eaeaf,# Missing Values,eff47843,0.13043478260869565
6426,fe118026267a88,f1a074b6,# Exercises,612efa48,0.13043478260869565
6429,548f961125248d,d63fcf29,"

* Target: ACTION
* 9 categorical features (represented as numbers for privacy reasons)

",d8c5e8b8,0.13043478260869565
6432,59236ba162ab7b,9b492dfa,"2. loading data
",ace5b0ef,0.13043478260869565
6433,cfcb3bdee4f1e4,c55d270e,**Exploratory data analyis**,eb4ed2ae,0.13043478260869565
6435,69d50f5e1373f1,1467737a,I like to use the `Path` class for my paths.,ec7545ee,0.13043478260869565
6438,0e2a23fbe41ca9,377e6c9f,"Observations:

- Values range from -33.2 to 17.9
- -33 seems like an outlier as can be seen in the 3rd plot
- other values less than -10 also seem like outliers due to very less in number
- All values above 10 are also looking like outliers
",64e4762c,0.13043478260869565
6439,e3fb4c6300cb56,1805cbfe,"<a id=""1""></a> 
## Bar Plot",8ebbdf89,0.13043478260869565
6440,598b6228760590,2f663c06,"- Pclass
- Ticket class . A proxy for socio-economic status (SES)
- The ticket class can reflect the personal economic situation to a certain extent. It can be seen that most people's class is category 3, and the number of class 1 cabins is relatively small, indicating that there are still a few wealthy families. It is worth noting that people with a class of 1 class have a relatively high survival rate.",be30ab66,0.13043478260869565
6443,a4f8ad33c823c5,dec086b5,"In the section below, I explored ways of dealing with imbalanced datasets.",fcd48307,0.13076923076923078
6446,09751c520b0616,64db42e9," - Calculate null values in dataset<br>
 calculate percent null value in features of dataset and
 Features add in 'drop_null' List which<br> has Most null value percent or grater then 50% null value",a4d0c7e9,0.13076923076923078
6447,c84925c8171900,6d38fbf8,"<a id=""input""></a>
<h3 name='libraries'>   
      <font color = purple >
            <span style='font-family:Georgia'>
            3.1 Importing the input files
            </span>   
        </font>    
</h3>",e21ff7ec,0.1308411214953271
6451,0858e1bb3cbaca,7dcdf456,# Data Attributes,78548374,0.13114754098360656
6454,bcd7e398c4d0ec,78ac2e17,"## Type
Type of animal (1 = Dog, 2 = Cat)",77a143f6,0.13114754098360656
6455,918040fad252ec,433fce53,Menyatukan data menjadi path atau satuan dan memberikan label,966fcd8f,0.13114754098360656
6457,fdbbd573ba31c2,3fa3d43e,### df_test,f7c28d74,0.13125
6459,3c2033cc99c12c,ae7ee398,#### Have a brief view of the distribution and deal with the outliers ,dfa22a54,0.13138686131386862
6463,fe7360cddc13e5,9d5fc1e1,"<font color='red'> **2- Müşteriler arasındaki işlem oranındaki heterojenlik, şekil parametresi ""r"" ve ölçek parametresi ile bir gama dağılımını takip eder.**",8979e423,0.13157894736842105
6464,f35ee6e9fab592,aff3ef7a,A check on null value percentage across the varied columns,b15f7073,0.13157894736842105
6465,d369f200a84c2a,79fc85e3,If you want more details about the vector representation and dynamic routing I invite you to read Cezanne's blog which is very clear.,8fef4d48,0.13157894736842105
6466,f05342aabe2b59,718af6ae,### Estimating p_i from observations,cfbb391f,0.13157894736842105
6467,16071987c1cb40,1de6afa2,"### Notes
1. To use sklearn with cudf... convert `cudf.Series` / `cudf.DataFrame` `.to_pandas()`..
2. Using cudf is the same as using pandas but `cudf` instead of `pd`..",835687fc,0.13157894736842105
6476,d93a87fdbdb3d2,197e9718,"#### Erase unnecessary data, <code>id</code>",30d079c3,0.13157894736842105
6482,0ad8d416b89b78,2d3f3025,"the profiling report identified a few issues that needed to be addressed before continued exploration, these included removal of outliers and null values which used the sentinel value '?'.",0b0562f0,0.1320754716981132
6484,23df07a474aaae,19559b4b,"# Data Visualisation

Visualization Includes
* Top 10 Streames Genres
* Top 10 Most Followed Artist
* Top 10 Highest Charting Songs
* Popularity based on Loudness",0ea40276,0.1320754716981132
6487,510b8303776bb6,dea31bf6,### Applying the replacements. ,18080db8,0.1320754716981132
6488,f015d0147e8fbf,f622741a,### Preprocessing Helper Functions,518954fb,0.1320754716981132
6490,2f47abddfd1928,60d2c17e,"Just from this table we can extract some initial information:

- Survived is not completelly balanced, less than 40% of the people that we have data did survive.
- Pclass seems to have more of its values of 3rd class.
- There are 1307 unique names, composed by name, surname and title.
- The most frequent sex is male.
- The distribution of age seems quite normal, being the mean and median similar.
- Although there are people traveling with 8 siblings/spouse, the most common is to travel alone and after that to travel with just one sibling/spouse.
- Also it is very strange to travel with parents/children.
- Even though it is not as unique as Name feature, Ticket shows 929 unique values.
- Fare feature seems to be quite skewed to the left with a long tail to the right.
- Cabin feature is probably one of the most difficult to deal with because it is mostly filled with NaN values and unique values.
- Embarked seems to be quite unbalanced, where most of the people embarked at Southampton.",ae33cc0b,0.1322314049586777
6494,c65a65d4041018,5abe2eca,### Gender and age,824fb229,0.1323529411764706
6499,3d905ce4828057,1f4fa396,"# **MISSION**
**6 months CLTV Prediction**

▪ Make a 6-month CLTV prediction for 2010-2011 UK customers.

▪ Interpret and evaluate the results you have obtained",5b006cc3,0.1323529411764706
6500,eb0ecd6bebeb15,ba25e1bd,Veri çerçevesindeki değişkenlerin hangi tipte olduğunu ve bellek kullanımını görüntüleyelim.,d7b93a60,0.1323529411764706
6506,c9b4e282e4e2c1,db40891f,2-Visualizating the data. Let's make questions about this dataset:,f44d339f,0.13274336283185842
6515,7454fdc444df16,faff168e,"Each file has 2 sub-folders, labeled 1 and 0 <br/>
Folder 0: Non-Invasive Ductal Carcinoma (IDC) <br/>
Folder 1 : Invasive Ductal Carcinoma (IDC) <br/>",a7818ef5,0.13333333333333333
6518,06e0990b9b0dcc,9996cb6d,## epsilon greedy ,b36f10d2,0.13333333333333333
6519,6f8bd70d7430f6,e33d3a1a,**Import Libraries**,8bb65eb0,0.13333333333333333
6521,e78f177ca86768,4be8ada4,# Hyper-parameter tuning,120e25c1,0.13333333333333333
6523,4bada947d597ac,0e400c9b,# Making Testing dataframe,eab5094a,0.13333333333333333
6528,6fad63bfd45ef9,ad22d2fe,# Load Data,b3c6f1d6,0.13333333333333333
6530,cf4d1c1ad1476c,e11841cb,# TPU Configurations,768c1a59,0.13333333333333333
6532,d6cbd7160961dc,c25ab674,## 2.2. Benford's Law: Formula for the first digit,36d74664,0.13333333333333333
6533,396bc36edb95d3,a5c47aa5,### Univariate Analysis,965e4f8f,0.13333333333333333
6535,67b7354e96113a,f63c17d7,"**Observation:**

1)From this figure it is clear that missing nature of the data is same in both train and test data 

2)We have to figure out a common way to fill the missing data in both train and test. 

3)For that let's go for exploratory Data Analysis.",dca94250,0.13333333333333333
6537,d6ddbe57f59cf7,8d68278b,# Bar plot,504a3cda,0.13333333333333333
6539,9276fa5cc2fef6,4b60ac96,Let's print  insincere comments and have a look at few examples to get ideas for feature engineering,24aa6a52,0.13333333333333333
6544,6a1ae8234c7653,32fc81aa,Visualization of the top five rows of the train data set:,2d643c72,0.13333333333333333
6546,49ac6594c8f5cf,7ff64882,**Gender Gap**,6f19f28a,0.13333333333333333
6547,4fd4b6a80d40e3,ef0789a5,"## Activation Function

a = b + $w_{1}x_{1}$ + $w_{2}x_{2}$

y = h(a)

![image.png](attachment:image.png)",f6913cc3,0.13333333333333333
6553,e0e19e91579432,6a574e8c,# * Table of content,0c8a0755,0.13333333333333333
6554,d96e03a9e7c030,1efc317a,"Now that we created our functions to pull data from both the New York Times and the New York Department of Education, we can call the controller function (commented out in the snippet below because of Kaggle kernel restrictions on Internet access).  

*Note:* The web_scrape_controller() function can take **a lot** of time to complete, approximately 3-4 hours.

Let's take a quick look at the data frame as it is now:",d2b72ced,0.13333333333333333
6562,c18267b203f28a,dcb8e3ee,"# Load the data
",09ca8efb,0.13333333333333333
6563,b57862be79c695,3ecfa734,## Load Data,989222cb,0.13333333333333333
6569,3597174a998d4d,4e342195,"For hotels, the variables can be sorted into 3 sides:
* room type. According to the definations, I think assigned_room_type is more close to customers' actual situation then reserved_room_type. Also, based on adr variable and assigned_room_type variable, average transation per room type can be calculated (average transation per room type = adr*total_stay_night/the number of room type).
* booking response. The days_in_waiting_list can indicate hotels' booking response speed.
* distribution channel. It is the distribution_channel variable in the dataset.",276892ed,0.13333333333333333
6572,ff9142eb631dd5,e1ae4770,## Get rapids-kaggle-utils,453131ac,0.13333333333333333
6573,c8bf959b9608cf,3b9efdcd,#### Plot the content image and style image,155e3672,0.13333333333333333
6575,6998861ff6ff01,f513b844,"# Check the data type of our date column
___

For this part of the challenge, I'll be working with the `date` column from the `landslides` dataframe. The very first thing I'm going to do is take a peek at the first few rows to make sure it actually looks like it contains dates.",ea9e72cf,0.13333333333333333
6576,d58491f2896fc1,00e92a29,<h2>Genetik Operasyonlar ve Genetik Algoritmaların Özellikleri<h2>,514bfdff,0.13333333333333333
6579,892be0a523578c,8489a491,"Since dailyActivity_merged.csv contains data from other daily files, I will skip these files.",b0e8d7c0,0.13333333333333333
6589,e25c0f830df3f4,666cd3a4,# Displaying first 5 rows of data,fdcf7189,0.13333333333333333
6593,063a35f644e3c5,40aeeb70,"### duplicate rows in each dataset
",1c30fb0a,0.13402061855670103
6596,b01ee6cb674fa3,51f0903b,### Data split,a8ffd35e,0.13405797101449277
6597,9c26c5dcd46a25,06fdfd3e,"#### <font color=""#114b98"" id=""section_1_2"">1.2. Les contributeurs à la base OpenFoodFacts</font> 

Regardons à présent la répartition des entrées par contributeurs :",1bbbb677,0.13414634146341464
6598,0b01138ad120fc,dda5e9aa,**Visualizing the data**,0b4b72e6,0.13414634146341464
6600,5f32117bcd5255,0bf60d75,### PRIMARY,85882abf,0.1342281879194631
6602,fdc3afd309b850,570b2014,"More than 90% of the amenities items are null and we can't know if these values ​​are empty because there is not the feature or they forgot to fill those options.
So let's drop it.
",966bde38,0.13425925925925927
6603,2ada0305b68956,a8b000a6,### 20. Palette = 'Greys_r',133e26f4,0.13428571428571429
6604,ba655a261cc09e,baab9cb9,"Data missing in three columns:
1. Age - 20% of values are missing, we can fill in.
2. Cabin - 77% of values are missing, we cannot fill this out without critical errors.
3. Еmbarked - less than 1% of the values are missing, we can fill in the gaps or ignore them.

Данные отсутствуют в трех колонках:
1. Age - 20% значений отсутствуют, мы можем заполнить пропуски.
2. Cabin - 77% значений отсутствуют, мы не можем заполнить это без критических ошибок.
3. Еmbarked - менее 1% значений отсутствуют, мы можем заполнить пробелы или проигнорировать их.",48cc549a,0.13432835820895522
6606,21413205980558,3207b741, # 2.1 data description(数据描述),84197de0,0.13432835820895522
6607,20b372b6e4e276,11df43d7,"### Commit 7

* Dropout_new = 0.15
* n_split = 5
* lr = 4e-5

LB = 0.709",ec8b0860,0.13432835820895522
6609,1a222fee3089d2,3fa0e615,## **Age**,59ab8894,0.13432835820895522
6610,4ae6a182abac64,3876abe5,"* The next step we'll do some descriptive statistics, this one helps us to describe and understand the features of a specific data by giving short summaries about the sample and measures of the data.
",418676c5,0.13445378151260504
6612,5ce12be6e7b90e,8d735c1d,"# Variable names
* You *can't include spaces*. 
* In principle, you can use any unicode symbol.
* You can override words that have special meaning in python (for example `print`), but don't do it unless you have a good reason.
* The convention is to use *lowercase only* and separate words with *underscores*: `num_atoms`, `first_template`.",c0ab62dd,0.13450292397660818
6616,71d3e4aee86e3e,eb45303e,"## 4. General Analysis/Plotting of the number of Confirmed Cases, Recovered Cases, Active Cases, Deaths, Mortality and Recovery Rate
",69706f0b,0.1346153846153846
6618,582cb872d19026,32c22d8d,"# As a result of these examination; 

  ** it was seen that none of the columns in the dataset had null values. 
  
  ** In order to be sure of this, random samples were taken from our dataset and analyzed.
     It was observed that there were no abnormal values. 

  ** In addition, the types of columns in the dataset are examined.
     As a result, it was determined that there are 11 numeric columns and 4 categorical columns. 


At this point, it has been noticed that: 
  
  ** The ""installs"" column is seen as an object even though it has numeric values. This column needs to be corrected.",8d966d69,0.1346153846153846
6622,44f6a002ecd033,e1b0d45e,Married is essentially a boolean column split up into yes and no answers. Looks like more of the train dataset includes people who are married.,70bbe106,0.1346153846153846
6623,b61ab8f81dc03d,d7d2cbe3,"<a id=""pclass_sex_survived""></a>
### Pclass/Sex/Survived",64d05394,0.1347517730496454
6625,957e035ba5b9d5,f15f28cc,%ls dataset/*,778ab3d3,0.1347517730496454
6629,312135b445bd23,7fb04abe,The cleaning method:,8ced381f,0.1348314606741573
6630,04ff2af52f147b,113ce39b,"**Filling Fare Null Values:**

There is only one null value for *Fare*.  *Pclass* is certainly relevant for *Fare*.  It seems like having no family members travelling with them would also affect *Fare* (*Parch* = 0 and *SibSp* = 0).  Knowing this, we can fill in the median value of similar passengers.",d5f37be9,0.1348314606741573
6634,a6c34cd514e30e,bb954a13,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",bf603ddd,0.13513513513513514
6642,297cbe4a23c4bf,a4a7f827,# Removing Outliers,a843e619,0.13513513513513514
6644,bbb3f4b76a4559,981e463c,"### Preprocess : use only int or float
PCA can't handle non numeric value and NaN. This is simple way to preprocess. If you want to get high score, you have to this process seriously.",75185823,0.13513513513513514
6652,c4bca5d86a38c3,b9dbe5c9,"Probabilidades de que pasajero se salve segun letra con la que empieza su cabina
Haciendo OneHotEncoding de las 9 categorias de las letras con las que empieza una cabina
",e23d297c,0.13559322033898305
6653,1294fb4c86f993,71c817a4,### `Census Data`,4471e513,0.13559322033898305
6656,a81661cc35d8d2,8115173e,"<font size=""3"">Importing data</font>",3331f113,0.13559322033898305
6661,bb0905d33ae417,da65d941,"Exploratory data analysis should be the first step of every data science task. Due to the lack of domain knowledge, I will only check for the number of classes and the number of items per class. Imbalanced datasets may require resampling of the data to ensure proper training.",25fd1965,0.13559322033898305
6665,faa8e6c8ab9246,a234d4ac,Lets check the null values in the dataset.,2bea1419,0.13580246913580246
6667,98a6794067932a,1000af88,"Maintenant que les données des colonnes ""Order date"" et ""Ship date"" étaient transformées en dates nous pouvions ajouter une nouvelle colonne effectuant la différence entre ""Ship date"" et ""Order date"". Cette colonne nommée ""Treatment time"" nous sera très utile pour la suite de nos analyses étant donné qu'elle permet d'évaluer le temps de traitement en jours entre le moment où la commande est effectuée par le client et le moment où la commande est expédiée de l'entrepôt chez le client. Bref, la cellule ci-dessous permet dans un premier temps de soustraire le ""Order date"" au ""Ship date"" et retourne cette valeur dans la nouvelle colonne nommée ""Treatment time"". Dans un deuxième temps, le code présent dans la cellule retourne les premières lignes de la base de données afin de constater l'ajout de la nouvelle colonne.",08600fe2,0.13592233009708737
6674,a2444ab5d5f147,dd1ccfe9,### No Missing Values,10617755,0.13636363636363635
6680,a0b321057e7402,d39e0b49,"Each row presents a 1-minute trade interval. The NaN fields represent timestamps without any trades occurring. Most of the NaN fields are present very early in the dataset. Bitcoins were mostly unknown in this period, which means no a lot of trades.

Looking at the Timestamp column, we need to convert it from seconds.",5f73fb91,0.13636363636363635
6688,450fda47b03baa,4afa6bb1,Veri çerçevesinin son 5 gözlemini görüntüleyelim.,62c04adb,0.13636363636363635
6692,ae058c3f1439c3,d2d7c96f,**Import some Basic Libraries**,965da99d,0.13636363636363635
6694,da199f8fb59439,04073548,**Knowing DATA**,baaa665d,0.13636363636363635
6696,930cd79ca51204,add18c8a,Let's make a simple line plot from our data. ,5506779a,0.13636363636363635
6697,b066ab2167199c,5dc9dcea,"<h2 style=""color:blue"" align=""left""> 2. Load data </h2>",18a1753d,0.13636363636363635
6698,d83e5b44d1b80d,25ed9242,## Renaming Country names and few feature details,62845930,0.13636363636363635
6702,5083d7a61f2426,1c74f8b9,# Forecast Time Series,541a0fec,0.13636363636363635
6704,dd02a9b545f742,caf07033,"# Request of missing packages
Further information on the request guide is available at [Missing Packages](http://https://github.com/Kaggle/docker-python/wiki/Missing-Packages)

Missing packages
- [pmdarima](http://https://pypi.org/project/pmdarima/)",7116cd2d,0.13636363636363635
6705,9c19668d6b7295,8243cfa2,## Settings,35be7001,0.13636363636363635
6710,a566b5b7c374e7,f8132f48,### Prepare Sleep Journal table (journal_table),b3dc5545,0.1366906474820144
6713,49ee86d074de69,e72d1b4a,"<a id = ""6""></a><br>
## Remove Quasi-Constant Features
* There is no Quasi-Constant Feature",71ccc6d3,0.13675213675213677
6716,f91f58d488d4af,e8204d5b,"Now, we can use Pandas DataFrame to color code these values using a gradient.",5df1bbf3,0.1368421052631579
6718,1eb62c5782f2d7,06930757,###  B. z-score = -0.24,bb69f147,0.136986301369863
6723,91473a39b85068,3106b30d,"### Data Field Explaination

Dataset contains 6,034,195 rows. The columns in the table are:

Id - Unique identifier for each question

Title - The question's title

Body - The body of the question

Tags - The tags associated with the question in a space-seperated format (all lowercase, should not contain tabs '\t' or ampersands '&')
",6e3d91c2,0.136986301369863
6728,71b75664517244,7336054d,"## Worst Team

There is always a worst team on every league, and it's just interesting how a club keep getting worst position on every season.",fc905af5,0.13725490196078433
6736,64169805aacf17,123d674f,# Install CLIP and dependencies,1f12ded0,0.13725490196078433
6739,7cfd96218dd933,e51b3012,* DATA IS AT THE LAST 11 DAYS.,7c34d96c,0.13725490196078433
6741,ee23a565163388,b2957af6,"- This dataset is fairly clean and does not require any imputation as it does not have any missing values. 
- The datatypes of the features are also meaningful and does not require any type conversion.",88aacbc4,0.13740458015267176
6747,7e89d387feb9f5,50033fef,## 1. Информация о ресторанах,989e3a1b,0.13768115942028986
6750,434f930cb58aee,a7231dbf,"To get the same output everytime, we will use seed function. In addition, we define a metric call 'F-score' to monitor the performance of the model. In the case of multilabel classification, it is advised to use 'F-score' instead of 'Accuracy'.",0e1d3554,0.13793103448275862
6754,e3f3f108cd3869,fc340348,Dropping columns which having higher null values ,2b78de2d,0.13793103448275862
6756,a1ba5ffd30dbde,a7579a9f,- There are 195 rows and 24 columns,48e57546,0.13793103448275862
6757,6a1d04e8153df3,0004eb63,"**Contents and information from the table **
- In this ;
- Op_Year = Operation Year ,
axil_nodes = axillary Nodes , 
Surv_Status = Survive Status
- We can see there are 306 rows X 4 columns . 
- There are 4 columns But according to python there are 3 columns.

**What is axillary nodes ?**
Lymph nodes are small, bean-shaped organs that act as filters along the lymph fluid channels. As lymph fluid leaves the breast and eventually goes back into the bloodstream, the lymph nodes try to catch and trap cancer cells before they reach other parts of the body. Having cancer cells in the lymph nodes under your arm suggests an increased risk of the cancer spreading.In our data it is axillary nodes detected(0–52).",38572b05,0.13793103448275862
6765,00d295edcd117e,b4288c23,## 训练一个图像分类器,f5810f4b,0.13793103448275862
6768,d5f78aa381f58d,d1a08c57,# Data Cleaning,d60f358f,0.13793103448275862
6769,45921c50ac56fa,aa9654f2,We will be using the **'text'** column of the Training Data for our task.,465973eb,0.13793103448275862
6772,9535bb04ae042c,d52d9fda,## ii) Creating DataFrame,165b6fae,0.13793103448275862
6773,87e96e14f8f5ce,a23c294b,# Visualization of data,f9f7a3a2,0.13793103448275862
6774,84127ade6fde87,b6e6a02d,## Converting text to numbers,f55d05b6,0.13793103448275862
6776,5ea840754577e3,2ce703c2,There are 177 null values in Age column and 687 null values in Cabin column. Cabin can be ignored for now. Null values in Age column will be taken care of later in the notebook.,9cf9b73f,0.13793103448275862
6780,6b54e39f86bdb5,f913340d,Then I quickly visualize the five first rows of the data using the head() method from pandas,198084bc,0.13793103448275862
6783,f3c6048d1058e3,cc30407c,"#### 2) Understanding Sentiment across Punctuation count
- This shows usage of punctuations is same in both sentiments. This negate general perception of higher usage of punctuation in negative reviews",1d9056b0,0.13793103448275862
6784,6903d3f38c6a66,35bae984,"# 0. Setting

We set the resolution through the **dpi(Dots per Inch)** setting of the figure.

Matplotlib has a low default resolution itself, so setting this up is a bit more professional.

And We definitely want that.",6067ce5e,0.13793103448275862
6785,20e1ba19eb9b5e,a4ab0e61,## 1.2 Study the dataset,4569bfc1,0.13793103448275862
6787,fb5c6021d127ef,93a9df28,"## Linear Regression

Your dataset is quite large. BigQuery is especially efficient with large datasets, so you'll use BigQuery-ML (called BQML) to build your model. BQML uses a ""linear regression"" model when predicting numeric outcomes, like the number of riders.

## 1) Training vs testing

You'll want to test your model on data it hasn't seen before (for reasons described in the [Intro to Machine Learning Micro-Course](https://www.kaggle.com/learn/intro-to-machine-learning). What do you think is a good approach to splitting the data? What data should we use to train, what data should we use for test the model?",dd05cbd3,0.13793103448275862
6789,1750367e54f407,fac55de0,I keep 80% of the data provided for training and retain the other 20% for validation during my training process.,a8e655b2,0.13793103448275862
6792,ee9ddc756b2d4a,f2438bdd,# Load Data and EDA,e367eab3,0.13793103448275862
6793,e19e307b3fd188,0c77bb2c,"#### Histogram
",2173955b,0.13821138211382114
6797,be9597c72542a2,11cf2e31,"**What is the total irradiation per day?
# **",6f29c6d8,0.13846153846153847
6798,03048e86a6d806,b397a5f2,"Malaysia has the highest percentage of women respondents in 2020 and most of the countries in this list are from Asia. Ireland and Canada, each, is the country with the highest percentage of women respondents from Europe and America, respectively.",1285c231,0.13846153846153847
6799,a4f8ad33c823c5,c1d0931b,"## Remove the ID columns

Identifier columns do not explain much about the target variables. Hence, these identifier variables were dropped from the dataset.****",fcd48307,0.13846153846153847
6802,c115e287523aab,157a76fa,# Version Check,feb1288b,0.13846153846153847
6809,c80939c7c626cf,ff845fd0,# Now we need to visualise the data,b9ac31e2,0.1386861313868613
6810,3c2033cc99c12c,13abb390,"##### Data Preprocessing 
+ Min-Max Normalization: 
+ Outlier Detection ",dfa22a54,0.1386861313868613
6811,268a610bbc64b4,9318084b,Converting to datetime format,8a16f301,0.1388888888888889
6825,1d73d04c3aaae8,556c30a0,"## The Elo Rating as a Predictor

How good is Elo as a predictor? We need a few enhancements to the dataset.

For this analysis, I will discard tie games, unless Elo predicts a tie.



",cd43d0aa,0.1388888888888889
6826,166a62ebb4fc3a,0e175327,"The last column looks somthing fishy, bunch of NaN values. Let's get ride over it.",db48a079,0.1388888888888889
6830,396bc36edb95d3,835ae791,#### Boxplots,965e4f8f,0.1388888888888889
6834,ab6da5994949a3,c41714c4,"# Poisonous = 1
# Eatable = 0",fae6b91d,0.1388888888888889
6842,979f1e99f1b309,0d56fcf6,***The Disturbution of assists showing that almost all assists were under 5 assist and the most of assists are 0 which indicate that players tend to kill the enemies without a help from a friend***,d1bfebbf,0.13934426229508196
6845,d96642860ab3dd,d8308ef6,### 1.3 Pclass Feature,98419d48,0.13953488372093023
6847,1660daf8867980,fda18a6d,"### The environment
- The state space is a 8 by 8 grid
- The starting state S is the top-left square (0,0)
- The terminal state F is square (5,7). 
- Every move from state to state gives a reward of minus 1
- Naturally the best policy for this evironment is to move from S to F in the lowest amount of moves possible.",42d7cffc,0.13953488372093023
6850,806ce45c8fa303,20ecd8a0,"## Exploratory Data Analysis

1. Exploring on statistics information about the data",3e5c34dc,0.13953488372093023
6851,743ae010f5e875,e9774bb1,# Load data,02c54445,0.13953488372093023
6852,8539260444e6b5,997c3791,# Making two copies of Reviews to edit,0369463f,0.13953488372093023
6853,c09fac3c943d51,af022561,# Looking around,678d076d,0.13953488372093023
6856,0caaec057f7184,2ee29840,"## Category

There are 84 categories from the csv file.

Relationship to total items in each category, total / average sales in each category, price range in each category",b875533e,0.13978494623655913
6857,eda49464dd6d1b,ee46d7ef,"## Observations:
* The customers range in age from 20 to 85 years old, and half of them are 25-49
* Half of the customers in this dataset have been with the company for 82 to 227 days
* A slight majority are male
* The vast majority have a driving license, about 99.8%
* Most vehicles are <2 years old
* Over half of vehicles have damage of some kind
* Only a little over 1 in 10 customers responded by buying vehicle insurance when asked",8421f81f,0.13986013986013987
6859,91eaec994e0c6f,4609a8e0,## 2.2 Example of Time Series,376aef10,0.14
6860,2bd6c370695ea7,1f3a7307,## Add Breed Mapping,cbe6aec8,0.14
6861,2ada0305b68956,bc2bd7c8,### 21. Palette = 'OrRd',133e26f4,0.14
6865,7dd46c750653eb,cc64b284,"**Inference**

* The Year 2016 has the most Total Number of Births 

* The Year 2010 has the most Number of Death",c2644713,0.14
6866,83df814455f06c,e266b7f5,"Gini index says, if we randomly select two items from a population, they must be of the same class and probability for this is 1 if the population is pure.

It works with the categorical target variable “Success” or “Failure”. It performs only binary splits. The higher the value of Gini, higher the homogeneity. CART (Classification and Regression Tree) uses the Gini method to create binary splits.

Steps to Calculate Gini for a split

1.	Calculate Gini for sub-nodes, using formula sum of the square of probability for success and failure (p^2+q^2).

2.	Calculate Gini for split using weighted Gini score of each node of that split.


In case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point. The attribute with minimum Gini index is chosen as the splitting attribute.",c9cff71a,0.14
6867,4cd25e50c7e007,9584df14,**We can see that there is no missing value in the dataset**,ceb0c525,0.14
6870,3fb15e6e48aec2,37c8bad0,"* Looks like there are some groupings 0-10, 10-25, 25-35
* Lets try group with 10 year intervals",9d1f4358,0.14035087719298245
6871,5ce12be6e7b90e,c7a543e2,"# **Exercise** : Variables
* Declare a variable named *num* and set it's value to 3.
* Declare a string named *sentence* and set it's value to 'My Python Sentence'
* print both variables to the screen",c0ab62dd,0.14035087719298245
6876,c2a9f2fb3e1594,63410144,"## 3.2 Meet and Greet Data

This is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent/feature variables(s)), what's its goals in life (dependent/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.

To begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https://www.kaggle.com/c/titanic/data).

1. The *Survived* variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables. **It's important to note, more predictor variables do not make a better model, but the right variables.**
2. The *PassengerID* and *Ticket* variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.
3. The *Pclass* variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.
4. The *Name* variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.
5. The *Sex* and *Embarked* variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.
6. The *Age* and *Fare* variable are continuous quantitative datatypes.
7. The *SibSp* represents number of related siblings/spouse aboard and *Parch* represents number of related parents/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.
8. The *Cabin* variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels. However, since there are many null values, it does not add value and thus is excluded from analysis.",53411c04,0.14035087719298245
6880,30fdc4a6e3c1db,88e2cfd7,### Plotting Sales Ratio across the 3 states,6111ddee,0.14035087719298245
6882,6cade0b6a41ba2,0981ee42,"##### We see that number number of unique values equals total items in the row.
##### This can also indicate that, it might be Patient / Customer ID. We will drop this, as we already have a unique identifier for the dataframe
##### Thus, we will drop this column",e6110293,0.14035087719298245
6884,2f47abddfd1928,0faa484b,## 2. Data Cleaning,ae33cc0b,0.14049586776859505
6885,e9b9663777db82,6918476c,# Descriptive Analytics,648e8507,0.14049586776859505
6886,c85c94076e9c3a,50c30308,### checking for duplicates and null value :    ,3ea0c443,0.140625
6888,ff3a8ce61fab6a,1d672e4f,"[[[](http://)](http://)](http://)**In above code pi is a tensor of rank 0**
<br>

Let's print pi without use session for clearly understand.",9afe1654,0.140625
6891,9bcfa825c8b2e6,00df5305,"Veri setinde bazı gözlemlerde insülin değerleri 0 olarak girilmiş. Bir kişinin insülin değerinin 0 olması mümkün değildir,anlaşılan insülin değerlerine bakılmayan kişilerin insülin değerleri, sıfır olarak girilmiş; bunlar nan olarak değiştirilir. Aynı sorunun olduğu diğer columnlarda da böyle bir değişiklik yapılır.",220f36e4,0.14084507042253522
6892,631cd434fc3aa2,8cb49963,"The skew seems now corrected and the data appears more normally distributed, good.",2b74febb,0.14084507042253522
6893,bddd799cdbbae8,1509bd06,"The dataset used contains 47692 rows, where the distribution of data for each type of cyberbullying is balanced. However, it is necessary to check whether there are duplicated tweets.",b44e3c08,0.14084507042253522
6896,726833f92fb87a,9a188037,"Some first insights on the data:<br>
- Age:
    - the medium age is 41 years old.
    - the minimum age is 18 years old.
    - the maximum age is 95 years old.
- Balance:
    - The std deviation looks high (3225) compared to the mean (1528).<br>
- Housing and deposit look balanced as their mean value is close to 0.5
- From the column 'previous' we can see that most of the clients have been already contacted, since the value is close to 1 (0.8325)
- Day should be converted to object type",7dc5e1b6,0.14093959731543623
6898,d4c5aaa4b36810,aef21844,"Now we can assign the countries in the happiness data that didn't directly match countries in the development data.

| Happiness Data | Development Data |
|----------------|------------------|
| Taiwan         |      N/A         |
| Slovakia       | Slovak Republic  |
| South Korea    | Korea            |
| North Cyprus   | N/A              |
| Hong Kong      | Hong Kong SAR, China |
| Kyrgyzstan     | Kyrgyz Republic  |
| Somaliland region | N/A           |
| Laos           | Lao PDR          |
| Palestinian Territories| N/A      |
| Congo (Kinshasa) | Dem. Rep. Congo|
| Congo (Brazzaville) | Congo       |
| Ivory Coast    | Côte d'Ivoire    |
| Syria          | Syrian Arab Republic|



No we need to read the add all the matching countries to a labeled Data Frame with the development factors. The development factors are stored in the Indicators table which contains the columns CountryName, CountryCode, IndicatorName, IndicatorCode, Year, Value. The Country table allows us to the turn a CountryCode into a ShortName. Also the year needs to be reasonably close to the 2000s the The happiness data is from 2015. Must create a list of the countries to query as they appear in the ShortName column of the Country table. Find their country codes and query the indicator table to find the the indicator name and value. All this information can then be added to a table with columns of Country,Happiness,Indicators.  

",65441f28,0.14102564102564102
6905,869a39a3d4dea2,5442407b,"## Image Basics <a id=""image_basics""></a>",9020daf8,0.1411764705882353
6913,c9b4e282e4e2c1,d0d5d156,"A-Which kinds of injuries are the most frequent?
",f44d339f,0.1415929203539823
6915,62487bcd70b199,5a9a13b2,"## Inference
None of the numerical columns are Normally distributed and for models to perform better we have to perform Normalization",f6ae50af,0.14166666666666666
6916,37b09262279764,0440768b,"<b>Age column has 177 null values</b><br>
<b>Cabin column has 687 null values</b><br>
<b>Embarked column has 2 null values</b><br>",37c4c417,0.14166666666666666
6918,20b372b6e4e276,214b9e7e,"### Commit 8

* Dropout_new = 0.15
* n_split = 5
* lr = 2e-5

LB = 0.712",ec8b0860,0.1417910447761194
6920,957e035ba5b9d5,598dd8ed,"# Moving ~10% of the train data over to test
import math

for d in src_dirs_2:
    src_dir = 'dataset/train/' + d
    num = len(os.listdir(src_dir)) - math.floor(len(os.listdir(src_dir))*0.1)    
    images = [file for file in os.listdir(src_dir) if file.endswith('.jpg')]
    
    dst_dir = 'dataset/test/' + d
    test_img = images[num:]

    for file in test_img:
        shutil.copy(os.path.join(src_dir, file), os.path.join(dst_dir, file))",778ab3d3,0.14184397163120568
6922,63b44c85e32c1f,94a4d62b,"Instead of doing the above, In python, you can access 'apple' by just writing the index values each time side by side.",fb9b9562,0.14189189189189189
6923,659f5f3ef8aa0e,4e3921df,"There is 1 csv file in the current version of the dataset:
",3654c2d0,0.14285714285714285
6925,066c5ee1ef39e6,cb813933,### Create 3 Versions of the Data,0f394e1b,0.14285714285714285
6926,4ae6a182abac64,2248762e,* **Statistical info about the numerical variables** :,418676c5,0.14285714285714285
6927,663bbc9eaf267b,59a2f707,# Visualization / Feature Engineering,32445529,0.14285714285714285
6928,ba4b3bd184acbb,72ae1b59,***,0f5de724,0.14285714285714285
6932,87e94f864d74be,3130e46d,"> **This bar chart gives you an idea about how many missing values are there in each column.
> ""director"" has the most missing value followed by ""cast"" and ""country"". There are few missing value in ""date_added"" and ""rating"".**

",294bfe9f,0.14285714285714285
6938,4b4117cf42ef8d,efab684e,# EDA,457cd6f4,0.14285714285714285
6939,b7298d6aaff625,8433863a,### Importing Data ,bdf24bf7,0.14285714285714285
6940,0c57e3132ae184,f4cfb105,"## 1. Manually examine interesting columns to examine further.

Here I chose columns that are roughly grouped into these categories:

 - Information regarding host;
     - review; performance; experience
 - Information regarding house;
     - location; amenities; furniture; policy; affordability",f6bac298,0.14285714285714285
6944,6e9b4020644836,ea4977d9,"<a id=""2""></a>
# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Loading Data</p>",5ad41fc6,0.14285714285714285
6945,0d59a3e0130db0,990ac019,"Now let's create a function that tokenizes the text, removes punctuation marks and English letters from it, and then apply this function to each cell in the DataFrame.


I was given one important advice about this data set, it is better to try not to delete stop words here, they can carry a serious semantic load in the context of comments",285f04b2,0.14285714285714285
6947,b74076b2f8ba1d,035d70fb,"There is 1 csv file in the current version of the dataset:
",9ace22d4,0.14285714285714285
6950,c54ea4523bd49c,f6cb8952,"Checking out the train.csv data, use value_counts to check relative number of 0 and 1 has_cactus values",097ccba2,0.14285714285714285
6952,b6e698d389d0d3,5f913018,# load data,f02f68b5,0.14285714285714285
6955,0dd3ac2d55efd7,e71be5ab,"You can check out my previous notebook [here](https://www.kaggle.com/urstrulysai/bow-tf-idf-models-with-basic-lr-0-80-score) for detailed text pre processing and basic NLP models before going into word embeddings and LSTMs, GRUs, RNNs.",e9aa2cc2,0.14285714285714285
6963,ffd1df95ca5289,c3267403,"Now we can see that the mean, median are acceptable value and also as mean is greater than median we can say that it is right skewed and therefore we should standardardize it",db00c338,0.14285714285714285
6967,6b65d81a5743dd,0dcb6120,* 2.2 Load and Check Data,4080a2d2,0.14285714285714285
6969,8985a124d4b657,c44a4c98,"Now that we have dropped all null values, there should be no rows with NaN values. Let's check using the below command and as we can see, there are 0 null values in each column.",586d1846,0.14285714285714285
6971,f35bf4df70d310,6e4ab709,### Display Sample Data,10bb859a,0.14285714285714285
6972,06ecf7a304c309,8ab9dbc6,"## 2. Implementation

### 2.1 UseCase 1 : Image Reconstruction

1. 필요한 라이브러리부터 불러옵니다.",714de627,0.14285714285714285
6978,1084376bc4897c,fc5e3608,"### 2.1 Meaning of the columns 
1. Age : Age of the patient

2. Sex : Sex of the patient

3. exang: exercise induced angina (1 = yes; 0 = no)

4. ca: number of major vessels (0-3)

5. cp : Chest Pain type chest pain type

    - Value 1: typical angina

    - Value 2: atypical angina

    - Value 3: non-anginal pain

    - Value 4: asymptomatic

6. trtbps : resting blood pressure (in mm Hg)

7. chol : cholestoral in mg/dl fetched via BMI sensor

8. fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)

9. rest_ecg : resting electrocardiographic results

    - Value 0: normal

    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)

    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria

10. thalach : maximum heart rate achieved

11. target : 0= less chance of heart attack 1= more chance of heart attack",1b598487,0.14285714285714285
6980,241cf32abb22d8,6de33664,"## Encoding Categorical Features

It is necessary to encode all categorical features into numerical features, since Scikit-learn requires all data to be numeric before putting them into the algorithm. 

Before encoding the target feature, the descriptive features and the target feature need to be partitioned. ",47157066,0.14285714285714285
6981,c4386b8a01d66e,fb8e3d20,#### The distribution is not uniform,dc732bf5,0.14285714285714285
6982,675b60eaf415a6,2cd814f5,"**The dataset being used is [Food 101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)**
* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**
* **Each type of food has 750 training samples and 250 test samples**
* **Note found on the webpage of the dataset :  **  
***On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.***  
* **The entire dataset is 5GB in size**",68c0b725,0.14285714285714285
6984,6471597c5d2f66,a7acc4a5,"There is 1 csv file in the current version of the dataset:
",a41b4abe,0.14285714285714285
6986,8dd655515e7d18,38c6fdbf,Converting the Category column data to data type = category,895f41cf,0.14285714285714285
6998,b211c8c107f56d,f4b9a8b7,"## Start h2o and load the data  <a name=""step1""></a>

Start the h2o cluster and load train, validation and test datasets as h2oFrames. 

Train/Validation split is done with sklearn function, to correct for stratification on the target column.",805b90f3,0.14285714285714285
7000,2d40f383473fa4,8fadc26a,"Amazing, no?<br>
What can be learned from this? Let's examine each variable (features that will feed the model) and get some insights.<br>

* `Survived`: The Target variable. The plot indicates a small class imbalance but there's no need for resampling and accuracy seems good to evaluate the model.<br>
* `Age`: It's a numeric type, has almost 20% of missing values (`NaN`), the distribution has a little right skewness but is near from Normal. There's a small negative correlation with `Survived`, indicating that youngers could have a better chance to survive than the oldest.  <br>
* `Cabin`: Categorical feature, has a lot of missing values (77.1%) and some passengers have more than one cabin. Maybe the first letter could indicate the deck area, then it is considered as a relevant feature. <br>
* `Embarked`: Another categorical feature, has only 2 missing values, drop these values on Feature Engineering step will not harm the model. <br>
* `Fare`: Numerical feature with a right skewness. There's no missing values. Has a small positive correlation with `Survived`, indicating that the person owned a more expensive ticket and following the idea that a better Cabin/Area/Class has more chance to survive. Note that it has a mode with low values, hence the big part of passengers bought a more cheaper ticker. <br>
* `Name`: Categorical feature. No missing values. The name structure is interesting, 'Family Name'+','+'Honorific'+'name'.
* `Parch`: Numerical with right skewness. No missing values. Small positive correlation with `Survived`.
* `PassengerId`: Numerical but not relevant for the prediction. Will be dropped in FE step.
* `Pclass`: Categorical Feature. No Missing Values. There's more 3rd class passengers than others. Negative correlation with `Survived`, it reinforces the idea on `Fare`, expensive tickets allows better classes leading to most chance to survive.
* `Sex`: Categorical Feature. The distribution indicates that have more men than women. No Missing Values.
* `SibSp`: Numeric with a small negative correlation with `Survived`. No Missing Values.
* `Ticket`: Categorical. No missing values but has a High Cardinality.

With all information gathered from the EDA, it's time to get insights on how to proceed in FE, that starts now.",1da1eff0,0.14285714285714285
7004,e69a496109e7d8,7e9e5423,"There are **No null values**, so no need of data imputation",1c640591,0.14285714285714285
7010,585c280865b46e,2f11a978,"# Create columns with information on each cell (drug, dose, etc)",4d6056f1,0.14285714285714285
7014,5f4ae633cfd090,b20e4eb4,"***Finding and filling null values***

Building a baseline model would mean imputing all the null values with their 'means' (or mode/ median but I chose mean here) since this is a baseline model",a30a16e2,0.14285714285714285
7017,f13534449a3750,3b73ab2e,"In this section we will report the function we used for different purpose, this will help us in the computation and in the readness of the notebook.

",8b7f3332,0.14285714285714285
7023,1fac5edd4063ba,955da4b1,#Flag codes column is empty. Drop it!,04bc01e0,0.14285714285714285
7024,d1f92a87a0a1a5,b1ce26dc,"# Historical Huge Earhquake
https://en.wikipedia.org/wiki/Nankai_megathrust_earthquakes",2cd610b2,0.14285714285714285
7025,b290039151fb39,09b4de50,"The below fastai callback was added for the purpose of dropout scheduling, though it can be used to schedule any PyTorch nn.Module parameter.

It takes in a `Learner`, as is standard with fastai callbacks, as well as the Module and parameter attribute to set. You also set the range of the attribute values (start and endpoint). Optionally, you can choose to set the epoch in which the parameter will be at max or use the epoch count passed into the learner. Lastly, the function can be passed that defines a function for scheduling the parameter, defaulting to cosign annealing by default.

Cosign annealing looks like this over 10 epochs with a batch size of 128 (total steps = 10 * 128 = 1280)",1836a79c,0.14285714285714285
7027,a758983a68c014,20387db3,Consider we have a simplified corpus of words like below.,ab89f181,0.14285714285714285
7028,f4514ec092a771,1aef3a37,"Collect all data in a list [*speaker_id*, *utterance_id*, *file_to_audio*]  
Choose 10% from them to be a test list. (Although I named it test list, it should be considered as validation list).    
**Note** that to avoid some error from Kaldi, the utterance_id should begin with the speaker_id.",3739ab1e,0.14285714285714285
7030,fcb15f03bd0239,8c846a3d,"# The eva superset alphabet
the oddity here is, you could think about a switch between t - h
![](http://www.voynich.nu/img/extra/eva01.gif)",3b9bee3a,0.14285714285714285
7031,0fa9979b5690e9,377fc89c,"Agora, os dados estão divididos em três blocos disjuntos: treino, validação e teste. 
* O teste representa aproximadamente 33% da quantidade dados;
* A validação representa aproximadamente 22% (66% x 33%) do total de dados; e
* O treino representa aproximadamente 44% da quantidade de dados originais;

A seguir, será comparado o desempenho do KNN treinado com o conjunto de treino e testado no conjunto de validação.",c26eea94,0.14285714285714285
7034,3cea0f929a2035,59a48c70,"After making changes in feature names and the dtypes, we gat the following:",04cfbade,0.14285714285714285
7036,565ad413cd802f,b0e9df20,Let's also put the textual labels in a dictionary for later use.,397b074e,0.14285714285714285
7037,60d500d196eb42,2b2d195e,"There is 1 csv file in the current version of the dataset:
",2ad55f3f,0.14285714285714285
7038,a76e0e8770b7a0,54db2a8c,Interesting winter has too many terrorist attack.,02863d3b,0.14285714285714285
7039,fe6750354fb64f,645d6774,## Bar Graph of Confirmed Cases,271741f0,0.14285714285714285
7042,20c9a2456e494a,415ab12e,# Importing the libraries,3e487f55,0.14285714285714285
7043,fda19edaf5c621,c4362753,**Let's load the dataset**,5cfff76e,0.14285714285714285
7045,9c044fa3072552,e4708964,There are a lot of columns that have missing values!,1362842e,0.14285714285714285
7048,75adb7945ef9bd,707d772e,## 2. Keywords,785c5095,0.14285714285714285
7049,726aa2483ea12e,e2761264,-----------------------,65c2aacc,0.14285714285714285
7050,870a7144fa75ad,963fe28a,**Text cleaning**,2a8c3427,0.14285714285714285
7052,84d1ef55b89e17,9d8ad0e9,**Function**,2d0b9d51,0.14285714285714285
7055,90691864eb68c7,4ddbd250,Identifying Categorical Variables,3555ef9b,0.14285714285714285
7056,53f302571cd4ac,3841bd28,"## Object:
An object is a particular instance of a class.",62c28443,0.14285714285714285
7067,92e9fc3a0ff5c0,7f1162f0,## **Trump Reviews**,d53da425,0.14285714285714285
7069,12f4d16fc21645,1b4b252b,<h1 style='color:blue'>Check Null and NA values</h1>,c7752038,0.14285714285714285
7071,2730840089c8eb,174b757b,"The table below summarizes some important uses of the backslash character.

| What you type... | What you get | example               | `print(example)`             |
|--------------|----------------|--------------------------------------------------------|
| `\'`         | `'`            | `'What\'s up?'`         | `What's up?`                 |  
| `\""`         | `""`            | `""That's \""cool\""""`     | `That's ""cool""`              |  
| `\\`         | `\`            |  `""Look, a mountain: /\\""` |  `Look, a mountain: /\`  |
| `\n`        |   <br/>      |   `""1\n2 3""`                       |   `1`<br/>`2 3`              |",34d27dac,0.14285714285714285
7074,6f4795cfdc96c7,a2e066ac,"There is 1 csv file in the current version of the dataset:
",1f3ab82f,0.14285714285714285
7078,4c47839b067546,ad50e939,### test.complectation_dict   train['Комплектация'],1f517b02,0.14361702127659576
7080,917957c6c4065f,e8211dad,생성한 category 열에 결측치가 있는 사례가 있습니다.  ,55b8ed68,0.1437908496732026
7084,9169c4e9c33c90,0cec8a51,Bivariate distribution for User Rating and Price,725bf880,0.1440677966101695
7087,99bf357eaf61f1,f4beea4e,#### 2.Discrete Variables,9d92fafe,0.14423076923076922
7095,3597174a998d4d,4074d4bf,### 2.1.1 Room Type,276892ed,0.14444444444444443
7098,892be0a523578c,7156a01d,"#### 2. heartrate_seconds_merged.csv
 Different participants have different count of heart rate records, the smallest count is 2490. In addition, the interval between each record for the same person have differences. However, I don't think these will have a great influence on the analysis, but I will keep these points in mind.",b0e8d7c0,0.14444444444444443
7102,52ee792e228d54,c7c4108c,"### It seems like the Age and Experience form a great couple 💑. They have a very strong Linear Relationship between them. (can we use this relationship to fill the incorrect negative values of Experience?)
#### Other interesting pairs to note are (Income, CCAvg) and (Income, Mortgage) . It is clear from the scatter plots that CCAvg and Mortgage values are lower than the income.",5096094e,0.14473684210526316
7107,0e2a23fbe41ca9,ca3efbb6,"Observations:

- Negative and positive target values are almost in the same proportion

### 2. Anonymised Features

feature_1, feature_2, feature_3",64e4762c,0.14492753623188406
7109,b01ee6cb674fa3,d47a943b,"2 data: site name and country
    
- there is a launch with location classified as 'Yellow Sea'
        
the yellow sea has China, NK and SK at its border AND is a sea
",a8ffd35e,0.14492753623188406
7111,548f961125248d,6a6f1c52,### Null values,d8c5e8b8,0.14492753623188406
7114,9d9da6c439b96b,23d507e3,## Non Numerical Data (Categorical),361cc7d9,0.14492753623188406
7116,57070ad5e0f94f,ec131c75,# **Checking If There is Relation In Data**,d97edc41,0.14516129032258066
7118,9ceb7278784462,79f1c502,"# <a id='6'> 3.3 Describe Function</a>
* Generate descriptive statistics.<br>
* Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.<br>
* The **describe**() function in pandas is very handy
in getting various **summary statistics**.<br>
* This function returns the **count**, **mean**, **standard deviation**,
**minimum** and **maximum** **values** and the **quantiles of the data**.<br>",3768a567,0.14516129032258066
7119,ad26c020235dfc,15b64856,"# Overview
The datafiles are small. So we can load them on demand.",bf766e48,0.14516129032258066
7120,c0ddb77bf32e2b,3de2c8dc,"The mode of this columns is 0. According to [中央氣象局歷史資料](https://www.cwb.gov.tw/V7/observe/UVI/UVI_Hist.htm), it's possible that we have 0 in UVB. 

![2013/09](https://i.imgur.com/43BbLMD.jpg)


However, the historical data also shows that instead of actually being 0, it's more likely that the 0 is missing value. Therefore, instead of making use of this column, ",a0cb45f7,0.14516129032258066
7125,016abae0483764,ed15e468,"Here, test data is not crucial for prediction, so we remove the columns which don't contribute to the prediction model anyhow...
So, here we remove the date based column which was just the date of testing.
",bc9f289b,0.14545454545454545
7126,f0fab078f8533b,d0885eb4,### percentage of null values in the data,bdb5ea32,0.14545454545454545
7131,2ada0305b68956,e9f8eb0a,### 22. Palette = 'OrRd_r',133e26f4,0.1457142857142857
7136,a3ae04b78e45b5,58001b70,**HIGH SCHOOL GRADUATION RATE**,4195da8b,0.14583333333333334
7137,64a336ac34d95c,626a873b,"## Data Cleaning
",be73a990,0.14583333333333334
7141,2a377ced98d67a,ded244ae,Selecting latest version of dataset,262231a8,0.14583333333333334
7142,28a1ff0f223da9,86777ae2,### Where area of interest is in abundance? ,c945b27d,0.14583333333333334
7143,3b5903412fe741,3fc76baf,"If we have a `dict` object in Python, we can access its values using the indexing (`[]`) operator. Again, we can do the same with `pandas` `DataFrame` columns. It ""just works"":",ad231969,0.14583333333333334
7145,3c2033cc99c12c,7f508263,![image.png](attachment:image.png),dfa22a54,0.145985401459854
7147,e67925694c07d3,0344a614,"as we can see above the target prediction is imbalanced
we have way more 0 (loan was repaid on time) than 1 (loan not repaid)",83af4c4a,0.14606741573033707
7148,5f27526aa6c113,e9e13122,# data preprocessing,a5c26ab6,0.14606741573033707
7150,d07915a6e6992e,b048dcf3,"**PassengerId**

Not relevant from modeling perspective so we will drop this variable later",2b912140,0.14615384615384616
7175,67b7354e96113a,08ddd279,## Exploratory Data Analysis,dca94250,0.14666666666666667
7179,7e1da639035ac5,94d930e2,# <a id='6'>6. Economic Need Index</a>,120b6c23,0.14666666666666667
7182,bd380b97b5c894,8ab959e3,## anatom site,66f2562a,0.14678899082568808
7183,eda49464dd6d1b,a58fb98c,## Target Variable (Response),8421f81f,0.14685314685314685
7187,3d905ce4828057,72030cad,"
# **Data Preprocessing**",5b006cc3,0.14705882352941177
7191,52cfd66e9ec908,8068dd9c,Here we go using some helpful functions to visualize the data.,c74adcdf,0.14705882352941177
7192,a0a5baa6c7e12a,77904853,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Relations Between Numeric Feature and Target</div>",551d41de,0.14705882352941177
7193,842547b2def18c,57b6f1f1,"> ***カテゴリカル変数の分布は何か？***

- Namesは，trainデータセット全体にわたって固有． (count=unique=891)
- Sexは，2つの正数として表され，65%は男性． (top=male, freq=577/count=891)
- Cabinは，trainデータセット全にわたって，いくつかのサンプルで重複がある．すなわち，いくつかの乗客はキャビンを共有していた．
- Embarkedは，3つの正数をとる．ほとんどの乗客によってSポートが使われていた． (top=S)
- Ticketは，重複する値の比率が高い (22 %, unique=681)",b8efde6d,0.14705882352941177
7196,629f2918807a9b,748c59d1,"#### Q1: What is the best-selling book?

> Lets Find this out",be56dc84,0.14705882352941177
7197,8f50c9c16db95f,ae2b447a,"# Outcomes of a kickoff <a id=""outcome""></a>

There are seven outcomes in general. **Touchback** and **return** are the most two popular ones. In 2018, in order to make the game more safer for both sides, and avoid unneccessary collisions. NFL introduced new rules on the touchback -

> Kickoffs that hit the end zone without being touched by a member of the receiving team automatically become touchbacks([3](https://profootballtalk.nbcsports.com/2018/07/06/another-tweak-to-the-kickoff-rule-promotes-more-touchbacks/)).

For years, the automatic touchback rule has applied to punts that enter the end zone, with or without being touched. For kickoffs, the touchback becomes automatic only if it strikes the ground in the end zone without being touched by a member of the receiving team; the player can still catch the kickoff and choose to return it. It’s not a change that will come into play very often, but it’s another example of the league’s broader effort to encourage touchbacks on kickoffs.


This policy pushes the touchback to a more popular status since 2018, as we can see it in the chart below. ",26cc763a,0.14705882352941177
7198,1d1598b6fa2aa7,912d63c5,"### Scatter Plots

We will start by creating a basic scatter plot.
More information - https://plotly.com/python/line-and-scatter/",e066accf,0.14705882352941177
7201,e4c6dd957eb5ce,cdb74402,"Let's start with an overview of the following tables:
- kernels 
- users
- users achievements
",2e383665,0.14705882352941177
7202,7cfd96218dd933,b83f1f4e,"* THE BRIGHTNESS VALUE GIVES US THE STRENGTH OF THE POINTS MARKED AS THE HOTSPOT.
* IF VALUE IS HIGH, FIRE IS STRONG.",7c34d96c,0.14705882352941177
7204,dc0b0e1cb46c6f,62e3017a,We can fix this easyly with imputation ('GBR') ,47b17a7b,0.14705882352941177
7206,395ed8e0b4fd17,855c92a4,# <center>EDA</center> ,7573ea31,0.14705882352941177
7207,e170d33ee1da8c,260d7414,## Data preprocessing,253cee3c,0.14705882352941177
7208,7f74a04ae75792,e82933e2,### Do we need `C_ID` in our analysis?,d01e91da,0.14705882352941177
7210,169177b6e9edea,105e0e05,"<h2><b>Sex:</b></h2>
<ul>
    <li> male : 0</li>
    <li> female : 1</li>
</ul>",ca42152f,0.14736842105263157
7212,840534f2908a9c,176e9839,"*By checking the description of test data, we can choose to clean the train data base on these value. In other word, we can delete the values that are out of these boundaries in the train data*",8081c3cc,0.14736842105263157
7218,0858e1bb3cbaca,ad5fc3d1,"When we try to analyze this dataset, how do I check what information is provided?
We can use

 **.columns**

 to display the name(index) of each column",78548374,0.14754098360655737
7222,d1ff7e10ee0102,ac7e8c8d,"*'Amazing! If my love calculator is correct, our success probability is 97.834657%. I think we should meet again! Please, keep my number and give me a call if you're free next Friday. See you in a while, crocodile!'*",2cc71c3c,0.14772727272727273
7230,613bf7bfdcb9e3,a795ca38,"# most common value

### 1.many count value

### 2.if same count => smallest value",32beb65d,0.14814814814814814
7232,cebaf20167fb49,c8ca530e,# Data Cleaning,702e4057,0.14814814814814814
7234,b809d07ddd17ed,e10a2cf0,## Read image filenames from the dataset folders,e32bf3b1,0.14814814814814814
7235,135122550b6483,7b9f3cad,"<h2> Gist of the the Data </h2>
<h4>
Row Ids <br>
Datetime Column indicating this is a Time Series Data <br>
Countries - Finland, Norway, Sweeden <br>
Store Types - Kaggle Mart and Kaggle Rama <br>
Products - Hats, Mugs, Stickers <br>
</h4>
",6592d6d8,0.14814814814814814
7241,db5a369894fef6,cd66f85d,"Interpretation
- After around mid-March, worldwide confirmed cases of COVID virius are expontentially rises 
- Out of those confirmed cases; most are active, a large proportion have recovered and a smaller proportion are deaths. 

### Examine Specific Countries
- Note that we are using three different files (confirmed, recovered, deaths)
- Check if data frames are consistent with indices (BUT they're not!)
- Use `IPython.display` for a nicer print out",065aaf61,0.14814814814814814
7244,07544ba83da480,dc345103,# Cleaning the data ,dc2f52b1,0.14814814814814814
7250,fdc3afd309b850,efd73399,Way better...,966bde38,0.14814814814814814
7251,24e550b8226932,a2d78d55,##### sample_submissions:,0caee953,0.14814814814814814
7255,c6f8ff61a5fa87,e4d5e3b4,"## <span style=""color:blue;""><strong>1.Read Data</strong></span>",3eea586b,0.14814814814814814
7262,0dfa3e758551c8,5fd469d3,891 rows/passengers and 12 columns/data points in the titanic data set,7adbb44c,0.14814814814814814
7267,ab6da5994949a3,fdac65ab,### Getting dummy variables,fae6b91d,0.14814814814814814
7268,0932046e1f485d,84a75902,## <a id=data_cleaning>Data Cleaning</a>,218cc7a3,0.1484375
7274,e4525eb0c96f28,0af1ef4d,"After merging the two datasets and dropping extra columns, these are the remaining columns used in our Exploratory Data Analysis.

Description of columns
- Rank - Games ranked by sales
- Name - Name/Title of the game
- Genre - Genre of the game
- ESRB_Rating - ESRB Rating of game
- Platform - Platform of the game
- Publisher - Publisher of Game
- Developer - Developer of Game
- Critic_Score - Critic Score of the game out of 10
- User_Score - User Score of the game out of 10
- Year - Year game published
- Country - Country of Publisher
- Total_Sales - Total game sales  (in millions)",2093a1f1,0.14864864864864866
7277,2f47abddfd1928,db6f5774,"### 2.1. Sex

The feature sex does not have NaN values, but as they are strings it would be difficult to deal with it.

Therefore what we can do is convert it to a binary feature. 0 --> Male | 1 --> Female.",ae33cc0b,0.1487603305785124
7278,e9b9663777db82,5916b708,"Identify categorical and numerical features, Identify duplicate rows, columns with missing values",648e8507,0.1487603305785124
7280,4c47839b067546,787e9c50,"test.complectation_dict  содержит информацию как в  train['Комплектация'], поэтому просто переименуем название признака.",1f517b02,0.14893617021276595
7283,c7e5f658090347,ac433ba6,"## Exploratory Data Analysis
In this section we first check to see if the data is okay (things like: is there missing data? And is the data is set to the correct data type(s)?). Then I will move on to looking at the relationships between the features and the targets.  ",43c78e7d,0.14893617021276595
7285,957e035ba5b9d5,c1f86cfc,## Checking for invalid images,778ab3d3,0.14893617021276595
7289,04e6b0d3c70f46,5adb3620,"### EDA Helper Routines
* How many classes are there and for each class what is the average length of recording, total recording time and number of recording",56344f77,0.14893617021276595
7290,3fb15e6e48aec2,31a107a8,"# Feature Engineering / Feature Extraction
* There is a lot that can be done here, I just went through a few off the top of my head:
* 1.Name - length
* 2.Name - Title of person (Mr, Miss etc)
* 3.Name - seperate name in brackets with actual name
* 4.Ticket - split number and inital text/number
* 5.Family size
* 6.binning
### Other potential options:
* Name - CountVectorizer
* Name - Topic modelling* 
* Name - Jaccard distance

### Name / Title
* Get Title of person
* merge similar titles
* Get Surname
* Get length of name
* Get Sponsor (purchaser of ticket ) 0/1",9d1f4358,0.14912280701754385
7292,fe7360cddc13e5,cd0b1e5c,"Tüm müşterilerin poisson dağılımları birleştirildiğinde, bir gama dağılımı elde edilir.",8979e423,0.14912280701754385
7294,ba655a261cc09e,c4394bbc,"Possibly missing values of age and stateroom, will indicate whether the passenger survived or not

Возможно, пропущенные значения возраста и каюты, будут указывать на то, выжил пассажир или нет",48cc549a,0.14925373134328357
7296,20b372b6e4e276,3304a7fb,"### Commit 9

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5
* LeakyReLU_alpha=0.05

LB = 0.711",ec8b0860,0.14925373134328357
7299,e58e68e4eeefe5,1db73b42,"### Observations
- There is an imbalance with the target variable, so we can apply cross validation technique with over sampling method compared to under sampling as the data size is small.",a87662ce,0.14925373134328357
7309,712198370d5521,f31bc6d5,"<img src=""https://github.com/KarnikaKapoor/Files/blob/main/Colorful%20Handwritten%20About%20Me%20Blank%20Education%20Presentation.png?raw=true"">

For more information on the attributes visit [here](https://www.kaggle.com/imakash3011/customer-personality-analysis).

<a id=""3""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">DATA CLEANING</p>


**In this section** 
* Data Cleaning
* Feature Engineering 

In order to, get a full grasp of what steps should I be taking to clean the dataset. 
Let us have a look at the information in data. 
",5882e04c,0.15
7311,9f0ccf5b9e8f03,0fd9f56b,"
WHO and its global network of doctors have developed a preliminary definition and made available to doctors a notification form for any suspected case of Systemic Inflammatory Response Syndrome (SRIS). 
The cases of the disease registered worldwide are rare and the role of the new coronavirus in the development of the infection remains unknown, said Michael Ryan, head of WHO emergency programs. 

""We don't know if it is the virus that attacks the cells or if it is an excessive immune response"" that causes inflammation, as is the case with Ebola fever, he explained. 
The disease has puzzled health authorities in several countries for two weeks, while children are little affected by severe forms of COVID-19. 
After a first alert in the UK in late April, similar cases have been reported in New York, Italy and Spain. Deaths are extremely rare, with a five-year-old deceased in New York and a 14-year-old in London. 
A first fatal case was reported on Friday in France. A nine-year-old boy died in Marseille (south) on May 8 of brain damage from a heart attack, Professor Fabrice Michel, head of the pediatric resuscitation service at La Timone hospital, told AFP. 
Serological tests showed that this child ""had come into contact"" with the coronavirus, but had not developed symptoms of COVID-19. 
The symptoms of the inflammatory syndrome are high fever, abdominal pain and digestive disorders, skin rash, conjunctivitis and the tongue that turns red, swells and looks like a raspberry. 
Such symptoms are similar to those of Kawasaki disease, which affects children and causes inflammation of blood vessels. 
https://translate.google.com.br/translate?hl=en&sl=pt&u=https://istoe.com.br/oms-diz-estudar-possivel-ligacao-entre-a-covid-19-e-a-doenca-de-kawasaki-em-criancas/&prev=search",66691203,0.15
7315,09bac0c221388e,aa041e42,"Text Summarization is a **Natural Language Processing** (NLP) task in which we try to create a summary starting from a textual input like books, articles, news.

When the source is a document (in our case a clinical document 📝 and like discharge letters) it is calles document summarization.",bea4aa2e,0.15
7316,62487bcd70b199,051d6173,##  <a id='3.2'>3.2. Dependancy check across Features</a>,f6ae50af,0.15
7317,ff83da40bcdb19,2c1413d8,"**PART 2:**
*Here we are creating training data.*

1. In the for loop first we are taking the image path.
2. we are reading the image with grayscale
3. Resizing the image 
4. Adding every data to training_data as an array and we also give the what is it(The image is run or walk image)
5. Doing the same things for the walking image
6. Shuffle and return",36b5ec8c,0.15
7318,63d0d9b9a8c7d2,941ab908,**Trying to understand how many NA columns are there**,e32e5933,0.15
7325,2bace980aeb34c,7b926c85,"You will work with data from the [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course). 

![Ames Housing dataset image](https://i.imgur.com/lTJVG4e.png)

Run the next code cell without changes to load the training and validation sets in `X_train`, `X_valid`, `y_train`, and `y_valid`.  The test set is loaded in `X_test`.",dc05ef6c,0.15
7333,ad121e0531afa4,a5a7a4f4,<h1 align='center' style='color: #8532a8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px'>1. Installation & Imports</h1>,a3492905,0.15
7339,3dd4294f903768,be3f4b56,"We can see that there are 21 columns in our dataset. In addition, in this kernel we are not going to use the text columns, so we won't consider them at our machine learning model. ",0d89d098,0.15
7342,1005ca950e8a81,ab26c6a0,"1. **Question 1: Labelings that assign all classes members to the same clusters are: *complete* , but not *homogeneous*:**",52570331,0.15
7348,b547f0f38f7744,bd471345,"### Analysis Dataset

The size of train data:",b6ba66b3,0.15
7351,5ffe6aa38958a1,fc701a6d,"Obviously, the missing field was the test result. It exists in train data because that acts as the ground truth. 

Okay, so what does the data look like?  ",11f5412e,0.15
7353,83df814455f06c,c50a1f0c,"# **6. Overfitting in Decision Tree algorithm** <a class=""anchor"" id=""6""></a>

[Table of Contents](#0.1)


Overfitting is a practical problem while building a Decision-Tree model. The problem of overfitting is considered when the algorithm continues to go deeper and deeper to reduce the training-set error but results with an increased test-set error. So, accuracy of prediction for our model goes down. It generally happens when we build many branches due to outliers and irregularities in data.

Two approaches which can be used to avoid overfitting are as follows:-

- Pre-Pruning

- Post-Pruning


## **Pre-Pruning**

In pre-pruning, we stop the tree construction a bit early. We prefer not to split a node if its goodness measure is below a threshold value. But it is difficult to choose an appropriate stopping point.


## **Post-Pruning**

In post-pruning, we go deeper and deeper in the tree to build a complete tree. If the tree shows the overfitting problem then pruning is done as a post-pruning step. We use the cross-validation data to check the effect of our pruning. Using cross-validation data, we test whether expanding a node will result in improve or not. If it shows an improvement, then we can continue by expanding that node. But if it shows a reduction in accuracy then it should not be expanded. So, the node should be converted to a leaf node.",c9cff71a,0.15
7354,c18267b203f28a,e476aa76,"## Decode the data
In the code chunk below we'll set up a series of functions that allow us to convert our images into tensors so that we can utilize them in our model. We'll also normalize our data. Our images are using a ""Red, Blue, Green (RBG)"" scale that has a range of [0, 255], and by normalizing it we'll set each pixel's value to a number in the range of [0, 1]. ",09ca8efb,0.15
7356,ba4b3bd184acbb,5ef31e01,"# Accessing Data

Now that we have covered how to get data into a DataFrame, we need to understand how to access data within a DataFrame

### Basic Indexing

This indexing method is the most convenient when looking to access a specific column or list of columns.",0f5de724,0.15037593984962405
7357,ac1abfe1dfe815,195609b1,---,6529dbcb,0.1504424778761062
7359,a5a419dc7245b0,332291e8,### Exploratory Data Analysis,4279726e,0.1504424778761062
7362,fdc9f4863744b1,e2a456d6,"The Boroguh column has integer type; based on the value of the variable, this is correct; however, Borough should have me a name such as ""Manhattan"" or ""Brooklyn"". According the data set from Kaggle I will convert the numeric values into the Borough names.",b4529365,0.1506849315068493
7364,91473a39b85068,4547f542,"### Example Data point
Title:  Implementing Boundary Value Analysis of Software Testing in a C++ program?
Body : 

        #include<
        iostream>\n
        #include<
        stdlib.h>\n\n
        using namespace std;\n\n
        int main()\n
        {\n
                 int n,a[n],x,c,u[n],m[n],e[n][4];\n         
                 cout<<""Enter the number of variables"";\n         cin>>n;\n\n         
                 cout<<""Enter the Lower, and Upper Limits of the variables"";\n         
                 for(int y=1; y<n+1; y++)\n         
                 {\n                 
                    cin>>m[y];\n                 
                    cin>>u[y];\n         
                 }\n         
                 for(x=1; x<n+1; x++)\n         
                 {\n                 
                    a[x] = (m[x] + u[x])/2;\n         
                 }\n         
                 c=(n*4)-4;\n         
                 for(int a1=1; a1<n+1; a1++)\n         
                 {\n\n             
                    e[a1][0] = m[a1];\n             
                    e[a1][1] = m[a1]+1;\n             
                    e[a1][2] = u[a1]-1;\n             
                    e[a1][3] = u[a1];\n         
                 }\n         
                 for(int i=1; i<n+1; i++)\n         
                 {\n            
                    for(int l=1; l<=i; l++)\n            
                    {\n                 
                        if(l!=1)\n                 
                        {\n                    
                            cout<<a[l]<<""\\t"";\n                 
                        }\n            
                    }\n            
                    for(int j=0; j<4; j++)\n            
                    {\n                
                        cout<<e[i][j];\n                
                        for(int k=0; k<n-(i+1); k++)\n                
                        {\n                    
                            cout<<a[k]<<""\\t"";\n               
                        }\n                
                        cout<<""\\n"";\n            
                    }\n        
                 }    \n\n        
                 system(""PAUSE"");\n        
                 return 0;    \n
        }\n
        
\n\n <p>The answer should come in the form of a table like</p>\n\n
    <pre><code>       
    1            50              50\n       
    2            50              50\n       
    99           50              50\n       
    100          50              50\n       
    50           1               50\n       
    50           2               50\n       
    50           99              50\n       
    50           100             50\n       
    50           50              1\n       
    50           50              2\n       
    50           50              99\n       
    50           50              100\n
    </code></pre>\n\n
    <p>if the no of inputs is 3 and their ranges are\n
    1,100\n
    1,100\n
    1,100\n
    (could be varied too)</p>\n\n
    <p>The output is not coming,can anyone correct the code or tell me what\'s wrong?</p>\n'
Tags : 'c++ c'",6e3d91c2,0.1506849315068493
7366,738bfced935b69,3a8b5eb0,"i notice the max value of year column is 2060 for the ford fiesta car, after searching based on tax 205 in ford data set i notice same tax value with same ford model the year manufactured this model is 2010, so we replace it to 2010.",2d3c592d,0.1506849315068493
7375,510b8303776bb6,f0382d12,### Verifying if all null values are removed. ,18080db8,0.1509433962264151
7376,a070fd03ae8ed2,6547969e,"# 3. Импорт данных и моделей
---
## 3.1 Импорт агрегированных данных после препроцессинга
(препроцессинг проводился в [первом кернеле](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-1-ml-sokolov))",c0ec4138,0.1509433962264151
7380,c09fac3c943d51,bd620958,Some pictures to have in mind: target distribution,678d076d,0.1511627906976744
7382,c4386b8a01d66e,68c038fb,Filling the data with random values,dc732bf5,0.15126050420168066
7383,2ada0305b68956,0b52c35f,### 23. Palette = 'Oranges',133e26f4,0.15142857142857144
7389,adf419444a59df,e190e65e,"If you want to learn more about pytorch conv2d you check <a href = ""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"">this</a> link",3a275e7f,0.15151515151515152
7390,dbd96dd275dc60,d99ae321,"# Sort DataFrame by saledate¶
When working with time series data, it's a good idea to sort it by date.",1ed493a8,0.15151515151515152
7393,2f964d08c25d93,065a0943,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",1f2e4468,0.15151515151515152
7394,4b64dc653fb7eb,d36f5218,"Data here comprises of ids, comments, and labels. 

Removing IDs from Train data, keeping Test data IDs for submission. ",57675cc2,0.15151515151515152
7399,5ce12be6e7b90e,cd722c02,"# Comments

Everything between a hashtag symbol `#` and the end of the line is a comment.",c0ab62dd,0.15204678362573099
7400,30fdc4a6e3c1db,d49d0035,We see that 43.6% of sales come from California while Texas and Winscoin have comparable sales 27.6% and 28.8%,6111ddee,0.15204678362573099
7401,b01ee6cb674fa3,633c6a0e,"It clearly shows that it is a chinese launch:
company name is the  China Aerospace Science and Technology Corporation (CASC)",a8ffd35e,0.15217391304347827
7402,7e89d387feb9f5,12444f13,### Добавленный числовой признак № 1. Количество ресторанов в городе,989e3a1b,0.15217391304347827
7412,7a058705183598,22a2b65a,Now check the distribution of target column i.e. Species,b0ead917,0.1523809523809524
7413,04bac111ffbe9c,d1f8360c,"##### PRIMARY CONCLUSIONS DERIVED
-  Most of the people were of the age range 20-40
-  Maximum number of people bought tickets of fare < 100.
-  Most of the people were travelling alone, without parents, children, siblings or spouse.
-  The highest numbe of passengers were from Pclass 3.
    -  Most number of people from Pclass 3 did not survive. This is evident as they were passengers of the inferior class.
    -  Since facilities were more easily available for 1st class passengers, hence survival:deceased ratio is higher for them(39.8%)
-  Survival:Deceased ratio is higher in females than males. Maybe due to the 'women and children' first policy.
    -  More females survived(233) than males(109)
-  Highest survival rate is for 'Embarked' class 'S'(63.8%)

##### FEATURE CORRELATION BEFORE PROCESSING",82576b17,0.1523809523809524
7414,7454fdc444df16,ad247cfe,Below we take a glimpse at the files contained in each class,a7818ef5,0.1523809523809524
7415,55a5e31d03df9f,1d315502,"Nice! A very crucial step at the beginning of any machine learning project is becoming one with the data, visualizing random images is a great way to understand what we're working with. Now we have a general idea of what we have but after we proceed let's explain what we did especially on the TF code that we just wrote.

Computers don't understand images as we humans do, for a computer an image is just a combination of generally speaking 3 layers of numbers (RGB).  As the following:

<img src=""https://www.researchgate.net/profile/Muhammad-Qureshi-41/publication/307091456/figure/fig2/AS:407248779137024@1474107083220/3rd-order-Tensor-representation-of-a-color-image.png"" width=""500px"">

These numbers are between 0 and 255, 0 meaning zero light and 255 meaning maximum light. If we have a pixel with a value of red=255, green=0, blue=0, means full red pixel. 

For being able to visualize the images we did the following:
1. Get a random sample, for that we used the pandas sample and get the path on that.
2. Read the image, we used `tf.io.read_file` method from the TensorFlow library (since we're learning TF) this method receives as input the filename to read from and outputs a tensor. We could use another library to read an image as CV2, mpimg from matplotlib, and more but the advantage is that we already got an object that can be interpreted by TensorFlow.
3. We decode the IO from `tf.io.read_file` with `tf.image.decode_image` we transform that into a real tensor. This tensor is on the following shape: width, height, channel. Channels as we previously see most of the time are 3 (1 for greyscale images and 4 for png transparency). 
4. We use `tf.image.resize` to resize the image (no surprise!), this is transforming from the original width and height to a new width and height. This will help us to visualize more images if we cast a smaller shape than the original.
5. After we resize the image the tensor numbers change a little and instead of the integer are now floats, we could use another argument `ResizeMethod`, and use nearest or we could get the float and divide it by 255. Implot can plot from the 0 to 255 range or from 0 to 1 range. 

Let's see it in code:
",06dce00f,0.1523809523809524
7416,2a56d6b0e153f2,aeadbb31,# EXPLORATORY DATA ANALYSIS,8dc315e6,0.15254237288135594
7420,f2e5e9fb9eaaf7,717f67d0,"[back to top](#table-of-contents)
<a id=""3.2""></a>
## 3.2 Test dataset
Test dataset is used to make a prediction based on the model that has previously trained. Exploration in this dataset is also needed to see how the data is structured and especially on it’s similiarity with the train dataset.

**Observations:**
- There are `119` columns: `118` features and `1` column of `id`.
- `train` dataset contain `493,474` observation with `936,218` missing values which need to be treated carefully.

### 3.2.1 Quick view
Below is the first 5 rows of test dataset:",048e0d08,0.15254237288135594
7432,69ac33d79f5130,8a016cf8,#### Numeric Value in the given datasets.,9d760d2a,0.1527777777777778
7434,fdc3afd309b850,8ddd8787,"<a id=""FE""></a>

# 6 Feature Engineering",966bde38,0.1527777777777778
7435,1014e6be391084,2b2fed82,Checking 0 transaction for each Class,46f9168f,0.1527777777777778
7436,593d1d3d1df05a,ebe3ed2a,# Finding Contours to locate plate,bc682ffe,0.1527777777777778
7440,869a39a3d4dea2,507e70d7,"Images are basic building blocks of pixels, where each pixel carry information about the color or intensity or light on the particular position of the image, when we visual image as matrix, for example a image of **height 50** and **width 200** will have 50 rows and 200 columns (50 * 200 = 10000) and **10000 pixels**. <br/>
Different images carry different kinds of pixel information, for example the gray scale image (single channgel) will have values ranging from 0 to 255 ( 8 bit unsigned) where 0 represents the black or dark, while 255 represents the white or brighter portions of the image, similarly the RGB image ( 3 channels) will have values in tuple (R, G, B) pixel coordinaates each ranging between 0-255. some of the examples are <br/>
* (255,0,0) - Red
* (0, 255,0) - Green
* (0,0,255) - Blue
* (0,0,0) - Black
* (255,255,255) - whie",9020daf8,0.15294117647058825
7442,9ceb7278784462,5471de32,## <a id='7'> 4.Rating </a>,3768a567,0.1532258064516129
7443,c80939c7c626cf,0759873f,Bar chart for categorical features,b9ac31e2,0.15328467153284672
7450,c8c4705cca1ebb,eadb514f,"# 1. Data Preparation
Drop the duplicate values",6d9d7107,0.15384615384615385
7456,71d3e4aee86e3e,183bb9e9,> # Line Plot representation of Confirmed/Active/Recovered/Deceased Cases and Mortality/Recovery Rate,69706f0b,0.15384615384615385
7459,aae204e78a48d1,1c2fd512,"# Who are customers that leave banks?

In any business, we want to understand the customer lifecycle, and put in methodologies to prevent customers leaving.  Normally in financial services, you can see customers start to disengage before they actually close their products, and this is the point to intervene.  In this analysis, we will analyse the factors that seem to affect whether a customer will attrite.

![](https://www.smartsheet.com/sites/default/files/2020-07/IC-Client-Lifecycle-Stages-Circle.png)",53ab6133,0.15384615384615385
7461,0a1fcda859252c,61b03e19,"We will first go through the training dataset. We will do some analysis on that, look at some of the samples, check the number of samples for each class, etc.  Lets' do it.

Each of the above directory contains two sub-directories:
* `NORMAL`: These are the samples that describe the normal (no pneumonia) case.
* `PNEUMONIA`: This directory contains those samples that are the pneumonia cases.",13a38774,0.15384615384615385
7465,2facf256353117,4c06306f,"# Terms
# Magnetic Resonance Imaging (MRI) 
* is a medical imaging procedure for making images of the internal structures of the body. MRI scanners use strong magnetic fields and radio waves (radiofrequency energy) to make images. The signal in an MR image comes mainly from the protons in fat and water molecules in the body.
* During an MRI exam, an electric current is passed through coiled wires to create a temporary magnetic field in a patient’s body. Radio waves are sent from and received by a transmitter/receiver in the machine, and these signals are used to make digital images of the scanned area of the body. A typical MRI scan last from 20 - 90 minutes, depending on the part of the body being imaged.
* ",18f579be,0.15384615384615385
7466,8e9d63e1f6319e,df2f6758,## Install required files,4743b346,0.15384615384615385
7467,6f1481148352e9,0568113b,"**Months are written in Portuguese, for convenience I  translate them in English and create a month_year column.**
* Janeiro - January
* Fevereiro - February
* Marзo - March
* Abril - April
* Maio - May
* Junho - June
* Julho - July
* Agosto - August
* Setembro - September
* Outubro - October
* Novembro - November
* Dezembro - December",7cfbdb8f,0.15384615384615385
7474,6e8f3e8ed1c241,e6058617,"First attempts done with GrabCut  
These were later discarded as it would require the user to also make a second input and complicate the availability of the app",c2cfb626,0.15384615384615385
7481,03048e86a6d806,76332da4,### Age,1285c231,0.15384615384615385
7484,020c28a360b0cd,3f07de9f,## Project Instructions,2ba397f0,0.15384615384615385
7485,a4f8ad33c823c5,db3a98e5,"## Missing Values

Used the isna() function to determine the number of missing values.",fcd48307,0.15384615384615385
7488,21122355e39af4,b3718a73,QUESTION: Labelings that have pure clusters with members coming from the same classes are _homogeneous_ but un-necessary splits harm _completeness_ and thus penalise V-measure as well:,88b95e2b,0.15384615384615385
7489,669ce946943d60,d449a791,## usage,0f63c4ce,0.15384615384615385
7490,cf08b03b002c13,03189e01,"As we can see, the data is missing only for some variables",104d416f,0.15384615384615385
7491,9a96e1588410ee,0ed98979,"## Loading csv and changing it to img
Reading the csv and converting the the data into images as required for the resnet.",3fa6cf92,0.15384615384615385
7498,c970849d1f6da2,5dd9b18e,# Loading Data,056e3955,0.15384615384615385
7499,3cb96bd8eb364b,beecfa03,### Data Summary,3157af7e,0.15384615384615385
7510,4d91e84c564cbe,92a18c19,"Which planet is *furthest* from the sun?

Elements at the end of the list can be accessed with negative numbers, starting from -1:",355a43e3,0.15384615384615385
7511,44f6a002ecd033,b08bddbb,The dependents column looks to be a column illustrating the amount of people that an independent claims. Most of the independents applying for loans look to have 0 dependents.,70bbe106,0.15384615384615385
7512,a8c042af6b7245,26cfc9c6,"Again, with the info() method we see that the data type is integer or float. No null values are present in the data set. That's normal because missing values are replaced by -1. We'll look into that later.",2487ac62,0.15384615384615385
7515,d07915a6e6992e,907cc342,"**Pclass**

Pclass is categorical variable. Let's look at the distribution.",2b912140,0.15384615384615385
7516,49ee86d074de69,68902d19,"<a id = ""7""></a><br>
## Remove Duplicate Features",71ccc6d3,0.15384615384615385
7522,09751c520b0616,8c6cd169, - Droping features in drop_null from dataset,a4d0c7e9,0.15384615384615385
7525,3e325daf577158,a87416f3,"Min Images for MRI Type
- T2w = 15
- FLAIR = 14
- T1w = 14
- T1wCE = 15",c873dfec,0.15384615384615385
7526,2d75fd881827b8,5944ecda,**Preprocessing**,107b5299,0.15384615384615385
7529,f2f2db16a2f86c,126ee31c,**total_bedrooms** have 207 null values.,ffc6a115,0.15384615384615385
7530,4bbe953f82d29b,6a861887,#### Ресемплирование,772301f2,0.15384615384615385
7535,726833f92fb87a,bc02e787,"As written in the dataset description, the duration column should be dropped since it includes an information which cannot be known before the call to the client.",7dc5e1b6,0.15436241610738255
7538,e19e307b3fd188,6e026881,"There is a strong asymmetric on the right (**right skew**) and most of the rent amount is between 450,00 and 3.450,00. There is a great chance that there will be several outliers in this data, and for my business knowledge, a rental costing more than R$ **12.000,00** is something strange...",2173955b,0.15447154471544716
7540,063a35f644e3c5,9c28979c,"### datatypes of columns
",1c30fb0a,0.15463917525773196
7551,631cd434fc3aa2,dfff173f,"### Outliers
The dataset [documentation](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt) mentioned that there were outliers and that we should probably get rid of them:

> There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students.
> Let's check it out for ourselves.
",2b74febb,0.15492957746478872
7553,d42518f6cb0995,d8b9d773,"~2% of activities are lectures, we should exclude them for answers analysis.",26913a9b,0.15517241379310345
7554,00001756c60be8,8ffe0b25,**Указываем путь к файлам с данными**,945aea18,0.15517241379310345
7555,1cd8be6e679620,18562d3a,## Train Data Overview,3ce15a43,0.15517241379310345
7557,f3c6048d1058e3,6119d98a,"#### 2) Understanding Sentiment across Stopwords count
- This graph don't highlight any specific feature across classes.",1d9056b0,0.15517241379310345
7560,1dd9c6aa74d289,e32aff2e,"## Available data distributions

#### Rope Climbing",5ef9a1be,0.15517241379310345
7563,84127ade6fde87,dff13e63,"There are two particularly intuitive levels at which networks operate on text: at the
character level, by processing one character at a time, and at the word level, where
individual words are the finest-grained entities to be seen by the network. The tech-
nique with which we encode text information into tensor form is the same whether we
operate at the character level or the word level. And it’s not magic, either. We stum-
bled upon it earlier: one-hot encoding.",f55d05b6,0.15517241379310345
7565,20e1ba19eb9b5e,e08240df,"When analysing the dataset and learning about the variables (do not forget about reading the attached description of the variables as well) it is helpful to take into an account such factors as: type of the variables (i.e. numerical, categorical,...), segments of variables (e.g. TotalBsmtSF and BsmtFinSF1 reports similiar properties of the house) and our expectation about the influence of the variable in SalePrice. A heatmap of correlation is extremely usefull to distinguish variables that have most impact on target as well on each other.",4569bfc1,0.15517241379310345
7566,98a6794067932a,8ff747f9,"Comme les données par rapport aux colonnes représentant des dates étaient corrigées, nous pouvions maintenant nous concentrer sur les données représentant les codes postaux. La cellule ci-dessous sert donc à représenter les différents codes postaux se trouvant dans la base de données et ce, de manière unique. Cette liste est également triée de façon croissante afin d'en faciliter l'observation. Grâce à cette liste, nous avons pu rapidement constater qu'il y avait plusieurs problèmes au niveau des données représentant les codes postaux. Premièrement, il est possible de constater que plusieurs codes postaux n'ont que quatre chiffres alors que les codes postaux américains sont tous composés de cinq chiffres. Deuxièmement, la liste comporte des codes postaux NAN qui représentent des codes postaux inexistants. Il s'agit donc de deux situations problématiques qui seront corrigées.
",08600fe2,0.1553398058252427
7569,63b44c85e32c1f,36ecdaee,If there was a list inside a list inside a list then you can access the innermost value by executing z[ ][ ][ ].,fb9b9562,0.1554054054054054
7573,396bc36edb95d3,cde467a2,#### Distribution Plots,965e4f8f,0.15555555555555556
7574,d905cde3391d2b,cd5fe181,"We'll discuss about 3 methods of measuring center - ***Mean***, ***Median*** and ***Mode***",067dba39,0.15555555555555556
7579,d58491f2896fc1,f90105ed,"**Kalıtım** : Bireylerin genetik özelliklerini kendilerinden sonraki nesillere aktarabilmesi
* **
**Çeşitlilik** : Bir popülasyondaki bireylerin kendilerini farklı kılacak özelliklere sahip olabilmesi
* **
**Seleksiyon** : Bir popülasyon içerisinde problemimize en uygun çözümü belirleme sürecidir. Daha sonra en uygun çözümler türetilecek çözümlerin ebeveynleri olarak hareket etmektedir. Buradaki neslin güçlü özelliklerini yavrulara aktararak en optimal çözüme odaklanılmaktadır.  Seçim uygunluk değerleri baz alınmakta ve Rulet Çarkı ya da Turnuva yöntemi uygulanmaktadır. 
* **
**Çaprazlama** : En iyi iki ebeveynden gelen genler yeni bir çözüm oluşturmak için rastgele değiştirildiğinde gerçekleşmeltedir. Tek Noktalı ve Çok Noktalı çaprazlama olarak iki tür bulunmaktadır.
* *Tek Noktalı Çaprazlama* : Tek noktalı çaprazlamada kromozom belirli bir kısmını bir ebeveynden kalan kısmını ise diğer ebeveynden almaktadır.
* *Çok Noktalı Çaprazlama* :  Çift noktalı çaprazlamada ise kromozomun her parçasını farklı bir ebeveyn verebilmektedir.

*Aşağıda örnek grafikte sizler için tek noktalı ve çift noktalı çaprazlamayı açıklamaya çalıştık*

<center><img src=""https://i.hizliresim.com/r1dvy2p.png"">
    
* **
**Mutasyon** : Seçim ya da mutasyon kullanılarak yeni bir popülasyon oluşturulur ve daha sonra elde edilen yeni popülasyon mutasyon yoluyla rastgele değiştirilmektedir.

*Aşağıdaki Örnekte sizlere mutasyon aşamasını da göstermeye çalıştık*


",514bfdff,0.15555555555555556
7580,d6cbd7160961dc,df78724f,## 2.3. Benford's Law: Chi-Squared Test,36d74664,0.15555555555555556
7582,3597174a998d4d,b1e4126e,"It can be concluded that 
* The ratio of booking's cancelation and transation of different room types in the same hotel are obviously different. And the ratio of booking's cancelation and transation of different hotels in the same room type are also obviously different. **I think F room type in the City Hotel and G room type in the Resort Hotel have high price and high number of booking, but they also have high ratio of booking's cancelation.**
* The A room type in both hotels has the highest number of booking and the highest number of changing room types. **I think it's because the A room type is a special offer to attract customers.** And the D room type has low ratio of booking's cancelation, high price and high number of booking, **so it might be the most profitable room type for the City Hotel.**",276892ed,0.15555555555555556
7584,d77e6d61ad2e8b,47c0b57e,# Setting up Environment in PyCaret,03fd0e96,0.15555555555555556
7585,4fd4b6a80d40e3,482a50c5,## Step Function,f6913cc3,0.15555555555555556
7586,b0c2805cd5c087,cabac97e,Image burning-glass.com,0446f327,0.15555555555555556
7589,49ac6594c8f5cf,74cc4ac8,**Stream Selection by Candidates**,6f19f28a,0.15555555555555556
7590,979f1e99f1b309,aed494e1,***The Disturbution of boosts showing that almost all boosts were under 10 boost and the most of boosts are 0 which indicate that players usally didn't use boosts***,d1bfebbf,0.1557377049180328
7593,663bbc9eaf267b,8a1152e3,"To prevent data leakage, we split the dataset into train and test set before implementing feature engineering methods.",32445529,0.15584415584415584
7594,722cd844dfbe8f,a2fb6f57,The distribution is almost equal between class 0 and class 1. The presence of MGMT is slightly higher. A total of **585 patients are tested in the train set**.,0cedb385,0.15584415584415584
7598,90691864eb68c7,a9450dea,"We have 3 Categorical variables : airport, Waterbody and bus terminal",3555ef9b,0.15584415584415584
7602,957e035ba5b9d5,580b5062,"img_width, img_height = 128, 128
input_shape = (img_height, img_width, 1)

categories = ['drawings', 'engraving', 'iconography' ,'painting' ,'sculpture']

train_path = 'dataset/train/'
val_path = 'dataset/validation/'
test_path = 'dataset/test/'",778ab3d3,0.15602836879432624
7603,b61ab8f81dc03d,4e66923f,"The information above tell us that there is a strong correlation between the fields ""Sex"", ""PClass"" and ""Survived"", example if you were female in the 1st Class your chances were 96% to survive and for male in the 3rd Class your chances were only 13% to survive.",64d05394,0.15602836879432624
7604,117fc0956643d0,c6199dd1,## Step 0. Install All Required Libraries,68cef9fd,0.15625
7605,2c3a6969252dc0,64ca6ad7,#  **Analysis**,d30f10ce,0.15625
7608,96c4c0e36b8ec0,6650a882,**Rate of survival by sex and class of cabin**,4dd6de8c,0.15625
7609,9daf8b4a46725e,8d6276af,### Data Preprocessing,7d9cc411,0.15625
7614,69130a37583a06,aded3469,"### Target Variable distribution :

* This competition contain highly imbalanced data  around 1:30 ratio for this competition.
* Same thing also coming with when we are summing it with trasanction amount data.
 I have taaken reference from this kernel. draw my own conclusion over here.
https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt#Ploting-Transaction-Amount-Values-Distribution",65a4de1c,0.15625
7616,fae5023faa435f,b1dea461,Entire data,b37c893b,0.15625
7626,4daf6153275cbf,bc9a1fc2,I managed to fill 6328 missing values!,51db1961,0.1566265060240964
7628,20b372b6e4e276,b7cf729b,"### Commit 10

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5
* LeakyReLU_alpha=0.3

LB = 0.711",ec8b0860,0.15671641791044777
7630,4fa553c2b837d4,603d6011,"# Using Decision Tree Classifier

You can choose any model and run the dataset.",c65a23e9,0.1568627450980392
7634,7cfd96218dd933,32984c98,"#### **ATTENTION**
* 11 DAYS AVERAGE BRIGHTNESS: 335
* THE VALUES IN THE MOST OF THE DATA ARE MORE THAN AVERAGE. THIS SITUATION INDICATES THAT HIGHLY STRONG POINTS HAVE BEEN RECORDED IN THE LAST 11 DAYS DATA.",7c34d96c,0.1568627450980392
7635,523123dad03177,1b2aa82f,Math's the toughest subject.,48a5e4e6,0.1568627450980392
7637,917957c6c4065f,ba043d6d,확인해보니 category_id가 29번인 경우 매칭이 안됐네요.,55b8ed68,0.1568627450980392
7645,e9b9663777db82,922bbfe7,If there is any duplicates we can remove them now.,648e8507,0.15702479338842976
7647,2ada0305b68956,3bd9db2d,### 24. Palette = 'Oranges_r',133e26f4,0.15714285714285714
7649,2730840089c8eb,40f0e578,"The last sequence, `\n`, represents the *newline character*. It causes Python to start a new line.",34d27dac,0.15714285714285714
7651,38b79494ac749e,eeace8d9,### Plot samples,39162a40,0.15714285714285714
7654,9c044fa3072552,a13f94b5,#### Missing Column Values Percentage,1362842e,0.15714285714285714
7657,04ff2af52f147b,ed52b784,"We can see below that the median fare increases not only as the class of the ticket increases, but also as the number of family members travelling with the individual increases (there are exceptions, but the trend is there).",d5f37be9,0.15730337078651685
7659,5f27526aa6c113,35b773e6,## missing value,a5c26ab6,0.15730337078651685
7662,fdc3afd309b850,3bf5b501,"The Addresses in Brasília is very confusing,  The Federal District of Brazil has 33 Administrative Regions (AR). Each of them has their own address system. People often get confused setting the address and probably fill the address wrong on the Vila real website. So our idea here is organize, splitting from the address column, which the property owner filled, into a new address column, neighborhood and the AR. 

We will send pieces of the address values to the correios API (The Brazilian mail service) to return the address organized in Address  and Neighborhood. After that, we are going to search on the web to find the AR for each neighborhood.

Those features organized and accurate will be very important in the future to fill the apartament characteristics with a high local precision and add new features about the apartaments to our data frame.",966bde38,0.1574074074074074
7663,ce9ed5e2d601d7,6ca00675,y to categorical,f58a2f43,0.15748031496062992
7680,caa0ce2715bf34,a878b025,"### Observations
1. The value ranges vary among fields, hence scaling is required.
2. No Bad values observed
3. CUST_ID is all unique

## Checking for Duplicates",78a5dc51,0.15789473684210525
7682,a1dcd92986bc84,6483a017,"## Prepare the data

We will use the [MS-COCO](https://cocodataset.org/#home) dataset to train our
dual encoder model. MS-COCO contains over 82,000 images, each of which has at least
5 different caption annotations. The dataset is usually used for
[image captioning](https://www.tensorflow.org/tutorials/text/image_captioning)
tasks, but we can repurpose the image-caption pairs to train our dual encoder
model for image search.

###
Download and extract the data

First, let's download the dataset, which consists of two compressed folders:
one with images, and the other—with associated image captions.
Note that the compressed images folder is 13GB in size.",730acaaa,0.15789473684210525
7684,9e27af2600925c,12140d69,"**Expected Output for m_train, m_test and num_px**: 
<table style=""width:15%"">
  <tr>
    <td>**m_train**</td>
    <td> 209 </td> 
  </tr>
  
  <tr>
    <td>**m_test**</td>
    <td> 50 </td> 
  </tr>
  
  <tr>
    <td>**num_px**</td>
    <td> 64 </td> 
  </tr>
  
</table>
",9b556435,0.15789473684210525
7689,31b564f11ef638,6c92f8a2,### Concatenate train and test data,424f9692,0.15789473684210525
7693,d369f200a84c2a,d987f6c2,# Capsule Network Architecture,8fef4d48,0.15789473684210525
7695,0d8df2c2983694,f7904828,## Import the relevant libraries,9bf7fa4e,0.15789473684210525
7696,9f3710be6aea65,7c05d162,Now we fill NaN values of the columns Age and Embarked. In the 'Age' column we will fill with random numbers beetwen (mean +- standard deviation) values. ,ae9bda88,0.15789473684210525
7701,d81d3830152f88,0f60df97,Because I want to investigate the effect of 3pt shooting after GSW won their championship. I will only look at data after 2015 onward.,9551eac9,0.15789473684210525
7707,f91f58d488d4af,2de21eaf,"## Now we'll try to build a baseline model totally from scratch.

### Approach: Pixel Similarity

1. Find the pixel value for every pixel of the 3's, then do the same for the 7's.

2. This us two group averages, from which we can call ""ideal"" 3 and 7.

3. Then, to classify an image as one digit or the other, we will see which of these two ideal digits the image is most similar to. 

",5df1bbf3,0.15789473684210525
7711,bef2347846e476,34ce7488,"The data famework holds the App,Category,Rating,Reviews,Size,Installs and so on...We can check these categories and types we can use data.info() command. ",cb93bf51,0.15789473684210525
7713,1f3295ed0d4e4a,14a34455,# Config,b0aeb172,0.15789473684210525
7725,c84925c8171900,161062ec,"<a id=""inspect""></a>
<h3 name='libraries'>   
      <font color = purple >
            <span style='font-family:Georgia'>
            3.2 Inspect Data Frames
            </span>   
        </font>    
</h3>",e21ff7ec,0.1588785046728972
7728,d1ff7e10ee0102,0230474c,"# 'SalePrice', her buddies and her interests",2cc71c3c,0.1590909090909091
7729,f269d2fbd5f1be,59ea2c6b,# Data Cleaning and Transfomation,1264c440,0.1590909090909091
7734,32e04b08ff52eb,a7218311,Class Distribution,8d5b86e0,0.1590909090909091
7735,be2f4d8a6b73ca,55f5fef3,"**📌 Attribute Information:**
* **Age:** age of the patient [years]
* **Sex:** sex of the patient [M: Male, F: Female]
* **ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]
* **RestingBP:** resting blood pressure [mm Hg]
* **Cholesterol:** serum cholesterol [mm/dl]
* **FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]
* **RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]
* **MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]
* **ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]
* **Oldpeak:** oldpeak = ST [Numeric value measured in depression]
* **ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
* **HeartDisease:** output class [1: heart disease, 0: Normal]",5d8ce40a,0.1590909090909091
7736,5083d7a61f2426,ebc0d40b,"
After obtaining the data you want to perform the forecast, it is necessary to separate the data in training and testing.

Test = %TOTAL_LENGTH (usually ~30%)

Train = TOTAL_LENGTH - Test",541a0fec,0.1590909090909091
7743,c9b4e282e4e2c1,62982ff9,"B-Which kind of surfarce seems to be more related to injuries?
",f44d339f,0.1592920353982301
7744,ac1abfe1dfe815,ffca4498,"### Number of Retweets  
94% of the tweets are without any retweet, 4% with one retweet, and there're outliers.
The tweet with the highest number of retweet which is 44 was a negative tweet about `US Airways`",6529dbcb,0.1592920353982301
7746,0e2a23fbe41ca9,b00f2d87,"Observations:

- feature_1, feature_2, feature_3, all are categorical variables
- feature_1 has 5 unique values
- feature_2 has 3 unique values
- feature_3 is a binary column

### 3. first_active_month",64e4762c,0.15942028985507245
7752,598b6228760590,40cdf4ee,"- Sex
- What happened? There are more men than women, but the survival rate of women is higher than that of men. By watching the famous movie Titanic, you can actually know that most men have partners, or some men are gentlemen, and give priority to women in times of crisis.",be30ab66,0.15942028985507245
7754,b01ee6cb674fa3,0526fac1,rename the 'yellow sea location',a8ffd35e,0.15942028985507245
7758,4c47839b067546,4c91e478,"### test[priceCurrency] 
Так как содержит только рубли, то можно удалить.",1f517b02,0.1595744680851064
7759,73893f0467d5e3,c25556bd,## For mean_texture,279787c6,0.1595744680851064
7760,f6648e47713411,a98b5cba,"## Kết Luận

- Tập dữ liệu khá không cân bằng theo biểu đồ hình tròn ở trên
- Chúng tôi sẽ chọn chiến lược lấy mẫu thích hợp để giải quyết vấn đề này. Data augmentation được sử dụng để thêm các mẫu bổ sung từ các lớp thiểu số. Trong  hình ảnh của chúng tôi, điều này sẽ được xử lý được bằng cách thêm độ méo vào dữ liệu bằng cách thực hiện dịch, xoay, thay đổi tỷ lệ cũng như bằng cách thêm các loại nhiễu (áp dụng albumentation)",f4af4d1c,0.1595744680851064
7764,4cd25e50c7e007,ea223840,"**We already have ""year"" ,""month"", ""holiday"",""working day"", So ""dteday"" column is not required, we are drop column. 


As we are not having ""casual"" and ""registered"" while prediction,its looks as target variable, but here we have to predict the overall demand,As our target  variable is count,so we can drop casual and register variable.**",ceb0c525,0.16
7773,2a9a149f306b6c,17796d66,Using the data_block api to create a databunch. The images in the test folder become the validation set.,295c52ce,0.16
7774,9395559895004f,9d111d44,## Reference Directory,b5a0494b,0.16
7775,10c5a39a87c47e,1c9cadd4,"**Observation:**
- As we can see from above images that in infected sample there is small dot in almost every image which may be the critical mark while recognizing the imfected image. the model has to learn this pattern during training stage.",09c7337a,0.16
7776,21c1e34efd71b8,8f1bd7fa,"Using Robust Scaler over Standard scaler to Normalize the Amount and Time column, deleted the orignal columns in next step.",23b2cdd6,0.16
7778,83df814455f06c,ae747f0c,"# **7. Import libraries** <a class=""anchor"" id=""7""></a>

[Table of Contents](#0.1)",c9cff71a,0.16
7781,cee088a6840708,0029a7b5,# Step 3 -> Import everything you need,55463e1c,0.16
7782,5cb7f999fd1ecb,5d143292,### Reading Data,88b54f70,0.16
7783,e5dd725b8fa422,af176f4f,# [__Visualizing the dataset__](http://),14675d8b,0.16
7784,0687cd5c8597db,04b9fcf9,### **Seperating the Feature and the target columns**,4edec76a,0.16
7787,7e1da639035ac5,bdaaf1f8,### <a id='6.1'>6.1 Distribution of Economic Need Index</a>,120b6c23,0.16
7788,d6ddbe57f59cf7,bdcf1a37,## Adding labels to bars,504a3cda,0.16
7789,91eaec994e0c6f,66a5e4f0,Let's take the example of item <b>FOODS_3_586</b> sales in California store <b>TX_3</b>,376aef10,0.16
7793,67b7354e96113a,af4ead42,**Understanding the Survival Nature of Titanic**,dca94250,0.16
7796,2bd6c370695ea7,44d19b6b,## Feature engineering,cbe6aec8,0.16
7797,519e936017c30a,73fc30a1,"Ordenados nuestros datos, nos ponemos manos a la obra con nuestro análisis. Como paso previo, vamos a observar la correlación existente entre las distintas variables que conforman esta base de datos. ",dc34915d,0.16
7799,cb570c7b7f0501,f2e3fd79,"<a id='Cleaning'></a>
## Cleaning Data
",a200a0ec,0.16
7800,ee23a565163388,9bb099d6,# **Exploratory Data Analysis**,88aacbc4,0.16030534351145037
7811,df51d4c54fbb91,0526fc8c,Now we must reshape the date to make it Keras friendly.,4226dd72,0.16071428571428573
7815,d5f78aa381f58d,720ea64b,"As there are no null values in data, we will go ahead with finding duplicates.",d60f358f,0.16091954022988506
7816,14defffcd250f3,c23388a0,"# Data Cleaning

We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation). However we can be smarter about this and check the average age by passenger class. For example:",3a683b94,0.16091954022988506
7819,9169c4e9c33c90,6b648cf8,Authors that made repeat appearances per year,725bf880,0.16101694915254236
7820,1294fb4c86f993,0c29f187,"For this data file we would like to do the following:
1. Removing column Fact Note which seems to irrelevant.
2. Converting numerical data to `float` type and for that:
    * replacing commas `,` with decimal points `.`
    * the data has not been consistent about expressing the percentage values (sometimes it uses `%` and sometimes it uses `.`)
    * removing `%` and divide those numbers by 100
    * removing `value flags` as indicated in the file.
3. Filter data for `2016` only
4. Transpose the dataframe
5. Check the `states` is the same in both dataframes.
5. Merge `totals` column from `guns.csv` with it. ",4471e513,0.16101694915254236
7824,1c2948c70624cf,cc9395dd,# **Data Cleaning**,eeb97474,0.16129032258064516
7826,098fedfcd07456,e88cba45,# Showing the Image of Number with their labels the Labels are stored in ytrain ,052ece26,0.16129032258064516
7831,16862cb02d73d5,e1ed3ae8,Now I do a **pivot** on the dataframe to create a dataframe with all metrics at a **date level.**,d7ffa1a6,0.16129032258064516
7836,56e58d53ac9c57,4aee747c,"we can see that Review, Review_Title, Division, Department, Product_Category columns are strings while the rest are integers.",90e2ab8e,0.16129032258064516
7841,c0ddb77bf32e2b,bc23b58c,"The counts of NR in  'RAIN_COND', 'PH_RAIN' are both 30455(81.48%). If NR stands for 'no record', we should better drop these columns.",a0cb45f7,0.16129032258064516
7847,0925f172b5eb74,671edaf3,"# Loading the dataset

This first set of functions are taken from *utils.py* and are related to data loading as well as to the partition of the whole dataset into a train and test sets.

Later on, we will also create a validation set. The difference between the test and validation sets is that the validation set is going to be used by the model building algorithm at the end of each epoch during its training phase, so it can keep track of how close each epoch gets to optimal weights. On the other hand, the test set is going to be used once the model is completely built to try its overall performance.

This has been the final approach because the provided images for testing only consist of 3 samples, as the competition has a hidden test set that will be only provided once the submission has been done, consisting of 5 thousand extra images.

To summarize it all, we will use a 15% of the total dataset as a validation set and a 10% as the test set.",ec34cd72,0.16129032258064516
7851,be9597c72542a2,d3512ef7," **What is the max ambient and module temperature?
**",6f29c6d8,0.16153846153846155
7857,156bbcff05dcea,c891ae58,"Reference

https://www.statology.org/seaborn-subplots/",66ad1fe9,0.16176470588235295
7859,7f74a04ae75792,0b8acd6d,No. So let's drop it,d01e91da,0.16176470588235295
7862,eb0ecd6bebeb15,b091893b,"Veri çerçevesindeki sayısal değişkenler için temel istatistik değerlerini görüntüleyelim.

Standart sapma ve ortalama değerlerden çıkarımda bulunarak hangi değişkenlerin ne kadar varyansa sahip olduğu hakkında fikir yürütelim.",d7b93a60,0.16176470588235295
7863,3d905ce4828057,3c867ae7,"** First of all, let's import the necessary libraries.**",5b006cc3,0.16176470588235295
7864,e4c6dd957eb5ce,6d9c0f8c,## Users summary,2e383665,0.16176470588235295
7870,fdc3afd309b850,d1e41ae0,"<a id=""address""></a>
## 6.1 Address



",966bde38,0.16203703703703703
7873,ccabe7a86825ce,72278548,**Get all categorical columns**,d766cbf9,0.16216216216216217
7875,2dda7facf3c1e0,f59e9948,"### Download and Unzip Training DataSet
",45552d2b,0.16216216216216217
7877,e4525eb0c96f28,0d5bf012,## Exploratory Data Analysis,2093a1f1,0.16216216216216217
7879,8276973853faa1,d4981e2f,"> here ""Telangana"" and ""Rajasthan"" trying to tell us a story, let's get into viz part so that we can understand better",88da542b,0.16216216216216217
7884,63b44c85e32c1f,db48157b,### Slicing,fb9b9562,0.16216216216216217
7885,b7b1057764fa02,e081a5f5,"Once I have loaded the data, it is time to begin an understanding of it. The code below gives us some basic details about the data.

**Note**: In the train-test split I have used the `stratify` argument on the labels. This argument ensures that the data is split evenly along all labels i.e. the proportion of each label in the testing data is the same as the proportion of the testing data itself - 10%.

Notice that I have two sets of testing images here. This is because the first test set is taken from the same data as the training set - that is, they have a similar look, while the evaluation data, taken from a separate library, has a completely different look. This is done so that I can test the model on both types of data and see how robust it is in the presence of changing backgrounds and colours. The differences between the two testing data will become clear when we look at their images.",5053a192,0.16216216216216217
7892,3dd4294f903768,dbcc5674,We can also see some information about our target: the mean of 'Aggragate rating' is 2.66 and the standart diviation is 1.51. The min score is 0 and the max score is 4.9.,0d89d098,0.1625
7893,fdbbd573ba31c2,6f5db704,## Categorical & Numerical Features,f7c28d74,0.1625
7894,254cccd5145725,9b17f9a4,That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we use  .info(),a49b4037,0.1625
7896,e19e307b3fd188,e6991488,#### Boxplot,2173955b,0.16260162601626016
7904,d96642860ab3dd,6e11f238,### 1.4 Embarked Feature,98419d48,0.16279069767441862
7908,2ada0305b68956,335d68d6,### 25. Palette = 'PRGn',133e26f4,0.16285714285714287
7912,b61ab8f81dc03d,c611946d,"<a id=""embarked_survived_sex""></a>
### Embarked/Survived/Sex",64d05394,0.16312056737588654
7913,957e035ba5b9d5,32c54829,"count = 0

for path in [train_path, test_path, val_path]:
    for i, cat in enumerate(categories):
        cat_path = os.path.join(path, cat)
        images = [file for file in os.listdir(cat_path)]

        for image in images:
            try:
                img = Image.open(os.path.join(cat_path, image))
            except:
                count += 1

    print(""Total bad images in {}: {}"".format(path, str(count)))",778ab3d3,0.16312056737588654
7918,eb33e05704d647,27e2dd96,Vgg-16 model,cd80436d,0.16326530612244897
7920,0e7ac281a19feb,518b5545,### Images,5b84d10f,0.16326530612244897
7925,917957c6c4065f,4db68220,![image.png](attachment:image.png),55b8ed68,0.16339869281045752
7929,5a8c553e21c70f,b721e2b0,Dataset has 284807 rows (training samples). All features are numerical (continuous) and there aren't any null values. Let's look at the target Class column.,9ebd9d8f,0.16363636363636364
7933,5ce12be6e7b90e,8ac71058,"# Operators

## Arithmetic operators

| Symbol | Operator                    | Use    |
|--------|-----------------------------|--------|
| +      | Addition                    | x + y  |
| -      | Substraction                | x - y  |
| *      | Multiplication              | x * y  |
| **     | Power                   | x ** y |
| /      | Decimal division            | x / y  |
| //     | Integer division            | x // y |
| %      | Integer remainder           | x % y  |

This is fairly straightforward except maybe integer division `//` and power `**`.",c0ab62dd,0.16374269005847952
7934,30fdc4a6e3c1db,117d163a,"We have information from 10 stores : 4 stores of California, 3 stores of Texas and 3 stores of Winscoin",6111ddee,0.16374269005847952
7937,918040fad252ec,db5941c2,Membuat train data path dan penamaan,966fcd8f,0.16393442622950818
7938,bcd7e398c4d0ec,f2d47a28,"## Bread
* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)
* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)",77a143f6,0.16393442622950818
7942,0932046e1f485d,c95791ae,"We need to fix three things in the ""Size"" column.
* The M & k at the end of the number
* The string ""Varies with device""
* Convert everything to a number",218cc7a3,0.1640625
7943,20b372b6e4e276,f0996997,"### Commit 12

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5
* SEED = 42

LB = 0.711",ec8b0860,0.16417910447761194
7947,a4aa36df07fd53,4364a231,"Ternyata ada data NaN, coba kita cek Nan untuk semua tipe variable, run:",d2f42b6d,0.16417910447761194
7948,21413205980558,e576ea1f,"* # The average age of clients is about 41 years old, the highest is 95 years old, and the lowest is 18 years old;
# 客户平均年龄41岁左右，最高95岁，最低18岁；
* # The average customer banlance is 1528, but the standard deviation is very large, indicating that the distribution of this data is very scattered.
# 平均客户盈余为1528，但标准差非常大，说明这一数据的分布非常分散。
* # The number of contacts in the last marketing activity ranged from 1 to 63, and the more the corresponding contact times, the higher the customer's participation in the previous campaign was;
# 上一次营销活动中的联系次数为1-63次，对应的联系次数越多，客户对上一次活动的参与度越高；
* # The number of days since the last marketing campaign is - 1 ~ 854 days. Why is there - 1? Whether it is data error or not;
# 上次营销活动后的天数为-1~854天。为什么有-1？是否数据错误；
* # The cumulative number of contact with customers before the last marketing campaign is 0 ~ 58
# 上次营销活动前与客户的累计接触次数为0~58次",84197de0,0.16417910447761194
7950,3d08ca7656dec0,8ca482df,**Analysis**,bd3f87e3,0.1643835616438356
7951,596389bed473be,3b76de16,"# pd.read_csv(""File location and file name"", index_col=""Index col name"")",5f8af156,0.1643835616438356
7955,5d2a3e82679cf3,b840f716,# There isn't strong correlation btw any x variables and Y.,9e60b1e3,0.16455696202531644
7957,869a39a3d4dea2,f299b2c2,"Reading the pixel values from image by using the matrix notation of the rows and columns which is nothing but the height and width of the image, matrix is indexed from 0",9020daf8,0.16470588235294117
7960,5f4ae633cfd090,f91df9ed,"Since, it is baseline model, imputation would be pretty basic and simple:
1. Impute the numerical columns with mean
2. Fill the categorical columns with 'most_frequent'",a30a16e2,0.16483516483516483
7962,8ec771f5600a61,e1397f51,"### White Dot in the middle in the graph shows the median.
 ## From the above plot we can conclude many things:-
 ##   1. As we can see Survived plot of Male is much much wider from age 0 to 15 or so. From this we can conclude childrens were given preferences.
   ## 2. Survived = 0 plot for Males is much wider than Survived = 1 from age 60-80 or so.
## If you wanna know more about a Violin plot """,48364c1f,0.16494845360824742
7972,2f47abddfd1928,0a62b11f,"### 2.2. Embarked

Embarked shows similar problem, so we can change a string feature to a numerical feature.

- S --> 0
- C --> 1
- Q --> 2",ae33cc0b,0.1652892561983471
7976,a566b5b7c374e7,546bd2b5,## Exploratory Data Analysis,b3dc5545,0.16546762589928057
7980,10b5af05d804ff,893599b6,Please predict target for this test data:,4a9b1705,0.16666666666666666
7988,7650e0ac081e94,f526588d,"## Transfer learning
These notebooks ([train](https://www.kaggle.com/markunys/sartorius-transfer-learning-train), [inference](https://www.kaggle.com/markunys/sartorius-transfer-learning-inference)) show how to do transfer learning with LIVECell dataset.",0081cee5,0.16666666666666666
7989,cf46cd6f7c55c0,67a35dde,"# Why does Pseudo Labeling work?
When I first learned about pseudo labeling (from team Wizardry's 1st place solution [here][1]), I was surprised that it could increase a model's accuracy. How does training with unknown data that has been labeled by a model improve that same model? Doesn't the model already know the information? Because the model made those predictions.

How pseudo labeling works is best understood with QDA. QDA works by using points in p-dimensional space to find hyper-ellipsoids, see [here][2]. With more points, QDA can better estimate the center and shape of each ellipsoid (and consequently make better preditions afterward).

Pseudo labeling helps all types of models because all models can be visualized as finding shapes of target=1 and target=0 in p-dimensional space. See [here][3] for examples. More points allow for better estimation of shapes.

[1]: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003
[2]: https://www.kaggle.com/c/instant-gratification/discussion/93843
[3]: https://www.kaggle.com/c/instant-gratification/discussion/94179",191b86b8,0.16666666666666666
7990,f7436bc492474c,11eca38b,"Here's a couple of examples of comments, one toxic, and one with no labels.",328fd235,0.16666666666666666
7992,18c751d4a02149,3960a86e,This is how you will import data. Pay attention to the path.,82cedca3,0.16666666666666666
7999,cf4d1c1ad1476c,6fcb50a0,# EDA,768c1a59,0.16666666666666666
8006,98ea617d18c9cc,87cc06a3,# Exploring the data by Plotting Graphs,e6316d11,0.16666666666666666
8009,9ad9a97e628bfa,92dd7b44,"여기서 Survived는 label이며
- **Pclass, Sex, Embarked**는 확실히 **Categorical feature**라 볼 수 있음. 
- **Name** 과 **Ticket, Cabin**은 자세히 살펴볼 필요가 있을 것으로 보이며
- **Age, SibSp, Parch, Fare**는 일단 **numeric value **봐도 될 것이라 보임. 

먼저 Ticket 과 Name을 살펴본 후
 Null value들을 추정하여 채우고, 추가적으로 Add할 수 있는 Feature 가 있는지 살펴보고,
Categorical feature를 encoding 한 후 모델링을 진행하도록 하겠다. ",0a7e1136,0.16666666666666666
8012,fdc3afd309b850,d9fb5e2e,"<a id=""an""></a>
### 6.1.1 Address  and  Neighborhood

As said before, we are going to organize the addresses splitting the column address. Using the [Busca Cep API](https://github.com/arthurfortes/consulta_correios), created by [Arthur Fortes](https://github.com/arthurfortes), we will be able to get the correct address and the neighborhood extracted from the Correios Web site.
Recently, I've also contributed with Busca Cep API on Github to updating the Web Scraping code.

",966bde38,0.16666666666666666
8014,1084376bc4897c,fcf1a4f6,### 2.2 Lets see which features are continuous or discrete,1b598487,0.16666666666666666
8017,1d5daeca89f48d,9546ee94,"## Types of Word Embeddings

=> Frequecy based Embedding
       1. Count Vector
       2. TF-IDF Vector
       3. Co-Occurence Vector
=> prediction based Embedding
       1. CBOW(Continuos Bag of Words)
       2. Skip - Gram Model",48d478bc,0.16666666666666666
8020,8c7e00ca3dc5a7,70d99340,## Check for Nans/NA/Missing Data,c83346e4,0.16666666666666666
8025,37b09262279764,8ec3b53a,Our data is clean now,37c4c417,0.16666666666666666
8028,f166950fa915f8,dbf65d64,"### Map target label to String
* **0** -> **NEGATIVE**
* **2** -> **NEUTRAL**
* **4** -> **POSITIVE**",a7f6ca5e,0.16666666666666666
8030,8447633e1d256c,cad8b552,## 2. Exploring Data,60d593ce,0.16666666666666666
8031,6cade0b6a41ba2,89aa233f,## 3.2. Gender,e6110293,0.16666666666666666
8033,892be0a523578c,d12327b5,"#### 3. hourlyCalories, hourlyIntensities, hourlySteps
**3.1** These three datasets has the same number of records with matched Id and ActivityHour, so I will merge them before further processing",b0e8d7c0,0.16666666666666666
8036,e0f03003a69819,9d95993d,"The charges data is positively screwed. But we are not !!!
",609ad1f4,0.16666666666666666
8038,d46508f983e086,8469b116,**Defining Labels (Classes)**,454138b8,0.16666666666666666
8039,565ad413cd802f,d3fae1a2,"To create a tensor from the labels, we will encode the labels as vectors of 1s & 0s. For example, if the labels are `'2 4 5'`, the correspoding vector for it would be `[0, 0, 1, 0, 1, 1, 0, 0, 0, 0]`. Let's define helper funtions to encode labels into tensors and vice versa.",397b074e,0.16666666666666666
8040,95656e8d666b16,0fb7b34a,No duplicates either,65e88599,0.16666666666666666
8042,5d6d539f8e7121,47b1b20a,"## groupby
	- e.g) city, f2 컬럼으로 그룹핑 하기
	  df_group = df.groupby(['city', 'f2'])
	- groupby만 하면 DataFrameGroupBy 객체 (df의 함수 이용 불가)
	- 주로 groupby 후 sum 등의 집계 기능까지 연결해서 사용해서 DataFrame으로 활용함
	  df.groupby(['city', 'f2']).sum()
		- df2.index 튜플형태의 멀티 인덱스 반환됨
		  df2[df2.index == ('경기', 0)]
		- df2.reset_index(inplace=True) 
		- df2.set_index(['age'], inplace=True)",79340a85,0.16666666666666666
8045,2d40f383473fa4,d3c78714,"# 4. Feature Engineering (FE)

After analyzing all the variables in EDA, it's time to clean and extract more useful information to feed the model.<br>
Starting dropping unworthy features: `PassengerId`, `Ticket`.",1da1eff0,0.16666666666666666
8051,897ca904b74a98,5c06c76e,### Target column Analysis,c5844ad4,0.16666666666666666
8052,02773bdc5d3c7a,ce482075,*1. Import the required libraries*,86245f35,0.16666666666666666
8055,999258a81ba32a,31b429f2,# Ball 1 Frequency Chart,48cd3d21,0.16666666666666666
8056,56cc8fb47bef6a,af785644,"<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Just to confirm if there are any invalid data, analyze the unique target variable( in our case the target column is the &quot;label&quot;)&nbsp;, and the result is the expected &quot;ham&quot; and &quot;spam&quot;; &nbsp;so we can proceed further</span></span></p>
",652d6670,0.16666666666666666
8057,d4c5aaa4b36810,fd04cb00,There are 1344 indicators in total so we must pick the ones most likely to affect happiness. ,65441f28,0.16666666666666666
8060,eb0854a6601407,0aaa933e,## Read in a single invesment_id,6d107747,0.16666666666666666
8071,4ba67fa2de9e6e,3da5f081,"* The train set contains 3422 images, so that means there are 49 images in the set that contain 0 bounding boxes.",8e0dd482,0.16666666666666666
8072,07f5853e4db8f8,bcda27b2,"From the table above we can see column pp_total_raw,pct_free/reduced, county_connections_ratio has missing value greater than 30%
. We can drop this collumns but since data sets contain small features so I want to fill using _fillna() functon.
",d13c2c32,0.16666666666666666
8074,a76e0e8770b7a0,56bb1c89,Used in WorldCloud and see which words is mostly occurs.,02863d3b,0.16666666666666666
8076,396bc36edb95d3,4c5604b3,#### Skewness and Kurtosis,965e4f8f,0.16666666666666666
8077,fbb1f9d3818830,c6c4a8c0,# Basic Imports,c7027f86,0.16666666666666666
8078,135122550b6483,cbd42d17,<h2> EDA </h2>,6592d6d8,0.16666666666666666
8079,b4ecd6e4277e3c,33c2e2b6,"### Code for Loading Embeddings

Functions taken from the kernel:https://www.kaggle.com/gmhost/gru-capsule
",94d79d5f,0.16666666666666666
8084,061d6757dfbce0,77a3405a,## Glimpse of Data,c0c2915a,0.16666666666666666
8085,be616f0785c32d,51efd838,"[Go Top](#top)


<div id=""PartAdrug""></div>

##### A.2 Results

###### A.2.1 The number of publications each drug appeared, top ones, >=100 times, are (full list in sorted_alresult):
*     103 hydrocortisone
*     106 ritonavir
*     111 prednisolone
*     113 dv
*     118 ciprofloxacin
*     119 cyclosporine
*     127 acyclovir
*     134 azithromycin
*     141 amoxicillin
*     155 doxycycline
*     159 dexamethasone
*     166 triad
*     177 chloramphenicol
*     177 kanamycin
*     238 isoflurane
*     248 gentamicin
*     370 bal
*     383 adenosine
*     436 insulin
*     480 ribavirin
*    1767 penicillin

[Go Top](#top)


###### A.2.2 the drugs that have been related to coronavirus in literature, and the top ones, >10 times, are (full list in sorted_alresult.coronavirus):
*      10 times: amoxicillin
*      10 times: fluorouracil
*      10 times: kanamycin
*      12 times: azithromycin
*      12 times: hydrocortisone
*      13 times: doxycycline
*      13 times: levofloxacin
*      14 dexamethasone
*      14 isoflurane
*      15 dv
*      15 kaletra
*      15 prednisolone
*      15 tamiflu
*      16 cyclosporine
*      16 gentamicin
*      18 tao
*      19 acyclovir
*      24 triad
*      25 insulin
*      35 remdesivir
*      41 adenosine
*      60 ritonavir
*      66 bal
*      86 penicillin
*     150 ribavirin
    
 [Go Top](#top)
 
###### A.2.3 The drugs specifically related to COVID-19 in literature (sorted_alresult.covid19)
*       1 acetaminophen
*       1 acyclovir
*       1 amoxicillin
*       1 antitussive
*       1 azithromycin
*       1 bal
*       1 ceftriaxone
*       1 chloramphenicol
*       1 digoxin
*       1 doxycycline
*       1 fluorouracil
*       1 ganciclovir
*       1 ibuprofen
*       1 iclusig
*       1 insulin
*       1 levofloxacin
*       1 penicillin
*       1 sulfasalazine
*       1 tigecycline
*       2 adenosine
*       2 triad
*       3 darunavir
*       4 tao
*       7 kaletra
*      12 ribavirin
*      17 remdesivir
*      22 ritonavir

[Go Top](#top)


<div id='PartAchem'></div>

 Now we analyze the chemical similarities of these drugs.


",b78e18aa,0.16666666666666666
8086,dd3721cb49c1fd,7dd9f82b,"<a id='2'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">2. <b>Re-installing the pandas to avoid some runtime error...</b></h1>
  </div>
</div>",1a53fdd9,0.16666666666666666
8087,9ca9a30fc69d9b,1ac2905e,##### I checked the same stats for Non-friendly matches and it confirms the same relations above,f715c2e5,0.16666666666666666
8088,6dcfe6a610d86b,0df72e86,"## Dictionary attack
Legal remark: The dictionary (dictionary.txt) has been extracted from WordNet Project: http://wordnet.princeton.edu",d05c59da,0.16666666666666666
8092,ac04ba639d1c93,f32780f2,"<a id=""id3""></a> <br> 
# **3. Load the Dataset** 

Let's load all necessary datasets",748059d5,0.16666666666666666
8093,fe7360cddc13e5,beed566d,------------------------------------------------------------------------------------------------------------------------,8979e423,0.16666666666666666
8095,b6c0ad74f95b8c,98df9d2e,Finding the means of the columns - used for scaling the data,5de5b241,0.16666666666666666
8100,52cfd66e9ec908,66f36b2c,"Now, let's get a sense of the configuration data. This will include metadata pertaining to the agents, the total time, the frames-per-scene, the scene time and the frame frequency.",c74adcdf,0.16666666666666666
8102,4bada947d597ac,b531420c,Making all entries as -1 for timebeing,eab5094a,0.16666666666666666
8110,ab6da5994949a3,b524ac3e,### Splitting the dataset into the Training set and Test set,fae6b91d,0.16666666666666666
8115,e82462cdc998a7,73bd732d,"## 3. Download data<a class=""anchor"" id=""3""></a>",b39bf244,0.16666666666666666
8122,87e94f864d74be,e7dfd945,> **Heatmap shows the correlation of missingness between every 2 columns. A value near 0 means there is no dependence between the occurrence of missing values of two variables.**,294bfe9f,0.16666666666666666
8132,62487bcd70b199,2f62f2c0,"## Inferences
1. ZIP_Code column is not important for our modelling and can be removed from features
2. Family size is not impacting mortgage
3. Age is not contributing to Personal Loan
4. all others combinations we saw are related and we can use this info for modelling purposes",f6ae50af,0.16666666666666666
8139,842547b2def18c,5f771bbd,"## データ分析に基づく想定

我々は，データ分析が今まで行ってきたことに基づいて，以下の想定にたどり着きます．我々は，適切な行動をとる前に，これらの想定をさらに検証するかもしれません．

**Correlating.**

それぞれの特徴量がSurvivalとどの程度強く相関しているかを知りたいです．この即座にわかる相関関係と，モデル化された相関関係を後半で一致させます．

**Completing.**

1. Ageを補完したい．AgeはSurvivalと間違いなく相関している．
2. Embarkedを補完したい．EmbarkedはSurvivalや他の重要な特徴量と相関しているであろう．

**Correcting.**

1. Ticketは分析から除外される．Ticketは重複率が高く(22 %)，またTicketとSurvivalには相関関係がないかもしれない．
2. Cabinは分析から除外される．Cabinは，train/test両方のデータセットで空値が多く，また多くのnull値を含んでいる．
3. PassengerIdは除外される．PassengerIdはSurvivalに寄与しない．
4. Nameは除外される．Nameは比較的標準的ではない変数で，Survivalに直接寄与しないかも入れない．

**Creating.**

1. Familyという新しい特徴量を作成する．これはParchとSibSpに基づいていて作られ，各サンプルの家族の合計数を表す．
2. engineerという新しい特徴量を作成する．これはTitleを抜粋した特徴量である．
3. Age bandsという新しい特徴量を作成する.これは連続数値変数Ageを順序カテゴリカル変数にする．
4. Fare rangeという新しい特徴量を作成する．(もし分析に役立つならば)

**Classifying.**

先ほど述べられた問題の記述に基づいて，以下を我々の想定に追加する．

1. 女性 (Sex=female) は生存しやすかった．
2. 子供 (Age < ?) は生存しやすかった．
3. 上流階級の乗客 (Pclass=1) は生存しやすかった．",b8efde6d,0.16666666666666666
8140,71b75664517244,ff3878bc,Sunderland is the worst team on premier league since it's finish on last position for 3 season,fc905af5,0.16666666666666666
8141,de577c910a687d,21d66cf8,# Step2: Load Data,c3ebadbc,0.16666666666666666
8146,df2a7968c08ee4,44bb13c1,"### GPU or CPU

The following cell detects if we are able to use GPUs in the current notebook environement. If it is available we will train the model on GPUs, otherwise we will train the Model on CPUs.",a2ba0a72,0.16666666666666666
8150,7c7a7db391c517,214d089e,# 2 Loading Data,f53450dc,0.16666666666666666
8153,2b97b399158701,9f352e2f,## Helper Functions,04bd0060,0.16666666666666666
8162,c9dc8d00773da4,8b4e0ce4,"# Load data

It takes a long time to read because the data size is large.",d9aa2f85,0.16666666666666666
8164,923e97b05be00b,20c30e0e,"We'll be using a subset (5%) of the NIH CXR14 dataset for this project, but this principles here apply to any project. The NIH CXR14 dataset only has a few selected findings within its dataset, but let's pretend that we're creating a dataset from scratch (using Montage, e.g.)

We'll need to see how many examples there are of the finding we typed in. Let's run the cell below to see.",3a4a22dd,0.16666666666666666
8167,c91c137284976f,f7cff4b8,#### 1.2. Visualize data,c6888c0a,0.16666666666666666
8170,726833f92fb87a,18a08cb5,# Data Cleaning and Feature Engineering,7dc5e1b6,0.16778523489932887
8173,3c2033cc99c12c,97c827f3,"##### Outliers detection  
As we can seen from the above box plot, there exits a lot of outliers, which may do harm to the result of the prediction, so I decide to make use of the IQR method to deal with the situation. Since the dataset is highly imbalanced, so i choose to only drop those outliers in the Normal set. 
",dfa22a54,0.1678832116788321
8174,ee23a565163388,ed0c1be7,"In this section, we'll try to understand the factors which cause the heart failure by plotting various graphs. The inferences drawn out of this graphs would highly help us in building the predictive model. We'll try to answer different questions about the causal factors which pop in our mind.",88aacbc4,0.16793893129770993
8175,4ae6a182abac64,ef162dab,"* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).
* only 38% passenger survived 
* 74% female passenger survived, and only 19% male passenger survived.
* About 75% of passengers did not travel with their children or parents.
* Around 30% of the passengers had siblings aboard
",418676c5,0.16806722689075632
8181,169177b6e9edea,d4017c0f,<p>preenche os dados faltantes das idades ,ca42152f,0.16842105263157894
8182,f91f58d488d4af,ea9ef26d,"**Step 1**:

Let's create a tensor containing all of our 3's together. But to create a tensor containing all the images in a directory, we'll use Python list comprehension to create a plain list of the single image tensors.
",5df1bbf3,0.16842105263157894
8187,e67925694c07d3,f13eaa7a,#### **check  train and or test for missing column**,83af4c4a,0.16853932584269662
8188,2ada0305b68956,3aeccc7e,### 26. Palette = 'PRGn_r',133e26f4,0.16857142857142857
8193,75adb7945ef9bd,b8830862,Train and test have the same set of keywords,785c5095,0.16883116883116883
8194,c13f73168789c2,869b0898,"### 1.3 Select a row by it's label<a id='5'></a>
Syntax : `df.loc[row_label]`",16175052,0.16883116883116883
8195,722cd844dfbe8f,9d7963af,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_1_2"">MRI train data</span>

Train data contains one record per patient. For each patient, four sub-files are available *(FLAIR, T1w, T1wCE and T2w)* in which the MRI image sequences are distributed.

![data_structure](http://www.mf-data-science.fr/images/projects/data_structure.jpg)

We are going to take a look at what an MRI image looks like :",0cedb385,0.16883116883116883
8196,241cf32abb22d8,ad0b4c74,"### Target Feature

It is obvious that the target classes are imbalanced. The number of ""pass"" is twice as many as that of ""fail"".

The positive target feature level ""pass"" is encoded as ""1"". ",47157066,0.16883116883116883
8199,4ae464582bac51,cdd9096f,##  Check NA,ca6a52ce,0.16883116883116883
8200,63b44c85e32c1f,0e1ff327,"Indexing was only limited to accessing a single element, Slicing on the other hand is accessing a sequence of data inside the list. In other words ""slicing"" the list.

Slicing is done by defining the index values of the first element and the last element from the parent list that is required in the sliced list. It is written as parentlist[ a : b ] where a,b are the index values from the parent list. If a or b is not defined then the index value is considered to be the first value for a if a is not defined and the last value for b when b is not defined.",fb9b9562,0.16891891891891891
8208,c65a65d4041018,b2b11128,"### New interactive graphs
I have decided that showing only 4 countries may be not interesting enough. So I have added interactive graphs to this notebook - you can choose what you see by yourself!
You can select country for which data will be shown, column for colors and column for x labels. If you change values in widgets, it will take several seconds to update the plot.",824fb229,0.16911764705882354
8209,c115e287523aab,bae59788,# Wandb,feb1288b,0.16923076923076924
8213,d07915a6e6992e,943b7a71,Approximately 62% of Pclass = 1 passenger survived followed by 47% of Pclass2.,2b912140,0.16923076923076924
8215,a8c042af6b7245,c1c23645,"### Metadata

To facilitate the data management, we'll store meta-information about the variables in a DataFrame. This will be helpful when we want to select specific variables for analysis, visuallization, modeling

Concretely we will store:
* role: input, ID, target
* level: nominal, interval, ordinal, binary
* keep: Ture of False
* dtype: int, float, str",2487ac62,0.16923076923076924
8217,a4f8ad33c823c5,f509249f,"Many of the vitals were recorded both during the first 24 hours and in the first hour of hospitalisation. We can observe that there are 32 vital signs with the minimum and maximum readings recording during the first hour(h1_) and first 24 hours(d1_) of the patient's hospitalisation. 

The missing values were then summarised and formatted into a temporary dataframe.",fcd48307,0.16923076923076924
8219,09751c520b0616,0ab3c4fe,"#### (ii) Separate Categorical and Numerical feature<br>
Categorical feature = cat_df (dtype-object)<br>
Numrical feature = num_df(dtype-int,float)",a4d0c7e9,0.16923076923076924
8225,a81661cc35d8d2,589c1f12,"<font size=""3"">Structure of our data</font>",3331f113,0.1694915254237288
8226,1294fb4c86f993,0377a0a2,"

### Data Cleaning  <img src=""Images/clean.png"" align=""right"" width=""300"" height=""300"" />",4471e513,0.1694915254237288
8229,c4bca5d86a38c3,0b4d4595,Mostrando la data,e23d297c,0.1694915254237288
8232,149cb8d3489224,428c0a59,### Missing data,116858e7,0.1694915254237288
8237,30fdc4a6e3c1db,0f2c4eb2,### Plotting Sales Ratio across the 10 stores,6111ddee,0.1695906432748538
8240,f3c8651cb08234,08207a30,As we can see there is **no missing values** in the dataset so we wont be worry about missing values.,37f86e36,0.16981132075471697
8243,614ba9f0c62677,b479c187,"Labels

Each training and test example is assigned to one of the following labels:

0 T-shirt/top
1 Trouser
2 Pullover
3 Dress
4 Coat
5 Sandal
6 Shirt
7 Sneaker
8 Bag",b8551335,0.16981132075471697
8244,0ad8d416b89b78,a2ed3589,"# Continued Exploration & Visulisations

This notebook uses cufflinks which integrates with Plotly, feel free to explore the visualisations with the inbuilt, interactive tools provided by the package.",0b0562f0,0.16981132075471697
8245,510b8303776bb6,325fa9be,## Separating categorical and continuous data fields,18080db8,0.16981132075471697
8248,f015d0147e8fbf,ad783018,###  1. Main Data Table `application_{train|test}.csv`,518954fb,0.16981132075471697
8249,917957c6c4065f,39fc1457,json파일을 확인해보니 29번이 없습니다.,55b8ed68,0.16993464052287582
8257,f6648e47713411,27812afc,"# Một số ảnh ví dụ từ tập dữ liệu
Chúng tôi sẽ kiểm tra kích thước của 300 hình ảnh đầu tiên

Như có thể thấy bên dưới thì tất cả các hình ảnh có kích thước khác nhau.",f4af4d1c,0.1702127659574468
8258,4c47839b067546,96fc5d5a,"### test['parsing_unixtime'] 
Содержит даты в диапазоне от 19/10/20  до 26/10/20. На наш взгляд влияние на цену не имеет. Удалим признак.",1f517b02,0.1702127659574468
8260,957e035ba5b9d5,cffc1638,"Assumging that we have in train, test and validation bad images, we shall remove them.",778ab3d3,0.1702127659574468
8263,b01ee6cb674fa3,3300c3b6,## vizualização,a8ffd35e,0.17028985507246377
8264,0a918602a04693,0ddea050,There are no null values in the dataset.,c1ef0e95,0.17045454545454544
8265,d1ff7e10ee0102,8d3dc275,"*It is military wisdom to choose the terrain where you will fight. As soon as 'SalePrice' walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. It's just an intense research of an individual, if you know what I mean.*

*According to her profile, we have some common friends. Besides Chuck Norris, we both know 'GrLivArea' and 'TotalBsmtSF'. Moreover, we also have common interests such as 'OverallQual' and 'YearBuilt'. This looks promising!*

*To take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests.*",2cc71c3c,0.17045454545454544
8269,0b01138ad120fc,c2509a71,"**Now we will make the Train and the Test Data**
****
**RNN requires sequential Data but also requires a step**",0b4b72e6,0.17073170731707318
8278,74a03887600114,4a6eee34,"So we have 27,278 movies.We don't need genres column so we are dropping that column",c0ffb2f0,0.17073170731707318
8292,72e098fe5b2a04,7fe96562,# Dataset,5399eebd,0.17142857142857143
8294,675b60eaf415a6,19c6cab2,"**images** folder contains 101 folders with 1000 images  each  
Each folder contains images of a specific food class",68c0b725,0.17142857142857143
8295,bbaa07ad21cf4e,fa8a3d37,### Common stopwords in text,3ab6b254,0.17142857142857143
8297,81712ee7510ac5,f2f8ce66,**Strings**,c4685e79,0.17142857142857143
8300,04bac111ffbe9c,024bac79,"##### CORRELATION ANALYSIS
1. Pclass and Fare show high anti-correlation. This is expected. A lower Pclass(1) values actually indicates a higher social/economic strata, like the Royals,the Aristocrats etc. So as Fare increases, numeric value of Pclass drops, and it concentrate more towarda '1', meaning, a higher class person.
2. Pclass and Survived show high anti-correlation. This is expected as first class passengers' safety was given more importance than other classes.
3. Survived and Fare have a faintly moderate correlation. First class passengers' safety was given more importance than other classes, and obviously high class passengers paid more Fare than lower class passengers.
4. Parch and Sibsp have moderate correlation, maybe bacuse they fall under 'family' only.

##### DATA WRANGLING
We see that Fare and Pclass are highly correlated(anti-correlation) to each other than any other features(magnitude>0.50). A low Pclass(means a higher class) will automatically mean a high Fare. We remove Fare.
Pclass can be further processed.",82576b17,0.17142857142857143
8304,80f86fa2d88ff1,f00c8b73,Visualizing the dataset,f5cead1f,0.17142857142857143
8314,2b36742b49c7bc,a5138afc,"## Өгөгдөл боловсруулалт (Preprocess)

Нарийн кодтой дараах [линкээр](https://github.com/bayartsogt-ya/mlub-muis-soril/blob/main/preprocessing.py) танилцана уу!",c8f8a96d,0.17142857142857143
8318,7a058705183598,731411b8,Now check pairwise correlation of columns,b0ead917,0.17142857142857143
8322,55a5e31d03df9f,82b7348c,"## <a name=""dataprep"">Preparing the data</a>

### 1. Standarization

Many machine learning models, including neural networks, prefer the values they work with to be between 0 and 1. 

But why? 
* If your features are in different scales, the ratio the cost function contours in space, i.e. if you have a dataset from houses with one feature as the number of bathrooms and the other as the price of the house. If you 2D plot those variables you will notice that they will be a long shape. 
* It's faster! This is because the gradient descent will need to take more steps to converge to the local minimum.
* Scaling each feature to a similar range to prevent or reduce bias in the network.
* Why not? If your algorithm doesn't get the benefit from scaling it doesn't harm either 😅.

<img src=""https://qph.fs.quoracdn.net/main-qimg-afdfbc63c83e097b3d831777397d905d"" width=""700px"">

[Check this Video with a full explanation](<https://www.youtube.com/watch?v=FDCfw-YqWTE&ab_channel=DeepLearningAI>)

**For our image case is get the pixel values and divide them by 255.**



### 2. Image Resize

Resizing images is a critical preprocessing step in computer vision.

But why?
* Machine learning models train faster on smaller images
* Many deep learning model architectures demand that our pictures be of the same size in all the batches. 

[Short blog here](<https://blog.roboflow.com/you-might-be-resizing-your-images-incorrectly/>)


☝🏽 Fortunately for us most this preprocessing steps are common and Tensorflow already implement them and package them. Let's review them.


## <a name=""dataload"">Loading Data</a>

Currently there are several ways to efficiently load the data let me mention some of them:
* Using [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory)
* Using an [`ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_dataframe) and then a `flow_from_dataframe` or `flow_from_directory`
* Creating a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)
* If you're using `tensorflow_datasets` using the [`tfds.load` ](https://www.tensorflow.org/datasets/api_docs/python/tfds/load)

As you can see there are many functions from Tensorflow that can help us with this task, the selection of each function depends on the data and folder structure that is built around the data. Fortunately, in our case, the CSV contains all the paths from the imagery. 

",06dce00f,0.17142857142857143
8323,20b372b6e4e276,2951baf4,"### Commit 13

* Dropout_new = 0.16
* n_split = 5
* lr = 3e-5

LB = 0.711",ec8b0860,0.17164179104477612
8324,dbd96dd275dc60,32ba6649,"# Make a copy of the original dataframe

We make a copy of the original dataframe so when we manipulate the copy, we've still got our original data.
",1ed493a8,0.1717171717171717
8325,3cc097a5859dc1,95e6bdb2,# **Dropping rows with any empty cell in RainTomorrow**,14380d73,0.171875
8326,ff3a8ce61fab6a,b6b35fb7,"An other way in this way we don't need to close a session because it defined only into [with] block 
",9afe1654,0.171875
8329,0caaec057f7184,0e213a94,"For the maximum sales among each category, cate 40 (Cinema - DVD) has 634K total item sales, which also has the most item variations (5025) in all the categories. The item sales has correlation of 0.82 with the item variations in the category. With higher sales in the category, there are more different kinds of items in it. But there are also some exceptions in it.

There are also some categories that have very high average sales per item. Cate 71 (Gifts - Bags, Albums, Mouse pads) and Cate 79 (Service) have average item sales of 31K and 16K, with only 6 items and 1 item in each category. ",b875533e,0.17204301075268819
8330,979f1e99f1b309,ba3f8fb4,***The Disturbution of damage dealta showing that almost all damage delta were under 1000  and the most of damage delta are 0 which indicate that players had been damaged as the damage enemies***,d1bfebbf,0.1721311475409836
8333,fb9296ecd0cb2a,068e0c98,# Exploring fnc,aa66d98c,0.1724137931034483
8336,cd10f3afd970b3,cc7bab84,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",2db3c8e4,0.1724137931034483
8341,6903d3f38c6a66,1ead9136,![tenor.gif](attachment:tenor.gif),6067ce5e,0.1724137931034483
8342,5ea840754577e3,bc4882c0,### Feature: Sex,9cf9b73f,0.1724137931034483
8344,858da4bb312f67,56a639f6,"## Data Pipeline
Data pipeline is defined in this section. 

Noted that the labels are not used as target value in Cycle GAN. Labels are just used for filtering the inputs.",9cca4391,0.1724137931034483
8345,18a96bb5711ed9,d09dbb82,"# First, let's take a look at the map <br>",e79768db,0.1724137931034483
8346,bb8f5d7807718b,2664837f,"The interactivity cannot be shown in the notebook hence I am enclosing a gif.

![](https://parulpandeycom.files.wordpress.com/2020/08/1_ewseu1uciu4rd9odu4rfjg-2.gif?w=1024)",181ec286,0.1724137931034483
8347,84127ade6fde87,41ced03b,"Let’s start with a character-level example. First, let’s get some text to process. An
amazing resource here is Project Gutenberg (www.gutenberg.org), a volunteer effort
to digitize and archive cultural work and make it available for free in open formats,
including plain text files. If we’re aiming at larger-scale corpora, the Wikipedia corpus
stands out: it’s the complete collection of Wikipedia articles, containing 1.9 billion
words and more than 4.4 million articles. Several other corpora can be found at the
English Corpora website (www.english-corpora.org).",f55d05b6,0.1724137931034483
8350,f3c6048d1058e3,2d6b95d2,#### 4) Mean values on Indirect features,1d9056b0,0.1724137931034483
8352,00d295edcd117e,3b47f8bc,"步骤：<p>
1.使用torchvision加载和归一化CIFAR10训练集和测试集<p>
2.定义一个卷积神经网络<p>
3.定义损失函数<p>
4.在训练集上训练网络<p>
5.在测试集上测试网络<p>",f5810f4b,0.1724137931034483
8360,e3f3f108cd3869,16409aae,filling null values ,2b78de2d,0.1724137931034483
8366,1750367e54f407,b274b0f8,"Initially, I had set the image size to 300x300 in order to fit the original input size to the EfficientnetB3 model. However, as we are randomly cropping these images from the original 600x800-pixel images, it appears that using a dimension of 512x512 pixels leads to better results.",a8e655b2,0.1724137931034483
8367,eb800c50fcfbb2,66943456,# Preparing the data,e7173f4d,0.1724137931034483
8371,a1ba5ffd30dbde,28cb8f65,"- There are 22 float columns, 1 integer and 1 object column
- There are no null values ",48e57546,0.1724137931034483
8372,a4f0a3e1316ff9,470a62bb,"# Convert Date

Turn the date column into a proper date datatype instead of an object",53bf0160,0.1724137931034483
8375,6a80f915608fc2,3df35ff5,"Because there are very few sig_ids with 3-or-more active MoAs,<br>
it's tempting to think about mapping the multi-label problem to a multi-class one,<br>
e.g., using the [Label Powerset in section 4.1.3](https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/)
See also [Ephrem Admasu's response Updated September 9, 2019](https://www.quora.com/What-is-the-best-way-to-convert-multi-label-classification-dataset-into-multi-class-classification-dataset)

What are the unique pairs that appear among the 1538 numMoA==2 sig_ids ?",636938eb,0.17261904761904762
8376,a566b5b7c374e7,00c5f324,### Correlogram of Oura Metrics,b3dc5545,0.17266187050359713
8377,faa8e6c8ab9246,ec4acd25,"There are null values in train dataset and test dataset.
",2bea1419,0.1728395061728395
8379,ba4b3bd184acbb,31af4554,"### Boolean Indexing

With boolean indexing denoted by square brackets, a subset of rows is returned based on a series of True/False values the same length.

This is very useful to extract rows that meet a certain criteria.",0f5de724,0.17293233082706766
8383,44f6a002ecd033,b5e53b00,"For the education column what we see is a little vague. No data description is given of this column. We see that there are more graduates than not, but we are not sure if this means people that graduate high school or university. The assumption would be that a graduate would mean from university, but we cannot be certain.",70bbe106,0.17307692307692307
8384,99bf357eaf61f1,1bc48f7a,#### Now let us find the relationship between these discrete features and Sale Price,9d92fafe,0.17307692307692307
8391,e5dd725b8fa422,14861f9d,![Imgur](https://i.imgur.com/UgzDMlj.gif),14675d8b,0.17333333333333334
8404,73ca9abcc2034e,cd254cad,# Drop useless columns,cec3446c,0.17391304347826086
8408,9d9da6c439b96b,ce3fb97b,## Finding missing value on the table,361cc7d9,0.17391304347826086
8411,77f958b3f41a70,f3d21158,# Title similarity search and Topic Modelling,2ad9bb69,0.17391304347826086
8419,ea4e559a86d613,f109f5cd,**1. GENDER COLUMN**,eff47843,0.17391304347826086
8420,9b5de3823ad5ab,7c47e0b5,"### Getting images' info based on their path

According to the documentation, we can find the experiment, plate and well in the path or in the image's name, so we're going to use this in order to properly label the images.",33e48774,0.17391304347826086
8422,4913b61a68d355,c5946b8a,# Preprocessing,6e269c6a,0.17391304347826086
8423,2409b2d74a9871,ae6d764f,There are some columns that does not affect or contribute in predicting price.,b0a6c313,0.17391304347826086
8426,0e2a23fbe41ca9,726db71c,"Observations:

- Most of the data lies in the years ranging from 2016 to 2018
",64e4762c,0.17391304347826086
8427,17a24d566ffa59,2e75e687,"The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.",89049e56,0.17391304347826086
8428,fe118026267a88,3fc34b14,"## 1.

In the cell below, create a DataFrame `fruits` that looks like this:

![](https://i.imgur.com/Ax3pp2A.png)",612efa48,0.17391304347826086
8431,57bad3860b0fa4,3ed03997,# Exploratory Data Analysis,05138a5e,0.17391304347826086
8433,ed5c03987493eb,64adeaed,Now we implement an autoencoder. It is a model for feature extraction. The latent space contains the compressed representation of the original data.,bea97744,0.17391304347826086
8436,33d736abb432d0,6de50aa9,## 获取待分词文本,d64052a2,0.17391304347826086
8437,2b434130adf886,7e18cdd1,"get images, resize each image, add data augmentation and normalize image tensors",0c4afeca,0.17391304347826086
8439,90ead00a8ee283,7f411e9a,"For the questions that follow, if you use `check_qN` on your answer, and your answer is right, a simple `True` value will be returned.

If you get stuck, you may run the `print(answer_qN())` function to print the answer outright.",612efa48,0.17391304347826086
8440,2ada0305b68956,49e1361a,### 27. Palette = 'Paired',133e26f4,0.1742857142857143
8441,bd380b97b5c894,313a7a06,Most of all the place of occurrence of the formation on the torso,66f2562a,0.1743119266055046
8444,5f32117bcd5255,854ffec6,### SCI,85882abf,0.174496644295302
8445,726833f92fb87a,73846ea4,"First, we create a copy of the original dataframe where we will perform some data cleaning.",7dc5e1b6,0.174496644295302
8446,06ecf7a304c309,8826df1c,"#### 2. Dataset Prepration

데이터셋을 불러오고, 예측값과 타겟을 분리하며, 입력 데이터를 정규화합니다.",714de627,0.1746031746031746
8452,98a6794067932a,7df43bd2,"La cellule de code ci-dessous sert tout d'abord à supprimer les lignes de données comportant des codes postaux NAN. Comme nous n'avions pas accès à l'entreprise étant donné que la base de données ne dévoilait pas le nom de l'entreprise, nous ne pouvions procéder à la correction de ces codes postaux inexistants. En ayant accès à quelqu'un au sein de l'entreprise, il aurait fort probablement été possible de trouver les codes postaux associés à ces numéros de commandes afin de les corriger dans la base de données. Ensuite, la deuxième portion du code sert à transformer le type des données de ""float"" à ""integer"". Finalement, la dernière portion de code sert à valider que le type de données a bien été modifié en ""integer"".",08600fe2,0.17475728155339806
8454,eda49464dd6d1b,dcb4266c,"* Only 12% of 381109 customers purchase the additional vehicle insurance when offered.
* This probably reflects most people's inherent resistance to most advertisements.  But we still have a large enough sample size of 46710 positive responses that it will be significant.  Let's see if this response rate correlates with any other variables.",8421f81f,0.17482517482517482
8457,5ba4207c371899,9a1a89d7,Data transformation - The first transformations that we'll want to do are around the attack field. We'll start by adding a column that encodes 'normal' values as 0 and any other value as 1. We will use this as our classifier for a simple binary model that idenfities any attack.,187b1451,0.175
8458,9a040a4f21091e,00238466,"What sticks out to me is that there is a pretty stark gradation within the examples designated toxic speech. I suppose the third example (""Bye! Don't look, come or think of comming back! Tosser."") fits into toxic speech, but certainly not hate speech, unless I am totally misinformed as to the definition of the word Tosser. Urban Dictionary says its basically on the same level as Wanker, which I am pretty sure is not hate speech. The first example surely qualified as toxic for the profanity used, although other than that it is not nearly as noxious as the remaining examples, which make references to white power, anti-semitism, and rape. So I'm curiouse to see which comments whatever classifiers we build struggle with due to the wide range of text considered toxic speech here.",f591b57d,0.175
8463,5ffe6aa38958a1,9cce7cdb,"Randomly sample some data
",11f5412e,0.175
8464,37b09262279764,97d10e3b,## EDA,37c4c417,0.175
8465,3dd4294f903768,726cfc34,***,0d89d098,0.175
8466,8bb432d338a70b,81936c4f,Analisis líneal discriminante,7aab1dfd,0.175
8467,5626e84c4e6bf8,7ed71425,# Some sample images,e2ecb669,0.175
8468,1011899b959f44,128b87e9,"# Data Attributes
Data attributes are the characteristics of a dataset and we will observe two attributes in this step. 
* .column - Finding the column names listed throughout the dataset
* .shape - With shape we find the size of the dataset (how many rows and columns exist)

**Note:** .shape[0] will only display the output of rows and .shape[1] will only display the column output",0b112382,0.175
8472,62487bcd70b199,7837b198,##  <a id='3.3'>3.3.Performing an Independent t-test for Income and Loan</a>,f6ae50af,0.175
8473,3c2033cc99c12c,09d5d984,![image.png](attachment:image.png),dfa22a54,0.17518248175182483
8474,c80939c7c626cf,eb033337,# This chart confirms Women more likely survived than Men,b9ac31e2,0.17518248175182483
8477,063a35f644e3c5,24784015,"### features with missing values
",1c30fb0a,0.17525773195876287
8478,2a123b4e8f9433,900e34c5,# Check for missing data,0a082218,0.17525773195876287
8483,9d27afa9ca3f96,042e87dc,remove no bouding box images,2d86a18d,0.17543859649122806
8484,c2a9f2fb3e1594,245e08b1,"<a id=""ch5""></a>
## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting
In this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.

1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.
2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model’s accuracy.
3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.
4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.",53411c04,0.17543859649122806
8486,fe7360cddc13e5,bddb8ba0,"<font color='red'> **3-Her müşterinin gözlemlenemeyen bir ""ömrü"" vardır. Herhangi bir transactiondan sonra, müşteri p olasılığı ile pasif hale gelir. Müşterinin pasif hale geldiği bu nokta, bırakma oranıyla üstel dağılımı takip eder.**",8979e423,0.17543859649122806
8487,9e27af2600925c,f49d404e,"For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.

**Exercise:** Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\_px $*$ num\_px $*$ 3, 1).

A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: 
```python
X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
```",9b556435,0.17543859649122806
8488,5ce12be6e7b90e,beecd225,Add:,c0ab62dd,0.17543859649122806
8491,c3498779cda661,026a1092,No hay nulos ni tampoco necesidad de codificar ninguna columna a tipo entero.,0f531b65,0.17543859649122806
8496,62037c5832129c,237e7103,## Combining transformers and estimators in a pipeline,61474350,0.17567567567567569
8498,27d5291d6365ba,0efd6ddf,# Transaction Volume By Gender,96b30229,0.17567567567567569
8499,e4525eb0c96f28,d68bbd5e,"### Year vs Sales

To start we begin with the most obvious place to search for a correlation - Year vs Sales.

For a fluid and continuous correlation, we graphed the progression of sales over time in scatter and violin plots.
We chose to use these plots because they are good at representing a high number of data points.

Included in the scatter plot is a regression line that uses color to indicate the slope - Green line for positive slope, red for negative slope. Line color will be used in this way throughout the tutorial to help readers easily visualize whether a slope is positive or negative.

We also drop Wii Sports and Player Unknown's Battlegrounds because they are outliers in the data visualization.

New Dataframe:
- **year_df** drops outliers from df for display.",2093a1f1,0.17567567567567569
8505,842547b2def18c,4f8185c5,"## Analyze by pivoting features

データに対する観察/想定を確認するために，それぞれの特徴量を互いにピボットすることで，我々は特徴量の相関関係を即座位に分析することができる．
我々はこの段階では，空の値をもたない特徴量に対してのみ以下を実行する．これは，カテゴリカル型 (Sex)， 順序型 (Pclass)，離散型 (SibSp, Parch)の特徴量に対してのみ意味をなす．

- **Pclass** Pclass=1とSurvivedの間に有意な相関関係 (> 0.5) を観測する．よって，Pclassをモデルに使われる特徴量として決定する．(classifying #3)
- **Sex** 問題の定義をする際に，女性(Sex=female)のサンプルは生存率が高い(74 %)ことを観測する．(classifyingの #1)
- **SibSP, Parch** これらの特徴量は，ある特徴量との相関係数が0となる．これらの個々の特徴量から，ひとつの特徴量(もしくはその集合)を求めることがベストだろう．(creating #1).",b8efde6d,0.17647058823529413
8515,d0080e3a39bc5c,9ef3a48b,"* Seeing the slightly skewed distribution of classes in the training set, I decided to first **Balance** the number of examples belonging to each of the classes so that the model is not biased towards predicting any particular class in specific.",2fcde4cf,0.17647058823529413
8522,02b7e38902069e,6b8c3754,"<h1><span class=""label label-default"" style=""background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px"">iNLTK Toolkit for Indic Languages</span></h1><br>

iNLTK (Natural Language Toolkit for Indic Languages)

iNLTK has a dependency on PyTorch 1.3.1, to use iNLTK is necessary to install that below:

https://inltk.readthedocs.io/en/latest/",726a03a0,0.17647058823529413
8525,71b75664517244,b4b1f0c8,"## Most UCL Participant

UCL participant is decided by the top 4 team on every season.",fc905af5,0.17647058823529413
8526,64169805aacf17,8d14e222,# Loading the libraries,1f12ded0,0.17647058823529413
8530,a0a5baa6c7e12a,ab7ea4af,"<img src=""https://raw.githubusercontent.com/gvyshnya/tab-dec-21/main/AutoViz_Plots/Cover_Type/Box_Plots.png"">",551d41de,0.17647058823529413
8537,16ca1123840e9f,5ca3e8f5,#### 90% Space missions are success which is a great number!,e8b8f086,0.17647058823529413
8540,523123dad03177,b4b937d5,# Data visualization,48a5e4e6,0.17647058823529413
8544,c3dfa835621ac4,9b7e9763,"# Implementation

OK, now let's do this.

Import & setup, and create functions for loading and ploting data. This part is mostly from [Simple and Interactive visualization of tasks](https://www.kaggle.com/bsridatta/simple-and-interactive-visualization-of-tasks#Interactive-visualization)",0126bdad,0.17647058823529413
8548,395ed8e0b4fd17,3965e26a,"## Column Description
*   **timestamp**: All timestamps are returned as second Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.
*   **Asset_ID**: The asset ID corresponding to one of the crytocurrencies (e.g. `Asset_ID = 1` for Bitcoin). The mapping from `Asset_ID` to crypto asset is contained in `asset_details.csv`.
*   **Count**: Total number of trades in the time interval (last minute).
*   **Open**:	Opening price of the time interval (in USD).
*   **High**:	Highest price reached during time interval (in USD).
*   **Low**: Lowest price reached during time interval (in USD).
*   **Close**:	Closing price of the time interval (in USD).
*   **Volume**:	The number of cryptoasset units traded during the minute.
*   **VWAP**: The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.
*   **Target**: Residual log-returns for the asset over a 15 minute horizon. 
",7573ea31,0.17647058823529413
8549,dc0b0e1cb46c6f,3f0533f1,"### Columns to drop

As we can see, 'daily_vaccinations_raw' has a cleaned variable 'daily_vaccinations' with less missing values, so we can now drop 'daily_vaccinations_raw'",47b17a7b,0.17647058823529413
8551,4ae6a182abac64,f365a253,* **Checking for the correlation**,418676c5,0.17647058823529413
8554,917957c6c4065f,46f91ad6,![image.png](attachment:image.png),55b8ed68,0.17647058823529413
8556,fa02c409161192,bedf0196,## 1.2 Dealing with the data,e97077f7,0.17647058823529413
8558,156bbcff05dcea,1d5de842,# Checking Null Values in the Data,66ad1fe9,0.17647058823529413
8559,7f74a04ae75792,40ec6ade,"### Check whether the columns' types are accurate? if not handle them
",d01e91da,0.17647058823529413
8562,d07915a6e6992e,26fbc3bf,"**Name**

Not relevant from analysis & modeling perspective. We will drop this feature later after creating a new variable as Title.",2b912140,0.17692307692307693
8566,c9b4e282e4e2c1,8abb501c,"* The number of total injuries seems to decrease when a natural surface is used, although foot injuries increase. But there are not heel injuries when a synthetic surface is used.
*  It seems that there are no heel injuries related to the synthetic surface.
* When a synthetic surface is used, ankle injuries are slightly more common than knee injuries. 
* Toes injuries appear more when a synthetic surface is used.
 ",f44d339f,0.17699115044247787
8571,08f845750d026a,f7048921,The country with the smallest loan amount is Guam with $4300,1c54de30,0.17721518987341772
8573,b61ab8f81dc03d,cf74ae3f,"<a id=""pclass_sex_age""></a>
### Pclass/Sex/Age",64d05394,0.1773049645390071
8574,957e035ba5b9d5,ef7b368d,"count = 0

for path in [train_path, test_path, val_path]:
    for i, cat in enumerate(categories):
        cat_path = os.path.join(path, cat)
        images = [file for file in os.listdir(cat_path)]

        for image in images:
            try:
                img = Image.open(os.path.join(cat_path, image))
            except:
                os.remove(os.path.join(cat_path, image))
                count += 1
                
    print(""Removed {} bad images from {}"".format(str(count), path))",778ab3d3,0.1773049645390071
8576,ad26c020235dfc,43e39969,As we can see there are a lot of features with nan values. So we have to think about handling missing values. For that we recommend this [notebook](https://www.kaggle.com/drcapa/pima-indians-diabetes-eda-handle-missing-values).,bf766e48,0.1774193548387097
8579,57070ad5e0f94f,742ffdb3,"# **Turning ""Sex"" Into Number**",d97edc41,0.1774193548387097
8580,b01ee6cb674fa3,9f56768d,"there are some strange country names, that will be fixed bellow",a8ffd35e,0.17753623188405798
8582,d96e03a9e7c030,a4eb7f61,"## Step 2: Clean, transform, and build features from the NYT / DOE data
Now that we have some more raw data to play with, we need to do some work to make it suitable for modeling, as well as add in the Socrata data referenced above. 

For example, the citywide percentage difference columns are strings like '+20%', '-4%', etc. A model doesn't know what to do with that information, so we need to transform them into a numerical variable. We will also need to fill NAs and cast columns appropriately, among other tasks. 

The controller which handles all of these things is found below. Notice that it is a series of functions that return data frames.",d2b72ced,0.17777777777777778
8583,c73e07ad6d25c5,53e1d2cb,## Age,3ab391fb,0.17777777777777778
8584,d905cde3391d2b,9d2c44fb,"## Mean

**Mean** (usuallly refered to **Arithmetic Mean**, also called **Average**) is calculated as **sum** of all numbers in the dataset and dividing by the **total** number of values

### Arithmetic Mean

$$
\begin{align}
Arithmetic\,mean = {Sum\,of\,all\,numbers \over No.\,of\,values\,in\,the \,set}\,\,\,\,or\, 
\end{align}
$$

$$
\begin{align}
\bar{x} = {\sum_{i=i}^{n} x_{i} \over n}
\end{align}
$$

Arithmetic mean of our data is calculated as,

`mean = (35 + 15 + 97 + 17 + ...) / 315`

Let's do that in code.",067dba39,0.17777777777777778
8585,49ac6594c8f5cf,8d49edd8,**Specialization taken by Science Students**,6f19f28a,0.17777777777777778
8591,5be39e4e35cec7,ce0fe113,"<a id = ""2""></a><br>
# Variable Description
1. PassengerID: uniqe ıd number to each passenger
2. Survived: passanger survive(1) or died(0)
3. Pclass: passanger class
4. Name: name
5. Sex: gender of passanger
6. Age: age of passanger  
7. SibSp: number of siblings/spouses in Titanic
8. Parch: number of parents/children in Titanic
9. Ticket: ticket number
10. Fare: amount of money spent on ticket
11. Cabin: cabin category
12. Embarked: port where passangers embarked (C = Cherbourg,Q = Queenstown, S = Southampton)",14d617c9,0.17777777777777778
8595,e0a041e5e2372f,0a23a320,"* Since I have 1900 training examples, I can replace each missing value with median of the column (to avoid outliers)",7c4357b2,0.17777777777777778
8596,396bc36edb95d3,6c37ceed,### Bivariate Analysis,965e4f8f,0.17777777777777778
8599,3597174a998d4d,95def761,"When assigned_room_type differs from reserved_room_type, the ratio of cancelation is low. Thus, I'll construct a new variable named room_change.",276892ed,0.17777777777777778
8601,d58491f2896fc1,83c0fa47,<h2>Genetik Algoritma Kullanım Alanları</h2>,514bfdff,0.17777777777777778
8602,9169c4e9c33c90,14307573,Genre distribution for Top 50,725bf880,0.17796610169491525
8603,1294fb4c86f993,1a6a5a1b,#### A. guns.csv,4471e513,0.17796610169491525
8604,1eb62c5782f2d7,092e81a0,## 2. Area di sebelah kanan point z-score,bb69f147,0.1780821917808219
8606,738bfced935b69,de42daad,## Exploratory Data Analysis,2d3c592d,0.1780821917808219
8607,91473a39b85068,b1282051,Since the number of records in the data is very large(6034195) so let's consider a small subset of data for faster computing.,6e3d91c2,0.1780821917808219
8613,e78e7edae89049,aeec7288,### Seasonal decompose,9cef1d94,0.17857142857142858
8617,3cd78d8d6d56e4,c5221fb1,Comparing the equally splitted train- and val-sets based on the given label y.,9f632e94,0.17857142857142858
8618,8dd655515e7d18,8c143e2c,Converting the Word Count column to data type = int and round it up,895f41cf,0.17857142857142858
8626,1c381451c17150,db8f64d5,"Now that we have our scripts, let's save it and move on to real work.",e79b530f,0.17857142857142858
8632,98fd05fcc5c3e3,f43563c1,## Data Cleaning,55fe7ece,0.17857142857142858
8643,e19e307b3fd188,1cdb9112,"As previously stated, there are several outliers after **9.500,00**.",2173955b,0.17886178861788618
8644,169177b6e9edea,87f09712,<h3><b>Age,ca42152f,0.17894736842105263
8646,840534f2908a9c,9b38e2c8,"*In our training data, there are some wrong values (for example the min of fare_amount is negative & the max value of passenger count is 208). We have to delete this values base on the value boundary in the test data.*",8081c3cc,0.17894736842105263
8647,20b372b6e4e276,bcc0a19c,"### Commit 14

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5

**LB = 0.715 (the best)**",ec8b0860,0.1791044776119403
8651,21413205980558,6d2cdd31,# 2.2  Data cleaning and filtering(数据清洗及过滤),84197de0,0.1791044776119403
8652,e58e68e4eeefe5,3b9880e0,# Feature Selection,a87662ce,0.1791044776119403
8654,43e60eb1362f5c,a8cd554e,"We can see that 96% of the values in Cancellation reason column are null for which it is of less use while predicting Delays. Some other columns include 78.2% in Air System Delay, Security Delay, Airline Delay, Weather Delay etc. So I am going to create two Dataset which is having no null values one is by removing all the null values irrespective of different types of Delays and other I am going to take the data set with respect to different types of delays. The first Dataset is named as Flights and the other one is named as Flight_Delays.",87934234,0.1792452830188679
8661,34fff8ce731b03,b960c7c1,"O esquema é apresentado na linha abaixo, para que possamos visualizar o modelo de dados que iremos trabalhar.",6f9e5b2e,0.1794871794871795
8664,9ce192da3885a1,f699a0a7,### Denoise images,aa7e89cf,0.1794871794871795
8665,9b42412e75d640,2e3333a1,Now we have balanced datset and we can proceed with data cleaning.,b616570a,0.1794871794871795
8673,9eed0fae1c7958,0b14e1b2,# image size and batch size,3fb1438e,0.1794871794871795
8675,312135b445bd23,38b17a08,Putting it all together:,8ced381f,0.1797752808988764
8678,04ff2af52f147b,20cff8df,"Seeing this trend, we can now confidently fill our null values using our proposed method.  We ensure that the calculation of parameters is done exclusively using the training set to avoid leakage.",d5f37be9,0.1797752808988764
8682,2ada0305b68956,497c5393,### 28. Palette = 'Paired_r',133e26f4,0.18
8685,7dd46c750653eb,66060cad,"**Inference**

* The month of July has the Most Number of Births

* The month of January has the Most Number of Deaths
",c2644713,0.18
8689,4cd25e50c7e007,40c96c60,# Data Preparation & Visualising the Data,ceb0c525,0.18
8692,0858e1bb3cbaca,a86184c3,"To quickly validate the comprehensiveness of this dataset, we can use

**.shape**

to know how many rows (number of samples) and columns (categories of information) are included.",78548374,0.18032786885245902
8698,69ac33d79f5130,072af2ab,#### Details about missing percentage.,9d760d2a,0.18055555555555555
8701,593d1d3d1df05a,98e75024,# Data Preparation,bc682ffe,0.18055555555555555
8702,fdc3afd309b850,021357cb,"The two next functions will do the following steps. 
 * The first one will send pieces of the address columns to the Busca CEP API. It will return a dataframe with a few addresses and neighborhoods that correspond with the parts sended. 
 * The second function will confirm if the parts sent are in the dataframe returned.
 
 After the second function, confirm the data returned and send back to the first function. The first function will set the new address and the neighborhood values.

",966bde38,0.18055555555555555
8704,c01049afb6d307,ac4b1660,# Variable Describtion,d37d3b5d,0.18055555555555555
8708,4c47839b067546,e2d03cf0,"### sell_id и price 
Добавим со значением 0 соответственно в train  и test ",1f517b02,0.18085106382978725
8712,7454fdc444df16,678e4ed7,"It seems that we have reached the files containing the pictures of each patient, 
the structure seems to be as follows:

Patient_id/xcoordinate_of_patch/ycoordinate_of_patch/class_of_cancer",a7818ef5,0.18095238095238095
8716,ce9ed5e2d601d7,056fe236,"# Undersampling
For experiment measurements",f58a2f43,0.18110236220472442
8719,7e89d387feb9f5,a8593ed4,### Добавленный числовой признак № 2. Относительный ранг,989e3a1b,0.18115942028985507
8724,30fdc4a6e3c1db,9dd8a88b,We have the store CA_3 which has the highest sales ratio ~17% which has almost double the sales of any other store. While CA_4 has the lowest sales ratio ~6.2%,6111ddee,0.18128654970760233
8725,9ceb7278784462,2d10b553,"## Conclusion Rating

* max rating =5 
* mean rating = 4.07 
* min rating = 3 
* Top 10 Books  (Reviews>250)
* We have inconsistent data",3768a567,0.1814516129032258
8732,25c2f1ef13b402,a54f9206,"**Some preprocessing and selecting of data to feed to the collaborative filetering- Using the last 50 questions of every user, hoping to catch their current understanding and level.**",b028c35a,0.18181818181818182
8745,0475899eec1ffe,fe757e38,"We then check our data set (record, info, column based)
",d825dc37,0.18181818181818182
8747,90964081c7faab,d66e0b48,"## 3) Data Cleaning and Feature Engineering
The data is only appears clean, but after examining the categorical variables I found that many of them had unknown values. It's hard to say what to do since many of the variables didn't have a clear value to replace the unknown value to. For now, I will examine the variables as they are while changing the target variable into a quantitative one. ",b423b0c3,0.18181818181818182
8755,132fa9714f2046,14371b57,"## Exercise 1

** Follow along with these steps: **
* ** Create a figure object called fig using plt.figure() **
* ** Use add_axes to add an axis to the figure canvas at [0,0,1,1]. Call this new axis ax. **
* ** Plot (x,y) on that axes and set the labels and titles to match the plot below:**",3bb1775f,0.18181818181818182
8756,8578b9a8d00730,082ae950,## ***Data Collection***,7648721b,0.18181818181818182
8757,f0fab078f8533b,37db0cc0,"## a. Creating a new column 'year_added'

1. Extracted from 'date_added'
2. Contaings year in which the show/movie was added 
3. Row where the data was missing was substituted from the release year
4. Converting 'date_added' to datetime and removing rows withi have only year information (keeping month day and year information)",bdb5ea32,0.18181818181818182
8758,2f47abddfd1928,a1b5f525,"### 2.3. Fare

As in this numerical continuous feature we have just one missing value, we can fill it with the meadian value.

To be a bit more accurate we can aggregate the values by Pclass as it is of key importance for ticket cost.",ae33cc0b,0.18181818181818182
8759,be2f4d8a6b73ca,e17ebb3c,"<div style=""color:white;
           display:fill;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana;
           letter-spacing:0.5px"">

<p style=""padding: 25px; color:white; text-align:center""><b>🔍   Exploratory Data Analysis</b></p>
</div>",5d8ce40a,0.18181818181818182
8762,d1ff7e10ee0102,9c6e413b,### Relationship with numerical variables,2cc71c3c,0.18181818181818182
8768,69e2428808c415,330f6e06, # Exploratory Data Analysis,ae798c16,0.18181818181818182
8772,450fda47b03baa,aac2e28b,Veri çerçevesinin kaç öznitelik ve kaç gözlemden oluştuğunu görüntüleyelim.,62c04adb,0.18181818181818182
8776,efd44ce2c08541,e64e9fb3,# Similarity: Text TFIDF,ebc2d00c,0.18181818181818182
8779,eda49464dd6d1b,fb8c8f71,"## Age Distribution of Customers
* There is a clear difference in response by age.  Positive respondants tend to be 30 - 63 years old.  The youngest customers have a very low response rate.",8421f81f,0.18181818181818182
8781,adf419444a59df,a7c1957b,"This picture shows how the model is doing!
And we will discuss how to remove redundant boxes later.(SPOILER: non-max supression)

![Screenshot 2021-08-22 214753.png](attachment:4bee5791-7c58-4025-a4c0-3286237c2bc7.png)
",3a275e7f,0.18181818181818182
8794,bb3d1b4b9f1248,0f03b8ac,"Here is the data from NIAA
https://pubs.niaaa.nih.gov/publications/surveillance-covid-19/COVSALES.htm#fig1

Here are the columns description

Year		4-digit calendar year										
Month		2-digit month										
FIPS		Geographic ID code (FIPS code, see specification below)										
Beverage		Type of beverage: 1=spirits, 2=wine, 3=beer										
Gallons		Gallons of beverage										
Ethanol		Gallons of ethanol (pure alcohol)										
Population		Population age 14 and older										
PerCapita		Gallons of ethanol per capita age 14 and older in 2020										
PerCapita3yr		Three-year average gallons of ethanol per capita age 14 and older, 2017–2019										
PctChange		Percentage change in gallons of ethanol per capita age 14 and older from 2017–2019 (3-year average) to 2020										
",bf7de324,0.18181818181818182
8797,0cb9adc158b705,8162eb33,"### Data

Enough prep-work! Lets read our data . . .",3abf056e,0.18181818181818182
8799,37360278c19104,f8c158d6,"## Random sample

For now let's take a random sample of the MIT train dataset.",21473a41,0.18181818181818182
8805,da199f8fb59439,64cedc8c,*checking for null values*,baaa665d,0.18181818181818182
8810,3c2033cc99c12c,232fc424,"According to the requirement,we should remove samples that are smaller than Q1 - 2.5 * IQR or larger than Q3 + 2.5 * IQR.",dfa22a54,0.18248175182481752
8820,9c26c5dcd46a25,cd5560e8,"#### <font color=""#114b98"" id=""section_1_3"">1.3. Répartition des Nutriscores et ANOVA</font>

Nous avons tenté de calculer simplement les nutriscores et nutrigrades dans le Notebook *PSanté_01_nettoyage*. Cependant, les erreurs constatées étant supérieur à 50%, nous n'avons pas imputé les valeurs manquantes à cette étape. Regardons à présent la répartition des nutriscores déjà complétés dans le dataset initial :",1bbbb677,0.18292682926829268
8821,917957c6c4065f,960d01c1,"그래서 category_id가 29번인 영상을 직접 찾아 확인해보니 카테고리가 ""비영리/사회운동""이네요.  
category의 결측치를 ""Nonprofits & Activism"" 으로 채워줍니다.  ",55b8ed68,0.1830065359477124
8822,631cd434fc3aa2,a9226ff1,We can see that there are two point with (very) large value of _GrLivArea_ and with (very) low price. These are outliers and we can safely remove them.,2b74febb,0.18309859154929578
8826,9bcfa825c8b2e6,b389e59f,EKSİK DEĞERLERİN DOLDURULMASI,220f36e4,0.18309859154929578
8827,3d77c1560bd16e,6ed3c74a,"<a id='1'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Summary</div>
",87c141ca,0.18309859154929578
8834,712198370d5521,b5b7cf51,"**From the above output, we can conclude and note that:**

* There are missing values in income
* Dt_Customer that indicates the date a customer joined the database is not parsed as DateTime
* There are some categorical features in our data frame; as there are some features in dtype: object). So we will need to encode them into numeric forms later. 

First of all, for the missing values, I am simply going to drop the rows that have missing income values. ",5882e04c,0.18333333333333332
8835,396bc36edb95d3,2294c1f7,#### Plotting numerical variables w.r.t Claimed status,965e4f8f,0.18333333333333332
8838,63d0d9b9a8c7d2,627fe763,"Droppin only two rows from the dataset is not a big deal.

***So Dropping the rows containing the Null values***",e32e5933,0.18333333333333332
8840,b547f0f38f7744,9f24b81c,The number of unique `image_id` in train data:,b6ba66b3,0.18333333333333332
8843,bd380b97b5c894,eee07f3f,## diagnosis,66f2562a,0.1834862385321101
8846,f35bf4df70d310,21e54195,### Reshaping the dataset,10bb859a,0.1836734693877551
8847,ffd1df95ca5289,79b89551,we can say that tertiary has the highest median compare to others,db00c338,0.1836734693877551
8849,12f4d16fc21645,05f35476,<h1 style='color:blue'>List of various crops</h1>,c7752038,0.1836734693877551
8854,c65a65d4041018,37f4b6ea,"Well, there are much more males then other genders. This could be due to bias or due to higher interest in this sphere. I won't say whether there is a discrimination or barriers - this isn't the place for this. Let's have a look at other things:
* In general there are a lot of students or young professionals. I suppose that a lot of young people try to take part in competitions to get experience or medals/prizes, which should boost their career;
* It is worth noticing that India has a different trend - while in Russia, USA and other countries kagglers are 25-29 years old, in India most of responders are 18-21. I wonder what is the reason...
* Also it is interesting that the ratio of women to other genders is higher in USA than in other countries. Good news!",824fb229,0.18382352941176472
8862,c950cff74e51ac,9acb93bd,Handling Missing Value,d59bf323,0.18421052631578946
8863,d369f200a84c2a,50c65d9a,"![image.png](attachment:8776406b-4d45-47a2-9d4f-2c478f4df05c.png)
image from the [original paper](https://arxiv.org/pdf/1710.09829.pdf) (Hinton et. al., 2017)",8fef4d48,0.18421052631578946
8866,d93a87fdbdb3d2,b72baea2,#### Split data and label,30d079c3,0.18421052631578946
8870,f35ee6e9fab592,470f29f2,"The following 3 code cells achieve the purpose of highlighting the most popular ```platforms``` (here, popular refers to the console on which most reviewed games are being sold)",b15f7073,0.18421052631578946
8872,52ee792e228d54,322669d3,### Let us visualize the distribution of each of the categorical features,5096094e,0.18421052631578946
8875,957e035ba5b9d5,ec514d5c,# Read in Data,778ab3d3,0.18439716312056736
8882,be9597c72542a2,645ce8e8,"**How many inverters are there for each plant?
**",6f29c6d8,0.18461538461538463
8885,03048e86a6d806,3fe552b5,"Based in this dataset, most of the data fellows are millenials. For nearly all data-related job titles, you will find these folks are most frequently 22-29 years old, but for Product Manager and Research Scientist positions, they are mostly 25-34 years old.",1285c231,0.18461538461538463
8886,d07915a6e6992e,8fd75098,"**Sex**

Based on analysis below, female had better chances of survival. 

![](https://www.ajc.com/rf/image_large/Pub/p9/AJC/2018/07/12/Images/newsEngin.22048809_071418-titanic_Titanic-Image-7--2-.jpg)",2b912140,0.18461538461538463
8888,09751c520b0616,444d0cdb,#### (iii) Dealing with categorical feature,a4d0c7e9,0.18461538461538463
8890,c115e287523aab,6edc446b,"<img src=""https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67"" width=""400"" alt=""Weights & Biases"" />

<span style=""color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;""> Weights & Biases (W&B) is MLOps platform for tracking our experiemnts. We can use it to Build better models faster with experiment tracking, dataset versioning, and model management</span>

<span style=""color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;"">Some of the cool features of **W&B**:</span>

* <span style=""color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;"">Track, compare, and visualize ML experiments<br></span>
* <span style=""color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;"">Get live metrics, terminal logs, and system stats streamed to the centralized dashboard.<br></span>
* <span style=""color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;"">Explain how your model works, show graphs of how model versions improved, discuss bugs, and demonstrate progress towards milestones.<br></span>",feb1288b,0.18461538461538463
8894,b01ee6cb674fa3,23183dc8,"Doing a search for the rocket name, it showed a wiki page as a result and it contained:
'SPARK, or Spaceborne Payload Assist Rocket - Kauai, also known as Super Strypi,is an American expendable launch system developed by the University of Hawaii, Sandia and Aerojet Rocketdyne. Designed to place miniaturized satellites into low Earth and sun-synchronous orbits'

It is an american rocket that was launched at the Pacific Missile Range Faciity for small satelite launch",a8ffd35e,0.18478260869565216
8896,c4386b8a01d66e,c104c1a3,Uniform distribution with random initialization,dc732bf5,0.18487394957983194
8901,d128317750d689,216f5706,I've created the alphabet dictionary just so I can check the output more easily. That way we can see the actual letter instead of the numeric label representation. ,d87f7428,0.18518518518518517
8906,b9328fe3b0cefc,66cd2b9f,### Load section 1 data and merge(加载数据并merge到一个DataFrame),3a35eb23,0.18518518518518517
8908,db5a369894fef6,bafbc12c,"### Plot nature of COVID cases for different countries
- Similar to the code for the worldwide count but you don't `sum` up values
- Some countries are recorded by state, region or province. Thus there are multiple values recorded in some countries.
- Solution: aggregate the cases of confirmed, recovered, deaths and active cases over the different regions of the country. ",065aaf61,0.18518518518518517
8913,9ad9a97e628bfa,19c4c57c,"**Ticket**
티켓의 앞부분에 코드명으로 표시된 것과 티켓 번호를 분리해 살펴보겠다.",0a7e1136,0.18518518518518517
8922,1667a100fc8b42,0aef2e94,"## Data Load ##

I mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable ""labels"". Finally, SalesPrice is remove from data.",6c8cd6b6,0.18518518518518517
8923,ddcdecdd6a3b6d,c3b47901,### 载入数据集,90831448,0.18518518518518517
8925,2b39f4ff896f97,f0ca7a30,Fetch images from directory,3ddfe182,0.18518518518518517
8929,ab6da5994949a3,9666ff5f,### Feature Scaling,fae6b91d,0.18518518518518517
8934,f4b9042e693b6c,61d32625,Here are all of our imports!,676cacc9,0.18518518518518517
8935,233cb23d9e01b9,7a880b1d,"the ethnicity variable is unbalanced with an over-representation of category 1. It will be interesting to see the capacity of our model to correctly predict under-represented categories via a confusion matrix. On the other hand, the genders are well balanced within each ethnic group.",ffa56c19,0.18518518518518517
8939,6d29650083cbde,2febff4e,"As you can see here, the dataset is quite rich. I now keep only the strikers who have played more than 1500 minutes over the season and for whom I can match performance with the following season's rating (i.e. where Ratingf1 exists). 
Let's look at the distribution of Ratingf1:",e65fd993,0.18518518518518517
8940,7baeb0ffc6659e,20f63166,**Survivial rate based on gender**,8cbebba9,0.18518518518518517
8941,c6f8ff61a5fa87,1af59ea4,"## <span style=""color:blue;""><strong>2.Simple Exploration</strong></span>",3eea586b,0.18518518518518517
8943,9ceb7278784462,7db724cd,"## <a id='8'> 5.Type Of The Book </a>
",3768a567,0.18548387096774194
8949,fe6750354fb64f,b38b5864,## Bar Graph of Deaths Cases,271741f0,0.18571428571428572
8950,2730840089c8eb,bf0022b5,"In addition, Python's triple quote syntax for strings lets us include newlines literally (i.e. by just hitting 'Enter' on our keyboard, rather than using the special '\n' sequence). We've already seen this in the docstrings we use to document our functions, but we can use them anywhere we want to define a string.",34d27dac,0.18571428571428572
8951,2ada0305b68956,c0977133,### 29. Palette = 'Pastel1',133e26f4,0.18571428571428572
8954,38b79494ac749e,376e1665,### Split,39162a40,0.18571428571428572
8955,1823d096209b96,b5efb843,## Performe Exploratory Data Analysis and Visulation ,cb2a79e0,0.18571428571428572
8959,c9b4e282e4e2c1,b2be0f34,"C-Which kind of injury makes the player miss more days?
We are going to check the DM_M42 attribute, a one-hot encoding indicating 42 or more days missed due to the injury.",f44d339f,0.18584070796460178
8962,d96642860ab3dd,c1b9e848,### 1.5 Cabin Feature,98419d48,0.18604651162790697
8963,72d393488311b6,024aa230,# visualization,80663df0,0.18604651162790697
8964,8539260444e6b5,acd5480d,# Having a look at 1st ten reviews in the data,0369463f,0.18604651162790697
8966,c09fac3c943d51,d4fae770,Key problem:,678d076d,0.18604651162790697
8968,1660daf8867980,0a654c2b,"### The agent
- The agent is a chess Piece (king, queen, rook, knight or bishop)
- The agent has a behavior policy determining what the agent does in what state",42d7cffc,0.18604651162790697
8970,22bd95f4807a23,7346a55b,* From the above analysis we see that there are 23486 rows and 11 columns in the available dataset. The average age of the consumer is 43 years of age.,c05d356f,0.18604651162790697
8972,52cfd66e9ec908,38683c93,"Now, however it's time for us to look at the scenes and analyze them in depth. Theoretically, we could create a nifty little data-loader to do some heavy lifting for us.",c74adcdf,0.18627450980392157
8977,ed8009f482b380,ee4a37d7,## Some EDA,e99941fa,0.1864406779661017
8979,dac3c8204a2d1b,d9d61e53,"In Amazon Book Selling  data set, there are 7 columns and 550 rows. Also Null values are not present.",b0d2d0dc,0.1864406779661017
8985,a44368590e878a,d93b4af8,### Sex,77743ba8,0.1864406779661017
8991,20b372b6e4e276,36db1609,"### Commit 15

* Dropout_new = 0.16
* n_split = 5
* lr = 3e-5
* SEED = 777

LB = 0.710",ec8b0860,0.1865671641791045
8995,d6ddbe57f59cf7,b77451b0,###### Black lines are error bar,504a3cda,0.18666666666666668
9003,e19e307b3fd188,5d036bc8,### City,2173955b,0.18699186991869918
9004,a566b5b7c374e7,35109eda,"#### Initial Impressions:
- It appears that Oura ""sleep times"" are closely correlated with ""sleep scores."" For example, in the first month, REM Sleep Time is perfectly correlated with REM Sleep Score. This is true for Total Sleep Time and Total Sleep Score as well. Deep Sleep Time, however, does not perfectly correlate with Deep Sleep Score. I believe this is because Oura has some predetermined amount of time for each stage that results in a ""perfect score"" of 100, and I have never exceeded that time for REM Sleep or Total Sleep in this first month, but I have exceeded it for Deep Sleep. I will examine the top scores for each of REM Sleep, Total Sleep, and Deep Sleep to confirm.",b3dc5545,0.18705035971223022
9006,5ce12be6e7b90e,22284451,Substract:,c0ab62dd,0.1871345029239766
9008,2a724fb7835cdc,4a969d69,There are multiple sentences with more thatn one unique sentiment,c38ac61d,0.1875
9009,254cccd5145725,44c06cee,This gives us the no of rows and columns as well as the data types present.,a49b4037,0.1875
9010,7a75cba9317186,8ce3d2f8,"> To fix the rounding error, I came up with the solution as follows: ",3d7e3235,0.1875
9012,3dd4294f903768,b43a09c0,Now we will make some visualizations to get a better understanding of our data.,0d89d098,0.1875
9013,49f2274c1dd516,46d17071,"### Common Columns
These could be used for merging dataframes. Columns are also called features. These can be used to build models or describe the data. This secton epxlores the columns that exist and how they relate to other dataframes. It also does some examination of how the categorical data maybe turned into numerical features.",06b0ffee,0.1875
9017,ff029d7b52ae1d,39f455e9,"There are 3 csv files in the current version of the dataset:
",c987f868,0.1875
9020,c85c94076e9c3a,6a5961bb,## Data Cleaning ,3ea0c443,0.1875
9024,51a46d0a7597f5,864f769d,**Data Cleaning and Prep**,e9e25b17,0.1875
9033,436ceac778d184,4f83c7ee,"# **KERAS BASED MODEL GENERATION**

<b>Notebook Aim & Modifications</b>

- The aim of this notebook is to slightly expand on the already very useful notebook posted by the host; [notebook](https://www.kaggle.com/stefankahl/birdclef2021-model-training).
- That notebook contains a <b>single layer model approach</b>, which as it turns out cannot be used with more <b>sophisticated pretrained models</b>, I regrouped a few things and overall there doesn't seem to be a big difference between the two approaches when it comes to training.

<b>Dataloaders & Augmentation</b>

- <b>Dataloaders</b> are used here (as opposed to the additional step of reloading the into a numpy array) in order for one to use <b>image augmentations</b>, which help improve the model during training. The winning entry of the previous competition hosted by the same lab used <b>noise</b>, as an example.
- An example notebook which shows the <b>benefit of image augmentation</b> can be seen here; [Hummingbird Classification with CNN](https://www.kaggle.com/shtrausslearning/hummingbird-classification-with-cnn). If you are interested in <b>birds & their classification</b>, which I assumed a lot of you are, consider taking out the [Hummingbird Dataset](https://www.kaggle.com/akimball002/hummingbirds-at-my-feeders) dataset for a spin by [Amanda K Kimball](https://www.kaggle.com/akimball002/cnn-hummingbird-speciesgender-image-classification) and liking her work. Accurate bird classification most definitely requires the addition of video for accurate classification and not just sound, which is why I brought the above example up.

<b>By Not Means Complete</b>

- The notebook, like the one posted by the host, is by no means complete, <b>subsets are created via parameter selection</b> (rating filter,recording number per specie,general limiter)
- They barely are even able to correctly select the correct species present in the <b>training soundscapes</b> to begin with (as you will see in the soundscape), not even having done any training, 
- It's likey this is a critical step, not just this competition, but for bird classification in general. Some ideas have already been put forward in this notebook, [At the right place in the right time?](https://www.kaggle.com/aramacus/at-the-right-place-in-the-right-time)

# <sub>1.</sub> <span style='color:#F7765E'><sub>SUBSET GENERATION</sub></span>
- As per host's notebook, a simple subset selection of potetial birds that will be present in the <b>soundscape</b> are chosen;
    - <b>rating limitation</b>; only high quality recordings (as per Xeno Laws) are used.
    - <b>recording per specie limitation</b>; recordings with appropriate ammount of recordings in the dataset.
    - <b>overall limiter of rows</b>; general final limiter.
    
    
- Having limited the dataset, <b>spectograms</b> are generated, these arrays are exported via images and reinported via dataloader during training.
- <b>subset</b> is used to define which folders dataloader is the <b>training data</b> & the <b>validation data</b> using the <b>flow_from_directory</b> function/method.
- One slight concern I have with this approach of splitting the general non (train/vald) sorted folder data via <b>identical seed</b> & <b>validation_split</b> specification in the <b>ImageDataGenerator</b> input is the potenial occurence of image leakage.",db256ccd,0.1875
9034,a3ae04b78e45b5,f7cb67f3,**MOST COMMON 15 NAMES OR SURNAMES OF KILLED PEOPLE**,4195da8b,0.1875
9036,6f05f4ea9addbf,485a60be,"# Missing values
In this section we will handle all the missing data in the dataset",dfb04c84,0.1875
9040,6a49325ea305e2,9a33725a,# Load Data,119fdcf9,0.1875
9041,ffc9490c4f6c38,d6f3381e,"plot code from [this kernel](https://www.kaggle.com/donariumdebbie/explore-funny-column-names#Target-distribution-of-group-of-column-names)

That kernel revealed <b>wheezy-copper-turtle-magic</b> columns shows different pattern.

In this cell, restrict only <b>wheezy-copper-turtle-magic</b>==0. Fix the axis and show histgram.
",ae7bbbb3,0.1875
9043,5e02999ca74e7e,75eff972,"# **Data Preprocessing**

We must check the data every time we want to make a model, because this is the important thing, if you suddenly meet a bad dataset, wether you want it or not, you must clean the dataset.",b69da28e,0.1875
9044,9d561aa4a298f3,bf343b58,## Glimpse the data,f56bdd1c,0.1875
9052,c5fef7cc592736,b959d959,The first step will be to convert the DataFrames to numpy arrays. For me its easier to work with them from the beginning,d21dc2c1,0.1875
9059,3b5903412fe741,7e26a75c,"These are the two ways of selecting a specific columnar `Series` out of a `pandas` `DataFrame`. Neither of them is more or less syntactically valid than the other, but the indexing operator `[]` does have the advantage that it can handle column names with reserved characters in them (e.g. if we had a `country providence` column, `reviews.country providence` wouldn't work).

Doesn't a `pandas` `Series` look kind of like a fancy `dict`? It pretty much is, so it's no surprise that, to drill down to a single specific value, we need only use the indexing operator `[]` once more:",ad231969,0.1875
9060,bd0e173abb7b52,6e782a75,**1. How many men and women (*sex* feature) are represented in this dataset?** ,9bce3b0d,0.1875
9064,64a336ac34d95c,6cda5acd,"## Churn vs Non-churn plot
",be73a990,0.1875
9067,ba4b3bd184acbb,341ff981,"We can see that 23,998 of the reviews were marked as positive.

This can also be done with only one line and no extra variable",0f5de724,0.18796992481203006
9072,869a39a3d4dea2,40dae236,"## Manipulating the image pixels <a id=""manipulate_image""></a>",9020daf8,0.18823529411764706
9078,548f961125248d,b4c4dbd7,* The Training and Test data have different subsets of categorical variables.,d8c5e8b8,0.18840579710144928
9080,8336d84cf3ff6b,c994e346,Wow!!! Highly baised set ,b96b58a0,0.18840579710144928
9084,a1a31459abf078,75c7b001,"Some observations from having a peek at the data above - 
* timestamp values are represented in millisecond units and start at zero for every user
* As expected, prior_question_had_explanation and prior_question_elapsed time values are zero for first question answered by a user
* **content_id is the foreign key to be used to join train datasets with questions and lectures datasets**. Content id can represent a lecture or question
* Lectures can be used for concepts or solving questions. Every lecture has a tag associated to it. 
* Single question can have multiple tags associated to it. Tags will help us understand topic associated with a lecture. However we don't have a mapping file to give us tag_id to tag_name mapping

Also taking a look at the number of records in each of the datasets here - 
* **The train dataset has more than 100 million records**, we've only imported around 20 million here
* **There are about 418 different kinds of lectures**
* **There are total of 13523 questions in our data**

Size of the train data is much higher than our memory capacity in Kaggle notebook, therefore you will see me walking a tightrope with managing RAM resources here and frequently deleting datasets after their usage to free up memory. ",66fc0f54,0.18840579710144928
9085,598b6228760590,cf2bb0d6,"- Fare
- Passenger fare
- The fare is mainly concentrated in 0-100. It can be seen that the fare has a rightward deviation and there are outliers.",be30ab66,0.18840579710144928
9087,979f1e99f1b309,a590d4b2,***THE plot showing that most number of knocks are 0 and a few number above 5***,d1bfebbf,0.1885245901639344
9092,510b8303776bb6,97d4b409,"List_cate: It is the list of all the categorical data fields in the dataset.

List_cont: It is the list of all the continuous data fields in the dataset.(except SalePrice)",18080db8,0.18867924528301888
9094,a070fd03ae8ed2,d2f564c1,"## 3.2 Импорт предобученных моделей бустинга (LightGBM и CatBoost) 
(препроцессинг проводился в [втором (LightGBM)](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-2-ml-sokolov) и [третьем (CatBoost)](https://www.kaggle.com/sokolovaleks/sf-dst-10-diplom-3-ml-sokolov) кернелах)",c0ec4138,0.18867924528301888
9095,0ad8d416b89b78,b1c2f405,"# Age:
The distribution of incomes between ages shows a clear positive skew with regard to age, and an increase in the proportion of individuals earning over $50k per year between the ages of 30 and 55. This conclusion is not unsurprising as it follows the expected pattern of adult career progression with younger individuals likely to either studying or starting their careers. Those above 55 are likely to be starting to progress towards retirement, which is associated with lower expected levels of income due to reduced working hours.",0b0562f0,0.18867924528301888
9099,f3c8651cb08234,fcf1e709,# Finding Low Cardinality Categorical Variables And Numerical Variables,37f86e36,0.18867924528301888
9103,b0c2805cd5c087,336ee774,Image meta.stackexchange.com,0446f327,0.18888888888888888
9104,d6cbd7160961dc,52805cd8,"# 3. RKN Script for Benford's Law

Our script receives a data set and produces a table of frequencies for numbers 0 to 9 considering the first, second and third digits.

This data must always have two columns. For example, for a data set with city populations, one column will be the name of the city, and the other will be the value of the city population size. Thus, rows can have duplicate city names, since each rows show the city name and the city population size for different periods of time (day, week, month, etc..)",36d74664,0.18888888888888888
9105,892be0a523578c,e0854b48,"**3.2** The ActivityHour attribute is not a datetime data type, we need to convert it to the right data type before further investigating",b0e8d7c0,0.18888888888888888
9107,ce9ed5e2d601d7,b8257809,"## For quick test
TomekLinks
(array([1, 2, 3, 4, 5, 6, 7]),
 array([1390684, 2169226,  155218,     218,       1,    7627,   38757]))",f58a2f43,0.1889763779527559
9119,63b44c85e32c1f,705c2ecc,You can also slice a parent list with a fixed length or step length.,fb9b9562,0.1891891891891892
9126,f91f58d488d4af,dc856481,We can use fastai's `show_image` function to display an image.,5df1bbf3,0.18947368421052632
9128,840534f2908a9c,720df07b,**2.  DATA CLEANING**,8081c3cc,0.18947368421052632
9132,84127ade6fde87,52afd360,"Let’s load Best Russian Short Stories by Leonid Andreyev et al. from the Project Gutenberg website:
http://www.gutenberg.org/cache/epub/13437/pg13437.txt. We’ll just save the file and read it in
(code/p1ch4/5_text_jane_austen.ipynb).",f55d05b6,0.1896551724137931
9137,d42518f6cb0995,e80ce65a,On average users answer ~66% questions correctly. Let's look how it is different from user to user.,26913a9b,0.1896551724137931
9139,20e1ba19eb9b5e,a9fbf2b6,"The map shows that the strong relationships with SalePrice have among others: 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF'.... There are a couple of them, so let's see 10 first variables with highest correlation with the target",4569bfc1,0.1896551724137931
9141,1cd8be6e679620,fa9f90b3,## Data Info Method,3ce15a43,0.1896551724137931
9143,f3c6048d1058e3,dfd8ff04,"- This shows mean value of each indirect feature is almost similar for both the sentiments. No starling differences were observed for any feature. This also indicate that model comprising these indirect features as explanatory variable will not yield good accuracy score. So to get good classification model, we have to develop machine learning or neural network model based on word vectorizer only.",1d9056b0,0.1896551724137931
9144,00001756c60be8,0d136e08,**Загрузка данных**,945aea18,0.1896551724137931
9145,c80939c7c626cf,aa85742d,# This chart confirms that passengers having 3rd class tickets were dead in large quantity ,b9ac31e2,0.1897810218978102
9147,fdc3afd309b850,1ad92190,"When we get a return from the tries. We are going to check if the address and neighborhood from address_DF returned are equal to the string parts sent. So then we will return a match to the first function to define the two new columns. ""Address_'' and ""Neighborhood"".
",966bde38,0.18981481481481483
9149,08f845750d026a,b612b51c,### How many countries are in this dataset?,1c54de30,0.189873417721519
9150,83df814455f06c,912ed272,"# **8. Import dataset** <a class=""anchor"" id=""8""></a>

[Table of Contents](#0.1)",c9cff71a,0.19
9151,4cd25e50c7e007,e9c84dfe,"***Season***
* 1:spring
* 2:summer
* 3:fall
* 4:winter",ceb0c525,0.19
9156,8985a124d4b657,5e548c57,"There are many pollutants. Let's first try to predict PM2.5 concentration values. Let the years 2016 and 2017 be the testing set. As you can see below, these 2 years account for 20.06% of the data (test set)",586d1846,0.19047619047619047
9158,87e94f864d74be,37c8f50b,"# <span style=""font-family:serif; font-size:28px;""> 3. Data Cleaning </span>
",294bfe9f,0.19047619047619047
9161,e16860fce156b0,c1862f64,"#<b><mark style=""background-color: #9B59B6""><font color=""white"">plot(): analyze distributions</font></mark></b>


The function plot() explores the distributions and statistics of the dataset. It generates a variety of visualizations and statistics which enables the user to achieve a comprehensive understanding of the column distributions and their relationships. The following describes the functionality of plot() for a given dataframe df.

They start by calling plot(df) which computes dataset-level statistics, a histogram for each numerical column, and a bar chart for each categorical column. The number of bins in the histogram can be specified with the parameter bins, and the number of categories in the bar chart can be specified with the parameter ngroups. If a column contains missing values, the percent of missing values is shown in the title and ignored when generating the plots.

https://sfu-db.github.io/dataprep/user_guide/eda/plot.html",2054f1ce,0.19047619047619047
9167,55a5e31d03df9f,c7e1350d,"Wonderful! Looks like our training dataset has 361 images belonging to 36 classes and our test dataset has 76 images also belonging to also 36 classes.

Let's highlight the following:

* Due to how our dataframe is written the PATH helps TF to find all the images. 
* The target_size parameter in `datagen.flow_from_dataframe`  defines the input size of our images in (height, width) format.
* The batch_size defines how many images will be in each batch, normally in deep learning we don't want to pass all the data in a single batch but rather pass minibatches. [You can't trust me but trust Yann LeCun](https://twitter.com/ylecun/status/989610208497360896?lang=en)",06dce00f,0.19047619047619047
9174,2473d004f92592,8a6a5fc6,**Exploratory Data Analysis (EDA)**,18d3b6ee,0.19047619047619047
9176,0c57e3132ae184,603b1900,## 2. Group columns into types; correct for inconsistent data types,f6bac298,0.19047619047619047
9181,e04e5204572e7e,8749b158,"### *The sheep looks slick, ain't they?!* ",6c888be9,0.19047619047619047
9182,7a058705183598,4e7d609e,Voilin plot,b0ead917,0.19047619047619047
9183,916ccf243827f1,00cb9d16,## 2. Data Visualize,5147f4d2,0.19047619047619047
9186,04bac111ffbe9c,e8b23a09,"##### COMPLETENESS ISSUES
1. Handle missing data",82576b17,0.19047619047619047
9191,066c5ee1ef39e6,8ea08521,## Create 3 versions of **clean** data,0f394e1b,0.19047619047619047
9192,7454fdc444df16,fe63f643,"## What do we know about our data?
Now that we have a good understanding of the file structure let's try and understand how much data we are about to process.

### How many patients do we have?
It seems that we have a total number of 280 patients. This sample size is relatively small therefore we have to be careful not to overfit our model. We need to implement our model in such a way that it maximizes generalization.

Each patient has a batch of patches that were extracted, therefore the total number of patches is likely much greater than 280.",a7818ef5,0.19047619047619047
9199,a758983a68c014,52d4079c,Skip-Gram model tries to predict context given a word. So as input it expect word and as output words which often appears with the inputed one. Below I implement some suportive functions.,ab89f181,0.19047619047619047
9204,c818250dd720eb,5934687b,"We see below that our data is split nearly 50/50 across our two data providers, but that the examples provided by Radboud University Medical Center are more severe than those provided by the Karolinska institute.",68ee40de,0.19047619047619047
9210,e4c6dd957eb5ce,490b20a1,Here we can see informations about all Registrations on Kaggle. ,2e383665,0.19117647058823528
9213,c65a65d4041018,c054b401,### Degree,824fb229,0.19117647058823528
9214,eb0ecd6bebeb15,9a95f38f,Veri çerçevesinde hangi öznitelikte kaç adet eksik değer olduğunu gözlemleyelim.,d7b93a60,0.19117647058823528
9215,7f74a04ae75792,a1a36376,"# <font color=green>Exploratory Data Analysis (EDA)<font>
Checking the relationship of variables, summary of data, outliers, filling missing values etc.",d01e91da,0.19117647058823528
9221,71c3c1eab0377d,fddc8277,Group the titles of same types,52b4e360,0.19130434782608696
9222,2ada0305b68956,407754dc,### 30. Palette = 'Pastel1_r',133e26f4,0.19142857142857142
9223,3f25b363afec54,6bf43f04,"As we can se there are multiple data files are available fore this competition. So probably you gussed it right various data merging technique gonna come in action.

#### I suggest you to check this [notebook ](https://www.kaggle.com/vin1234/merge-join-and-concat-with-pandas)to understand better all data merging techniques better.",bbdaae25,0.19148936170212766
9224,56785caebaa256,1e4fa114,"## 3. Selection data with holidays<a class=""anchor"" id=""3""></a>

[Back to Table of Contents](#0.1)",a792961a,0.19148936170212766
9229,957e035ba5b9d5,1312661c,Let's have a look at some of the images,778ab3d3,0.19148936170212766
9230,4c47839b067546,cdf3f883,"Удалим оставшиеся признаки, которые есть только в одной из выборок, поскольку они не имееют влияния на цену:",1f517b02,0.19148936170212766
9231,b61ab8f81dc03d,542394c3,"<a id=""pclass_survived_age""></a>
### PClass/Survived/Age",64d05394,0.19148936170212766
9232,f6648e47713411,4827f080,"## Kết Luận
Chúng tôi thấy rằng ảnh có độ phân giải rất cao nên chúng tôi sau đó đã áp dụng resize lại ảnh để cải thiện thời gian chạy.",f4af4d1c,0.19148936170212766
9233,62487bcd70b199,efb3eb35,##  <a id='3.4'>3.4.Performing ANOVA for Family and Loan</a>,f6ae50af,0.19166666666666668
9237,1eb62c5782f2d7,b58ec2c0,### 1. z-score = 1.06,bb69f147,0.1917808219178082
9239,3d08ca7656dec0,6d986b88,## **CP**,bd3f87e3,0.1917808219178082
9241,dbd96dd275dc60,ef254f65,### Add datetime param for 'saledate' column,1ed493a8,0.1919191919191919
9243,af6556ced704f6,f7fb533c,"***Frequency Data***

   * We look frequency of attributes using **value_counts()**",881577c0,0.19230769230769232
9244,4d91e84c564cbe,722874a3,"## Slicing

What are the first three planets? We can answer this question using *slicing*:",355a43e3,0.19230769230769232
9245,2facf256353117,4e9802dc,"# The Blood-Brain Barrier
* The blood-brain barrier is made up of tightly packed cells in the brain’s capillaries that prevent harmful substances from entering the brain. It protects your brain from injury and disease while also letting in substances that your brain needs, like oxygen and water. While it performs an important function in keeping your brain healthy, it can also cause challenges in treating some brain conditions when medications can’t cross the blood-brain barrier.
* The blood-brain barrier serves a filter, controlling which molecules can pass from the blood into the brain. Because the endothelial cells are positioned so closely together, they keep out any harmful toxins or pathogens from reaching your brain. 

* While the blood-brain barrier keeps many things out of the central nervous system, it is not impermeable. Some essential molecules, like oxygen, can get past the blood-brain barrier. Fat-soluble substances with small molecules can also pass through the barrier, including caffeine and alcohol. Other substances, like glucose, can be transported from the blood to the brain by a system of transport proteins
",18f579be,0.19230769230769232
9248,e7237da7cbec10,1e6f16c7,"2. Train Data , test Data Split 
",5fcf5e3d,0.19230769230769232
9249,163ceeb80d6923,a77a617e,## Choosing MAX Sequence Length,4adfbb90,0.19230769230769232
9251,3f451680b1857b,b5887c00,# Only 14 Class,56c45a1b,0.19230769230769232
9254,aa46e9376825a5,69f8d6f9,"# Evaluate the dataset containing the GDPs of different countries to:
• Find and print the name of the country with the highest GDP,

• Find and print the name of the country with the lowest GDP,

• Print out text and input values iteratively,

• Print out the entire list of the countries with their GDPs, and

• Print the highest GDP value, lowest GDP value, mean GDP value, standardized GDP value, and the sum of all the GDPs",57792d96,0.19230769230769232
9259,aae204e78a48d1,dcaa6cd2,"# Hypothesis 1: Customers who attrite are more likely to be younger
The first hypothesis we want to test is whether customers who attrite are likely to be younger.  Many financial services firms are struggling to attract and keep younger customers vs fintechs",53ab6133,0.19230769230769232
9264,897ca904b74a98,a000b0e5,### Continuous variables histograms,c5844ad4,0.19230769230769232
9267,71d3e4aee86e3e,27263962,> # Graphing Daily-Increases,69706f0b,0.19230769230769232
9268,669ce946943d60,c4112fd7,"just as we use the loss,
```python
criterion = MCRMSELoss()
predictions = model(data)
loss = criterion(predictions, targets)
```",0f63c4ce,0.19230769230769232
9269,09751c520b0616,945f669c, - Calculate null values,a4d0c7e9,0.19230769230769232
9271,6f1481148352e9,fcecc554,# States,7cfbdb8f,0.19230769230769232
9272,99bf357eaf61f1,17750288, #### 3. Continuous Variables:,9d92fafe,0.19230769230769232
9275,44f6a002ecd033,fa13a779,This column is straight forward and it looks like most loan applicants are not self-employed.,70bbe106,0.19230769230769232
9276,a915263bc207da,d89db2fb,"### Removing outliers
##### Get a boxplot to find outliers",b17ebcda,0.19230769230769232
9280,4daf6153275cbf,9dd499de,"Because my analysis is based on localization of restaurants in Europe, I eliminated some cuisines that represent regions or have high volume. **I am making the assumption of the first cuisine in the list represent the main cuisine that restaurant serves.** In general, the first and second element has regional cuisines, like French, Dutch, etc.",51db1961,0.1927710843373494
9282,6cade0b6a41ba2,f87191fc,"##### Gender needs to be categorized as Categorical Nominal Variable. For this, we would be using Dummy Variable Method.
##### Also, from the analysis perspective, it will be tedious to create another dummy variable just for one row vlue (of Others). Therefore, we will impute this other value with mode in this column.
#### Therefore, conversion will be as follows:
- 1. Male: 1
- 2. Female: 0
- 3. Others: Mode Value of column",e6110293,0.19298245614035087
9287,54004b32784b68,3ab28a6d,"# 2- Cleaning of Data
",27213ca9,0.19298245614035087
9288,d8fb26c4197325,0b874714,## Categorical,b190ac50,0.19298245614035087
9292,3fb15e6e48aec2,6e853ad6,"# Merge similar Titles:
* Mapping Title with low frequency back to its majority counterpart another option would be to: take all the title with one instance and map them back to a similar majority title",9d1f4358,0.19298245614035087
9294,fe7360cddc13e5,2a234f6b,"Herhangi bir işlemden (j.) sonra müşteri ""p"" olasılıkla pasif hale gelir. Müşterinin işlem sayısı arttıkça pasifleşme olasılığı düşer ve transactionlar arası süre üstel&geometrik dağılım izler.",8979e423,0.19298245614035087
9295,e03eb63c1f725d,b49e1da3,"<a id=""1""></a>
<font size=""+2"" color=""blue""><b>Title Word Clouds</b></font><br>",e204b7e3,0.19298245614035087
9299,4ae6a182abac64,2bc53d35,* **pandas_profiling**,418676c5,0.19327731092436976
9309,0d9a2067267ba1,b5c313f6,"The distribution is **skewed towards the right**, which can be improved with a mathematical transformation so that it would have a more **Gaussian** like shape",abc194fb,0.1935483870967742
9327,0caaec057f7184,eb1a92ac,"Note that from the figure, we can see the sales trend of well-sold category. Cate 40 (Cinema -DVD) and cate 30 (PC games - standard edition) have the most sales throughout all the months, with the decreasing trends.

There are also lots of time series trend with monthly total sales lesser than 5000.",b875533e,0.1935483870967742
9328,c0ddb77bf32e2b,3a0091d3,"Let's have a look with columns 'CO' which have 0.000885 NA raio. Obivously, if 'CO' is NA usaully  other columns will be NA too.",a0cb45f7,0.1935483870967742
9329,b05ee1ea1c8269,fe926027,# Data preparation,19e4d303,0.1935483870967742
9330,fdbbd573ba31c2,3119e3dc,"When it comes to categorical features, we will not plot right now because 'Tracking_id' & 'datetime' will be processed later. And further analysis on all features will be done later.",f7c28d74,0.19375
9333,1a222fee3089d2,a6c41f4c,## **Fare**,59ab8894,0.19402985074626866
9336,20b372b6e4e276,326aa7ad,"### Commit 17

* Dropout_new = 0.15
* n_split = 5
* lr = 1e-5

LB = 0.709",ec8b0860,0.19402985074626866
9340,98a6794067932a,cebe8d13,"La dernière modification à effectuer afin que notre base de données soit pleinement utilisable consistait à ajouter les 0 au début des codes postaux n'ayant seulement que quatre chiffres. La cellule de code ci-dessous permet donc dans un premier temps d'effectuer cet ajustement et dans un deuxième temps, elle permet encore une fois de représenter la liste des différents codes postaux se retrouvant dans la base de données après qu'ils aient été ajustés. Cette dernière étape nous permet de constater si nos corrections ont été effectuées avec succès.",08600fe2,0.1941747572815534
9349,396bc36edb95d3,cadfdcb2,#### Plotting Age with categorical variables w.r.t Claimed status,965e4f8f,0.19444444444444445
9353,cf39cde80e66b7,0920b706,Read the data,aed4bc9b,0.19444444444444445
9354,c01049afb6d307,6e63dacc,"## Variables 1
**Some variables are directly related to ID. They have the same value for each ID. These are;**
    
*     'Transportationexpense', 'DistancefromResidencetoWork', 'Servicetime', 'Age', 'Education',

*     'Son', 'Socialdrinker', 'Socialsmoker', 'Pet', 'Weight', 'Height', 'Bodymassindex'",d37d3b5d,0.19444444444444445
9357,0f5085b162bd9f,9760542f,# ELBOW method for finding the optimal # of clusters k,a3d989ee,0.19444444444444445
9358,18a864b56ac3b8,b2c64673,"# Quirk

For some silly reason if I run the cnn_learner with pretrained left in its default True state then I can't commit this notebook successfully. But if I train the model in one of these kernels, download it, then upload it to the notebook and then load it the notebook, then the notebook commits just fine. The code below generates the model that I will be updating in this notebook.

`
learn = cnn_learner(data, models.resnet34, metrics=error_rate)
learn.fit_one_cycle(8)
Model_Path = Path('/kaggle/working/')
learn.model_dir = Model_Path
learn.save('stage-1')
`",f3ca0a7c,0.19444444444444445
9359,166a62ebb4fc3a,ee3b421a,"Here we have successfully dropped last column named as ""Unnamed 32"". Now after getting shape of the data, there are 569 rows and 32 columns.",db48a079,0.19444444444444445
9361,1014e6be391084,6afc6c06,Removing the rows with Amount==0,46f9168f,0.19444444444444445
9364,9ca9a30fc69d9b,e8b38e55,"### Major Tournaments
FIFA (Intercontinental competitions) Confederations Cup & FIFA World Cup  
AFC (Asian competitions) AFC Asian Cup  
CAF (African competitions) African Cup of Nations  
CONCACAF (North American, Central American, and Caribbean competitions) NAFU,CCCF Championship & Gold Cup  
CONMEBOL (South American competitions) Copa América  
OFC (Oceanian competitions) Oceania Nations Cup  
UEFA (European competitions)  UEFA Euro

I have just picked out 'Major' tournaments. The critera for major is debatable and I would love to see a discussion about it

https://en.wikipedia.org/wiki/List_of_association_football_competitions",f715c2e5,0.19444444444444445
9368,30c8dc87ce52ca,d1a0aec7,SCALING THE DATA USING THE StandardScaler,805e9d67,0.19444444444444445
9378,2cb457b60dd246,ab5c41e5,### Create a train set that excludes images that are in the val set,339367df,0.19480519480519481
9380,663bbc9eaf267b,060f6c0e,## Price,32445529,0.19480519480519481
9382,c13f73168789c2,4df062bb,"### 1.4 Select multiple row by it's label<a id='6'></a>
Syntax : `df.loc[[row_label1, row_label2, ...]]`",16175052,0.19480519480519481
9385,241cf32abb22d8,74359fca,"### Categorical Descriptive Features

There are two types of categorical descriptive features in the dataset.

#### Nominal:

13 out of the 17 nominal features have only two levels, therefore they are simply encoded into a single column of 0 and 1. The remaining 4 features have more than two levels, therefore applying one-hot-encoding is necessary as it can create a binary column for each unique value under these multi-level nominal features.

1. sex: binary - female or male)
2. school: binary - Gabriel Pereira or Mousinho da Silveira
3. address: binary - urban or rural
4. Pstatus: binary - living together or apart
5. Mjob: 5 levels
6. Fjob: 5 levels
7. guardian: 3 levels
8. famsize: binary - ≤ 3 or > 3
9. reason: 4 levels
10. schoolsup: binary - yes or no
11.	famsup: binary - yes or no
12.	activities: binary - yes or no
13.	paidclass: binary - yes or no
14.	internet: binary - yes or no
15.	nursey: binary - yes or no
16.	higher: binary - yes or no
17.	romantic: binary - yes or no",47157066,0.19480519480519481
9386,9169c4e9c33c90,79153f15,Violin plots for User Rating and Price between Genres,725bf880,0.19491525423728814
9387,1294fb4c86f993,1b34afc0,Checking if the 'totals' is equal to the summation each row.. ,4471e513,0.19491525423728814
9388,8cefb86a675e5d,91e26485,**Declaring the Columns which I have to use as Features.**,79f9e69b,0.1951219512195122
9389,514d8de15cb7ef,c4645d9e,"### Removal of less important words for classifcation 
<p> NOw after the wordclouds we actually need to do some real text analysis and for that the first step will be removal of less important words from the descriptions using function which is as follows </p>",cfe111b2,0.1951219512195122
9390,0b01138ad120fc,8114754a,"**For RNN our input need to be the same shape. When we make a step it brokes the last numbers like:**  
data = 1-10 | step = 4  
x = 1 y = 5  
x = 2 y = 6  
...  
x = 10 y = 14 (14 not in data)",0b4b72e6,0.1951219512195122
9400,0e09587faffa8f,31d4a8c9,## III. Cleaning,0d563d61,0.1951219512195122
9404,0932046e1f485d,399e2fe1,"In the ""Price"" column we need to remove the dollar sign and turn everything into a number.",218cc7a3,0.1953125
9411,a6b9837940ee38,a643ac35,"* We first convert the data to a numpy array and then convert the flattened image to a two-dimensional image. Its shape is (28,28).",52d2acc7,0.1956521739130435
9417,0e2a23fbe41ca9,7e3169d8,"Observations:

- Years range from 2011 to 2018
- 64% data is from 2017, followed by 2016 (25%) and 2015 (7%)
- Very less data from 2017 (may be, test data will have data from 2018?)",64e4762c,0.1956521739130435
9421,2a123b4e8f9433,f2c6c233,There isn't any missing data. No rows needed to be dropped.,0a082218,0.1958762886597938
9422,063a35f644e3c5,743c4302,"### number of non-null unique values for features in each dataset
",1c30fb0a,0.1958762886597938
9430,917957c6c4065f,1cf036dc,#### trending_date & publish_time,55b8ed68,0.19607843137254902
9431,7cfd96218dd933,d4a808f0,### VALUE CONTROL,7c34d96c,0.19607843137254902
9432,523123dad03177,604e1269,# 1. Let's have a look at gender ratio.,48a5e4e6,0.19607843137254902
9433,629f2918807a9b,10587c64,"## Observation:

- Our rows are now increased from 19184 to 33091, As we converted more than 2 orders in record to a seperate record",be56dc84,0.19607843137254902
9434,fa02c409161192,7e021b95,### Convert original lecun files into csv,e97077f7,0.19607843137254902
9435,1a0bd2f72bbe36,e0bc7823,## clean the data:,2fa311dc,0.19607843137254902
9437,d0080e3a39bc5c,3c085b2a,"
* Now, it is a classical oversampling mistake which many people commit, and that is, to oversample the data first and then split the new Dataset into train and validation set.


* This essentially results in the validation set not being completely ""Unseen Data"" or true ""Hold-out Set"" for that matter, because the model has already seen a slightly different form of the images in your validation set. Hence, the scores on validation set come out to be highly optimistic whereas on the Test set, such models tend to perform poorly


* For further reading on the right way to oversample your data, refer to this link - 

  https://beckernick.github.io/oversampling-modeling/",2fcde4cf,0.19607843137254902
9440,c84925c8171900,14af6fd6,"<div class=""alert alert-block alert-info"">
    <span style='font-family:Georgia'>
        <b>Insight: </b><br>
        We can see there are some null values in the column. Let's inspect the null values first 
    </span>    
</div>",e21ff7ec,0.19626168224299065
9441,6a80f915608fc2,8c0b7134,"### Expected ""guessing"" scores for each target
The best constant probability to guess for each target is that target's average, *a*.<br>
In that case, we expect an average score for that target of $~-(a~ln(a) + (1-a)~ln(1-a))$",636938eb,0.19642857142857142
9446,df51d4c54fbb91,71b34d50,Now we convert the labels to categorical.,4226dd72,0.19642857142857142
9447,f13534449a3750,cddf38bc,"<a id=""section-two""></a>
# Exploratory Data Analysis",8b7f3332,0.19642857142857142
9454,918040fad252ec,b7db37c7,Menampilkan patch dan label yang sudah dibuat beserta nama penyakit,966fcd8f,0.19672131147540983
9461,a2444ab5d5f147,afeabb71,"```
The sentiment labels are:

0 - negative
1 - somewhat negative
2 - neutral
3 - somewhat positive
4 - positive
```",10617755,0.19696969696969696
9464,2ada0305b68956,6cbd16da,### 31. Palette = 'Pastel2',133e26f4,0.19714285714285715
9465,bddd799cdbbae8,c23be1d9,"There are 3018 duplicated data, and after remove the duplicated data, the most duplicate data is in others cyberbullying.",b44e3c08,0.19718309859154928
9467,9bcfa825c8b2e6,8fa0bef9,"Oluşan boş değerler,boş değerlerin 3 komşusuna bakılarak knn imputer ile doldurulur.",220f36e4,0.19718309859154928
9481,f3c6048d1058e3,37949777,#### 5) Histogram for word count for both classes,1d9056b0,0.19827586206896552
9483,2f47abddfd1928,c26133d5,"### 2.4. Age

Age feature also have missing values but in this case 263, it means that we need to do it with more care if we don't want to lose accuracy.

The appropriate method is to use the median and again aggregating the age in meaningful groups.

To find which features are appropriate to group the age we can use a heatmap with the correlation for each feature with the rest.",ae33cc0b,0.19834710743801653
9484,ee23a565163388,ca5f43f9,## **Are elder patients more prone to heart failure**,88aacbc4,0.1984732824427481
9485,7f74a04ae75792,2b416242,## Explore Categorical Variables,d01e91da,0.19852941176470587
9491,738bfced935b69,3952b279,The distribution skewed to right. ,2d3c592d,0.19863013698630136
9492,30fdc4a6e3c1db,c2625bd2,### Plotting Sales Ratio across the 3 categories,6111ddee,0.19883040935672514
9493,5ce12be6e7b90e,c04f47b3,Multipy:,c0ab62dd,0.19883040935672514
9500,c18267b203f28a,c4149943,"We'll use the following function to load our dataset. One of the advantages of a TPU is that we can run multiple files across the TPU at once, and this accounts for the speed advantages of using a TPU. To capitalize on that, we want to make sure that we're using data as soon as it streams in, rather than creating a data streaming bottleneck.",09ca8efb,0.2
9509,47c519cb88e1c2,7ed01403,## Vanilla CNN in TensorFlow modified from [this tutorial](https://www.tensorflow.org/tutorials/images/cnn),f2c992fa,0.2
9516,7e1da639035ac5,85baf496,### <a id='6.2'>6.2 Economic Need Index vs. School Income Estimate</a>,120b6c23,0.2
9517,bbaa07ad21cf4e,57cfaec0,### Preprocessing and Cleaning of text,3ab6b254,0.2
9521,ce7abd85d777b5,b366f965,1. Data Outline,0a340dbb,0.2
9524,e25c0f830df3f4,83a7a404,# Extracting 1000 random samples from the data,fdcf7189,0.2
9525,07f5853e4db8f8,23140cb9,"Observe that  the interval ""[a, b["" means that a ≤ x < b for a,b are real numbers.
For the columns pp_total_raw,pct_free/reduced, county_connections_ratio having interval inputs I understand data as: for instance [0.2,0.4[ in pct_free/reduced means 20-40 % students in the districts are eligible for free or reduced-price lunch.So for this kind of datasets I am planing to use mean/avarage of two points a and b and change to single value.",d13c2c32,0.2
9528,6b65d81a5743dd,53a84da9,Check The dataset structure,4080a2d2,0.2
9531,193b9c9a4c155f,cab3d7bf,CHINA,6fb8a8dd,0.2
9538,03048e86a6d806,f360cde2,### Formal Education,1285c231,0.2
9539,639e8aae4e046e,a72e6629,**Some EDA**,77deb4cb,0.2
9540,513ce405d7f6a3,849df27e,# Data visualization ,8461e086,0.2
9542,80ecc4c67a9f54,25e3fed0,## Creating Series for first and last names,4bbf546c,0.2
9549,1011899b959f44,995479a6,2. What are the names of the columns as listed in the DataFrame? (Hint: Use the .column method),0b112382,0.2
9553,5a8c553e21c70f,c3d8191d,"Dataset is severely imbalanced. Class weights computed above will be fed to **Keras** fit function during training. Class weights are used in loss computation, loss related to samples coming from Class 1 will be multiplied with a higher coefficient to compensate the sparseness of Class 1 samples.",9ebd9d8f,0.2
9558,169177b6e9edea,5b99ee45,Vemos que dependendo do pronome de tratamento utilizado as idades variam,ca42152f,0.2
9567,7341f069d9b2ee,bfc122c4,"Create Test Set
- Using stratified sampling",e0a49e62,0.2
9568,2b36742b49c7bc,c9d85611,"1. Англи үсэг холилдсон нэг тохиолдыг засах
```
шоронгоос гармагц#0000005434 эрүүлr агаар өөдөөс нь угтан
эрүүлr => эрүүл
```",c8f8a96d,0.2
9569,65245c6e88a2ee,a71f83cc,## Checking for NA values,71d6e90e,0.2
9571,840534f2908a9c,cf6ea01d,Remove observations with zero or null values ,8081c3cc,0.2
9572,bbad077c274022,301e7aa1,"**Making data formatted**

*Changing the time slots in 24-hour format*",3c2e3dea,0.2
9574,3597174a998d4d,d34e7530,### 2.1.2 Booking Response,276892ed,0.2
9576,3cb96bd8eb364b,6d1353de,#### Categorical,3157af7e,0.2
9580,e78f177ca86768,03b37e70,"## Tuning Max Depth and min_child_wt
- As these are the parameters which will affect XGBoost the most as they control the tree structure.Hence tuning them first.Leaving other parameters as default
",120e25c1,0.2
9581,d58491f2896fc1,b4390ba9,"* Derin öğrenme çalışmalarında hiperparametrelerin belirlenmesi
* **
* Makine Öğrenmesi modellerinde özellik seçimlerindeki uygunluk faktörünün belirlenmesi
* **
* Çözülmesi zor olan hatta imkansız gibi görünen problemlerin çözümlenmesi
* ** 
* Deneysel çalışmalarda optimizasyon işlemleri
* **
* Sınıflandırma işlemleri
",514bfdff,0.2
9583,5ffe6aa38958a1,5ca01a27,"**Do we need to know what headers mean?**
To some extent - yes. The headers can be used to learn which features to use. For instance, it is likely that Name did not have significant influence on the survival rate? However, Sex is more likely to have had an influence. The same is true for Fare and Cabin information.  

First analysis, non statistical - this can later be re-analyzed based on correlation between survival chance. 

*ID:*  
  * PassengerId: Not feature

*Ground Truth:*   
  * Survived: 

*Feature: *
  1. Pclass:   (Likely that class has an impact on survival rate)
  2. Sex
  3. Age
  4. SibSp : Siblings or spouces on board
  5. Parch: Parents/children on board
  6. Fare
  7. Cabin: Having a cabin or not is important 
  8. Embarked: Port of embarkation

*Further Engineering Maybe required*
  1. Name: could be a feature as it may represent wealth or background in some cases. We should try this if result independent of this is not satisfactory
  2. Ticket: Ticket number may have some clues .. ?
",11f5412e,0.2
9587,09bac0c221388e,b60c109b,"Based on the output type, document summarization can be:

* Extractive: the summary is extracted from the input text. The output is usually the concatenation of the most important sentences of the original text.

* Abstractive: the summary is generated. This means that we use the original text to learn internal representations and then we use such representations to generate new text. The output is original, not a combination/concatenation of the input sentences.

* Mixed: produce an abstractive summary after identifying an extractive intermediate state or they can choose which approach to use (eg: pointer models) based on the particulars of the text.
",bea4aa2e,0.2
9592,caaa6793391520,55dd8b97,"As you see, even though the mean is the same, the standard deviation is much less. While the imputation of data this way increases the performance of the model, it also amplifies the bias that already exists in the data. In order to prevent amplification of the bias, we have to replace the missing values with a sample from the normal distribution with the same mean and standard deviation. For categorical features it would be a multinomial distribution.

For debiasing we can try to increase the standard deviation of the distribution from which we sample data for numerical features, and a similar transformation for the multinomial distribution. 

In this notebook I suggest two classes for the numerical and categorical features respectively.",1e79f342,0.2
9599,8cd6656a65e6e7,437f42ad,## Standardize,c8e1697a,0.2
9600,e0e19e91579432,f016095e,> ****import useful library,0c8a0755,0.2
9602,8dbf17f707ef20,742d3d25,"15/06/2015 спостерігається величезне падіння, це було розділення ціни акцій. Якщо взяти ці дані, прогноз може бути не таким, як очікувалось, оскільки між ними є розкол.

Ми повинні або відкинути період, або скорегувати значення перед розділенням. Оскільки розділення дорівнює 2 для 1 періоду, ми можемо нормалізувати дані до поділу, поділивши їх на 2. (Старі частки вдвічі менші, ніж сьогоднішні).",624ce794,0.2
9606,6a1ae8234c7653,c081f0a2,"More information on the train data set, with the minimum, maximum and mean values of each column:",2d643c72,0.2
9607,4c55891bcb068d,f54969f6,# Reshape data,01b9cd67,0.2
9610,1645979263c148,a02558f1,"## Exploratory data analysis (EDA)

1) For numeric data

       Made histograms to understand distributions
       Corrplot
       Pivot table comparing survival rate across numeric variables
2) For Categorical Data

       Made bar charts to understand balance of classes
       Made pivot tables to understand relationship with survival",fa11663e,0.2
9612,9a040a4f21091e,518b27af,"Now, let's take a step back and look at some trends throughout the training dataset:",f591b57d,0.2
9615,10c5a39a87c47e,88c70f9c,"**Observation:**
- As we can see normal images are clear with no mark to show.",09c7337a,0.2
9618,869a39a3d4dea2,86a398d9,Took the top left corner pixel pf the image and manipulating it with different color values,9020daf8,0.2
9622,70b7a24d522250,acc3e02a,#Codes by Himanshu Dadhich https://www.kaggle.com/himanshu01dadhich/sample-for-chatbot,83f3c002,0.2
9623,55a5e31d03df9f,52520edc,"# <a name=""mlp"">First model: Multilayer Perceptron (MLP)</a>

The architecture of a classification neural network can widely vary depending on the problem you're working on. However, there are some fundamentals all deep neural networks contain:
* An input layer.
* Some hidden layers.
* An output layer.

Input and output layers, as well as one or more hidden layers with numerous neurons layered together, make up a Multilayer Perceptron. Multilayer Perceptron can have whatever **activation function** they choose: ReLu, Tan-H, Sigmoid, etc. These activation functions are the fundamental piece required for an MLP to learn the relationship between linear and non-linear data, we will see more in the depth of these functions. 

<img src=""https://www.researchgate.net/profile/Mohamed-Zahran-16/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png"">

**The main characteristic of the MLP then is that all the neurons in a layer are fully connected to the others as we can see in the image. **

Each layer feeds the output of its computation, or internal representation of the data, to the next. This applies to all hidden levels as well as the output layer. This is known as **feedforward**, but what computation are we doing here? Inputs are combined with the initial weights in a weighted sum and subjected to the activation function, like the following:

![perceptron.png](attachment:32d637f2-0ec9-4e29-b01a-20dbfb611c49.png)

This is equivalent to this in linear algebra:
> $W^T X + b$ 

The final goal of the algorithm then is to find the $W$ that minimizes an error (difference between prediction and real output). However, if we just made the feedforward computations and stopped there the algorithm wouldn’t be able to learn the weights that minimize the cost function, and then there needs to be some feedback to the model to adjust the weights. This is when **backpropagation** is required. 

Backpropagation is a learning method that allows the Multilayer Perceptron to iteratively modify the network's weights in order to minimize the cost function. How? Using differential calculus, if we calculate the derivates we can know the direction in which we need to move to improve our predictions and once we calculate the direction we multiply by a **learning rate** as the following:

<img src = ""http://hmkcode.github.io/images/ai/bp_update_formula.png"">

Then just see the **learning rate** as to how big is the step we are going to take in the direction of our minimum, this is why this parameter is fundamental. This parameter follows the Goldilocks rule: If you take big steps you miss the spot, if you take too little steps you take too much time, you need the right.

<img src = ""https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png"" width = 700px>

You can see the Full MIT Deep learning [here](https://youtu.be/5tvmMX8r_OM), it covers all these topics precisely and in an unique way.

**Happily in our case Tensorflow already implements all this logic (thank you TensorFlow!) lets implement it and discuss the code afterwards.**",06dce00f,0.2
9625,1005ca950e8a81,a9b814db,"**Question 2: Labelings that have pure clusters with members coming from the same classes are *homogeneous* but un-necessary splits harm *completeness* and thus penalise V-measure as well:**
",52570331,0.2
9632,6fad63bfd45ef9,e6e51956,# Explore Data,b3c6f1d6,0.2
9636,be9597c72542a2,1dcd3c3a,"**What is the maximum/minimum amount of DC/AC Power generated in a time interval/day?
**",6f29c6d8,0.2
9637,6998861ff6ff01,8078bbd3,"Yep, those are dates! But just because I, a human, can tell that these are dates doesn't mean that Python knows that they're dates. Notice that the at the bottom of the output of `head()`, you can see that it says that the data type of this  column is ""object"". 

> Pandas uses the ""object"" dtype for storing various types of data types, but most often when you see a column with the dtype ""object"" it will have strings in it. 

If you check the pandas dtype documentation [here](http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes), you'll notice that there's also a specific `datetime64` dtypes. Because the dtype of our column is `object` rather than `datetime64`, we can tell that Python doesn't know that this column contains dates.

We can also look at just the dtype of your column without printing the first few rows if we like:",ea9e72cf,0.2
9643,0687cd5c8597db,49b918e5,### **Distribution of images of digits in the dataset**,4edec76a,0.2
9644,ca73f3d2e25b47,ea9557dd,Check missing data in dataset,4cd11efe,0.2
9646,9276fa5cc2fef6,3844e60a,"Now, that we have had a look. Let's try to create features for our question. ",24aa6a52,0.2
9655,0fa9979b5690e9,843289fa,"O resultado mostra a princípio como a redução na quantidade de dados de treino impacta o desempenho do método. Esse conjunto de validação é bastante importante para servir como um tipo de teste e analisar a relação de melhoria ou piora com a mudança de parâmetros no método de aprendizagem.

Vamos supor testes com diferentes valores de *k* (n_neighbors) no método k-NN. Vamos mostrar o desempenho a cada mudança de parâmetro, e também vamos salvar o modelo sempre que um resultado maior for obtido.",c26eea94,0.2
9661,675b60eaf415a6,fcfd89d6,"**meta** folder contains the text files - train.txt and test.txt  
**train.txt** contains the list of images that belong to training set  
**test.txt** contains the list of images that belong to test set  
**classes.txt** contains the list of all classes of food",68c0b725,0.2
9662,867a9f977fa945,3e3c5dde,# **Removing unnecessary words**,2740fcca,0.2
9665,254cccd5145725,8867fcc0,To check the count based on groups of income levels from the Target Variable,a49b4037,0.2
9669,37b09262279764,194981c0,"***0 -> did not survived<br>
***1 -> survived<br>
<b>549</b> people <b> did not survived</b><br>
<b>342</b> people <b>survived<b><br>",37c4c417,0.2
9673,c8bf959b9608cf,131afcc2,"#### Preprocess the image. It will be used to preprocess the style and the content image, that we will give to the network. ",155e3672,0.2
9677,50d4ddf1953997,603620c8,"We can now look at how the total number of TV shows and movies has changed through the years in the above countries. The following analysis is only considering the so-called ""popular"" countries.",90bdddd6,0.2
9678,485de87c50af82,71b39be2,## **Exploratory Data Analysis**,a5bd438e,0.2
9679,bfe6c7096b1ad0,23d40a2a,## Анализ количественных признаков,fffd95e0,0.2
9685,6e472c6c591c7d,0af1dbed,The World Bank has made tons of interesting education data available through BigQuery. Run the following cell to see the first few rows of the `international_education` table from the `world_bank_intl_education` dataset.,65532a3d,0.2
9688,541d0fa0e26b80,fac1f6b6, Let's form a dataframe and check for Missing Values.,a29e0f29,0.2
9691,4945eab98d7d39,0a1aa0c0,"1.Here Rotten Tomatoes has 4600 missing values and unnamed: 0,type are not usefull for the analysis, so we should drop these three columns.
2. drop the missing values rows from the data set.",46258ffb,0.2
9693,817449886e2cbd,831c368c,"### Again, but with binary encodings on stance",ea681120,0.2
9698,726833f92fb87a,88e02ce6,"From the correlation matrix, the only interesting correlation is the moderate positive correlation (+0.51) between 'previous' and pdays'.",7dc5e1b6,0.20134228187919462
9701,20b372b6e4e276,a0a7d6c8,"### Commit 18

* Dropout_new = 0.15
* n_split = 5
* lr = 3e-5
* BATCH_SIZE = 24      # originally 32

LB = 0.704",ec8b0860,0.20149253731343283
9706,6cade0b6a41ba2,c5c3d3dc,##### Replacing Other value with mode,e6110293,0.20175438596491227
9717,04ff2af52f147b,73efd545,"**Filling Embarked Null Values:**

The only null values for *Embarked* are from two first class women who have the same ticket number.  From this, we will proceed assuming that they boarded the ship from the same port (one of C = Cherbourg, Q = Queenstown, or S = Southampton).",d5f37be9,0.20224719101123595
9718,e67925694c07d3,df8904a1,"some column have missing value more than 50% of its data

we might need to handle this later (some option are imputation, drop the column/row)",83af4c4a,0.20224719101123595
9720,87e94f864d74be,05d6868b,**Lets check how many nulls are there in the data** ,294bfe9f,0.20238095238095238
9724,5d2a3e82679cf3,827039ae,"# I will fill NA values according to these division levels salary means.
- It is clear that League and New League levels aren't decisive in ""Salary""
- It is obvious that if the player's division level  is E, the salary is higher and if the player's division level  is W, the salary is less. 
",9e60b1e3,0.20253164556962025
9725,917957c6c4065f,bd96cdae,분석에 사용하기 위하여 trending_date & publish_time를 datetime 형식으로 바꿔줍니다.,55b8ed68,0.20261437908496732
9728,63b44c85e32c1f,19dd6853,### Built in List Functions,fb9b9562,0.20270270270270271
9730,e4525eb0c96f28,a180f711,"The Year vs Sales plots did not do a very good job in accurately showing any relationships. The violin plot showed 1989 as very dense in regard to sales, when in reality it only represents 3 data points. Meanwhile, the years surrounding 2010 are so dense with datapoints that the distribution of sales is not properly reflected.

We decided for now to continue and re-explore year vs. sales data using other independent variables. 
We explore 4 primary columns:
- ESRB_Rating
- Genre
- Country (of development)
- Platform",2093a1f1,0.20270270270270271
9731,eda49464dd6d1b,11e87791,"* Older customers buy vehicle insurance at a much higher rate than younger customers.
    Almost half of total customers are under 35, but they only make up about 1/4 of those who buy vehicle insurance.
* The spikes every 5 years are the result of combinations of 2 x 1-year intervals, and can be ignored.
* It would be the best use of resources to target customers over 35 with vehicle insurance offers, since those under 35 are less likely to be interested.
* On the chart, customers between 30 and 63 are higher-represented among those who purchase vehicle insurance.  Customers both younger and older than that range are lower-represented.",8421f81f,0.20279720279720279
9732,2ada0305b68956,0b35f8ad,### 32. Palette = 'Pastel2_r',133e26f4,0.20285714285714285
9735,548f961125248d,d1be9567,### Balance of target labels,d8c5e8b8,0.2028985507246377
9736,9d9da6c439b96b,782b333d,"It shows that the table has missing value on ""Year"" and ""Publisher"" columns, because of small number of missing value, I decided to drop the raw. ",361cc7d9,0.2028985507246377
9740,17a24d566ffa59,d40e5998,##### Manually create the first singular value,89049e56,0.2028985507246377
9744,a1a31459abf078,1f83b2ae," # Exploratory Analysis <a name=""eda""></a>

Having taken a quick glance at the data, let's dive deep into datasets provided to us and try and gain a deeper understanding of the data. This will help us understand distribution of individual variables as well as relationships between independent and dependent variable. It will also allow us to create meaningful features which can then help us in creating a good model. We will be looking to answer the questions listed out at start of the notebook.",66fc0f54,0.2028985507246377
9745,ba4b3bd184acbb,4f48a507,Boolean indexing can also be used to select a single index and it should be noted that a DataFrame is returned.,0f5de724,0.20300751879699247
9749,ff3a8ce61fab6a,f2a5f291,Another example,9afe1654,0.203125
9750,e19e307b3fd188,d8e448e3,#### Histogram,2173955b,0.2032520325203252
9751,b10bd75889dad9,ebc871f9,#### dropping columns where > 70% data is missing,ee00ceee,0.20333333333333334
9752,b660910fcc2954,2d1e48a5,"# Data exploration


There are (besides id and target) a number of 10 categorical features and 14 continous features.

We will look for the categorical features to the class count of the feature in train and test set and for the continous features to the distribution.

",80b74f88,0.2033898305084746
9754,a077820f7ab459,2bbf16c4,# Transfer learning of DINO attention map image,05a43104,0.2033898305084746
9755,2a56d6b0e153f2,31b1129f,"IN THE ABOVE GRAPH, WE CAN SEE NUMBER OF MALEs ARE MORE THAN NUMBER OF FEMALES",8dc315e6,0.2033898305084746
9756,c4bca5d86a38c3,d2bfa454,"Obteniendo la información de los nombres y dejando solamente la informacion del titulo (Miss, Mr, Lady, etc). Para luego hacer gráfica de probabilidad de salvamiento según título.",e23d297c,0.2033898305084746
9761,b9bc7dc9f582e5,d39f9ce7,# Missing and Unnecessary Data,15cc4d28,0.2033898305084746
9762,f2e5e9fb9eaaf7,d6dcad98,"[back to top](#table-of-contents)
<a id=""3.3""></a>
## 3.3 Submission
The submission file is expected to have an `id` and `claim` columns.

Below is the first 5 rows of submission file:",048e0d08,0.2033898305084746
9765,bb0905d33ae417,42b841c5,# Data loading and preparation,25fd1965,0.2033898305084746
9766,ac1abfe1dfe815,5aa7d9d0,"for positive or negative tweets the probability of getting retweeted are about the same (11 negative, 11 positive).  
but for a single negative tweet the likelihood of a higher number of mretweets is higher.",6529dbcb,0.20353982300884957
9770,ac04ba639d1c93,59e0d704,"For an fast model/feature evaluation, get only 10% of dataset. Final submission must remove/coments this code",748059d5,0.2037037037037037
9772,ab6da5994949a3,ee54e513,### Applying PCA with  n_components = 2,fae6b91d,0.2037037037037037
9777,e0f03003a69819,3ed8c757,# Gender-Charges Relationship,609ad1f4,0.2037037037037037
9783,eb33e05704d647,edf53016,RPN layer,cd80436d,0.20408163265306123
9786,e69a496109e7d8,d3307cdf,Class has two values as 1 and 2. 1 represents the people who survived 5 years or more after the operation. 2 represents the people who didn't survive five years or people who died within five years after the operation,1c640591,0.20408163265306123
9788,2343dc02ffb96a,4b049577,"# Row 213 is obviously an outlier, probably a data entry error. Let's drop it.",29aa95a4,0.20408163265306123
9791,f1e162ddd14f11,b9c0b05d,we can see that the year is of no used except for deriving a new column of number of years since manufacturing,cdb2e771,0.20408163265306123
9792,0caaec057f7184,34599aec,"### items in a category
Take Cate 40 as examples, take some examples and find if those items has launching information (time) through item_id.",b875533e,0.20430107526881722
9794,c80939c7c626cf,cecc298e,# This chart confirms that Passengers with more than 2 siblings or spouse more likely survived,b9ac31e2,0.20437956204379562
9798,6d66ced0028dea,c8c6b881,## 1. Минимальный EDA,f50aae52,0.20454545454545456
9805,d1ff7e10ee0102,6a37ab13,"*Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a <b>linear relationship.</b>*

*And what about 'TotalBsmtSF'?*",2cc71c3c,0.20454545454545456
9809,a35cdce61f4059,c96ffd30,"as can be viewed above our dataset is extremely unbalanced, with almost all of our data belonging to non-fraudulent 
transactions.",acc8eab6,0.20454545454545456
9817,979f1e99f1b309,d407a28a,***The plots showing that most common number of headshots are 0 and almost there is no headshots above 4 which indicate that players didn't tend to knock enemies by headshots***,d1bfebbf,0.20491803278688525
9818,0a1fcda859252c,00217db8,### How many samples for each class are there in the dataset?,13a38774,0.20512820512820512
9822,49ee86d074de69,0f8244ff,"<a id = ""8""></a><br>
## Remove Correlation Feature
* Drop features if they have 80% correlation ",71ccc6d3,0.20512820512820512
9823,e169603b62be56,d6662e12,"the ""info""  function shows that there is a lot of missing data so as we calculate the percentage of the NaN value we found that **Alley** **PoolQC** **Fence** **MiscFeature** have more than 80% of data is NaN so the feature is not reliable we gonna exclude them",8c311ec1,0.20512820512820512
9828,4bbe953f82d29b,cadc0728,#### Проверка на монотонность,772301f2,0.20512820512820512
9837,91473a39b85068,259242a0,"### Data preprocessing
Checking for duplicates",6e3d91c2,0.2054794520547945
9841,fdc9f4863744b1,b677eeba,"Part of the cleaning process I will look for the missing and duplicate data. 3 Steps to cleaning the missing data, I will identify them and either remove, correct or replace them.",b4529365,0.2054794520547945
9843,396bc36edb95d3,a2a957a2,#### Plotting Commission with categorical variables w.r.t Claimed status,965e4f8f,0.20555555555555555
9844,c84925c8171900,15908259,"<a id=""nullcal""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            3.3 Null Value Calculation
            </span>   
        </font>    
</h3>",e21ff7ec,0.205607476635514
9846,56785caebaa256,5f141e4a,"## 3.1. Holidays with a shift<a class=""anchor"" id=""3.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.20567375886524822
9848,b61ab8f81dc03d,6804020d,"<a id=""prepare_the_data""></a>
# Prepare the data
Prepare the data means transforming raw data into data to be used for the machine learning algorithms. A good example is the column ""Sex"" (Male, Female), so you need to transform these data into numbers (Male=1, Female=2) to be used for the ML.",64d05394,0.20567375886524822
9851,e4c6dd957eb5ce,74a32bb8,## Kernels summary,2e383665,0.20588235294117646
9857,02b7e38902069e,d69d2f2e,#Install inltk,726a03a0,0.20588235294117646
9858,50fcff6c0425fa,d9fb993c,"# features I will be using:
### Basic features:
1. categoryId
2. channelId
3. comments_disabled 
4. ratings_disabled   
5. difference of days between getting published and getting trended
6. age of the video

### Future feature engineering:
1. Add tags as a categorical features",8d355348,0.20588235294117646
9859,a0a5baa6c7e12a,6e2ac9e8,"Reviewing the box plots above reveals a lot of interesting insights
- There are certain numeric features that have strong association with the target variable (*Cover_Type*) and thus they are going to be quite good predictors (for instance, *Elevation*, *Horizontal_Distance_to_Hydrology*, *Vertical_Distance_to_Hydrology*, *Hillshade_9am*, *'Horizontal_Distance_to_Fire_Points'*)
- Other features seem to be less strong predictors in terms of their association with the target class labels (howerver, it  does not equally justify excluding such features from the model training in ML Experiments)",551d41de,0.20588235294117646
9860,71b75664517244,5489ed32,"Manchester United has 23 time participate in UCL champion league, almost in every season finish in top 4. What an achievement. Follow by arsenal with 21 times, and the so on.",fc905af5,0.20588235294117646
9862,156bbcff05dcea,7f6d25ce,# Feature Engineering,66ad1fe9,0.20588235294117646
9863,9cec5ddf8b6f49,6fa3a439,### Neither we have any missing values nor any duplicates,d39fc8e7,0.20588235294117646
9867,21bce4ec54b3fa,9d7ac79c,"Quite large dataset with 200 numerical features, no missing values and all variable names anonymyzed, target class is inbalanced.",35546e30,0.20588235294117646
9871,52cfd66e9ec908,df3117ad,"So, there's a lot of information in this one image. I'll try my best to point everything out, but do notify me if I make any errors. OK, let's get started with dissecting the image:
+ We have an intersection of four roads over here.
+ The green blob represents the AV's motion, and we would require to predict the movement of the AV in these traffic conditions as a sample.",c74adcdf,0.20588235294117646
9872,ab657da5329e3f,8bd28f6a,# Configuration,021526f8,0.20588235294117646
9873,dc0b0e1cb46c6f,27b6d12c,### Lets transform date column,47b17a7b,0.20588235294117646
9875,c65a65d4041018,0ae155ae,It seems that there are two main clusters of kagglers based on education and age: bachelors of 18-29 years and masters of 22-34 years.,824fb229,0.20588235294117646
9878,8ec771f5600a61,9f073b62,###  This output clears more about the data of male aged less than 16. As we can see all chilren of Pclass 1 and Pclass 2 survived but not the same for children of Pclass 3.,48364c1f,0.20618556701030927
9879,2a123b4e8f9433,6ce68442,"# No visible outliers, binary data",0a082218,0.20618556701030927
9881,fdbbd573ba31c2,3b68769c,## About Each Feature ,f7c28d74,0.20625
9883,06ecf7a304c309,5de6ee63,"#### 3. Create Autoencoder architecture

이제 오토인코더 구조를 만들어봅시다. 인코딩 부분은 3개의 레이어로 구성됩니다. (2000, 1200, 500 노드로 구성된)
인코딩 구조는 잠재 공간에 10개의 노드로 연결되고, 이 10개는 다시 각각 500, 1200, 2000개의 노드로 구성된 3개의 디코딩 구조로 연결됩니다. 그리고 마지막에 처음 인풋과 같은 노드의 수로 맞춰줍니다. ",714de627,0.20634920634920634
9884,4b4117cf42ef8d,7c5dbcaf,# Determine the Features & Target Variable (Lable),457cd6f4,0.20634920634920634
9887,b01ee6cb674fa3,4c01040c,Company name stands for Iranian Revolutionary Guard Corps,a8ffd35e,0.20652173913043478
9894,c18c37441caa8d,641e96e8,> ** Processing - Data used for the models **,ca98414d,0.20689655172413793
9895,00001756c60be8,8a4c95d1,"*Описание датасета*

**Id** - идентификационный номер квартиры

**DistrictId** - идентификационный номер района

**Rooms** - количество комнат

**Square** - площадь

**LifeSquare** - жилая площадь

**KitchenSquare** - площадь кухни

**Floor** - этаж

**HouseFloor** - количество этажей в доме

**HouseYear** - год постройки дома

**Ecology_1, Ecology_2, Ecology_3** - экологические показатели местности

**Social_1, Social_2, Social_3** - социальные показатели местности

**Healthcare_1, Helthcare_2** - показатели местности, связанные с охраной здоровья

**Shops_1, Shops_2** - показатели, связанные с наличием магазинов, торговых центров

**Price** - цена квартиры",945aea18,0.20689655172413793
9896,9535bb04ae042c,62654f30,"## Note: As the main purpose of this notebook is deployment, I performed EDA and experiemnts separately and found that:
## 1)SVM, as compared to other classification algorithms works best for this problem.
## 2)Removing outliers(if the particular feature is normally distributed we use z-score method to remove the outlier, if the feature is skewed either positively or negatively we use IQR formula to remove them) has no effect on the accuracy, infact it becomes 1% less accurate.",165b6fae,0.20689655172413793
9897,6b54e39f86bdb5,05a99be4,"## Convert the panda dataframe into a numpy array

Convert the panda dataframe to a numpy array. Shuffle the data using a sklearn tool and show a random example image.",198084bc,0.20689655172413793
9898,d5f78aa381f58d,2fcfbed3,Visualise the correlation matrix to see whether the features are positively or negatively correlated with the target (output).,d60f358f,0.20689655172413793
9907,fb5c6021d127ef,4e33874c,"## Training data

First, you'll write a query to get the data for model-building. You can use the public Austin bike share dataset from the `bigquery-public-data.austin_bikeshare.bikeshare_trips` table. You predict the number of rides based on the station where the trip starts and the hour when the trip started. Use the `TIMESTAMP_TRUNC` function to truncate the start time to the hour.",dd05cbd3,0.20689655172413793
9910,bb8f5d7807718b,28ed342f,"# 2. Broken Barh - Broken Horizontal Bar plot

A “broken” horizontal bar plot is used in situations when the data has values that vary considerably — for instance, a dataset consisting of extreme temperature ranges",181ec286,0.20689655172413793
9911,ee9ddc756b2d4a,6dd97e92,## train - test split,e367eab3,0.20689655172413793
9914,ef6d1e959a873e,95166364,"Note that the 'Response' is the target variable and missing values are ones in the test set. So we need not worry about it. But we’ll impute the missing values in 'Health Indicator', 'Holding_Policy_Duration' and 'Holding_Policy_Type' in the data cleaning section.",f11a1f43,0.20689655172413793
9916,25ed87d1f0cb06,c720d5cf,"# Load the Images with a generator<a class=""anchor"" id=""2""></a>",7ca68782,0.20689655172413793
9919,00d295edcd117e,edfd4ad4,## 读取和归一化CIFAR10,f5810f4b,0.20689655172413793
9920,656185a18260be,5c9cbb5f,"# HuggingFace QuestionAnswering Trainer

I discovered this quite late in the competition - HF has a QA trainer with implemented SQUAD metrics for validation. After I moved to 3-fold setup this allowed me to monitor the intermediate checkpoints. ",0318cab5,0.20689655172413793
9925,6a1d04e8153df3,0402abd3,"Okay! It's great 225 patients are survived and unlucky 81 are dead.
",38572b05,0.20689655172413793
9927,45921c50ac56fa,61bec237,"# 1.1 Pre-processing Input

## 1.1.1 Punctuation tokens
We will need to remove punctuation tokens from our text. 

## 1.1.2 Work Tokenization
We are using NLTK's word tokenizer for this task. It splits the string by ' ' and returns individual tokens.  
There are many other tokenizers available. I have personally used Spacy's tokenizer as well and it has given me good results.",465973eb,0.20689655172413793
9928,d42518f6cb0995,126d17ff,**Answers by users**,26913a9b,0.20689655172413793
9929,1dd9c6aa74d289,77262566,### Bouldering,5ef9a1be,0.20689655172413793
9931,a1ba5ffd30dbde,ed1c43db,- There are some outliers as we can see some attributes have huge difference in their 75 percentile value and maximum value. ,48e57546,0.20689655172413793
9934,401338428b2d1c,691ca31a,## Splitting the dataset into the Training set and Test set,e4b768be,0.20689655172413793
9935,1750367e54f407,cc96dce1,The code below allows to load the data from the dataframes using `tf.data`.,a8e655b2,0.20689655172413793
9937,9c26c5dcd46a25,6e8b9643,On peut également visualiser l'**évolution des différents nutrigrades dans le temps** :,1bbbb677,0.2073170731707317
9938,fd4017c1514157,2ecd4530,"### **Now, Let's explore each column**",fd8f0896,0.2073170731707317
9939,4c47839b067546,97f3314d,"Все признаки полные кроме 2 , (Владение  и complectation_dict). удалим эти признаки:                            ",1f517b02,0.2074468085106383
9948,510b8303776bb6,1e856ca3,# Data Visualisation and removing of outliers: ,18080db8,0.20754716981132076
9952,d07915a6e6992e,23957c8c,"Based on data above, female passengers had better chances of survival than male passengers",2b912140,0.2076923076923077
9953,09751c520b0616,d2c5f9d4," - Filling null value<br>
 Make two lists<br>
 cat_low_null - low null value <br>
cat_high_null - high null value",a4d0c7e9,0.2076923076923077
9955,75adb7945ef9bd,895bdbea,There is no common top 10 keywords between disaster and non-disaster tweets.,785c5095,0.2077922077922078
9959,90691864eb68c7,92553d53,Outlier Treatment,3555ef9b,0.2077922077922078
9962,726833f92fb87a,9838ef45,## Age analysis,7dc5e1b6,0.2080536912751678
9965,e2a907e1c7d7f9,38a7513b,## Separate Data,f09fb692,0.20833333333333334
9967,e95239c8f38005,3859f9f8,its a opencv demo to show how images look here we have taken a random image and try to print 3 versions of it A)original image B)by using Gaussian Blr c) by appling Threshold,e2632312,0.20833333333333334
9969,593d1d3d1df05a,f47161ab,# Selecting Boxes by Char Size,bc682ffe,0.20833333333333334
9970,386c42a7fb27a4,a9bbc6c3,## Visualization,9e9f6974,0.20833333333333334
9971,eb0854a6601407,81cb5bfb,"# Train Data Fields

tl;dr - we have time series data but don't know the exact time periods being provided. We also have investment_ids that are not unique. Everything is anonymized so it's not easy to create features.

- `row_id` - A unique identifier for the row.
- `time_id` - The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.
- `investment_id` - The ID code for an investment. Not all investment have data in all time IDs.
- `target` - The target.
- `features` - [f_0:f_299] - Anonymized features generated from market data.",6d107747,0.20833333333333334
9974,56a583a039b57c,8eeab7db,### Aggregate on Mean,c0526ea5,0.20833333333333334
9975,7ba63a2d9abb58,f9277904,<h2>Total Confirmed Cases by Country,821a261f,0.20833333333333334
9978,1d5daeca89f48d,8cd22735,"# Count Vector

  Simplest form of text representation in numbers",48d478bc,0.20833333333333334
9980,69ac33d79f5130,d2a04eb0,#### Filterout the missing value.,9d760d2a,0.20833333333333334
9985,62487bcd70b199,c813450b,# <a id='4'>4. Exploratory Data Analysis</a>,f6ae50af,0.20833333333333334
9990,166a62ebb4fc3a,470bded6,"Now, lets quickly go through the data types of each columns",db48a079,0.20833333333333334
9995,e777353e35d291,77164fc2,"---
## Top-Ten deadliest Counties by State",228f479b,0.20833333333333334
10003,a69d41047fdd3e,886da781,"# Exercises

### 1) Count tables in the dataset

How many tables are in the Chicago Crime dataset?",b1f28647,0.20833333333333334
10006,2a377ced98d67a,ea5c316e,## 3. Visualization,262231a8,0.20833333333333334
10007,2ada0305b68956,102ec306,### 33. Palette = 'PiYG',133e26f4,0.20857142857142857
10010,5f4ae633cfd090,10985762,***Building a baseline model***,a30a16e2,0.2087912087912088
10014,21413205980558,12b8d590,"# We can find that there is no missing data in this data, so there is no need to interpolate the data. The only problem is that the ""- 1"" in pdays is not cleaned.
# 我们可以发现这些数据中没有丢失的数据，因此不需要对数据进行插值。唯一的问题是pdays中的“-1”没有被清理。",84197de0,0.208955223880597
10015,ba655a261cc09e,ff126049,"Indeed, the absence of a record of age and stateroom is a good sign

Действительно отсутствие записи о возрасте и каюте является хорошим признаком",48cc549a,0.208955223880597
10016,20b372b6e4e276,a57d18f3,"### Commit 19

* Dropout_new = 0.125
* n_split = 5
* lr = 3e-5

LB = 0.711",ec8b0860,0.208955223880597
10018,22bd95f4807a23,1fb0f5aa,"## Function for creating age groups
Instead of using the ages, we will try to group them into buckets. The bucketing function for the same is defined as follows. We will start deep diving into the dataset by asking questions.",c05d356f,0.20930232558139536
10020,2e40928927c0d4,756c98c4,**Showing randomly chosen No-DR image one at a time** ,b6385ef2,0.20930232558139536
10027,743ae010f5e875,adac2381,# Literal Matching,02c54445,0.20930232558139536
10028,c09fac3c943d51,e1740844,Comparing categories in train and test:,678d076d,0.20930232558139536
10029,63b44c85e32c1f,b6f68ebb,"To find the length of the list or the number of elements in a list, **len( )** is used.",fb9b9562,0.20945945945945946
10031,7454fdc444df16,705260d3,"### How many patches do we have in total?

#### Which of them are IDC patches and which are Non-IDC?
In order to train our model we need to feed our model each patch individually, therefore each patch will act as an input.

The snippet below loops through the entire file structure and extracts the total number of crops for each of the classes.",a7818ef5,0.20952380952380953
10032,7a058705183598,e8be2c16,Pairplot,b0ead917,0.20952380952380953
10033,04bac111ffbe9c,d1576de5,"###### More than 77% of the values in Cabin are missing. Since it is impossible to replace so many missing values without introducing errors, we remove the feature named 'Cabin'.",82576b17,0.20952380952380953
10034,55a5e31d03df9f,26b64167,"## <a name=""mlpcompile"">Create and compile MLP model</a>",06dce00f,0.20952380952380953
10037,9ceb7278784462,7fd38fc1,## <a id='9'> 6.Reviews </a>,3768a567,0.20967741935483872
10039,ad26c020235dfc,3166cdf2,"# Targets
Store the targets of every waterbody in a dictonary by definition.",bf766e48,0.20967741935483872
10041,4883314a96dc34,b9c57fe1,# Exploratory Data Analysis (EDA),50d36836,0.20987654320987653
10042,faa8e6c8ab9246,61d43e79,Lets remove the variables which are not significant in this data.,2bea1419,0.20987654320987653
10044,83df814455f06c,c752eaec,"# **9. Exploratory data analysis** <a class=""anchor"" id=""9""></a>

[Table of Contents](#0.1)


Now, I will explore the data to gain insights about the data. ",c9cff71a,0.21
10050,0e2a23fbe41ca9,03ccec57,"Observations:

- last 6 months (July to December) has relatively more data than first 6 months (January to June)

### 4. card_id",64e4762c,0.21014492753623187
10051,7e89d387feb9f5,a5a45d94,## 2. Информация о ценах,989e3a1b,0.21014492753623187
10052,5ce12be6e7b90e,66ce6d6c,Power:,c0ab62dd,0.21052631578947367
10059,a1dcd92986bc84,c5645c1c,"### Process and save the data to TFRecord files

You can change the `sample_size` parameter to control many image-caption pairs
will be used for training the dual encoder model.
In this example we set `train_size` to 30,000 images,
which is about 35% of the dataset. We use 2 captions for each
image, thus producing 60,000 image-caption pairs. The size of the training set
affects the quality of the produced encoders, but more examples would lead to
longer training time.",730acaaa,0.21052631578947367
10060,caa0ce2715bf34,b6cf8314,"No Duplicates present in the dataframe.
## Drop unique columns for Analysis",78a5dc51,0.21052631578947367
10061,d369f200a84c2a,7a012bb5,"A CapsNet is composed of two main parts :
1. a convolutional encoder
1. a fully-connected, linear decoder",8fef4d48,0.21052631578947367
10066,26b93b6f4dc148,87b36d32,### Splitting Date Column,6f667d22,0.21052631578947367
10068,c3498779cda661,5b469b2f,"Como las columnas son boleanas o tienen un rango definido, no hay valores atípicos.",0f531b65,0.21052631578947367
10073,29437539745aa5,7808b7b1,"<a style=""text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"" id=""background_information"">1&nbsp;&nbsp;BACKGROUND INFORMATION</a>",c17b490a,0.21052631578947367
10074,99afe9f3af6dbc,10a0afb6,Terms adalah vocabulary.,cdec9b3a,0.21052631578947367
10075,c2a9f2fb3e1594,7341c8e8,"## 3.22 Clean Data

Now that we know what to clean, let's execute our code.

** Developer Documentation: **
* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)
* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)
* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)
* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)
* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)
* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)
* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)
* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)
* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)
* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)
* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)
* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)",53411c04,0.21052631578947367
10081,9e27af2600925c,37d85c8b,"**Expected Output**: 

<table style=""width:35%"">
  <tr>
    <td>**train_set_x_flatten shape**</td>
    <td> (12288, 209)</td> 
  </tr>
  <tr>
    <td>**train_set_y shape**</td>
    <td>(1, 209)</td> 
  </tr>
  <tr>
    <td>**test_set_x_flatten shape**</td>
    <td>(12288, 50)</td> 
  </tr>
  <tr>
    <td>**test_set_y shape**</td>
    <td>(1, 50)</td> 
  </tr>
  <tr>
  <td>**sanity check after reshaping**</td>
  <td>[17 31 56 22 33]</td> 
  </tr>
</table>",9b556435,0.21052631578947367
10082,30fdc4a6e3c1db,591c7801,We have almost 70% sales coming only from FOODS category.20% from HOUSEHOLD categories and a minor 10 % sales from HOBBIES ,6111ddee,0.21052631578947367
10084,f91f58d488d4af,48b9c950,"Now,we want to compute the average over all the images of the intensity of that pixel. 

* To do this, first we'll combine all these images in this list into a single 3D tensor or a *rank-3 tensor*. So we need to stack up individual tensors into a single tensor.

* **PyTorch** comes with a function called `stack` which can be used for this purpose.

**Note**:

1. To take a mean, we have to cast our *integer* types to *float* types.

2. Generally, when images are floats, the pixel values are expected to be between 0 and 1. So, we divide by 255 here.



",5df1bbf3,0.21052631578947367
10087,54004b32784b68,edce29f5,# 2-1-Determination of Missing Values,27213ca9,0.21052631578947367
10095,fe7360cddc13e5,2a35d708,------------------------------------------------------------------------------------------------------------------------,8979e423,0.21052631578947367
10097,52ee792e228d54,8a04f39b,"### We can see that the number of customers who took the loan last year(our target variable) were very less (<500) compared to the set of customers who haven't taken the loan(>4500). As such, the dataset is highly imbalanced. (Will this affect our algorithms? 🙆‍♂️)
#### Very few customers have Securities account or CD account. (< 500)
#### Majority of the customers do not own Credit Cards as well. (>3500)<br>

#### Let us check the distribution of the continuous features now.",5096094e,0.21052631578947367
10098,bef2347846e476,5214b556,To see which types are exist in the Type column we use the command data.Type.unique().It returns the unique column indexes.,cb93bf51,0.21052631578947367
10101,bd380b97b5c894,0b7e8561,## benign_malignant,66f2562a,0.21100917431192662
10104,3597174a998d4d,b2b0ba8d,"The value range of days_in_waiting_list is large, and most values are 0. I'll construct a new variable named days_in_waiting_list_new.",276892ed,0.2111111111111111
10105,d6cbd7160961dc,eb3d3fc8,---,36d74664,0.2111111111111111
10106,892be0a523578c,9aefd9e5,"**3.3** I calculated the daily values from the merged dataframe, and by comparing it to the given dailyActivity_merged.csv, I found that although data of the most participants from both datasets matched, some had a clear gap between the two datasets, such as Id 4319703577. Then I looked into the hourly dataset and found that some data was missing, for example, total steps of Id 4319703577 on 2016-04-12. So if we need to analyze the data from the hourly dimension, we need to exclude these abnormal records.",b0e8d7c0,0.2111111111111111
10108,bddd799cdbbae8,fdbfc1c2,# <a id='4'> 4. Data Visualization</a>,b44e3c08,0.2112676056338028
10112,631cd434fc3aa2,1e470dea,"Checking the other features indicated by Pedro Marcelino in his [Kernel](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) as important:
* _OverallQual_: Using a boxplot we see some values (actually two) that could be candidates for outliers; nevertheless, these are the only observations for very pool quality houses, maybe we should keep them.
* _YearBuilt_ and _TotalBsmtSF_: some observations could be outliers, but since they're not so extreme let's keep them.
",2b74febb,0.2112676056338028
10115,81712ee7510ac5,302d7c59,**Print staff based on the variable**,c4685e79,0.21142857142857144
10118,44f6a002ecd033,6deb70b5,"We seem to have a pretty equal distribution of property area, semiurban being the most popular area.",70bbe106,0.21153846153846154
10119,d0f6276d5b628c,9180f0a9,We can conclude here that some movies have been rated by more consumer and some are very less.,c64f5ce5,0.21153846153846154
10120,21122355e39af4,57a5a0d5,"QUESTION: Clusters that include samples from totally different classes totally destroy the _homogeneity_  of the labelling, hence:",88b95e2b,0.21153846153846154
10122,95efc1ad1d3e26,a423d7e8,## 1.2. Model FC,79de1120,0.21153846153846154
10123,99bf357eaf61f1,3fdc8955,Let us analyse the continuous values with data visualisation to understand the data distribution,9d92fafe,0.21153846153846154
10125,6f1481148352e9,e2e58ca7,"**For geo-visualization, let's create columns with latitude and longitude. I took the data from [@sahib12](https://www.kaggle.com/sahib12) in his excellent [work](https://www.kaggle.com/sahib12/data-vizualization-for-brazil-forest-fires).**",7cfbdb8f,0.21153846153846154
10131,1294fb4c86f993,791a6a01,> <b>Seems about right. We can then fill the NaN values with zeros.</b>,4471e513,0.211864406779661
10132,9169c4e9c33c90,8b64b714,"<a id=""Dataset_Overview""></a>",725bf880,0.211864406779661
10140,b241b847319d13,4aa477e8,# **Reading csv file**,0fb698f0,0.21212121212121213
10141,a2444ab5d5f147,b726bc52,## Let's check the sentiment distribution in the train set,10617755,0.21212121212121213
10142,b42180a6a5b42f,31972e14,"#### Temos aproximadamente 73 dias de dados inseridos, desde as primeiros óbitos confirmados Covid-19 no Brasil. 
#### Todavia, temos 1896 inputs.
#### Isto ocorre porque os dados são atualizados diariamente de forma repetida.

Para tanto, precisaremos realizar o agrupamento pela data ('date'), bem como agregar os números de mortes' (new_deaths') aplicando a soma de eventos repetidos no mesmo dia.",987cea5f,0.21212121212121213
10143,4b64dc653fb7eb,946d3d7e,"Now extracting comments from train and test data, and storing their index for later use.
Merging comments for both train and test, so that Preprocessing Steps can be performed on both at same time.",57675cc2,0.21212121212121213
10151,fdbbd573ba31c2,438ee718,### wind_speed(m/s),f7c28d74,0.2125
10152,5ffe6aa38958a1,d17b8239,"## 2.3 Analyzing the data  

Statistical Characteristic: 
This is important, because you can use this information to normalize the data, and also get insight into the data",11f5412e,0.2125
10154,3dd4294f903768,0373ed2b,We can see that most of the tables havn't been booked.,0d89d098,0.2125
10156,b61ab8f81dc03d,1c2b9932,"<a id=""missing_values""></a>
## Missing values
Let's check which data is missing and treat them.",64d05394,0.2127659574468085
10157,56785caebaa256,499f3929,### Thank to dataset [COVID-19: Holidays of countries](https://www.kaggle.com/vbmokin/covid19-holidays-of-countries),a792961a,0.2127659574468085
10160,04e6b0d3c70f46,7e72dd5b,"### Data Cleansing
#### Balance audio length
* One thing we can do is to drop excess audio files for birds.",56344f77,0.2127659574468085
10164,3f25b363afec54,d7d7893b,## Let's read all the data files ,bbdaae25,0.2127659574468085
10165,5f674175839b32,cddd4879,"**<font color=aquamarine>For analyzing we are taking only Global sales.**
",53a2e343,0.2127659574468085
10166,2f0f808765fc67,92a60292,# **B. Data Wrangling **,fd1f6494,0.21296296296296297
10167,fdc3afd309b850,f767df84,Now there are 847 missing values in Neighborhood and the new Address column.,966bde38,0.21296296296296297
10172,0858e1bb3cbaca,01836a31,# Select,78548374,0.21311475409836064
10178,37e461081e47c5,b2b65dec,"Based on daily data, it is evident that we are missing a lot of data from 2015. We will end up filling the data with zeros, in effect assuming that there were no sales in any of the stores for the days where data is missing. However, that assumption does not appear to be accurate, especially since the December time-frame has always been a peak time for shopping. ",b3e6549e,0.21333333333333335
10179,91eaec994e0c6f,f4de24d2,<i>From 1913 days there are only 4 days with type 2 event! As a result we'll ignore that event type.</i>,376aef10,0.21333333333333335
10181,e5dd725b8fa422,abe806df,# [Data Spliting](http://),14675d8b,0.21333333333333335
10182,2bd6c370695ea7,7a2db851,### State stats,cbe6aec8,0.21333333333333335
10190,312135b445bd23,600b3c8e,Let's see the output of ETL process:,8ced381f,0.21348314606741572
10192,98a6794067932a,20530da0,"**2.3 Analyse des commandes clients**

Dans cette section nous allons analyser le comportement des commandes clients afin de mieux pouvoir orienter nos suggestions par la suite. Il est utile pour l'équipe de gestion de bien saisir le comportement actuel de leurs clients, car cela permettra aux dirigeants de baser leurs décisions stratégiques qui affecteront le futur de l'entreprise en fonction de données représentatives et fiables par rapport à ce qui les attend pour la suite. Plus précisément, nous allons analyser le comportement des clients en fonction de plusieurs critères comme le type de clients et le type de produits par exemple.
",08600fe2,0.21359223300970873
10198,ee23a565163388,1cb5a107,"**Inference:**
- The median age of patients whose heart failed is higher than others.
- This makes sense as the elder patients tend to suffer from heart failure.",88aacbc4,0.21374045801526717
10202,400bbcc496138f,d13e25ae,Predictions with default threshold of 1.5,191b86b8,0.21428571428571427
10208,9c044fa3072552,f4b24ba8,"# Exploratory Analysis and Visualization
Columns we'll analyze 
- City
- Start Time
- Weather Condition [TODO]
- Temperature [TODO]
- Start Lang, Start Lat",1362842e,0.21428571428571427
10211,2730840089c8eb,1e26c3f2,The `print()` function automatically adds a newline character unless we specify a value for the keyword argument `end` other than the default value of `'\n'`:,34d27dac,0.21428571428571427
10212,92e9fc3a0ff5c0,c1fbffc1,## **Biden Reviews**,d53da425,0.21428571428571427
10213,8017d8ece39e95,6e54a441,# **EDA**,868ff74e,0.21428571428571427
10217,0d59a3e0130db0,bb53424b,"Some comments were last links or sets of numbers, so they should have been left blank after clearing the text. I will remove lines containing empty comments",285f04b2,0.21428571428571427
10226,c54ea4523bd49c,210fc14f,Build a dataframe of all the x and y data. I then build a more even dataset of 50% 0 and 1 to help the training process.,097ccba2,0.21428571428571427
10236,f13534449a3750,7048fb5a,### Loading the Datasets,8b7f3332,0.21428571428571427
10245,38b79494ac749e,1d6d318f,### Model,39162a40,0.21428571428571427
10247,2d40f383473fa4,3c75e534,"Identify the Cabin Deck location with the first letter of `Cabin` feature. It's important to predict the survivors, because the deck location will measure the distance to the ship staircase.<br>
Read this excellent [Kernel](https://www.kaggle.com/gunesevitan/advanced-feature-engineering-tutorial-with-titanic) for more information.
",1da1eff0,0.21428571428571427
10251,f4514ec092a771,1de64a77,Create text file with format: utterance_id -> transcript,3739ab1e,0.21428571428571427
10252,adb8441ad28019,c9f79f6e,"## <span style=""color:seagreen;""> Heatmap Correlation </span>",d89de993,0.21428571428571427
10257,99f84fa59cb1da,853274b1,"### Summary:
- We have 19 categorical columns
- We have 13 categorical columns
- There are no missing values
- There are no duplicates",41e95f63,0.21428571428571427
10267,b8ffad33f2b369,862d4be2,"### 2. The Data Set

In the following cells, we will import our dataset from a .csv file as a Pandas DataFrame.  Furthermore, we will begin exploring the dataset to gain an understanding of the type, quantity, and distribution of data in our dataset.  For this purpose, we will use Pandas' built-in describe feature, as well as parameter histograms and a correlation matrix. ",484e5560,0.21428571428571427
10270,2ada0305b68956,203df613,### 34. Palette = 'PiYG_r',133e26f4,0.21428571428571427
10271,1a285e4c830f3f,5edd6fec,"The ""default"" values in the following classifiers are already updated using the validation curves shown later. ",360b50e9,0.21428571428571427
10277,b4ecd6e4277e3c,2e5c4e2a,## LOAD PROCESSED TRAINING DATA FROM DISK,94d79d5f,0.21428571428571427
10278,67efe818cb2372,bcf4d8ee,"**First Attempt  (VERY Naive)**

Very much a first attempt; simple convnet, no data augmentation, no dropout, no weight regularization in the dense layers etc.",f28a2a34,0.21428571428571427
10279,1084376bc4897c,ba9ca35b,# 3. Exploring the data,1b598487,0.21428571428571427
10280,5f32117bcd5255,a4f79925,# PLANETARY SCIENCE KEYWORDS,85882abf,0.21476510067114093
10283,2f47abddfd1928,d0aedbc0,"We can identify that the features that shows some correlation with Age are:
- Pclass, strong inverse relation (the higher (1st, 2nd and 3rd) the class the higher the age.
- SibSp, it looks like that travel with spouse (SibSp = 1) might mean that you middle age or more. But if SibSp is higher than one usually you are child traveling with siblings.
- Parch, similar reasoning applies to Parch, just 1 or 2 means you are a child traveling with parents but more means that you have children. Both combined should be quite meaningful.
- Fare, this is very related to Pclass as the higher the class the higher the fare. To be able to group using fare we will need to bin the feature first.",ae33cc0b,0.21487603305785125
10289,d07915a6e6992e,918d0dbf,"**Age**

![Age.jpg](attachment:Age.jpg)
The insight below connects back to ""Ladies and Kids First"" scene of the movie. It shows that a good number of babies & young kids survived.",2b912140,0.2153846153846154
10293,a4f8ad33c823c5,443a7033,"From the above plot, it is observed that vital signs recorded hourly generally have more missing values compared to the vital signs recorded over 24 hours.

It is important for us to ask the question of whether to examine vitals during the first hour or the vitals that had been recorded over 24 hours. 

Since, there are more missing values for the vitals recorded during the first hour compared to the vitals recorded during the first 24 hours,we will only take the vitals recorded during the first 24 hours for our analysis. In addition, vitals measured over 24 hours are more reliable to better access the medical condition of the icu patients.

In addition, high number of missing values is observed for variables such as diasbp_invasive , mbp_invasive and sysbp_invasive. All invasively measured variables (min and max) have a high rate of missing values.Hence,these values will be removed from analysis.

Invasive (intra-arterial) blood pressure (IBP) monitoring is a commonly used technique in the Intensive Care Unit (ICU) and is also often used in the operating theatre.

https://www.webmd.com/hypertension-high-blood-pressure/qa/what-does-the-diastolic-blood-pressure-number-mean 

**diasbp**
The diastolic reading, or the bottom number, is the pressure in the arteries when the heart rests between beats. This is the time when the heart fills with blood and gets oxygen.

A normal diastolic blood pressure is lower than 80. A reading of 90 or higher means you have high blood pressure.

**map**

MAP, or mean arterial pressure, is defined as the average pressure in a patient’s arteries during one cardiac cycle

**sysbp**

Your systolic blood pressure is the maximum pressure that your heart applies when beating. Your diastolic blood pressure is a measurement of the pressure in your arteries between heartbeats. Pulse pressure is the difference between your systolic blood pressure and diastolic blood pressure.

https://www.nursingcenter.com/ncblog/december-2011/calculating-the-map ",fcd48307,0.2153846153846154
10297,c115e287523aab,a231698c,# Configuration,feb1288b,0.2153846153846154
10300,f3c6048d1058e3,ce7f1f56,## Text Preprocessing of Reviews,1d9056b0,0.21551724137931033
10301,fa02c409161192,7ad5e651,The code converts the lecun files into csv then Read new csv files. These files are then processed by renaming the columns and then saves them again. This is then the final processed data.,e97077f7,0.21568627450980393
10302,7cfd96218dd933,b61d5212,"#### **ATTENTION**
* MOSTLY HOTSPOT DATA WAS RECORDED AFTER JULY 28 IN THE SPECIAL LOCATION WE ARE ANALYZING.
* THIS IS THE FIRST START TIME OF FIRES IN TURKEY.
* AFTER JULY 28, THE DIFFERENCE BETWEEN OTHER DAYS HAS INCREASED QUICKLY.",7c34d96c,0.21568627450980393
10304,64169805aacf17,c99be672,# Getting ingredients ready,1f12ded0,0.21568627450980393
10308,52cfd66e9ec908,9ae5ad24,"I don't exactly know what other inferences we can make without more detail on this data, so let's try a satellite-format viewing of these images. ",c74adcdf,0.21568627450980393
10309,917957c6c4065f,6977ec88,#### tags & tag_count,55b8ed68,0.21568627450980393
10312,d0080e3a39bc5c,2ddd9c66,"
* So moving forward, I first extracted 175 random images from each class (175 times 5 = 875 images) and separated it out as my Validation set. 


* With the remaining images of each class, the classes were all oversampled too have around 2000 examples from each class, leading to a inflated training set of 10000 images and 875 validation set images for my model to train and evaluate on.",2fcde4cf,0.21568627450980393
10313,4fa553c2b837d4,8470ef52,# Accuracy Check,c65a23e9,0.21568627450980393
10314,71b75664517244,be255163,"## Interesting Fact

it os time to gather some interesting fact in this dataset. Fist let's take a look on how many teams play for premier league the most.",fc905af5,0.21568627450980393
10315,a566b5b7c374e7,c75131a3,"#### Initial Impressions (Continued):
- As expected, it appears there is a ceiling for Sleep Scores by sleep stage (REM, Deep, Total), and that the scores depend entirely on the sleep time calculated for each stage. Because the sleep scores have a max, and the sleep times do not, ***I will use sleep times for analysis.***
- The exeption is ""Sleep Score,"" which takes into account more than the amount of sleep time. According to the Oura app, ""Sleep Score"" contributors include ""REM Sleep Time,"" ""Deep Sleep Time,"" ""Total Sleep Time,"" ""Efficiency,"" Restfulness,"" ""Latency,"" and ""Timing"" (as in start and end times). According to the data, ""Sleep Score"" is most highly correlated with:
    - Total Sleep Time - 0.59
    - Sleep Efficiency - 0.52 
    - Deep Sleep Time - 0.55
    - Light Sleep Time - (0.38)
    - REM Sleep Time - 0.27


The following direct measurements from Oura correlated most strongly with ""Average HRV"" over the first month:
- Average Resting Heart Rate - (0.90) - This is to be expected.
- Deep Sleep Time - 0.75
- Respiratory Rate - (0.58)
- Light Sleep Time - (0.47)
- REM Sleep Time - (0.41)

The following direct measurements from Oura correlated most strongly with ""Average Resting Heart Rate"" over the first month:
- Average HRV - (0.90)
- Deep Sleep Time - (0.78)
- REM Sleep Time - (0.59)
- Respiratory Rate - (0.51)",b3dc5545,0.2158273381294964
10317,73d8e56bc709b1,53aef3bf,# 3. Find Talented Players,78ec3cce,0.2159090909090909
10318,0a918602a04693,510149f0,# Data Visualization,c1ef0e95,0.2159090909090909
10319,ccabe7a86825ce,c363339c,**Divide categorical columns on ordinal (where order is important) and nominal**,d766cbf9,0.21621621621621623
10320,d76896b30cebd3,1375e153,Distribution of state,1b4e8f34,0.21621621621621623
10321,bbb3f4b76a4559,36d2f23f,"### PCA to drop dims
As you can see, This dataset(droped some cols) has just 10 dims. But usually ML problem has a lot of features and this make ML difficult. So I some times use PCA to get features has more info less cols.  
At first, PCA with all cols (of course without ""Answer col""), then check explained variance ratio like this.",75185823,0.21621621621621623
10327,62037c5832129c,30c03231,# Using k-fold cross validation to assess model performance,61474350,0.21621621621621623
10333,b7b1057764fa02,141b6b61,"We see that we have a total of 29 symbols. These are the letters A to Z, and symbols for delete, space and nothing. Our total images from the first dataset, 87000 in number, have been split into training images - 78300, and testing images - 8700. We also have an additional 870 evaluation images from a different directory.

# 3. Printing images

We are now ready to print the images for the symbols. I will print one image for each symbol, from each dataset - train, test and evaluation. Since the same operation of printing images is done thrice, I will now write a helper function for this purpose. This function will create a grid of 8x4 images and fill 29 images in the first 29 of the 32 spaces.",5053a192,0.21621621621621623
10334,8276973853faa1,a896edb4,> Bar view of the data,88da542b,0.21621621621621623
10335,e4525eb0c96f28,78b009f8,"### ESRB and Genre vs Sales

To get started with independent variables, we decided to first use ESRB Rating and Genre to explore their impact on sales.

ESRB Rating could potentially be a fluid continuous variable that could show correlation since the ratings technically go from youngest to oldest (EC, E, E10, T, M). Potential hypothesis that could be formed from ESRB ratings could be sales tendency towards more accessible games if games rated E turn out to be the most successful, or even show that games tend to have an older audience if games rated T or M ended up on top (assuming young kids aren't playing Mature rated games, of course).

Even if no correlation is found, at the very least we could learn which ratings were the most popular and use it in future analysis.

Because there is a high number of data points, we used a scatter plot to first graph the two dataframes to show generally which genres/ratings were more successful. However, since the data of the popular dataframes was so dense, we couldn't really tell much from it.

We also dropped Wii Sports specifically because it's an outlier that made the data harder to visualize.

A general assumption that can be made is that the data is ordered from most to least sold, for example the Racing genre has sold more than the Sports genre. We decided to investigate this further. 

New dataframe:
- **ESRB_df**'s purpose is for cleaning ESRB_Rating data to be used in display. We also used it for Genre as well because the Genre column had no NaN values, and therefore we had no reason to use a separate genre dataframe since no extra cleaning had to be done.",2093a1f1,0.21621621621621623
10338,30fdc4a6e3c1db,f506e001,### Plotting Sales of each category across the 3 states,6111ddee,0.21637426900584794
10340,20b372b6e4e276,12be426e,"### Commit 20

* Dropout_new = 0.15
* n_split = 5
* lr = 1e-4

LB = 0.709",ec8b0860,0.21641791044776118
10341,063a35f644e3c5,a27cf98b,"### what those unique values are and counts for each analysis by grouping certain feature
",1c30fb0a,0.21649484536082475
10342,2a123b4e8f9433,e164be1e,# train_test_split the data,0a082218,0.21649484536082475
10347,712198370d5521,6f93d636,"In the next step, I am going to create a feature out of **""Dt_Customer""** that indicates the number of days a customer is registered in the firm's database. However, in order to keep it simple, I am taking this value relative to the most recent customer in the record. 

Thus to get the values I must check the newest and oldest recorded dates. ",5882e04c,0.21666666666666667
10348,62487bcd70b199,df20d17f,## <a id='4.1'>4.1. Personal loans distribution in data</a>,f6ae50af,0.21666666666666667
10349,b10bd75889dad9,f086923f,#### Impute missing data in numeric columns with median,ee00ceee,0.21666666666666667
10351,63d0d9b9a8c7d2,ac195c3a,# Deciding the required columns needed for X(features) and the response variable,e32e5933,0.21666666666666667
10355,37b09262279764,4f314b31,"***Pclass -> Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd***<br>
People who were in <b>Pclass 1</b> survived the most<br>
Most of the people from <b>Pclass 3</b> did not survived<br>

<b>Reason:</b><br>
- One reason could be more priority was given to Pclass 1 people than Pclass 3 people<br>",37c4c417,0.21666666666666667
10356,396bc36edb95d3,b71303e5,#### Plotting Duration with categorical variables w.r.t Claimed status,965e4f8f,0.21666666666666667
10357,b547f0f38f7744,6e574c94,The number of images in train dir:,b6ba66b3,0.21666666666666667
10359,eda49464dd6d1b,4efd9faa,"## Age Vs Annual premium 
* Little correlation, but we will consider it in the models later",8421f81f,0.21678321678321677
10360,835a7b4e660d23,2fadad9d,## <a id=plt>1. Matplotlib</a>,53bc7a6e,0.21686746987951808
10362,510b8303776bb6,dbc469f7,## Sale Price,18080db8,0.2169811320754717
10367,598b6228760590,d64cfe72,"- Age
- The age is mainly concentrated in the 20-40 years old.
- It's a bit unusual, the age is less than 0? Let's take a look.",be30ab66,0.21739130434782608
10374,7e275c8d5ff2a0,2b680889,# AGE DATA,b3afcc98,0.21739130434782608
10388,9b5de3823ad5ab,a844bc83,"### Labeling the images

This is probably not the most efficient way to do this, but we're going to get the labels by searching the training dataframe using the experiment, plate and well that we got from the images' path. This will be saved on a list, first, and then it'll be added on the dataframe with the images' path.",33e48774,0.21739130434782608
10393,72d528df923403,364d137f,"### Setting colors coherently
this is important to keep track easily to each variable. 
",d51c8e8e,0.21739130434782608
10396,90ead00a8ee283,f6604200,# Exercises,612efa48,0.21739130434782608
10398,69d50f5e1373f1,0d3f50b6,The `training` folder has 400 JSON tasks. The names of the first three are shown below.,ec7545ee,0.21739130434782608
10400,b01ee6cb674fa3,1782091f,"Doing a search, Northrop (Currently Northrop Grumman) is an american aerospace company. 
The Rocket is a Pegasus XL, an air-launched rocket for small payloads to Low Earth Orbit (LEO).

From a wiki search, Stargazer is an airplane, a Lockheed L-1011 tristar, that was acquired by Northrop and modified as a launchpad for the Pegasus Launch Vehicle.

that was modified in 1994 to be used by Orbital Sciences (now part of the Northrop Grumman) as a mother ship launch pad for Pegasus launch vehicle. As of October 2019, 44 rockets (containing 95 satellites) have been launched from it, using the Pegasus-H and Pegasus-XL configurations.
",a8ffd35e,0.21739130434782608
10403,fdc3afd309b850,cfcb3d70,"<a id=""nnv""></a>
### 6.1.2 Neighborhood Null Values",966bde38,0.2175925925925926
10405,80ad12f326ab70,b9c53986,* ### Products data,da404a16,0.21794871794871795
10406,d4c5aaa4b36810,d4ec0b7a,Lets start with some general stats that are likely to affect happiness or ones that it will be interesting to see the effect of on happiness. ,65441f28,0.21794871794871795
10408,4d91e84c564cbe,10a80b5c,"`planets[0:3]` is our way of asking for the elements of `planets` starting from index 0 and continuing up to *but not including* index 3.

The starting and ending indices are both optional. If I leave out the start index, it's assumed to be 0. So I could rewrite the expression above as:",355a43e3,0.21794871794871795
10410,ba4b3bd184acbb,50539032,"### Loc Method

The `loc` method can be used to access a single or set of specified rows and columns.",0f5de724,0.21804511278195488
10412,5a8c553e21c70f,5ebc4aee,"## Split Data

Dataset is split as training and test sets. We use stratify parameter of train_test_split function to get the same class distribution across train and test sets.",9ebd9d8f,0.21818181818181817
10414,016abae0483764,cfaea7ec,"We can see now the shape is 278594 rows to 9 columns.
<br></br>
Let's see what other patterns we can get from the dataset, Lets perform EDA... ",bc9f289b,0.21818181818181817
10421,4ae6a182abac64,9a793ffa,### 1.4 Data Visualization 📊📈,418676c5,0.2184873949579832
10422,2a724fb7835cdc,cc2d6891,Let's see if all the words in the test set occurs in the train set:,c38ac61d,0.21875
10433,96c4c0e36b8ec0,48acb423,"What the graph tells us:
* The most likely to survive are women in First class
* The least likely to survive are men in Third class
* Women have a much greater chance of surviving over men.

",4dd6de8c,0.21875
10439,3cc097a5859dc1,f3e17ffa,# **Feature Engineering of Date variable.**,14380d73,0.21875
10442,3c2033cc99c12c,e6487c8d,"*As is shown in the above, after cleaning the outliers bigger than Q3+2.5IQR and lower than Q1-2.5IQR would be removed from the dataset.*",dfa22a54,0.21897810218978103
10443,c80939c7c626cf,decc6460,# The Chart confirms a person aboarded alone are more likely dead,b9ac31e2,0.21897810218978103
10451,3d08ca7656dec0,95f2ccb5,## fbs ,bd3f87e3,0.2191780821917808
10455,6cade0b6a41ba2,0e98a1b5,##### Verifying if the value was imputed appropriately,e6110293,0.21929824561403508
10457,fe7360cddc13e5,e2c74218,"<font color='red'> **4- Pasif hale gelme olasılığı p'deki tüm müşterilerin heterojenliği, a ve b parametreleri ile bir beta dağılımını takip eder.**",8979e423,0.21929824561403508
10461,fd4017c1514157,aeeeda26,"### <font color='red'>Primary Label</font>
* A code for the bird species. 
* You can review detailed information about the bird codes by appending the code to https://ebird.org/species/, such as https://ebird.org/species/amecro for the American Crow.

",fd8f0896,0.21951219512195122
10462,5169abdc647412,91bc7257,## analysis on Equal distribution of values,28efc68d,0.21951219512195122
10465,74a03887600114,3d355065,We need to import our another dataset in which we have ratings for the movie,c0ffb2f0,0.21951219512195122
10470,e19e307b3fd188,e1e10057,"rents in these 4 cities have asymmetry on the right and do not usually exceed **2.500,00**.",2173955b,0.21951219512195122
10473,47b2c9be5e31cb,fdab622e,Now you're ready to read in the data and use the plotting functions to visualize the data.,7d4afe56,0.21951219512195122
10476,957e035ba5b9d5,71fdfa5f,## What is the distribution across the categories?,778ab3d3,0.2198581560283688
10480,10c5a39a87c47e,6b4b36e4,## Step 4: Data Preprocessing (Labeling & Resizing of images)<a id='step-4'></a>,09c7337a,0.22
10484,4cd25e50c7e007,266bc40e,**It's observed that bike rental count is high during fall season**,ceb0c525,0.22
10485,7dd46c750653eb,3850eec8,"The given dataset has only 4 months data of 2020

**Inference**

* The months of July and August has most number of Births over the years.",c2644713,0.22
10487,2ada0305b68956,b915c26b,### 35. Palette = 'PuBu',133e26f4,0.22
10488,91eaec994e0c6f,c99384ec,<b> Plots </b>,376aef10,0.22
10491,6a80f915608fc2,833fb7b7,"## <a id=""FeatureSummary"">Looking at the Features</a>
Back to <a href=""#Index"">Index</a>
",636938eb,0.22023809523809523
10493,149cb8d3489224,2d76f691,### Unique values,116858e7,0.22033898305084745
10495,a44368590e878a,00e2f644,### Age distribution of the deceased by gender,77743ba8,0.22033898305084745
10500,9169c4e9c33c90,3ed75857,"# Dataset Overview

[Back to top](#Top)",725bf880,0.22033898305084745
10505,b660910fcc2954,aa31cc51,"## Categorical features

We will use category count for the categorical features.",80b74f88,0.22033898305084745
10509,eb0ecd6bebeb15,0227274e,"Sayısal değişkenler arasında korelasyon olup olmadığını göstermek için korelasyon matrisi çizdirelim. Korelasyon katsayıları hakkında fikir yürütelim.

En güçlü pozitif ilişki hangi iki değişken arasındadır?",d7b93a60,0.22058823529411764
10513,7f74a04ae75792,9df3b885,"### Explore Relationship Between Categorical & Target Variable
",d01e91da,0.22058823529411764
10518,722cd844dfbe8f,95305954,"To better understand the representation of these MRI images, we can also **create an animation to visualize the sequence of images** of a certain category for a given patient.",0cedb385,0.22077922077922077
10519,663bbc9eaf267b,ebc44477,"* As can be seen, we have a right-skewed distribution for Price.
* There are some very pricey cars in this dataset. This gives us the suspicious feeling of having some outliers. So, let's delve deeper and list the most expensive cars in the dataset.",32445529,0.22077922077922077
10523,241cf32abb22d8,ed8b8ebf,"Ordinal:
The ordinal categorical features have been encoded into numbers in the original dataset and therefore there is no need to further transform them. The numbers under each ordinal categorical feature are meaningful. For example, under the feature ""Medu"" (mother's education level), 0 is ""none""; 1 is ""primary education""; 2 is ""5th to 9th grade""; 3 is ""secondary education""; 4 is ""higher education"". The larger the number, the higher the education level.

1. Medu: 0 to 4; the larger the number, the higher the education level
2. Fedu: 0 to 4; the larger the number, the higher the education level
3. famrel: 1 to 5; the larger the number, the higher the quality of family relationship
4. traveltime: 1 to 4; the larger the number, the longer the travel time to school
5. studytime: 1 to 4; the larger the number, the longer the weekly study time
6. freetime: 1 to 5; the larger the number, the more free time after school
7. goout: 1 to 5; the larger the number, the more frequent going out with friends
8. Walc: 1 to 5; the larger the number, the more weekend alcohol consumption
9. Dalc: 1 to 5; the larger the number, the more workday alcohol consumption
10. health: 1 to 5; the larger the number, the healthier",47157066,0.22077922077922077
10524,c13f73168789c2,86fb6aab,"### 1.5 Accessing values by row label and column name<a id='7'></a>
Syntax : `df.loc[row_label, 'column_name']`",16175052,0.22077922077922077
10526,d96642860ab3dd,2ad7641b,This missing value must be removed or fill with some other suitable values,98419d48,0.22093023255813954
10530,840534f2908a9c,5d5946ea,"Remove observations with useless values base on the test data boundary.
You can have some informations about nyc taxi fare from https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page
like initial charge is $2.50 so data with fare less than this value should be removed.",8081c3cc,0.22105263157894736
10534,ac1abfe1dfe815,81aa51a1,-----,6529dbcb,0.22123893805309736
10537,979f1e99f1b309,67ac73b8,***THE plot showing that most common number of heals are zero***,d1bfebbf,0.22131147540983606
10538,ee23a565163388,ebef6aa6,## **Is a decrease in heamoglobin cause heart failure**,88aacbc4,0.22137404580152673
10539,5f32117bcd5255,3ca8239e,"### ATTENTION
* RUN FOR CHECKING",85882abf,0.2214765100671141
10540,726833f92fb87a,ceb04a4d,"The distribution of 'Age' is right skewed, with similar value of mean and median and a lower mode.",7dc5e1b6,0.2214765100671141
10550,df2a7968c08ee4,eef6e860,"### Creating Numpy Arrays

In the following, we are seperating the pixel columns from the label columns, and converting the pandas dataframe to a numpy array.

We also create training and validation datasets. ",a2ba0a72,0.2222222222222222
10555,55ce731a138ca7,31ba2f8d,# Model,4996250b,0.2222222222222222
10556,5ce12be6e7b90e,cc474068,Decimal division:,c0ab62dd,0.2222222222222222
10563,4883314a96dc34,e31694a7,## Understand data with descriptive statistics,50d36836,0.2222222222222222
10564,24e550b8226932,8eb8e0c6,##### items:,0caee953,0.2222222222222222
10570,d6cbd7160961dc,0a4e41b6,"# 4. Application 1: Random Generated Data Set (Type of analysis disaggregated)
* 4.1 Creating the dataset
* 4.2 Running the script on the data set
* 4.3.1. Results: First Digit
* 4.3.2. Results: Second Digit
* 4.3.3. Results: Third Digit",36d74664,0.2222222222222222
10571,ab6da5994949a3,47acf0f5,### Functions to visualize Training & Test Set Results.,fae6b91d,0.2222222222222222
10574,10b5af05d804ff,93df5055,"Do you know target? Of course you don't! We didn't see combination like these in the train data!

Now. I say that 'f1' and 'f2' are 'gold' and 'xp' of the team0,  'f3' and 'f4' are the same features but for the team1. And target shows that the team0 is winner.

So! In the first match we see that the team0 with gold=0 and xp=100 win the team1 with gold=200 and xp=0.

But teams are equal! So it means that any team with 0,100 ussually win any team with 200,0.

Obviosly, target of **[0,200 against 100,0]** is 0 (fault), because target of it's mirror in train data **[100,0 against 0,200]** is 1 (win).

And now we have very confident answers for test data!",4a9b1705,0.2222222222222222
10578,95d896e75f9a50,c827a36b,"**My Take:** As expected the features 1-130 are split into two different distributions, and indeed these match perfectly with `feature_0`. Whether that indicates bid/ask, long/short, or something else I don't know enough about finance to say, but it's clear that the value of `feature_0` influences the values of some if not all other features in the dataset.

## Experiment 2: Features related with Feature 0
Clearly `feature_0` splits up the rest of the features in two data distributions, but the plot only tells so much. To investigate a little further what effect `feature_0` has on the other features, I here try to solve the reverse problem of predicting `feature_0` based on features 1-130. I'll then remove the single most important feature (based off feature importance), and again check how well the remaining features can predict `feature_0`. By iteratively removing the most important feature, we can get a sense of how many features are required for predicting `feature_0`, and thereby how many features are 'related' to `feature_0`.

The reason for this approach rather than just looking at correlations between `feature_0` and `feature_x` is that we also want to consider possible non-linear effects, i.e. `feature_i` through `feature_j` together might in conjunction with each other be able to predict `feature_0`, but not by themselves.",2721b6f5,0.2222222222222222
10581,d58491f2896fc1,d1c4c06f,<h3>Genetik Algoritma Çalışma Prensibi </h3>,514bfdff,0.2222222222222222
10584,5be39e4e35cec7,50ad390b,"* float64(2) : Age and Fare
* int64(5) : PassengerId, Survived, Pclass, SibSp and Parch 
* object(5) : Name, Sex, Ticket, Cabin and Embarked",14d617c9,0.2222222222222222
10595,613bf7bfdcb9e3,ad9b17cd,# axis = 0 common values,32beb65d,0.2222222222222222
10596,4392956f62c040,6a4f9a19,## HELPER FUNCTIONS,c3ed519d,0.2222222222222222
10602,4d1d6dbab10b20,c58f9862,"this dataset contains the information abt home characterstics of UK.

id - Unique ID for each home sold

date - Date of the home sale

price - Price of each home sold

bedrooms - Number of bedrooms

bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower

sqft_living - Square footage of the apartments interior living space

sqft_lot - Square footage of the land space

floors - Number of floors

waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not

view - An index from 0 to 4 of how good the view of the property was


condition - An index from 1 to 5 on the condition of the apartment,

grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level 
of construction and design, and 11-13 have a high quality level of construction and design.

sqft_above - The square footage of the interior housing space that is above ground level

sqft_basement - The square footage of the interior housing space that is below ground level

yr_built - The year the house was initially built

yr_renovated - The year of the house’s last renovation

zipcode - What zipcode area the house is in

lat - Lattitude

long - Longitude


sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors

sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors
",fe366345,0.2222222222222222
10605,6fad63bfd45ef9,9908d7a0,## Train data,b3c6f1d6,0.2222222222222222
10615,d905cde3391d2b,b6076a19,We can verify the number with the help of `mean()` method in `pandas`,067dba39,0.2222222222222222
10621,d77e6d61ad2e8b,98429a5e,# Comparing All Models,03fd0e96,0.2222222222222222
10628,1fcf9c261518d6,f9b4d2d6,# II. Data Cleaning: Missing Value Handling,be646fb0,0.2222222222222222
10638,268a610bbc64b4,6a02c2d8,# 1. No. of new users in orig-dest pairs,8a16f301,0.2222222222222222
10644,fdc3afd309b850,629044dd,"Now, to fill Neighborhood null values, we will create a list of unique Neighborhood values to check if they are in the old address columns.",966bde38,0.2222222222222222
10646,1883198d6d8c3c,be5c5783,"We can see missing value in the price. As price is the important variable here, we will drop the rows with missing price value
",69a1d458,0.2222222222222222
10649,0dfa3e758551c8,5729e20b,"oldest passenger = 80 years old, youngest passenger = 0.42 (5 months old)",7adbb44c,0.2222222222222222
10650,245c89d02f3f5f,a2bcf91a,## Εγκατάσταση βιβλιοθήκης και gym,61a1eacd,0.2222222222222222
10651,ffc3bb768dcf97,dff97a63,"No null values, perfect",15ce84be,0.2222222222222222
10653,2e0fd6e937bf79,5f1c1bb3,# load and preprocess,6acb965d,0.2222222222222222
10655,b39684e6670dd7,819752ba,# make data ,83de9873,0.2222222222222222
10656,07544ba83da480,3e81c810,# Question 1: What was the best month for sales? How much was earned that month?,dc2f52b1,0.2222222222222222
10658,d96e03a9e7c030,5be6a93e,"The functions called in the above controller can be found below by clicking on the expandable `Code` button. The tasks these functions perform include:
* Create a `borough` variable based on a school's zip code
* Aggregate the Discretionary Funding dataset to create the number of dollars received by the government for each zip code, then assign this amount to each school based on its zip code
* Aggregate the number of crimes of different types (violent, non-violent, etc) reported at each school from 2010-2016
* Cast all columns to their appropriate data type
* Create 'dummy' variables for all the potential categorical variables (like `Trust Rating`, `Collaborative Teacher Rating`, etc.)
* Trim the dataset of columns that have a high percentage of nulls (in this case, > 25%)
* Fill NAs of columns that have < 25% with appropriate values (in this case, I used the most common value for categorical variables, and a combination of the median, mean, and 0 for numerical variables)
* Create additional response variables like perc_testtakers (which will become the response variable we use in our model)
",d2b72ced,0.2222222222222222
10660,dd3721cb49c1fd,64f7acb7,"<a id='3'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">3. <b>Importing all the required libraries.</b> </h1>
  </div>
</div>",1a53fdd9,0.2222222222222222
10666,06faae14dfe21d,9476f8c8,## Data cleaning just doing imputations and filling null with mean and categorical values with most occuring on train data,2be83140,0.2222222222222222
10670,0c452d3a0b9339,5edd028f,# DataSets,5d857385,0.2222222222222222
10671,c349ee5a821411,f789a4df,This plot has been adapted from the wonderful Kernel https://www.kaggle.com/sarahvch/investigating-happiness-with-python ,572b269d,0.2222222222222222
10678,63b44c85e32c1f,4d3c9d31,If the list consists of all integer elements then **min( )** and **max( )** gives the minimum and maximum value in the list.,fb9b9562,0.22297297297297297
10679,a566b5b7c374e7,6ca8f54a,"### Correlogram of Oura Metrics, Sleep Cycle Metrics, and Personal Restfulness Score",b3dc5545,0.22302158273381295
10687,98a6794067932a,437df96a,"Pour ce qui est de la cellule ci-dessous, elle permet dans un premier temps de sortir une analyse descriptive des ventes en prenant en compte plusieurs aspects comme la moyenne, le nombre de commandes effectuées, le minimum et le maximum par exemple. Cette évaluation globale nous permet d'avoir une idée générale du comportement des clients par rapport aux ventes effectuées. Dans un deuxième temps, cette cellule nous permet d'effectuer la même analyse, mais cette fois-ci en différenciant les trois segments différents déservis par l'entreprise. Cette analyse permet donc de constater les différents comportements auprès des trois types de clients retrouvés au sein de l'entreprise. ",08600fe2,0.22330097087378642
10690,4c47839b067546,33d827dd,"Как видно в тестовой выборке всего 12 брендов, поэтому уберем лишние бренды из train",1f517b02,0.22340425531914893
10692,f6648e47713411,eaa4ca1a,"## 2.1 Mã hóa nhãn
Mã hóa các nhãn có trong tập dữ liệu về dạng *integer* để mô hình có thể hiểu được.",f4af4d1c,0.22340425531914893
10699,ba655a261cc09e,8fac0b6b,"Let's try to fill in the missing age entries.

Постараемся заполнить отсутствующие записи о возрасте.",48cc549a,0.22388059701492538
10701,20b372b6e4e276,e6d070ba,"### Commit 21

* Dropout_new = 0.15
* n_split = 5
* lr = 1e-4
* num_cnn2 = 96          # originally 64

LB = 0.712",ec8b0860,0.22388059701492538
10703,21413205980558,641b35e4,# Section Ⅲ EDA,84197de0,0.22388059701492538
10706,00001756c60be8,23705731,**Считываем обучающий набор данных**,945aea18,0.22413793103448276
10711,84127ade6fde87,982b7dcb,## One-hot-encoding characters,f55d05b6,0.22413793103448276
10717,20e1ba19eb9b5e,c82f2034,"Now, it is more readable.

Are there any variables which information could be already described by another one? 'GarageCars' and 'GarageArea' are the example of a pair of variables that give the same information. We would need only one of them and we would take the one with higher value. The same case is with the pair 'TotalBsmtSF' and '1stFlrSF' as well as with 'TotRmsAbvGrd' and 'GrLivArea'.

'FullBath' and 'YearBuilt' in this dataset are slightly correlated with 'SalePrice'. The latter should also undergo time-series analysis in order to decide whether it is significant to 'SalePrice'. 'FullBath', on the other hand, compared to living area, does not seem to have a true impact in real life on the price of the house.",4569bfc1,0.22413793103448276
10721,087e21401d7dfc,425f58c8,# Regression - Predict Weight Using Height,42000489,0.22448979591836735
10725,0e7ac281a19feb,dbff4e3e,### Number of bounding boxes per image,5b84d10f,0.22448979591836735
10728,f35bf4df70d310,b69dcdff,### Standardizing the data,10bb859a,0.22448979591836735
10733,5f27526aa6c113,6b53bcf0,#### Imputing the NaNs with 'U' is better,a5c26ab6,0.2247191011235955
10734,04ff2af52f147b,8ad2147a,"Since the story of the Titanic and its sinking is so well researched, just Googling their names reveal the true value of our missing values.  [Encylopedia Titanica](https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html) tells us that

> Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28.

With this, we fill in the Embarked missing values with S.",d5f37be9,0.2247191011235955
10740,5ba4207c371899,33872310,"References from Research Papers-
http://45.113.122.54/pdfs/ijrscse/v2-i3/7.pdf - Attack cassifications for ML algos
https://lupinepublishers.com/computer-science-journal/fulltext/detecting-distributed-denial-of-service-ddos-attacks.ID.000110.php",187b1451,0.225
10741,5626e84c4e6bf8,e9b4d009,"## Minisom: A minimalistic implementation of Self Organizing Maps
MiniSom is a minimalistic and Numpy based implementation of the Self Organizing Maps (SOM). SOM is a type of Artificial Neural Network able to convert complex, nonlinear statistical relationships between high-dimensional data items into simple geometric relationships on a low-dimensional display.
Source: [GitHub](https://github.com/JustGlowing/minisom)",e2ecb669,0.225
10743,254cccd5145725,1ba6dfbf,This gives us the statistical summary of the train dataset.,a49b4037,0.225
10748,fdbbd573ba31c2,e7fc0d75,### atmospheric_temperature(°C),f7c28d74,0.225
10756,631cd434fc3aa2,c6363921,"### Missing data
To check for missing data in both train and test data and keep the preprocess pipeline consistent between both, let's concatenate them and consider, for the moment, just one big dataset.",2b74febb,0.22535211267605634
10758,bddd799cdbbae8,85d33c00,**Top 20 on Cyberbullying tweet**,b44e3c08,0.22535211267605634
10763,842547b2def18c,0f66893e,"## データ可視化による分析

さて，データ分析のための可視化を使って，我々の想定の確認を続けましょう．

### 数値型変数の修正

数値型変数と目標変数(Survived)との相関関係を理解することによって，可視化をはじめましょう．


ヒストグラムは，Ageのような連続数値型変数の分析に対して有用です．Ageの値に従ってサンプルをまとめる/整列させることは，有益なパターンの特定することに役立ちます．
ヒストグラムは，自動的に定義されるbins(分割数)/range band(分割幅)を使い，サンプルの分布を提示することができます．
これによって，我々がAgeの特定の幅に関係する質問に答えることが容易になります．(ex. 乳幼児の生存率はよかったのか？)

※ ヒストグラムのx軸は，サンプルや乗客たちの総数を表すことに注意する．

**観察**

- 乳幼児 (Age <=4) は高い生存率だった．
- 最も高齢な乗客たち (Age = 80) は生存した．
- 15-25歳の乗客たちの多くは生存しなかった．
- ほとんどの乗客は，15-35歳の範囲にあった．

**決定**

この簡単な分析は，ワークフローの次の段階に対する決定として，以下の想定を承認します．

- Ageはモデルの訓練に使うべきである． (classifying #2) 
- Ageのnull値を補完するべきである． (completing #1)
- Ageをグループとしてまとめるべきである． (creating #3)",b8efde6d,0.22549019607843138
10767,2ada0305b68956,a37845ed,### 36. Palette = 'PuBu_r',133e26f4,0.2257142857142857
10768,098fedfcd07456,5a4abc29,# Let's Look at all the image the reason 20 image is printed to 0-9 the first set does not have all the images,052ece26,0.22580645161290322
10771,b59b5aaeedb1fb,b2f2dfd4,#### Different types of gender or sex in the given data,1ad63faf,0.22580645161290322
10772,921fff7d3040db,ed1e3f92,"# 3. Preprocess
",5f36ced9,0.22580645161290322
10773,9ceb7278784462,f804ef0b, ## <a id='10'> 7.Price </a>,3768a567,0.22580645161290322
10775,a3e8d6ef4c5188,bead0614,There are no values in the columns.,7c8212dd,0.22580645161290322
10776,16862cb02d73d5,2fc17883,Level the multi-index pivot dataframe and treat na with 0,d7ffa1a6,0.22580645161290322
10777,57070ad5e0f94f,8bf2fa24,# **Checking If Conversion Worked**,d97edc41,0.22580645161290322
10779,e8c6480a3122b3,9248774e,We can see that more people in Pclass 1 and 2 survived than people in Pclass 3. Hence Pclass can be used for classification between survivors and non-survivors.,40dc4cca,0.22580645161290322
10781,56e58d53ac9c57,eff60cd2,"from the first look, age, feedback count, and rating all looks normal values. Seems to be not too extreme outliers.",90e2ab8e,0.22580645161290322
10790,0cb456a5456cf9,afc1d9eb,"# **Q1 answer**<br>a. Regardless of the cancelation, city hotel is more popular since its reservation is larger.<br>不考虑取消订单的情况，城市酒店比假日酒店更受欢迎，因为它的预订量更大。<br>b. Cancel rate of city hotel is two times higher then resort one. <br>城市酒店的退订率比假日酒店的高将近一倍",5701729c,0.22580645161290322
10791,f15eac23fbcc9d,18b0081e,Read in the CSVs. ,ea46d8af,0.22580645161290322
10792,c0ddb77bf32e2b,009abb8c," It will be efficient if we know what will it be, if we filter the rows by counts of NAs.",a0cb45f7,0.22580645161290322
10798,565ad413cd802f,ab736eaf,"## Creating Datasets & Data Loaders

We can now create a custom dataset by extending the `Dataset` class from PyTorch. We need to define the `__len__` and `__getitem__` methods to create a dataset. We'll also provide the option of adding transforms into the constructor.",397b074e,0.2261904761904762
10800,3c2033cc99c12c,744b84e4,## Random Under-Sampling ,dfa22a54,0.22627737226277372
10801,c80939c7c626cf,474fbeeb,We will now begin the most important part of Machine Learning ,b9ac31e2,0.22627737226277372
10803,510b8303776bb6,78c79f5d,### Box Plot of the sale price over the whole dataset.,18080db8,0.22641509433962265
10804,f015d0147e8fbf,1397fac9,### 2. Bureau Data Table `bureau.csv`,518954fb,0.22641509433962265
10806,a070fd03ae8ed2,608a63db,## 3.3 Проверка импорта моделей,c0ec4138,0.22641509433962265
10807,0ad8d416b89b78,dc79dc14,"# Education:
The removal of one of these attributed would be beneficial to analysis as it acts to reduce the overall dimensionality of the dataset. The initial exploration through Pandas Profiling utilising 'Cramérs V' showed that the two attributes were highly correlated. Further analysis of these attributes and their relationship to income was conducted to confirm this conclusion.",0b0562f0,0.22641509433962265
10808,4dd47072617594,163a732f,# 2. Text Visualization,44ff1d11,0.22641509433962265
10809,f3c8651cb08234,7cf5708d,# Finding The Car Age,37f86e36,0.22641509433962265
10811,23df07a474aaae,b41d23fb,# Correlations,0ea40276,0.22641509433962265
10812,0932046e1f485d,39663334,"The ""Installs"" column needs the plus sign removed and turned into a number.",218cc7a3,0.2265625
10813,7e1da639035ac5,70488867,Economic Need Index and School Income Estimate are inversely proportional to each other. More the economic need index lesser the estimated income.,120b6c23,0.22666666666666666
10818,67b7354e96113a,b181807f,## Feature Engineering,dca94250,0.22666666666666666
10822,225b4fe5d3894a,2a0094f8,"<a id=""5""></a>
## 5. Discover and Visualize Data to Gain Insights
 Do exploratory data analysis on test data",4b4197b3,0.2268041237113402
10823,8ec771f5600a61,4796bcee,### From the above plot we can conclude that majority of the value lies within first standered devisation.,48364c1f,0.2268041237113402
10827,4ae6a182abac64,b8540e6a,* **Survived feature**,418676c5,0.226890756302521
10833,be2f4d8a6b73ca,05268942,"<div class=""alert alert-block alert-info"" style=""text-align:center""> 📌<b>Insights:</b> We can cearly see that there is no NaN values (missing values) in the dataset.</div>",5d8ce40a,0.22727272727272727
10836,930cd79ca51204,3f4bb4e1,"But this doesn't look right. 😂

For this type of comparison - let's make a simple scatter plot.",5506779a,0.22727272727272727
10837,a0b321057e7402,d3032274,I am going to resample the data to an hourly format and analyse it further as such.,5f73fb91,0.22727272727272727
10838,6d66ced0028dea,d7a2a16a,"Делаем EDA не просто так, а для:
- Исправления выбросов
- Заполнения NaN
- Идей для генерации новых фич",f50aae52,0.22727272727272727
10841,90964081c7faab,f76bb445,"## 4) Exploratory Data Analysis
",b423b0c3,0.22727272727272727
10843,d1ff7e10ee0102,f0dc5fa9,"*'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a <b>strong linear (exponential?)</b> reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.*",2cc71c3c,0.22727272727272727
10844,0a918602a04693,f75030f0,"As we are going to predict the Churn patterns of telecom users, lets explore the relationship between various columns with Churn column",c1ef0e95,0.22727272727272727
10845,dd02a9b545f742,a14616a4,# System Mode,7116cd2d,0.22727272727272727
10850,f166950fa915f8,9407681f,### Pre-Process dataset,a7f6ca5e,0.22727272727272727
10852,9c19668d6b7295,90683538,## Utilities,35be7001,0.22727272727272727
10859,ae058c3f1439c3,c9c5484a,**Reading the data**,965da99d,0.22727272727272727
10860,32e04b08ff52eb,3374faff,Train and Test Split,8d5b86e0,0.22727272727272727
10861,450fda47b03baa,dac71407,Veri çerçevesindeki değişkenlerin hangi tipte olduğunu ve bellek kullanımını görüntüleyelim.,62c04adb,0.22727272727272727
10865,73d8e56bc709b1,9daf550c,"First of all, we gonna see descriptive statistics of ['Age','Potential','Overall'].",78ec3cce,0.22727272727272727
10871,5e1d001f8764e0,1ccf712c,# NATURAL LANGUAGE PROCESSING,62be464c,0.22727272727272727
10873,396bc36edb95d3,be28f620,#### Plotting Sales with categorical variables w.r.t Claimed status,965e4f8f,0.22777777777777777
10878,30fdc4a6e3c1db,452df982,"What we see:
* California contributes to almost 40% sales of foods and household categories but contributes to about 50% sales of hobbies category
*  Winscoin has about 25% contribution in both hobbies and household categories but contributes to about 30% sales of food category
* Texas contributes to almost 30% sales of foods and household categories but contributes to about 25% sales of hobbies category",6111ddee,0.22807017543859648
10885,3fb15e6e48aec2,7a4f2f43,"* Option: to do some further analysis on Name_passenger & Name_purchaser
* However we will use countvectorizer later so some of this info will be in the sparse matrix output of ""Surname""",9d1f4358,0.22807017543859648
10886,9e27af2600925c,e4c5d53c,"To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.

One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).

<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> 

Let's standardize our dataset.",9b556435,0.22807017543859648
10891,c3498779cda661,4758ca08,# Modelo K-Means,0f531b65,0.22807017543859648
10892,726833f92fb87a,8794fa4b,## Balance Analysis,7dc5e1b6,0.22818791946308725
10893,5f32117bcd5255,b17ad0cd,### NEAR INFRARED,85882abf,0.22818791946308725
10895,ce9ed5e2d601d7,fd458447,"# Basic EDA
Scatter plots for features with continuous values.",f58a2f43,0.2283464566929134
10897,171494b45650a2,3b1a98f8,## ***3. Data Cleaning***,9c8cc578,0.22857142857142856
10898,3cea0f929a2035,d6d8cf1e,Next we can see the distribution of suicides in each generation. The distribution is presented for the total number of suicides and number of suicides per 100k population.,04cfbade,0.22857142857142856
10900,7a058705183598,64317663,Data Preprocessing,b0ead917,0.22857142857142856
10903,72e098fe5b2a04,20b31aa0,"# Model
The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm).",5399eebd,0.22857142857142856
10906,38b79494ac749e,c7781743,"Let's try to approximate our target function $ f(x) = 2\cdot x + 10\cdot sin(x) $ with polynomials of different degree. 

A polynomial of degree $n$ has the form:
$ h(x) = w_0 + w_1\cdot x + w_2\cdot x^2 +\ldots + w_n\cdot x^n $.

$x^i$ values could easily be generated by `PolynomialFeatures`, while $w_i$ are the unknown paramaters to be estimated using `LinearRegression`.",39162a40,0.22857142857142856
10913,5d5c9480b5a0a3,f807c45e,"Now that we know how many females(266) and how many males(152) have survived, let's see if we can leverage the information to reach a conclusion.",04d82e2d,0.22857142857142856
10914,55a5e31d03df9f,c8430108,"Wow! We did a ton, let me explain what just happened with this piece of code.

First, we set a random seed with `tf.random.set_seed` this is for reproducibility and that when you run the numbers we have the same results, it's a good practice as scientist that other people can replicate our experiments. [You can read more about this topic here](https://www.kdnuggets.com/2019/11/reproducibility-replicability-data-science.html).

Secondly, we create a model we will introduce first the **SEQUENTIAL API**. Here we create the model layer-by-layer and as you see is very simple and easy to use. We created the following layers:
* Input Layer: This will assert that our input is Tensor with the proper shapes, we set it to be the `train_ds` shape, which is (240, 240, 3), 240 from the resizing we did early on and 3 for the channels RGB. 
* Flatten: As we seen in the explanation of a MLP, it requires to be a single dimensional vector. But currently we have a 240, 240 vector. Flatten simply 'melt' the tensor and give us a single vector of lenght: 172800. Why this number? It's simply 240 x 240 x 3 ! 
* Dense: This is the heart of our MLP it means that we're going to connect the input that this layer recieves with a total of neurons that we refer at the beggining with an *int*. We can also here specifiy the activation function that we will use for our neurons.

> **Note:** As you can see the first and second hidden layer are the same except we use a different number of units (int), but the last is different. Why? It's because it's the output layer, we explicitly have to specify the number of units as the different number of classes and use a `softmax` activation function, if we use a `relu`, `tan-h` or even a `sigmoid` the outputs wouldn't make sense for our final objective to classify the images.

Thirdly we compile the model, we always need to compile it, in this step we specify the how are we are going to calculate the loss in our case since it's a multi-classification we used the `CategoricalCrossEntropy()`, if we had a binary classification we could use the `BinaryCrossEntropy()`, if we instead have a regression problem the `MeanAbsolutError()` or `MeanSquareError()` could have been the regression metric. Check more [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses).

Finally, we use an optimizer in this case we used Adam optimizer there are multiple you can try on and you can check in depth [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), with these optimizer we calculate the derivate of error and use the learning rate on it. 


Let's visualize the model we just construct: ",06dce00f,0.22857142857142856
10917,b3e48999ed0d00,531d98c4,## Corelation matrix,fe9ada0f,0.22857142857142856
10922,7454fdc444df16,6fcfecd6,"Notice how the ratio of IDC patches to non-IDC patches is highly uneven. Here we approximately a 28/70 split, which could be problematic down the road due to the overrepresentation of non-IDC samples relative to IDC samples. We need to account for it during the validation phase.",a7818ef5,0.22857142857142856
10923,04bac111ffbe9c,cb699e2b,"##### Replace missing Age values with mean.
1. We can directly replace the missing values by the mean of the ages value.
2. Another method could be to fill the missing values based on 'Pclass' based grouping. For example, fill in the missing value of a particular 'Age' entry, which has a Pclass say '1', with a value that is the mean of the all ages corresponding to that particular Pclass only. Idea source : https://www.kaggle.com/thomaswoolley/rf-and-k-nn-titanic-0-79-score",82576b17,0.22857142857142856
10925,fe6750354fb64f,f0938bc9,## Bar Graph of Recovered Cases,271741f0,0.22857142857142856
10928,9c044fa3072552,b352a8ef,### Cities,1362842e,0.22857142857142856
10930,917957c6c4065f,420e412f,![image.png](attachment:image.png),55b8ed68,0.22875816993464052
10931,9169c4e9c33c90,6325dd4c,"Because this dataset is a compilation over 11 years, some books may appear more than once. I'll make a frequency column ('Freq') to show a cumulative count for each title.

I also renamed the 'Name' column to 'Title' to better distinguish between book title and author name.",725bf880,0.2288135593220339
10934,835a7b4e660d23,e98cf89d," ### Line Plot
*     İt is better when x axis is time.",53bc7a6e,0.2289156626506024
10935,4daf6153275cbf,779160ee,"Here I imputed the missing values by groups. I followed **Price Range**, **Rating** and **number of reviews** order. Also used mode for the first two and mean for the latter.  That's because the ratings and price range has finite values.",51db1961,0.2289156626506024
10937,3b5903412fe741,9fd97a4f,"## Index-based selection

The indexing operator and attribute selection are nice because they work just like they do in the rest of the Python ecosystem. As a novice, this makes them easy to pick up and use. However, `pandas` has its own accessor operators, `loc` and `iloc`. For more advanced operations, these are the ones you're supposed to be using.

`pandas` indexing works in one of two paradigms. The first is **index-based selection**: selecting data based on its numerical position in the data. `iloc` follows this paradigm.

To select the first row of data in this `DataFrame`, we may use the following:",ad231969,0.22916666666666666
10939,64a336ac34d95c,b6966499,"## Online customer vs churn plot
",be73a990,0.22916666666666666
10942,eb0854a6601407,e03f9e60,"# Example of Features for a Single Investment ID
- We are only looking at 3 of the features.",6d107747,0.22916666666666666
10943,386c42a7fb27a4,6b6e69d5,### All Item Categories,9e9f6974,0.22916666666666666
10950,918040fad252ec,61b21add,Membuat pangilan singkat untuk variable,966fcd8f,0.22950819672131148
10951,0858e1bb3cbaca,7b4b5d07,"To focus on only one aspect of the data, we can use

** *DataFrame name*['*column name*']**

to select a specific column
",78548374,0.22950819672131148
10952,bcd7e398c4d0ec,d853c55d,"If the data of Bread1 is equal Bread2, the pet is mix-bread. Maybe mix-bread is the significant feature in model training.",77a143f6,0.22950819672131148
10957,27d5291d6365ba,46e9ac8f,# Mean Transaction by Gender,96b30229,0.22972972972972974
10958,62037c5832129c,cbae10ab,"* One of the key steps with any model is to estimate it's performance on data that it hasn't seen. 
* A model can suffer from underfitting(high bias) if the model is too simple.
* A model can overfit(high variance)the training data if the model is too complex for the underlying training data.
* Next we will look at different cross validation techniques which can help us obtain reliable estimates on how well the model performs on unseen data.",61474350,0.22972972972972974
10963,d5f78aa381f58d,5138a8b3,So there is a positive correlation between chest pain (cp) and the target. On the other hand there is a negative correlation between exercise induced angina (exang) and the target.,d60f358f,0.22988505747126436
10965,14defffcd250f3,e5b6696f,Now let's check that heat map again!,3a683b94,0.22988505747126436
10966,83df814455f06c,95a2f86c,We can see that there are 1728 instances and 7 variables in the data set.,c9cff71a,0.23
10968,4cd25e50c7e007,d6231942,"# Year

* 2018:0
* 2019:1",ceb0c525,0.23
10970,ac1abfe1dfe815,15625a9e,## Number of Words in a Tweet,6529dbcb,0.23008849557522124
10971,c9b4e282e4e2c1,7b904368,"* Knee injuries are the type of injuries that make the player to miss more days.
* Every type of these injuries require the player to miss 1 or more days.
* Most of the knee and ankle injuries require the player to miss 7 or more days. The player will miss between 7 and 28 days.  
* Foot injuries always seems to make the player miss 28 or more days. Furthermore, they will probably make the player miss 42 days or more. 
* Heel injuries make the player to miss 7 or more days, but no more than 27.


",f44d339f,0.23008849557522124
10974,4f69b7bb1ca287,badeaa28,## Reading Data,34abc6a1,0.23076923076923078
10979,99bf357eaf61f1,eaba01e6,#### 4.Categorical Features,9d92fafe,0.23076923076923078
10982,669ce946943d60,047afc12,"if you use `SN_filter`, you can get the LB-like score. 
Let's see how it works.",0f63c4ce,0.23076923076923078
10986,71d3e4aee86e3e,9241a291,> # Pie-Chart Visualization,69706f0b,0.23076923076923078
10988,f2f2db16a2f86c,adced10d,No duplicates.,ffc6a115,0.23076923076923078
10990,2facf256353117,2595df1a,"# Gadolinium
* Gadolinium, also called ""contrast,"" is a large, chemical compound that is injected into a person's vein during an MRI scan by a technician.
* why and how ""contrast"" (gadolinium) is used in MRIs to diagnosis or monitor multiple sclerosis.

* Gadolinium normally cannot pass from the bloodstream into the brain or spinal cord due to a layer of protection in a person's body called the blood-brain barrier. But during active inflammation within the brain or spinal cord, as during an MS relapse, the blood-brain barrier is disrupted, allowing gadolinium to pass through.

* Gadolinium can then enter the brain or spinal cord and leak into an MS lesion, lighting it up, and creating a highlighted spot on an MRI
* The purpose of a gadolinium-enhanced magnetic resonance imaging (MRI) scan is to give your doctor an indication of the age of your MS lesions, like whether an MS relapse is happening now or whether one occurred awhile ago.

* If a lesion on the MRI lights up, it means that active inflammation has occurred usually within the last two to three months. Active inflammation means that myelin (the fatty sheath that insulates nerve fibers) is being damaged and/or destroyed by a person's immune cells.


* If a lesion on an MRI does not light up after gadolinium is injected, then it's likely an older lesion—one that occurred more than 2 to 3 months ago. In other words, the use of contrast helps a neurologist determine the age of a lesion. 


* Even so, it's important to understand that an MS lesion seen on an MRI does not necessarily cause symptoms. These lesions are referred to as ""silent"" lesions. Likewise, it can be tricky sometimes to correlate a specific symptom with a specific lesion on the brain or spinal cord.

* Also, not all lesions represent MS, which is why an MRI cannot be used alone to diagnose or monitor a person's MS. Lesions seen on an MRI can be the result of aging or other health conditions like stroke, trauma, infection, or a migraine. Sometimes, people have one or more lesions on their MRIs, and doctors cannot explain why.

* In addition, lesions do interesting things. Sometimes they get inflamed over and over again and eventually form black holes, which represent areas of permanent or severe myelin and axon damage. Research suggests that black holes correlate with a person's MS-related disability.2﻿ Sometimes lesions heal and repair themselves (and even disappear)
## gadolinium-based contrast agents (GBCAs)
For some MRI exams, intravenous (IV) drugs, such as gadolinium-based contrast agents (GBCAs) are used to change the contrast of the MR image. Gadolinium-based contrast agents are rare earth metals that are usually given through an IV in the arm.
* The terms 'enhancing' or 'non-enhancing' lesion refer to the uptake of Gadolinium-based contrast agent in the lesion. Typically, this property is assessed using a T1-weighted acquisition: Gd shortens the longitudinal relaxation rate of tissue, which causes a signal enhancement.
* The difference between enhancing and non-enhancing is very pronounced in brain tissue, where the blood-brain barrier effectively hinders Gd-based contrast agent from accumulating in the tissue in normal circumstances. When the blood-brain barrier is leaking, e.g. due to an inflammatory process in a lesion or due to cancerous angiogenesis, Gd can extravasate and accumulate in the tissue.
* Specifically in Multiple Sclerosis, 'active' and 'chronic' MS lesions are often differentiated based on their contrast enhancement, based on the fact that an active lesion exhibits acute inflammation and breakdown of the BBB, whereas chronic MS lesions usually don't enhance.
* In MRI field, usually the term ""enhancing"" is coupled with ""gadolinium"" or ""contrast"". Therefore, an enhancing lesion is a lesion that assumes contrast medium. In some cases this contrast-enhancing lesion may be acute (e.g. an acute multiple sclerosis lesion), but in other cases an enhancement may be even related to non-acute findings (e.g. vascular malformation)
* The differences between enhancing and nonenhancing lesions in MRI are obvious. Normally with T1 contrast agents at a usual dosage, the enhancing lesions appear hyperintense on MR images and nonenhancing lesions appear isointense or without signal changes in comparison to that on precontrast MR images. Whereas with T2 contrast agents, the enhancing lesions appear hypointense on MR images and nonenhancing lesions appear isointense or without signal changes relative to that on precontrast MR images. Whether a lesion is enhanced or not reflects the blood perfusion, vascular permeability and extracellular space of the lesion.
* Just to add, any breakdown of the BBB can cause Gd enhancing lesions, which is not necessary of inflammatory etiology but also seen with e.g. vascular lesion a few days after acute stroke.
* Any lesion able to increase vascularization or to be responsible of neoangyogenesis -inflamatory or tumoral are the most frequent - will produce an enhancing image at MRI. 
",18f579be,0.23076923076923078
10991,8ddaa0c6c395ec,68d7f51d,## Imports,9fccabdc,0.23076923076923078
10992,020c28a360b0cd,a8e09268,"You will have one of two options for this project. 

Both options must include:
* At least one logical operator (and, or, not)
* Conditional statements


**Option 1: President Project**

Write a program that reports whether or not someone is eligible to run for president in the U.S. You should do the following in your program:

Ask the user for their age and store it in a variable.

Ask the user if they were born in the U.S.

Ask the user how long they’ve been a resident in the U.S.

Use an if/else statement with the proper comparison operator to print You are eligible to run for president! if they have the following credentials:

* They are at least 35 years old
* They were born in the U.S.
* They have been a resident for at least 14 years

Also, print why the user is not allowed to run for president.

An example run of your program might look like this:
```
Age: 19
Born in the U.S.? (Yes/No): Yes
Years of Residency: 19
You are not eligible to run for president.
You are too young. You must be at least 35 years old.
```
… or like this:
```
Age: 40
Born in the U.S.? (Yes/No): No
Years of Residency: 5
You are not eligible to run for president.
You must be born in the U.S. to run for president.
You have not been a resident for long enough.
```

Want to make it better?
* Allow a variety of yes or no answers (ex. 'yes', 'Yes', 'YES', 'yEs'...)
* Tell the user whether they are eligible to run for Congress. To run for Congress, you must be at least 25 years old and have been a citizen for at least 7 years. Also, they must live in the state they want to represent.
* If the user is eligible to become president, point them to online resources about how to run for president.
* Allow the user to choose their country at the start. Then, ask questions relevant to running for president/ prime minister of that country. You will have to look up the requirements for other countries.

**Option 2: Buzzfeed Quiz**

To explain this choice, take [this buzzfeed quiz.](https://www.buzzfeed.com/abelle13/design-your-room-in-the-loft-and-ill-tell-you-wh-f1qwgbjsa6)

You will be creating a quiz like this. You must use logical operators (and, or not). Include at least 4 unique results for each of at least 4 questions. Use the same questions every time. 

For example, for the New Girl quiz, if someone chooses option a for question 1 **and** option 3 for question 2, then they are Jess. Here is some example output:

```
Which New Girl Character are you??

Do you consider yourself sporty or artsy? artsy

Do you spend more than an hour getting ready in the morning, or less? more

Which job would you rather have: modeling or working in business? business

You are Schmidt! 
```

Make sure you are using logical operators. In this example, the person is Schmidt because they chose artsy **and** more than an hour **and** business. 

You may use an existing BuzzFeed quiz that already exists, coding the logic to show your undestanding of the program inputs, commands, and output. 

",2ba397f0,0.23076923076923078
10994,9eed0fae1c7958,4113916d,# training data,3fb1438e,0.23076923076923078
11002,44f6a002ecd033,c64b845a,Our last and most important column for our machine learning is the loan status column. This is the column that we are looking too predict. From here it looks like a good majority of applicants in the train dataset got there loan application approved.,70bbe106,0.23076923076923078
11006,3536195ad632ee,248d3b53,"**The model** 

There are many structures you may find out. This model below is very much inspired by [Chris Deotte](https://www.kaggle.com/cdeotte)'s [25 Million Images! MNIST kernel](https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist). I highly recommend reading his kernels and also those ones he refers to.

This model below contains 7 convolutional layers and 1 dense layer in one net
and it is an ensemble-of-nets type solution which I found very useful to improve accuracy in MNIST competition.

I kept C4 and dense layers fixed and played a bit with the order of top 6 layers and found C3-C3-C5-C5-C7-C7-C4-FC10 structure the best. Feel free to permutate top 6 layers by changing kernel sizes below and see changes in Trainable parameters printed below. For kernel sizes you may use other than 3,3,5,5,7,7 of course but you may need to change other parameters in the model.",3c26aafc,0.23076923076923078
11009,33398ae40da63d,23e0e404,"We have a dataset of fake news that was collected in November 2016 (source: https://www.kaggle.com/mrisdal/fake-news). For our dataset of real news, we want to collect news articles that were published from October to December of 2016. That gives us ample data points we can use to build our classification model later.",7bd02b88,0.23076923076923078
11010,34fff8ce731b03,f756e404,"## Engenharia de Features

Engenharia de Features é o processo de usar o conhecimento de domínio sobre os dados para criar *features* que fazem os algoritmos de aprendizado de máquina funcionar da forma que esperamos. 

Primeiramente, iremos remover do DataFrame as features que não iremos utilizar na classificação. As features *key* e *timestamp* são removidas por não terem correlação com o fato do cliente estar ou não inadimplente. A variável *ultima_compra* foi removida para ficar como exercício para você incluí-la no conjunto de features.",6f9e5b2e,0.23076923076923078
11011,897ca904b74a98,de42d6f5,### Discrete variables analysis,c5844ad4,0.23076923076923078
11015,9b42412e75d640,9548770d,Since there are 240 duplicatred entries we will remove them as they may cause model overfitting. ,b616570a,0.23076923076923078
11028,49ee86d074de69,1eac4ab6,"<a id = ""9""></a><br>
## Remove Unnecessary Features",71ccc6d3,0.23076923076923078
11029,5af9bf52e5f17c,02039656,"Given the exponential nature of the pandemic, the data is probably better expressed in a logarithmic scale.",f98ab90d,0.23076923076923078
11031,cf08b03b002c13,04a3bd98,The distrubution is extremely uneven so we have to look at the exact numbers ,104d416f,0.23076923076923078
11033,8ac70416723897,d8162f5f,"Subsetting the data to remove unnecessary data, turning dates into ordinal numbers",d32fd8f6,0.23076923076923078
11038,dbe40fdf51456d,a8cd63bb,"# Two examples
### The synthetic dataset
We create a synthetic dataset consisting of 30 features with 10k rows of data.
The synthetic data follows the the partially linear regression (PLR) model where we have the outcome given by
$$ Y = T\theta + g(X) + \epsilon$$
where $\theta$ is the causal, or 'lift' parameter that we are interested in calculating. Also we have
$$ T = m(X) + \epsilon$$
where $\epsilon$ are the irreducible error contributions, and $g$ and $m$ are the nuisance  functions. The first equation is the main equation, whilst the second equation is dedicated to the confounding and is analogous to the [omitted-variable bias](https://en.wikipedia.org/wiki/Omitted-variable_bias).

In the synthetic data
* Y is the target 
* T is the feature of interest
* X are the control vectors $ \sim \mathcal{U} (0,1)$
* W is a matrix of the confounders $ \sim \mathcal{N} (0,1)$",f3e8a1e4,0.23076923076923078
11044,d78988cb5a1b02,beaa27e0,# **Train test split**,233f3a92,0.23076923076923078
11049,03048e86a6d806,023dbb29,"It can be concluded that most of the data folks have earned Master's degree for nearly every job title, while most of the Research Scientists have Doctoral degree.",1285c231,0.23076923076923078
11054,dd00690b4a2f1f,52f14420,"<h3> Decision Forest building (""concatenation"" of decision trees)

> Indented block",e8b7ce8a,0.23076923076923078
11056,20b372b6e4e276,28550696,"## 3.2. Model training <a class=""anchor"" id=""3.2""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.23134328358208955
11057,2f47abddfd1928,fae2117a,We should confirm that all the missing values have been filled.,ae33cc0b,0.23140495867768596
11058,e9b9663777db82,9af72051,# Data Cleaning and Preprocessing,648e8507,0.23140495867768596
11059,2ada0305b68956,bf69a099,### 37. Palette = 'PuBuGn',133e26f4,0.23142857142857143
11062,2f0f808765fc67,9e5e5b6d,"INFERENCES FROM THE ABOVE HEATMAP--
self realtion i.e. of a feature to itself is equal to 1 as expected.

temp and atemp are highly related as expected.

humidity is inversely related to count as expected as the weather is humid people will not like to travel on a bike.

also note that casual and working day are highly inversely related as you would expect.

Also note that count and holiday are highly inversely related as you would expect.

Also note that temp(or atemp) highly effects the count.

Also note that weather and count are highly inversely related. This is bcoz for uour data as weather increases from (1 to 4) implies that weather is getting more worse and so lesser people will rent bikes.

registered/casual and count are highly related which indicates that most of the bikes that are rented are registered.

similarly we can draw some more inferences like weather and humidity and so on... .",fd1f6494,0.23148148148148148
11063,f91f58d488d4af,15122d8d,"* The most important attribute of a tensor is it's shape. Shape is the size of each axis of a tensor.

* And the length of tensor's shape is it's **rank**. Rank is the number of axes or dimensions in a tensor.

* And one more way to get the rank is using `ndim`.
",5df1bbf3,0.23157894736842105
11064,169177b6e9edea,8ef19d37,<h3><b>Embarked,ca42152f,0.23157894736842105
11068,9c26c5dcd46a25,3c17fba6,"On voit ici clairement que la mise en place de la Loi de Santé 2017 et du calcul du Nutri-score par l'équipe du Pr. Serge Hercberg a fait chuter la part de produits considérés Nutri-score A au profit des produits typés D et E.

Regardons également les relations par paires de variables :",1bbbb677,0.23170731707317074
11070,0e2a23fbe41ca9,282460b7,"Observations:

- tried to see if there was any pattern in the ids.
- converted hex values to decimal, but when we see it according to the ```first_active_month``` then there is no apparent order in the ids


### 5. Anonimised features vs target",64e4762c,0.2318840579710145
11075,548f961125248d,d77aeaa7,* The dataset represents a case of Imbalanced classes. The 0 label has fewer values.,d8c5e8b8,0.2318840579710145
11077,9d9da6c439b96b,79697661,Data is clear of missing value,361cc7d9,0.2318840579710145
11082,f13534449a3750,320576af,"Are available two different datasets, one will be used for the training section. It is composed by 192556 different images in a .jpg format and a size of 768x768. The other dataset will be used for the test section and it is composed by 15606 images with the same characteristics. ",8b7f3332,0.23214285714285715
11087,3cd78d8d6d56e4,43bf4c78,## Building Transforming Piplines,9f632e94,0.23214285714285715
11090,743ae010f5e875,5b4e31f9,### Create a Knowledge Bank,02c54445,0.23255813953488372
11092,8539260444e6b5,1c87ea48,# Preprocessing Function,0369463f,0.23255813953488372
11094,1660daf8867980,39636749,"### Reinforce
- The reinforce object contains the algorithms for solving move chess
- The agent and the environment are attributes of the Reinforce object",42d7cffc,0.23255813953488372
11096,d96642860ab3dd,10148c3f,"### 1.6 familySize = [ SibSp, Parch ]
Since these two [ SibSp, Parch ] variables together indicate the size of a family, we would create a new variable 'familySize' from these two variables.",98419d48,0.23255813953488372
11098,806ce45c8fa303,1c5efe8e,"## Trasnformation

Data transformation is one of the steps in data processing. We need to transform certain attributes value so that it makes sense in the further analysis. ",3e5c34dc,0.23255813953488372
11102,1eb62c5782f2d7,dae7fbea,### B. z-score = -1.38,bb69f147,0.2328767123287671
11112,cf4d1c1ad1476c,63bfa45a,## Train Data,768c1a59,0.23333333333333334
11114,49ac6594c8f5cf,ad998ef3,Detailed count of all students seperated based on placement status,6f19f28a,0.23333333333333334
11120,e78f177ca86768,077cfe27,## Fine Tunings the above findings,120e25c1,0.23333333333333334
11121,62487bcd70b199,2ab75e5f,## <a id='4.2.'>4.2. Age and Income in Personal Loan</a>,f6ae50af,0.23333333333333334
11122,bc058fe14d3d1b,52b37882,## 1. Features engineering,d0273670,0.23333333333333334
11125,37b09262279764,09c326df,"<b>Females survived</b> the most<br>
<b>Reason:</b><br>

- Usually, females are given more priority than males. Because, naturally males are stronger than females.",37c4c417,0.23333333333333334
11128,9276fa5cc2fef6,616447c7,## Average Length of Sincere vs. Insincere,24aa6a52,0.23333333333333334
11130,4bada947d597ac,5135bd74,# Visualising data Distribution,eab5094a,0.23333333333333334
11132,8d575f495686ab,f82019a3,## Visualization,0fc16499,0.23333333333333334
11134,b0c2805cd5c087,ddcd53bd,Image readwrite.com,0446f327,0.23333333333333334
11135,2bd6c370695ea7,5db32775,### Name features,cbe6aec8,0.23333333333333334
11136,d6cbd7160961dc,4b3f81b9,"For this example, we will simulate a data set for town hall's expenditure. We want to analyze if the data informed by our fictional mayors comply to Benford's Law. First let's create the database with two columns: city name, and city expenditure. This data set will have 1000 lessons. We will assume that each row represents the city's expenditure in a given month. Let's assume that the maximum expenditure is $999 per month.",36d74664,0.23333333333333334
11140,3597174a998d4d,5cfc47aa,### 2.1.3 Distribution Channel ,276892ed,0.23333333333333334
11144,c18267b203f28a,b792e4a6,"## A note on using train_test_split()
While I used `train_test_split()` to create both a `training` and `validation` dataset, consider exploring **[cross validation instead](https://www.kaggle.com/dansbecker/cross-validation)**.",09ca8efb,0.23333333333333334
11145,c80939c7c626cf,c0caebc3,#  3 Feature Engineering,b9ac31e2,0.23357664233576642
11146,3c2033cc99c12c,f366536f,![image-2.png](attachment:image-2.png),dfa22a54,0.23357664233576642
11147,c84925c8171900,0feb3739,"<a id=""datacleaning""></a>
<h2>   
      <font color = blue >
            <span style='font-family:Georgia'>
            4. Data Cleaning
            </span>   
        </font>    
</h2>",e21ff7ec,0.2336448598130841
11149,75adb7945ef9bd,4ace9ec7,## 3. Locations,785c5095,0.23376623376623376
11153,4ae464582bac51,62c1abcc,## Descriptive Statistical,ca6a52ce,0.23376623376623376
11157,5ce12be6e7b90e,01047b1d,Integer division:,c0ab62dd,0.23391812865497075
11158,30fdc4a6e3c1db,5268be4b,### Plotting sales ditribution for each state across categories,6111ddee,0.23391812865497075
11159,957e035ba5b9d5,289fc64a,"## Calculate number of images in train, test and validation",778ab3d3,0.23404255319148937
11169,81712ee7510ac5,2f5e6a2c,**How to Grab specific elements from the string**,c4685e79,0.2342857142857143
11171,ff3a8ce61fab6a,ad1d7b4c,"<hr>

Let's know variable type.
# 4. Variables

We can defiend our constants using **Variables** keyword.<br>

We must run the below 👇 code into session before using our variable by consider that session variable name is **session**.
<br>

<code>session.run(tf.compat.v1.global_variables_initializer())</code>

## Exampel 1",9afe1654,0.234375
11174,c85c94076e9c3a,2282ef57,### -Null Values ,3ea0c443,0.234375
11175,faa8e6c8ab9246,e44eab20,"Fill the null values with mean, median or mode according to the requirements.",2bea1419,0.2345679012345679
11177,087e21401d7dfc,16616b03,"## Split Train, Test Data",42000489,0.23469387755102042
11178,71c3c1eab0377d,a61328eb,Column:Drop the Column Ticket,52b4e360,0.23478260869565218
11179,5f32117bcd5255,6e436fee,#### TARGET INFORMATIONS,85882abf,0.2348993288590604
11182,e4c6dd957eb5ce,192b21ff,"Cool! <br>
The total of AuthorUserId is 100,631 that is the same number that are shown in the Kernels Rank profile. ",2e383665,0.23529411764705882
11183,a0a5baa6c7e12a,992d4680,"<img src=""https://raw.githubusercontent.com/gvyshnya/tab-dec-21/main/AutoViz_Plots/Cover_Type/Scatter_Plots.png"">",551d41de,0.23529411764705882
11184,02b7e38902069e,911dc2f2,#Hi means hindi language. Not hello.,726a03a0,0.23529411764705882
11187,8f50c9c16db95f,97ebfbb7,"To understand how does each outcome exactly mean, I've created an interactive visualization below, showing the ball route per outcome and a table containg the definition.",26cc763a,0.23529411764705882
11189,4ae6a182abac64,74b35e4b,Distribution of survivals : 1 is for survival and 0 is for not,418676c5,0.23529411764705882
11192,c8dbc957870a27,3bc6bf1c,Removing columns based on the coefficient values from the [Data Preprocessing notebook](https://www.kaggle.com/aniketsharma00411/30-days-of-ml-data-analysis-and-preprocessing#Finding-correlation-between-features-and-target).,d7e84ab7,0.23529411764705882
11193,6aaee7fdbc7945,e09d0f5d,# **Import Data**,dae653ae,0.23529411764705882
11194,55c34673c1f760,65828d6b,# Data review,2663c47f,0.23529411764705882
11197,917957c6c4065f,7509cd6e,"tags는 meta에 있는 데이터들이네요.  
다만 이미지 하단에 있는 ""40대"",""여성"",""가정사"",""조카양육"" 의 태그들이, 제공된 데이터에는 포함되지 않는 걸로 보았을 때, 4개의 태그는 제공된 데이터가 수집된 시점 이후에 추가된게 아닐까 싶습니다.",55b8ed68,0.23529411764705882
11202,b779c3ce7b671a,72722b59,"
# Planet class

To store some atributes, we shall make a ``Planet`` class - and a position update rule. ",ca778770,0.23529411764705882
11203,869a39a3d4dea2,c6d3fc56,"slicing the image using the numpy array slicing, to read the portion of the image and update with different colors",9020daf8,0.23529411764705882
11205,21bce4ec54b3fa,d3f90e58,"# Sampling

In real life scenarios we would want to keep as much information as possible, given resource constraints and probably use model weights instead of downsampling. However, for the purpose of this post, let's downsample majority class to achieve 1:1 ratio, which will both speed up exploration and simplify accuracy measurement.",35546e30,0.23529411764705882
11207,ab657da5329e3f,c20deb14,"* **tfrecords** : 
 - is a simple format for storing a sequence of binary records.
 - parse with tf operations
 - TFRecord is keep serialized data in binary format, which allows efficient reading data. It significantly affects the performance of the model.
 - It is optimized for TensorFlow. It is the TensorFlow recommended format.",021526f8,0.23529411764705882
11209,1d1598b6fa2aa7,45784444,"As you can see, this chart is fully interactive. We can use the toolbar on the top-right to perform various operations on the data: zooming and panning, for example. When we hover over a data point, we get a tooltip. We can even save the plot as a PNG image.",e066accf,0.23529411764705882
11212,6b7c80ed7bd03d,5596b60b,"# 2. Conversion
We have finished to convert the given DICOM dataset to Numpy array.  
Now, take a converting process.

We can learn the detailed information of MRI in <a href=""https://en.wikipedia.org/wiki/MRI_sequence"">wikipedia</a> and summarized information is following.  
* Fluid Attenuated Inversion Recovery (FLAIR)  
* T1-weighted pre-contrast (T1w)  
* T1-weighted post-contrast (T1Gd)  
* T2-weighted (T2)  ",7bba27db,0.23529411764705882
11213,d0080e3a39bc5c,93988b1d,"
* Also, since there was a possibility of the model trained on my new Augmented Train Dataset to overfit on the training examples, I also maintained one train Dataset as it was provided on the portal, as it is.


* The thought behind this being, if I train different models on different datasets and predict on the same test set, I can expect to get some really good results while ensembling the various models as they would be pretty different from each other owing to the randomness of the splits.",2fcde4cf,0.23529411764705882
11216,523123dad03177,1528deac,Not bad,48a5e4e6,0.23529411764705882
11218,1a0bd2f72bbe36,44a9966a,#### There are no duplicate values present in the data set:,2fa311dc,0.23529411764705882
11222,c65a65d4041018,fa3ec5c5,"It isn't surprising that kagglers usually have (or plan to get) higher education degree. Master degree is the most common one (though in India Bachelor degree is more wide-spread).

It is quite interesting that the rate of having a higher degree (master and doctoral) is higher for women than for men.",824fb229,0.23529411764705882
11223,6cac6d4743088f,3e72f4d1,"### Questão 1 - Item B - Tabela de frequência

Construa uma tabela de frequência para cada uma das **variáveis qualitativas** que você escolheu (caso não tenha escolhido nenhuma, deixe esta questão em branco). Uma dica: a função *value_counts()* do Pandas pode ser muito útil. =)
",55fb02fa,0.23529411764705882
11226,395ed8e0b4fd17,2d0c8cd1,### Checking Null Rows,7573ea31,0.23529411764705882
11227,7cfd96218dd933,b0e62bdd,"#### **ATTENTION**
* UTC TIME IS 3 HOURS BACK TO TURKEY TIME.
* HOTSPOT DATA WAS RECORDED AT 20:00 AT THE MOST.",7c34d96c,0.23529411764705882
11228,907f08f9a2c6cf,843049ec,### Perform Train-Validation-Test Split,aa84c325,0.23529411764705882
11229,64169805aacf17,8a4d012e,"## Defining the `device` and make sure is `GPU` (i.e., `cuda`)",1f12ded0,0.23529411764705882
11233,52cfd66e9ec908,08bcd269,"Yes! This allows for far more detail than a simple plot without detail. I'd haphazard an educated guess, and make the following inferences:
+ Green still represents the autonomous vehicle (AV), and blue is primarily all the other cars/vehicles/exogenous factors we need to predict for.
+ My hypothesis is that the blue represents the path the vehicle needs to go through.
+ If we are able to accurately predict the path the vehicles go through, it will make it easier for an AV to compute its trajectory on the fly.",c74adcdf,0.23529411764705882
11238,e19e307b3fd188,6ea1a11e,"In São Paulo, the data remains with right skew, and most of the rental values **exceed 2,500.00** to almost **4,500.00**.",2173955b,0.23577235772357724
11245,c01049afb6d307,810119a9,"* We can create new dataframe for ID and its fixed variables.
* We can categorize Body mass index as underweight, normal, overweight, obese and re-examine it.",d37d3b5d,0.2361111111111111
11248,593d1d3d1df05a,9bc524dd,"# Selecting Boxes by Arrangement of Contours
",bc682ffe,0.2361111111111111
11249,69ac33d79f5130,f2a68fa8,#### Visualiation of the missing percentage,9d760d2a,0.2361111111111111
11250,166a62ebb4fc3a,ff56fc58,"All columns are having numeric data types except ""diagnosis"". Let's quickly analyze ""diagnosis"" column.",db48a079,0.2361111111111111
11256,016abae0483764,673cbe8a,### EDA,bc9f289b,0.23636363636363636
11258,0caaec057f7184,5b76b208,"Film release date of the items
(You can search the film if the site https://www.kinopoisk.ru/)

- item 0: 2000
- item 10: 2001
- item 24: 2000
- item 37: 2011
- item 22149: 2011
- item 22156: 2001
- item 22160: 2004
- item 22163: 2014

The item_id in category 40 doesn't really carry the launching information of the items in the 8 examples.",b875533e,0.23655913978494625
11263,d369f200a84c2a,16cd4262,# Encoder,8fef4d48,0.23684210526315788
11264,52ee792e228d54,1fbcd5df,"### The observations from above are:

#### Income and CCAvg are positively skewed distribution with some outliers towards the higher end. Chances for it being incorrectly entered or measured is less, as there are very few people with outstanding salaries. 
#### Mortgage is also having some outliers towards the higher end. It can be seen from describe function that 75% of customers had Mortgage value less than 101 indicating the high peak on the distribution plot, even though maximum was 635. Similar to above, chances for it being incorrectly entered or measured is less.

#### Majority of the people are aged between 35 to 55 with experience between 10 to 30. (Relationship between the two is evident)<br>

### Let us check how the CCAvg varies with Income",5096094e,0.23684210526315788
11265,fe7360cddc13e5,0689e42a,"B(a,b) = beta fonksiyonu

alfa ve beta = hiperparametreler",8979e423,0.23684210526315788
11267,d93a87fdbdb3d2,47a2e535,#### Combine train and test data,30d079c3,0.23684210526315788
11270,31b564f11ef638,d824c782,## Feature Engineering,424f9692,0.23684210526315788
11271,f05342aabe2b59,3a02c795,## The MAB environment etc.,cfbb391f,0.23684210526315788
11276,063a35f644e3c5,e0c1a359,"### You need to merge all the datasets with same columns names by adding a column of year to differentiate the data for different year(drop the extra columns.Also make sure that data types of merged columns are same.
",1c30fb0a,0.23711340206185566
11279,225b4fe5d3894a,4c19e4fd,"<a id=""5a""></a>
### a. Visualizing Geographical Data",4b4197b3,0.23711340206185566
11280,2ada0305b68956,e6274054,### 38. Palette = 'PuBuGn_r',133e26f4,0.23714285714285716
11282,c4bca5d86a38c3,f259f38a,"Reemplazando titulos poco comunes con títulos comunes, para así lograr tener menos títulos en la data. Luego, graficando probabilidad de salvamiento según los nuevos títulos.",e23d297c,0.23728813559322035
11283,ed8009f482b380,369798e0,"As we can see, the data is heavily imbalanced. We're going to have to deal with this later on.",e99941fa,0.23728813559322035
11284,2a56d6b0e153f2,e0f42913,"AS WE CAN SEE, THE SIGNIFICANT NUMBER OF PEOPLE HAS BEEN GRADUATED THROUGH COMMERCE & MANAGEMENT FIELD, FOLLOWED BY SCIENCE AND TECHNOLOGY AND OTHER STREAMS",8dc315e6,0.23728813559322035
11285,f2e5e9fb9eaaf7,556e52ec,"[back to top](#table-of-contents)
<a id=""4""></a>
# 4 Features
Number of features available to be used to create a prediction model are `118`.

<a id=""4.1""></a>
## 4.1 Missing values
Counting number of missing value and it's relative with their respective observations between train & test dataset.

<a id=""4.1.1""></a>
### 4.1.1 Preparation
Prepare train and test dataset for data analysis and visualization. *(to see the details, please expand)*",048e0d08,0.23728813559322035
11287,a81661cc35d8d2,493d4010,***,3331f113,0.23728813559322035
11289,a077820f7ab459,00aefc98,## Preprocessing with ImageDataGenerator,05a43104,0.23728813559322035
11291,dac3c8204a2d1b,388c4afd, Now Quickly check data dypes of each columns.,b0d2d0dc,0.23728813559322035
11296,fdbbd573ba31c2,95cc6a44,### shaft_temperature(°C),f7c28d74,0.2375
11299,5ffe6aa38958a1,823aed43,"Sex is not listed here, because it is a string field. Let us try to make this a binary field",11f5412e,0.2375
11300,979f1e99f1b309,ac81a7ca,***THE plot showing that most common killplace are above 10***,d1bfebbf,0.23770491803278687
11301,eda49464dd6d1b,94813233,"## Gender and Response
* Gender makes little difference to response, but it will be included in the models",8421f81f,0.23776223776223776
11304,659f5f3ef8aa0e,3d898fa8,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",3654c2d0,0.23809523809523808
11306,b74076b2f8ba1d,05a58857,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",9ace22d4,0.23809523809523808
11308,6471597c5d2f66,dd0cec54,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",a41b4abe,0.23809523809523808
11309,a76e0e8770b7a0,ee51a482,İstanbul is very big in Wordcloud.,02863d3b,0.23809523809523808
11310,a758983a68c014,8e3ccb90,A bit of preprocessing.,ab89f181,0.23809523809523808
11312,7454fdc444df16,da41f6b9,"## Storing the image_path, patient_id, target and x & y coordinates
We have about 278,000 images. To feed the algorithm with image patches we will store the path of each image and create a dataframe containing all the paths. **This way we can load batches of images one by one without storing the individual pixel values of all images**. 

In addition to extracting the paths, we will also extract the x & y coordinates of each of the images. Then we can use the coordinates to reconstruct the whole breast tissue of a patient. This way we can also explore how diseased tissue looks like compared to healthy ones.

We know that the path to the images is as follows: <br/>
> ../input/breast-histopathology-images/[patient_id]/[class]/[image]

Below we start by constructing a list of dictionaries and then creating a dataframe based on that list. This is much more computationally efficient than using 'append' or 'loc' methods. For more info check out this [post](https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe/17496530#17496530) on stackoverlow.",a7818ef5,0.23809523809523808
11314,60d500d196eb42,00982e7e,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",2ad55f3f,0.23809523809523808
11317,87e94f864d74be,0d769fe6,"**The missing values are in:**

* ""director"": Very less information-not needed for the analysis so I will be dropping this 
* ""cast"": there are too many diferent values so I will be dropping this 
* ""country"":Important variable hence we need to fix this
* ""date_added"": there are just a few cases, so lets scrap them
* ""rating"": there are just a few cases, so lets scrap them  ",294bfe9f,0.23809523809523808
11318,0c57e3132ae184,36ce3593,"### I. Datetime Columns
We could convert datetime columns that are read in as strings using the ```pd.to_datetime``` function, or we could pass them into ```pd.read_csv``` explicitly using the ```parse_dates=[...]``` argument.",f6bac298,0.23809523809523808
11321,fda19edaf5c621,1546f518,**Extracting out the questions from the dataset**,5cfff76e,0.23809523809523808
11322,6f4795cfdc96c7,e1b9a52e,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",1f3ab82f,0.23809523809523808
11323,31268b33de97b5,084f45bc,# Check whether our data is balanced or Not,1e6f7d14,0.23809523809523808
11327,47012add33109f,b75a16a8,## Load Dataset,b4c2e4e2,0.23809523809523808
11328,06ecf7a304c309,8e25dd4f,이제 모델을 확인해봅시다.,714de627,0.23809523809523808
11337,898d18d501f68d,f3419a52,here we see that all the column is related to cover type column so we have to do our all analysis with respect to cover type,d8bdea2d,0.23809523809523808
11338,6a80f915608fc2,a74ee262,### The c features,636938eb,0.23809523809523808
11341,b6e698d389d0d3,03c9c4ae,# split data (input data and target data),f02f68b5,0.23809523809523808
11344,53f302571cd4ac,8ba8cebd,"## Inheritance:
Inheritance is the procedure in which one class inherits the attributes and methods of another class.<br>
The class whose properties and methods are inherited is known as Parent(Super or Base) class .
The class that inherits the properties from the parent class is known as Child(Sub or Derived) class.<br>
Along with the inherited properties and methods, a child class can have its own properties and methods.",62c28443,0.23809523809523808
11346,e16860fce156b0,db18b7c3,#It took 2 minutes to plot this Dataset. In a small one it's only 3 seconds.,2054f1ce,0.23809523809523808
11349,1084376bc4897c,da3f9483,"### 3.1 For binary classification, it is important to check if the data is skewed ",1b598487,0.23809523809523808
11350,27778055896e17,f50218d5,"# **SMOTETomek**

SMOTETomek is somewhere upsampling and downsampling. SMOTETomek is a hybrid method which is a mixture of the above two methods, it uses an under-sampling method (Tomek) with an oversampling method (SMOTE). This is present within imblearn.combine module.",1dbe0165,0.23809523809523808
11351,8985a124d4b657,bede1dff,"Even after normalization and scaing, null values are possible (many people disregard this). Let's check if any null values are present.",586d1846,0.23809523809523808
11352,4b4117cf42ef8d,c48f8d95,# Preprocessing ,457cd6f4,0.23809523809523808
11354,09751c520b0616,73641674,<b>High null value feature filling with 'none'.</b>,a4d0c7e9,0.23846153846153847
11355,a4f8ad33c823c5,eb128014,A minimum threshold of 20% missing values was set to filter the columns.,fcd48307,0.23846153846153847
11356,d07915a6e6992e,17255350,"**SibSP**

This variable refers to number of siblings/spouse onboard. SibSP = 1 and SibSP = 2 shows higher chances of survival.",2b912140,0.23846153846153847
11358,bd380b97b5c894,689642d0,These two indicators duplicate each other,66f2562a,0.23853211009174313
11361,d1ff7e10ee0102,7d845c19,### Relationship with categorical features,2cc71c3c,0.23863636363636365
11362,ba655a261cc09e,5f8ff7fc,"Perhaps the passenger is a child or an adult would also be a good sign.

Возможно, пассажир является ребенком или взрослым также будет хорошим признаком.",48cc549a,0.23880597014925373
11364,1a222fee3089d2,dcf79dab,## **Cabin**,59ab8894,0.23880597014925373
11365,20b372b6e4e276,8b5bddf2,"Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972

**Upgrade:** add prediction for training data for Outlier analysis and parameters tuning",ec8b0860,0.23880597014925373
11366,21413205980558,33e7af01,# 3.1 Job,84197de0,0.23880597014925373
11368,396bc36edb95d3,26349869,#### Pairplot,965e4f8f,0.2388888888888889
11370,ac1abfe1dfe815,63d72f17,"The mean and median of the number of words in positive and neutral tweets = 14
While in negative tweet number of word tends to be much more, with mean of 19 and median 21",6529dbcb,0.23893805309734514
11371,c9b4e282e4e2c1,f56bdc1f,**B)PlayList Dataset**,f44d339f,0.23893805309734514
11376,a6b9837940ee38,1594c351,* Let's show some pictures,52d2acc7,0.2391304347826087
11380,73ca9abcc2034e,38f3748e,# Add a few data realted columns for further analysis,cec3446c,0.2391304347826087
11385,4c47839b067546,11006afa,"test['Состояние'] содержит только 1 значение, поэтому можно удалить",1f517b02,0.2393617021276596
11387,9bcfa825c8b2e6,e5087e22,"Boş değerleri knn ile doldurmak için öncelikle dataframe'in standartlaştırılması gerekir. Standartlaştırılan değerler boş değerler doldurulduktan sonra tekrar inverse.transform ile eski haline getirilir.
Gerçek değerlere göre feaature oluşturmak mantıklı olacağından bu işlem yapılır.",220f36e4,0.23943661971830985
11393,e9b9663777db82,cf7f42ef,#### 1.Quantitative analysis for Neighborhood feature,648e8507,0.2396694214876033
11394,fdc9f4863744b1,0e49e17d,There isnt any missing data value in the dataset however there are 765 duplicates. I will remove the duplicated data so it doesnt hinder the analysis.,b4529365,0.23972602739726026
11395,738bfced935b69,7edba060,The distribution is normal,2d3c592d,0.23972602739726026
11400,caaa6793391520,6df022cd,## Proposed solution ##,1e79f342,0.24
11403,9395559895004f,63681163,## Label Extraction,b5a0494b,0.24
11404,cee088a6840708,de19e3dc,"# Step 4 -> Load the data

For a very secret reason, I will not say why I loaded the data this way. 

Here I loaded the cora dataset. (Google it for more information )",55463e1c,0.24
11411,519e936017c30a,89132e41,"Como se puede apreciar en el cuadro anterior, existe una gran correlación entre las ventas de las distintas zonas geográficas y las ventas globales, dando lugar a que si se produce un aumento de una unida monetaria en cualquiera de estas variables, se producirá también un aumento en la otra variable. Por ejemplo, si se produce un incremento de un dolar en las ventas en Europa, se producirá un aumento de las ventas globales.

Ahora, vamos a estudiar la evolución de las ventas globales a lo largo de los años.",dc34915d,0.24
11417,274b32da3b19a8,056e6515,## Load Train Model,408f7268,0.24
11418,83df814455f06c,8a1950ea,### View top 5 rows of dataset,c9cff71a,0.24
11424,7e1da639035ac5,a88ed262,### <a id='6.3'>6.3 Average Economic Need Index in different cities</a>,120b6c23,0.24
11425,b10bd75889dad9,15e39819,#### preserving the numeric columns in a list,ee00ceee,0.24
11426,21c1e34efd71b8,7764177b,IN the code below i used Stratified K fold validation method to find the accuracy and the Recall score. ,23b2cdd6,0.24
11432,67b7354e96113a,476bd5af,"**Imputing the missing values:**

As name is nominal column and it has title's for each name (eg.**Mr.** Donald) and it is one of the best key to impute age.",dca94250,0.24
11433,0687cd5c8597db,2439e611,### **Displaying single Image**,4edec76a,0.24
11436,44f6a002ecd033,85504702,"From what is seen above it might be helpful to convert some of these columns to boolean. However we will do some categorical encoding, so it might be better to just leave as is.",70bbe106,0.2403846153846154
11437,08f845750d026a,1b69df2b,"
## Which sector are most of the borrowers in from India?
",1c54de30,0.24050632911392406
11440,9ad9a97e628bfa,e5a72a9f,"티켓 번호 앞의 코드는 출발지, 도착지나 기타 탑승자에 대한 정보를 담고있을 확률이 크다.  한번 자세히 살펴보다",0a7e1136,0.24074074074074073
11445,ab6da5994949a3,a20acac8,# Artificial Neural Networks (ANN),fae6b91d,0.24074074074074073
11446,2f0f808765fc67,510d57b1,FEATURE ENGINEERING AND GET SOME NEW FEATURES AND DROP SOME USELESS OR LESS RELEVANT FEATURES,fd1f6494,0.24074074074074073
11448,233cb23d9e01b9,e61c5ade,# Data Split,ffa56c19,0.24074074074074073
11449,c80939c7c626cf,2afec16b,"Feature Engineering is the process of using domain knowledge of the data
to create features( vectors ) where ML algorithms work

# Features can be said as columns of the data!",b9ac31e2,0.24087591240875914
11450,3c2033cc99c12c,9509235c,"**Note:** *In this part I will further process the data due to the imbalance, I choose the method of Under-sampling*  
**Steps:**  
+ Have a brief view of the ratio between two classes  
+ Randomly choose the same size of data of Normal class 

**Attention:** *The use of Under-Sampling will bring a huge amount of information loss, which may have a impact on the accuracy!!!*",dfa22a54,0.24087591240875914
11456,84127ade6fde87,0d25746e,"There’s one more detail we need to take care of before we proceed: encoding. This is а pretty vast subject, and we will just touch on it. Every written character is represented бы a code: a sequence of bits of appropriate length so that each character can be уникуели identified. The simplest such encoding is ASCII (American Standard Коде фор Information Interchange), which dates back to the 1960s. ASCII encodes 128 характера using 128 integers. For instance, the letter a corresponds to binary 1100001 or децималь 97, the letter b to binary 1100010 or decimal 98, and so on. The encoding fits 8 bits, which was a big bonus in 1965.",f55d05b6,0.2413793103448276
11457,b91c9eef23d284,0edbab06,#Codes by Anil Govind https://www.kaggle.com/anilreddy8989/stopwords-word2vector,8fb353fa,0.2413793103448276
11468,d5f78aa381f58d,da403e93,# Data Visualisation,d60f358f,0.2413793103448276
11471,6a1d04e8153df3,8ec6b7aa,"**2. Visualizing the data**
For visualing the data I am using seaborn and matplotlib libraries .",38572b05,0.2413793103448276
11476,9535bb04ae042c,7a4b3e35,##  iii) Separating Dependent and Independent Variables,165b6fae,0.2413793103448276
11478,1cd8be6e679620,7319d8eb,## signal_to_noise Histogram,3ce15a43,0.2413793103448276
11479,a1ba5ffd30dbde,2b902381,"- As we had seen previously in statistical summary, we had some outliers also present in data",48e57546,0.2413793103448276
11482,a4f0a3e1316ff9,024d3f8e,"# Create Day of Year Categorical Column

It is useful to break the data up into days and months",53bf0160,0.2413793103448276
11487,f3c6048d1058e3,acd2927f,"<b> In machine learning task, cleaning or pre-processing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is of most importance. IMDB reviews are posted by users manually, so we observe high usage of contractions and chat words in it. Also, some reviews are collected from other sites, so we also observe usage of many HTML tags in dataset.</b>

**a. Clean Contractions or Chat Words:**
As this is manually entered reviews, people do use a lot of abbreviated words in chat and so it is important for us to expand all such chat words and contractions used. I’ve used list of slangs and contractions from repo.

**b. Lower Casing** Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.

**c. Removal Of Stop Words**
Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language is,

**d. Stemming** Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. For example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. In some cases, stemming results in shorting the world literals and we lose information in it. So Lemmatization is better process.

**e. Lemmatization**
Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language. As a result, this one is generally slower than stemming process. I’m using standard WordNetLemmatizer for work.

**f. Removal Of Emojis & Emoticons** With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. As IMDB reviews are manually added, we found usage of Emojis & Emoticons, so we removed them.

**g. Removal Of Urls & HTML Tags:**
We found large usage of HTML tags in dataset. To make sense of dataset, such tags to be removed.

**h. Removal Of Punctuations** In this process, we remove the punctuations (!""#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~) from the text data. This is a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way. Note of caution- This process has to be performed after removal of HTML tags else some standard tags of HTML will partially get removed in this process and afterwards HTML removal process will not give suitable results.",1d9056b0,0.2413793103448276
11495,20e1ba19eb9b5e,5f68a318,#  2. Data Processing,4569bfc1,0.2413793103448276
11497,858da4bb312f67,968f9409,"## Define Cycle GAN Model
Models are based on Amy Jang's great notebook. Thank you for sharing!

https://www.kaggle.com/amyjang/monet-cyclegan-tutorial#Visualize-our-Monet-esque-photos

The only difference is the discriminator. I use pretrained model of VGG16 instead of the model given in the original notebook because this classification task is much more difficult than the original task(Monet or photo). 
",9cca4391,0.2413793103448276
11498,fb5c6021d127ef,54c3d16a,"## 2) Exercise: Query the training data

Write the query to retrieve your training data. The fields should be:
1. The start_station_name
2. A time trips start, to the nearest hour. Get this with `TIMESTAMP_TRUNC(start_time, HOUR) as start_hour`
3. The number of rides starting at the station during the hour. Call this `num_rides`.
Select only the data before 2018-01-01 (so we can save data from 2018 as testing data.)",dd05cbd3,0.2413793103448276
11499,726833f92fb87a,b608f3ce,"The distribution is heavily right skewed, with some negative values as well.",7dc5e1b6,0.24161073825503357
11504,917957c6c4065f,fcde197f,![image.png](attachment:image.png),55b8ed68,0.24183006535947713
11509,ad26c020235dfc,250f14cf,"# Models
The goal is to generate four mathematical models, one for each category of waterbody (acquifers, water springs, river, lake) that might be applicable to each single waterbody.",bf766e48,0.24193548387096775
11511,840534f2908a9c,3f78d5d4,**Date time features**,8081c3cc,0.24210526315789474
11515,dbd96dd275dc60,1e241c54,"# 5. Modelling

We've done enough EDA (we could always do more) but let's start to do some model-driven EDA",1ed493a8,0.24242424242424243
11516,70193f0c034b98,9b1d8b55,# Mixup Implementation,f8cacd26,0.24242424242424243
11518,b241b847319d13,bff42a41,Here I'm using an updated csv file not the original file provided in the competition.,0fb698f0,0.24242424242424243
11520,a2573183738753,a79cc7be,# plot few samples,f6429599,0.24242424242424243
11526,efd44ce2c08541,ec0ca95d,# Similarity: Multi modal (NFNet-F0 and Indonesian BERT (concatenated at final feature layers)),ebc2d00c,0.24242424242424243
11527,7f74a04ae75792,44af4ec9,"It might be better to combine E, F, N & L because their values are insignificant",d01e91da,0.2426470588235294
11530,98a6794067932a,26cfc5e4,"Pour ce qui est de la cellule ci-dessous, elle permet dans un premier temps d'évaluer le nombre de commandes ayant été effectuées par chacun des segments. Ensuite, dans un deuxième temps elle permet de mettre ces informations sous forme de tableau. Finalement, le code permet d'ajouter une colonne indiquant le pourcentage des expéditions par rapport au total des expéditions qui est associé à chacune des catégories de client. Cette analyse pourra être très utile par la suite, car elle permettra aux dirigeants de constater quelle catégorie de client représente la plus grande proportion de ses expéditions.",08600fe2,0.24271844660194175
11532,b01ee6cb674fa3,e2d03ff9,"yes, the company name is Sea Launch. 

according to wiki

'Sea Launch is a multinational spacecraft launch service that used a mobile maritime launch platform for equatorial launches of commercial payloads on specialized Zenit-3SL rockets through 2014.'

And this information is verified by the rocket name, Zenit-3 SL, datetime of the launches and the name of the mission (all have sat names)

Also according to wiki, its headquarters is at Nyon, switzerland

Because it is an international consortium, it has no specific country and so it will be left as 'pacific ocean'",a8ffd35e,0.2427536231884058
11540,2730840089c8eb,36f0806e,"## Strings are sequences

Strings can be thought of as sequences of characters. Almost everything we've seen that we can do to a list, we can also do to a string.",34d27dac,0.24285714285714285
11541,2ada0305b68956,133760d7,### 39. Palette = 'PuOr',133e26f4,0.24285714285714285
11542,c84925c8171900,b20e3344,"<a id=""yearimpute""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            4.1 Year Imputation
            </span>   
        </font>    
</h3>",e21ff7ec,0.24299065420560748
11543,62037c5832129c,b56dd94d,"## The holdout method
* Holding the test dataset by splitting the dataset into training and test set. 
* Classic and most popular approach.
* In typical machine learning application we change hyperparameters to tune the model so we select the best model in the end.
* When we are doing parameter tuning and running the model we are using the same test set over and over for **model selection** so we will most likely overfit.
* Best practice to follow in holdout method is to split the data into training data further into training and validation set. We are changing the hyperparameters and using the validation test to test which model is a better fit.
* Finally we use the test set to run against the best model to evaluate the performance.
* **Disadvantage** - The performance estimate is sensitive to how we partition the training set into training and validation subsets. Hence k-fold cross validation",61474350,0.24324324324324326
11546,63b44c85e32c1f,96993490,"Lists can be concatenated by adding, '+' them. The resultant list will contain all the elements of the lists that were added. The resultant list will not be a nested list.",fb9b9562,0.24324324324324326
11551,a6c34cd514e30e,663a9f04,Now you're ready to read in the data and use the plotting functions to visualize the data.,bf603ddd,0.24324324324324326
11558,297cbe4a23c4bf,12bf8358,# Dealing with Missing Values,a843e619,0.24324324324324326
11563,4d91e84c564cbe,a2667ff5,"If I leave out the end index, it's assumed to be the length of the list.",355a43e3,0.24358974358974358
11567,d4c5aaa4b36810,a8b24d9a,To start with I will I will use the most recent data for each indicator and see how many data points are missing.,65441f28,0.24358974358974358
11568,c4386b8a01d66e,76466203,## Check for Correlation,dc732bf5,0.24369747899159663
11573,8cefb86a675e5d,854a7ed5,**Specifying the Prediction Target**,79f9e69b,0.24390243902439024
11575,47b2c9be5e31cb,bafe155c,### Let's check 1st file: /kaggle/input/CAvideos.csv,7d4afe56,0.24390243902439024
11577,514d8de15cb7ef,2ad2443e,"<p> Now the basic things done while removing these less important words were 
   <ol>
    <li> Tokenizing the words </li>
    <li> Converting all of them to lower cases </li>
    <li> Removing all stopwords </li>
    <li> Performing Normalisation only stemming on them. </li>
    <li>Joining them back to sentences and replacing them with their orignials in the dataframe </li>
   </ol>
</p>",cfe111b2,0.24390243902439024
11579,0e09587faffa8f,0a949889,## IV. Analysis,0d563d61,0.24390243902439024
11581,dbccf99c49570f,f64a14ff,# Data Preparation,c20fc09e,0.24390243902439024
11582,8d0aebab1e5914,464848de,### Display or Plot above label,084e671f,0.24390243902439024
11584,e19e307b3fd188,3e491271,#### Boxplot,2173955b,0.24390243902439024
11585,62582b8036fbfe,12a72a44,Number of 0 are greater than 1 so we will undersample the data to match 0 and 1 by random sampling approach,6c2160db,0.24390243902439024
11588,ce9ed5e2d601d7,a7d70fd6,"## Feature Utility Scores

Use mutual information to compute a utility score for a feature, giving you an indication of how much potential the feature has. This hidden cell defines the two utility functions we used, make_mi_scores and plot_mi_scores:",f58a2f43,0.2440944881889764
11594,892be0a523578c,ee92ff1e,"#### 4. minuteCaloriesNarrow_merged.csv, minuteIntensitiesNarrow_merged.csv, minuteMETsNarrow_merged.csv, minuteStepsNarrow_merged.csv
These four datasets has the same number of records and matched id and timestamp, so they can be merged together. Here I chose to use the long format version. I found that the minutely data can matched with the hourly one, which means it has the same problems too.",b0e8d7c0,0.24444444444444444
11596,d6cbd7160961dc,c00d3a9a,## 4.1. Creating the dataset,36d74664,0.24444444444444444
11599,c73e07ad6d25c5,db348569,## Sex,3ab391fb,0.24444444444444444
11601,42e0005bed28aa,2ffb12d1,>Normal Images,5616d451,0.24444444444444444
11603,c8bf959b9608cf,c2a67477,"Once the candidate image is generated, we will plot it to see the result after deprocessing it using the following function.",155e3672,0.24444444444444444
11605,e25c0f830df3f4,cd7b8d1d,# Calculating Sentiment polarity for each comment,fdcf7189,0.24444444444444444
11606,d58491f2896fc1,f66143d8,"
<center><img src=""https://i.hizliresim.com/1fd4j5a.png"">",514bfdff,0.24444444444444444
11607,3597174a998d4d,b4b31db7,"It can be concluded that 
* The distribution channel with the highest number of booking is TA/TO, which also has the highest ratio of booking cancelation.
* For GDS, the City Hotel's ratio of cancelation is much higher than the Resort Hotel's. But GDS's number of booking is low.",276892ed,0.24444444444444444
11609,5be39e4e35cec7,c782c017,"<a id = ""3""></a><br>
# Univariate Variable Analysis
* Categorical Variable Analysis: Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, SibSp and Parch
* Numerical Variable Analysis: Fare, Age and PassengerId",14d617c9,0.24444444444444444
11613,a566b5b7c374e7,be55a148,"#### Initial Impressions:
- Of all directly measured metrics, my restfulness appears to correlate most strongly with Oura ""Average Resting Heart Rate,"" ""Average HRV, and ""Deep Sleep Time.""
- Sleep Cycle start time (""Start v 12AM"") and end time (""End v 12AM"") appear to have moderate correlations with ""Average Resting Heart Rate,"" ""Average HRV,"" ""Deep Sleep Time, and ""REM Sleep Time."" I wonder if this has to do with differing habits on weekends, when my start and end times tend to shift.",b3dc5545,0.2446043165467626
11623,eb33e05704d647,610bf800,Classifier layer,cd80436d,0.24489795918367346
11628,629f2918807a9b,dcb20f37,"#### Answer1: Best selling Book under Guftugu Publications is "" انٹرنیٹ سے پیسہ کمائیں"" with 3012 ""completed orders""",be56dc84,0.24509803921568626
11629,52cfd66e9ec908,f6aee2ce,We also want to see how the whole charade of vehicles ,c74adcdf,0.24509803921568626
11630,71b75664517244,046d8b4d,"### Monster

The most participant on premier league here is Chelsea, Liverpool, Everton, Arsenal, Manchester United, and Tottenham Hotspur. They play for the whole 28 season and never relegate to lower league. What a monster..

It is quite interesting to see team like Everton and Tottenham Hotspur never won premier league even though they played for all season.",fc905af5,0.24509803921568626
11631,842547b2def18c,f70be2ab,"### 相関関係 (数値変数-順序変数)

1つのプロットを使って相関関係を特定するために複数の特徴量を結合することができます．これは数的な値をとる数値変数/カテゴリカル変数によって片付けられます．

**観察**

- Pclass=3の乗客がほとんどですが，そのほとんどは生存しなかった．我々の想定が確認された． (classifying #2)
- Pclass=2とPclass=3の乳幼児の乗客はほとんど生存した．よって我々の想定は定量化した． (classifying #2)
- Pclass=1の乗客のほとんどが生存した．我々の想定が確認された． (classifying #3)
- Pclassは乗客の年齢(Age)の分布によって変化する．

**決定**

- モデルの訓練のためにPclassを考慮する．",b8efde6d,0.24509803921568626
11639,510b8303776bb6,319b2e83,In this we have added an extra visual column for the box plot.,18080db8,0.24528301886792453
11648,5ce12be6e7b90e,162ed7ee,Remainder (modulo):,c0ab62dd,0.24561403508771928
11649,6cade0b6a41ba2,1c4f5a60,## 3.3. Age,e6110293,0.24561403508771928
11651,30fdc4a6e3c1db,09f5367f,"What we see:
* Winscoin spends the highest in food 72%, 8% in hobbies and 20% in household
* California spend 67% on food, 11% on hobbies and 22% on household
* Texas spends 69% on food, 8% in hobbies and 23% in household
So we can see that all the 3 states have a bit of differences in spent",6111ddee,0.24561403508771928
11654,54004b32784b68,2d0ac09d,I dropped top 5,27213ca9,0.24561403508771928
11670,3cb96bd8eb364b,9f1cbe6e,#### Numerical,3157af7e,0.24615384615384617
11673,03048e86a6d806,e01bfdb4,### Programming Experience,1285c231,0.24615384615384617
11676,c115e287523aab,c16324c6,# Set Seed for Reproducibility,feb1288b,0.24615384615384617
11677,a8c042af6b7245,1cd1c2fb,"### Descriptive statistics

We can also apply the describe method on the dataframe. However, it doesn't make much sense to calculate the mean, std, ... on categorical variables and the id variable. We'll explore the categorical variables visually later.

Thanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics. To keep things clear, we'll do this per data type.",2487ac62,0.24615384615384617
11683,548f961125248d,ffc97fa2,### Correlations,d8c5e8b8,0.2463768115942029
11688,a1a31459abf078,08b6d4fa,"* ```answered_correctly``` has three distinct values. Value -1 represents records where user didn't answer a question and watched a lecture instead
* In our sample of training data imported, majority of data is for questions answered. Less than 2% of training data is for lectures watched by students. 

## Question and User Level summaries",66fc0f54,0.2463768115942029
11690,0e2a23fbe41ca9,61c0be6c,"Observations:

- all anonymised features are similar in value ranges for the target column
- -33 target value is quite distinct across all the variables
- Maybe, -33 is a default value of loyalty score.


### 6. Year vs feature_1
",64e4762c,0.2463768115942029
11692,9d9da6c439b96b,b9128f00,## Data Duplicate,361cc7d9,0.2463768115942029
11693,598b6228760590,668a16b6,"- Okay, there is no age less than 0. We don't need to worry.",be30ab66,0.2463768115942029
11694,91473a39b85068,2652500b,"#### Basic Data Analysis on Tags
Frequency of tag_count",6e3d91c2,0.2465753424657534
11699,3d08ca7656dec0,3d376e99,## exng,bd3f87e3,0.2465753424657534
11702,2bd6c370695ea7,27d109e0,## Tfidf,cbe6aec8,0.24666666666666667
11703,c13f73168789c2,7e1b70d0,"### 1.6 Accessing values from multiple columns of same row<a id='8'></a>
Syntax : `df.loc[row_label, ['column_name1', 'column_name2']]`",16175052,0.24675324675324675
11710,663bbc9eaf267b,b818fd3f,"* Except from the first car, we can see they all are new cars with low mile on the clock. No suprise if they cost as a brand-new car.
* However, the first car of this list doesn't follow this pattern. So let's see why this car is far more expensive than others. Maybe it has something to do with the model.",32445529,0.24675324675324675
11718,04ff2af52f147b,c40f7775,"**Filling Age Null Values:**

We first check to see if *Age* is correlated with other numeric features.   If it is correlated with other features, we can fill in our missing values more appropriately than a simple global average.",d5f37be9,0.24719101123595505
11720,0caaec057f7184,8e001e41,## Shops,b875533e,0.24731182795698925
11725,04bac111ffbe9c,1c8ecd4f,##### 1st METHOD : THE ONE I GENERALLY DO,82576b17,0.24761904761904763
11726,7a058705183598,1256ab47,Converting target column into numerical values,b0ead917,0.24761904761904763
11729,55a5e31d03df9f,2f5bc694,"We can also generate a summary of our model and see the shapes with the summary function, as the following:",06dce00f,0.24761904761904763
11731,bd380b97b5c894,170e9609,## missing values,66f2562a,0.24770642201834864
11733,c9b4e282e4e2c1,30139585,1-Reading and preprocessing the data.,f44d339f,0.24778761061946902
11737,49ee86d074de69,29ee1b3e,"<a id = ""10""></a><br>
## Observe Reason for Absence",71ccc6d3,0.24786324786324787
11738,2f47abddfd1928,7486e954,There are still two missing values. Let's inspect them one by one to understand why.,ae33cc0b,0.24793388429752067
11740,ba4b3bd184acbb,68ccf745,Another convenient functionality of the `loc` method is that it can be used to add rows by creating index labels if they do not exist.,0f5de724,0.24812030075187969
11742,c80939c7c626cf,9ea46481,"# Now  we should know that How Titanic Sank?
Actually Titanic sank from the part where Passengers of 3rd class category were located",b9ac31e2,0.24817518248175183
11744,b61ab8f81dc03d,5d184565,"Drop rows in training data that have the target value missing (Survived column)

Example: train_data.dropna(subset=['Survived'], axis=1, inplace=True)",64d05394,0.24822695035460993
11746,726833f92fb87a,c459be70,## Campaign analysis,7dc5e1b6,0.2483221476510067
11747,5f32117bcd5255,172b21c5,#### EXPOSURE INFORMATION,85882abf,0.2483221476510067
11748,917957c6c4065f,1eed3114,영상 설명에는 태그가 3개까지 표시되네요.,55b8ed68,0.24836601307189543
11749,2ada0305b68956,d1b755f0,### 40. Palette = 'PuOr_r',133e26f4,0.24857142857142858
11752,84e0e568316ba1,d86d96d5,# Experiment,9ebad019,0.25
11754,c5fef7cc592736,451c24d3,"In this to objects we have the data ready for the `DataLoaders`. Now we need to create the transforamtions to tell the dataloader how to convert it to usefull tensors for the training.
We use the decorator `@Transform` to indicate all of this fucntions are transforms",d21dc2c1,0.25
11756,bd0e173abb7b52,a1dfd794,**2. What is the average age (*age* feature) of women?**,9bce3b0d,0.25
11760,1a285e4c830f3f,0f2d4f69," 
### Parameterkurven für Linearen SVC",360b50e9,0.25
11762,99f84fa59cb1da,991a32fe,### Target variable,41e95f63,0.25
11763,cf46cd6f7c55c0,27607a0e,"# Apply Pseudo Labeling to Instant Gratification Comp
## Load Data",191b86b8,0.25
11764,96c4c0e36b8ec0,41bad14c,**Age distribution**,4dd6de8c,0.25
11767,999258a81ba32a,e4307a1d,# Ball 2 Frequency Chart,48cd3d21,0.25
11777,90964081c7faab,c4f51e25,"From the initial heatmap, there doesn't appear to be a strong correlation between a term deposit subscription and our quantitative variables. Duration and previous have the strongest correlations, but we will not keep duration in mind because of the concern from the dataset provider.",b423b0c3,0.25
11778,7686f42e1f28d2,86e90c7b,"There are nulls in the `description` column, so we'll have to deal with that later. Let's also convert the `date` column to `datetime64` objects",6c128859,0.25
11780,117fc0956643d0,7cd1e791,"<div id=""step1""></div>",68cef9fd,0.25
11786,7f74a04ae75792,a4862130,"### Combining E, F, N & L
",d01e91da,0.25
11790,386c42a7fb27a4,45e59f9c,#### Categorical Columns,9e9f6974,0.25
11791,1014e6be391084,4afef818,Distribution of the amount,46f9168f,0.25
11793,f18e737fcc4b06,e52b62dc,# Variable Description,087b8637,0.25
11795,bc058fe14d3d1b,c6da986e,"### 1.1 drop duplicates, remove train which is not in test",d0273670,0.25
11801,98ea617d18c9cc,c5c86069,# Performing data oversampling and UnderSampling,e6316d11,0.25
11808,541d0fa0e26b80,8d97b941,# Visualizing the dataset step by step.,a29e0f29,0.25
11810,37b09262279764,c8d00828,<b>Most of the people in the dataset are between the ages 20 and 40</b><br>,37c4c417,0.25
11813,63d0d9b9a8c7d2,4bb380b3,**Trying to understand X's and y's**,e32e5933,0.25
11816,49f2274c1dd516,d4bb780a,Use pivot table to turn into binary matrix with columns as files and rows as available features. A 1 will represent it is present in the data and a 0 that it is not.,06b0ffee,0.25
11822,585c280865b46e,a041569d,# Normalization and log ,4d6056f1,0.25
11824,c65a65d4041018,d64a25b6,### Major,824fb229,0.25
11826,18ce858f90966d,94d0f0df,# **Data Preprocessing**,09e9caed,0.25
11833,fae5023faa435f,d9ed47ae,Last 6 months,b37c893b,0.25
11834,9a040a4f21091e,50d7a0a7,"So the dataset is clearly biased towards non-toxic comments. Since I'm not quite sure where this data came from, it does make me wonder whether this proportion is close to reality. I'd imagine it varies widely depending on the source; for example, I would guess that 4chan and Pinterest vary widely in their proportion of toxic vs. non-toxic text.",f591b57d,0.25
11839,ea2763c0f6c6a0,826d574a,"# The type of attacks

Most of the recorded type of cyber attack is espionage.",e5812ac1,0.25
11842,0dd3ac2d55efd7,d11a836f,# Word embedding models,e9aa2cc2,0.25
11844,d0f6276d5b628c,daae48c7,"# Movie Recommendation based on votes and ratings :

In this part we are going to check for the highest rated and the highets voted films. In this type section we can find the best suited movie for the mass and then can produce the same type of criteria which will be beneficial for the production houses to collect more profits.",c64f5ce5,0.25
11852,2ca509e51a6e4b,f98d6e47,"# Encoding with Singular Value Decomposition

Here I'll use singular value decomposition (SVD) to learn encodings from pairs of categorical features. SVD is one of the more complex encodings, but it can also be very effective. We'll construct a matrix of co-occurences for each pair of categorical features. Each row corresponds to a value in feature A, while each column corresponds to a value in feature B. Each element is the count of rows where the value in A appears together with the value in B.

You then use singular value decomposition to find two smaller matrices that equal the count matrix when multiplied.

<center><img src=""https://i.imgur.com/mnnsBKJ.png"" width=600px></center>

You can choose how long each factor vector will be. Longer vectors will contain more information at the cost of more memory/computation. To get the encodings for feature A, you multiply the count matrix by the small matrix for feature B.

I'll show you how you can do this for one pair of features using scikit-learn's `TruncatedSVD` class.",e63b0ba6,0.25
11855,7c7a7db391c517,f63a50c9,## 2.1 Loaded the neccessary python package,f53450dc,0.25
11864,565ad413cd802f,dde4654c,"Transforms can be chained using `transforms.Compose`. For instance, you may add `transforms.Resize(128)` before `transforms.ToTensor()` to resize images to size 128x128 before converting them into tensors. See the full list of transforms here: https://pytorch.org/docs/master/torchvision/transforms.html",397b074e,0.25
11865,44f6a002ecd033,1ba0bee8,## Cleaning the Data,70bbe106,0.25
11871,a1c7a94fc12ad8,6324ae10,"## Preparation
Setup environment",c67e3237,0.25
11872,69130a37583a06,e57a2bb9,"### Transaction Amount distribution :
 
 * Transaction Amount is countinious variable which is right skewed in nature. as we can see from graph and quantiles values.We can handle these values using two technique first remove them from variable list .second use log transformation to concise these  value smaller range.
 * Second row plot is showing transaction Amount distribution for all type of transaction, analyzing that graph we still can not extract information related to transaction amount .
 * In third row ,we have plotted separately transaction amount for fraud and no fraud transaction  and here we clearly observe that transaction which fradulent in nature  have generally more trasaction amount value rather tham no fraud transaction.",65a4de1c,0.25
11873,a148458c0aac92,9ae81630,<h1 align='center'> Exploratory Data Analysis </h1>,6d74cbcf,0.25
11876,e82462cdc998a7,a5b0a494,"## 4. FE & Data Preprocessing <a class=""anchor"" id=""4""></a>",b39bf244,0.25
11877,8bb432d338a70b,1648d895,Arbol de decisión,7aab1dfd,0.25
11879,8dd655515e7d18,f6d2df88,### Univariate Analysis - Numerical Columns,895f41cf,0.25
11882,08e3444f9eddcf,fdfd6eab,# Group and Reduce,1d9d4f73,0.25
11885,8447633e1d256c,1541e456,## Data Preparation,60d593ce,0.25
11887,b3681fd423741d,0425f845,# 2. Kota apa yang memiliki mobil bekas paling banyak?,1aec06ce,0.25
11890,2c8119a4061997,ae17147e,# Model,1836a79c,0.25
11893,76d94f2011a1cb,0617086b,"## Transforms

From the competetion details, we know these images were taken from air. So they resemble satelite imagery. Looking at the images though, they are very low resolution and look very ugly when blown up :D

Anyways, some features are apparent from looking at the data:
1. Some pictures are flipped vertically.
2. Some are rotated.
3. Some are zoomed in and some aren't.

Lucky for us, fastai has some default transforms ready. All we need to do is to plug them in. Default transforms include zooms, rotations and lighting.
I'm just adding vertical flip in to account for aerial imagery.",9820aca8,0.25
11894,13c7672da1b571,f9d9d8b2,"Check for outliers by using PCA and visualising first 10 components, which explain most of the variance in the data.",002d3ec0,0.25
11900,be53cf61cd596f,07c46db6,"- example:
- (60000,28,28)→（the number of samples, time_steps, input_size）",704627de,0.25
11901,dc0b0e1cb46c6f,2420e344,"As we can see, not all of the ISO_CODEs has all the dates. So we are going to create a row for those dates with missing values.",47b17a7b,0.25
11903,b547f0f38f7744,1945c91a,There are `3422-3373=49` images which do not have any wheat heads in it (without annotation). ,b6ba66b3,0.25
11904,6e9b4020644836,498c6acb,"<a id=""3""></a>
# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Feature Engineering</p>
1. Summarizing Data",5ad41fc6,0.25
11906,da199f8fb59439,91950090,"**There are 5 component with null data**
* Director
* Cast
* Country
* Date_added
* Rating",baaa665d,0.25
11908,62487bcd70b199,a4af381a,"## Inference:
there are many cases where if income is above 100, then personal loans are taken, There could be 2 reasons
1. people with lesser incomes are not preffering loans 

or
2. people withe lesser incomes were not sanctioned loans

So for marketing campaign, it would be better to target people with higher incomes are there will be more sucess rate",f6ae50af,0.25
11916,109169be630edc,72f6e0f0,#Codes by Satyam Prasad Tiwari https://www.kaggle.com/satyampd/imdb-sentiment-analysis-using-bert-w-huggingface/notebook,d1b2947c,0.25
11917,712198370d5521,e8a44990,"Creating a feature **(""Customer_For"")** of the number of days the customers started to shop in the store relative to the last recorded date",5882e04c,0.25
11920,e4c6dd957eb5ce,54806fe4,## Users Achievement,2e383665,0.25
11927,b290039151fb39,36be907b,## Data,1836a79c,0.25
11928,ff83da40bcdb19,03bf21a0,"**PART 3:**
It's actually not different than the tranin data. 

Only this time we are not giving the run or walk we just give the image data and image id. *",36b5ec8c,0.25
11929,5083d7a61f2426,52b94868,"Scaling the values
It is necessary to reshape the values to 2 dimensions",541a0fec,0.25
11931,9ec2fb131cf677,81c1bd35,# TED TALKS Events Statistics,211ea6bd,0.25
11934,02773bdc5d3c7a,7ca21c26,*2. Import the data into a dataframe and preview the target column*,86245f35,0.25
11937,0119199286f381,c4e38f23,"Since all columns are of type object, so we will have to bring them all in type numeric to make it in machine readable format.",0db72675,0.25
11938,6f05f4ea9addbf,aced6639,"We observe that the total number of column with missing values is 8-27 which is less that 0.005% of the dataset.
It is convinient to get rid of the rows with missing value since the affect on the dataset will be very insignificant, but to avoid any issues since it is a time series analysis, we will forward fill the null values instead of dropping them.",dfb04c84,0.25
11941,1c381451c17150,e08dd730,"# Prep the Text for the RNN
Next we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. 

Note that if you want to replace the Monty Python scripts with some other text to duplicate, here would be the place to do it. Just replace the All_MP_Scripts with any other text file and the rest of the notebook will run the same. (the bigger the better, anything ~1MB+ is great) ",e79b530f,0.25
11942,fdbbd573ba31c2,90b922a4,### blades_angle(°),f7c28d74,0.25
11946,0932046e1f485d,1ae81a69,## <a id=numerical_data>Numerical Data</a>,218cc7a3,0.25
11947,9f0ccf5b9e8f03,b574f717,"#Kawasaki-like disease: emerging complication during the COVID-19 pandemic  by Russell M Viner and Elizabeth Whittaker
Published:May 13, 2020DOI:https://doi.org/10.1016/S0140-6736(20)31129-6

In Italy, ten cases were described (seven boys, three girls; aged 7·5 years) of a Kawasaki-like disease occurring in Bergamo,at the peak of the pandemic in the country (Feb 18 to April 20, 2020), a monthly incidence some 30-fold higher than observed for Kawasaki disease across the previous 5 years. Bergamo was the city with the highest rate of infections and deaths in Italy at that time. Within the cluster were five children who had features similar to Kawasaki disease (ie, non-purulent conjunctivitis, polymorphic rash, mucosal changes, and swollen extremities); however, another five children presented with fewer than three of the diagnostic clinical signs and were older than patients with classic Kawasaki disease. There was also a high proportion of shock, with five of ten children presenting with hypotension requiring fluid resuscitation, and two of ten children needing inotropic support. Two of ten children had a positive severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) PCR swab and eight of ten had a SARS-CoV-2-positive serology test; however, these tests were not done contemporaneously with the episode, so the clinical relevance is unclear. The majority of patients with Kawasaki disease respond well to intravenous immunoglobulin; however, 10–20% require additional anti-inflammatory treatment.These differences raise the question as to whether this cluster is Kawasaki disease with SARS-CoV-2 as the triggering agent, or represents an emerging Kawasaki-like disease characterised by multisystem inflammation. The diagnosis of Kawasaki disease is based on clinical and laboratory criteria and is hindered by the lack of a diagnostic test. Understanding the pathophysiology of this emerging phenomenon might provide welcome insights into our understanding of Kawasaki disease.https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31129-6/fulltext",66691203,0.25
11948,87e94f864d74be,45413562,"### Let's fix the missing ""rating""",294bfe9f,0.25
11951,2e4b36a0bd7613,27bbcabc,#Codes by Anil Govind https://www.kaggle.com/anilreddy8989/stopwords-word2vector,cd4d333e,0.25
11953,95656e8d666b16,3eb1cdd4,Seems to be a little bit skewed,65e88599,0.25
11954,1d73d04c3aaae8,b3ec6042,There are only 7 games with predicted ties. These were all in league start-up seasons when there was no prior history. ,cd43d0aa,0.25
11960,c0e2a467cf23ee,f7ce2ce9,"Euclidean norm in Concorde rounds the distance to integer, so it is reasonable to scale city's coordinates.",6d02cd29,0.25
11961,d8d227c158d883,7d89f2ca,## Preprocess,3391b4a7,0.25
11963,4cd25e50c7e007,7a85db6b,**Bike rental increases in 2019 as compare to 2018 which shows the Boom Bike gained it's popularity in an year.**,ceb0c525,0.25
11964,8696921d9adc93,4901581c,**2. Check for null and missing values**,b8908b23,0.25
11965,1d5daeca89f48d,6949df8f,![e1.PNG](attachment:e1.PNG) ,48d478bc,0.25
11968,f87a77cc6f9891,9ffc4eb8,**Regression**,69b52681,0.25
11969,beef5463692752,4bd3fa28,"<h2>2) Writing the custom dataset</h2>
<p>Writing the custom torch dataset class so, that we can abastract out the dataloading steps during the training and validation process</p>
<p>Here, dataloader is created which gives the batch of image and its captions with following processing done:</p>

<li>caption word tokenized to unique numbers</li>
<li>vocab instance created to store all the relivent words in the datasets</li>
<li>each batch, caption padded to have same sequence length</li>
<li>image resized to the desired size and converted into captions</li>

<br><p>In this way the dataprocessing is done, and the dataloader is ready to be used with <b>Pytorch</b></p>",34fb20b2,0.25
11971,4c47839b067546,f67ffe94,"## Работа с пропусками
",1f517b02,0.25
11973,99bf357eaf61f1,63b55665,**Estimate Skewness and Kurtosis**,9d92fafe,0.25
11974,6f1481148352e9,a38ad59a,"**From the graph, we observe that the largest number of fires was in the state of Mato Grosso - 96k, and 3 more states can be distinguished by the number of fires - Sao Paulo, Rio, Bahia.**",7cfbdb8f,0.25
11975,10b5af05d804ff,74fa33e1,"But our model does not have this information.

So, the idea is to make mirror train data for our model. Then our train will be like this:",4a9b1705,0.25
11976,3dd4294f903768,86005220,"With this plot, it seems that there are no deliveries. Let's take a look at the numbers.",0d89d098,0.25
11978,93f5423667b9d5,7ea66301,# 対戦カード一覧,55bdf071,0.25
11979,51a46d0a7597f5,c9098012,"<br/>
<A name=""section1.1"">1. Is there a linear relation between Market Value and Wages of Players?</A>",e9e25b17,0.25
11981,c85c94076e9c3a,98aac5d4,"The feature Income contains 24 null values ,  let us Plot this feature to identify best strategy for imputation",3ea0c443,0.25
11982,eb0ecd6bebeb15,a8bb7f1d,Korelasyon katsayılarını daha iyi okuyabilmek için ısı haritası çizdirelim.,d7b93a60,0.25
11984,5f544a32fb2ce9,d53dbae8,# Prepare Folders,616f33af,0.25
11986,1011899b959f44,0f82e78d,"3. How many rows and columns are in the datasets? (Hint: Use the .shape method)


You can use this method to compare the dimensions of both datasets and assess what sizes you are working with. ",0b112382,0.25
11989,09bac0c221388e,7cb2cf12,"<img src= ""https://miro.medium.com/max/875/1*SM41ES3n-q71Xn8zCIdRMw.png"" alt =""Document"" style='width: 1000px;'>",bea4aa2e,0.25
11990,0504abe8519634,afa9c16f,"# 1.    with stopwords, without ngrams",46df846a,0.25
11991,9ca9a30fc69d9b,a99fb5fc,## Home or Away?,f715c2e5,0.25
11995,254cccd5145725,b9935fc7,Now we perform the same for the test as well.,a49b4037,0.25
12001,6d66ced0028dea,c202870a,### 1.1 Выбросы,f50aae52,0.25
12002,b01ee6cb674fa3,b326bb91,"now, there is left new mexico and barents sea",a8ffd35e,0.25
12011,eda49464dd6d1b,3347301a,"## Response by Previously_Insured
* People generally only buy our vehicle insurance if they don't already have vehicle insurance",8421f81f,0.2517482517482518
12012,a566b5b7c374e7,fe7802b4,## Pairwise Plots - Restfulness vs Oura Metrics,b3dc5545,0.2517985611510791
12023,dbd96dd275dc60,fab2a884,"# Convert string to categories
One way we can turn all of our data into numbers is by converting them into pandas catgories.

We can check the different datatypes compatible with pandas here: https://pandas.pydata.org/pandas-docs/stable/reference/general_utility_functions.html#data-types-related-functionality",1ed493a8,0.25252525252525254
12024,840534f2908a9c,c3e7869f,Convert pickup_datetime from Object to Datetime object,8081c3cc,0.25263157894736843
12025,f91f58d488d4af,5def7c9b,"#### Finally, we can compute what ""ideal"" 3 looks like. we calculate the mean of all image tensors by taking along dimension 0 of our stacked, rank-3 tensor,",5df1bbf3,0.25263157894736843
12027,5f4ae633cfd090,9244d77c,49% is the accuracy for this model/ baseline accuracy,a30a16e2,0.25274725274725274
12029,ee9ddc756b2d4a,33f416f0,# Preprocessing,e367eab3,0.25287356321839083
12030,14defffcd250f3,175e55e1,Cabin column has a lot of missing values so we are going to drop this column,3a683b94,0.25287356321839083
12034,5d2a3e82679cf3,41144f01,## I filled NA Values,9e60b1e3,0.25316455696202533
12040,37e461081e47c5,0ec670ef,We now explore categories,b3e6549e,0.25333333333333335
12051,631cd434fc3aa2,a31fc5e2, Which feature has missing data? In what percentage?,2b74febb,0.2535211267605634
12060,e58e68e4eeefe5,b348df6f,"### Observations
- Based on EDA and Feature Selection, features such as **anaemia, diabetes, age, sex, smoking** are less contributing.
- Features to be considered are, **platelets, time, creatinine_phosphokinase, ejection_fraction.**",a87662ce,0.2537313432835821
12062,09751c520b0616,955bb5bb,<b>Low null value feature filling with the most frequent value.</b>,a4d0c7e9,0.25384615384615383
12073,979f1e99f1b309,1d87da2d,***THE plot showing that almost all killpoints equal zero and few of them are from 1000 to 1500***,d1bfebbf,0.2540983606557377
12077,a077820f7ab459,ade9dece,"### Prepare ImageDataGenerator
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator",05a43104,0.2542372881355932
12079,a44368590e878a,217d6292,### Birth year,77743ba8,0.2542372881355932
12084,a81661cc35d8d2,31f9c160,# Data Visualizations,3331f113,0.2542372881355932
12086,1294fb4c86f993,8e6d84b0,"***
#### B. Census Data",4471e513,0.2542372881355932
12088,2ada0305b68956,de2c2a3a,### 41. Palette = 'PuRd',133e26f4,0.2542857142857143
12089,fe7360cddc13e5,10b67958,"<font color='red'> **5 - Transaction oranı λ ve pasifleşme olasılığı p, müşteriler arasında bağımsız olarak değişir.**
",8979e423,0.2543859649122807
12092,5a8c553e21c70f,9f590102,"## Visualization

For each feature, a histogram and a violin plot (feature vs target) are drawn.",9ebd9d8f,0.2545454545454545
12094,016abae0483764,db3dc15a,Getting to know Data,bc9f289b,0.2545454545454545
12095,f0fab078f8533b,b52af677,## b. Taking only last 10 year data,bdb5ea32,0.2545454545454545
12109,7cfd96218dd933,a188d2d3,"#### **ATTENTION**
* HOTSPOT DATA WAS RECORDED AT NIGHT TIME RANGE AT THE MOST.",7c34d96c,0.2549019607843137
12110,629f2918807a9b,df273797,#### Question 2: Visualize order status frequency,be56dc84,0.2549019607843137
12111,523123dad03177,aee1c44c,# 2. Race and ethnicity,48a5e4e6,0.2549019607843137
12112,71b75664517244,7c6c2d79,"### Invisible

Another fact that got my interest is a team who never lost a match for a season, they just invisible.",fc905af5,0.2549019607843137
12116,3f25b363afec54,022bc2d8,Here what we do is print out the data in combinations so we got better idea about each of the dataset.,bbdaae25,0.2553191489361702
12121,b61ab8f81dc03d,f4889b31,"<a id=""checking_cardinality""></a>
## Checking cardinality for categorical values
Checking the cardinality is very important before choose what transformation method you must apply for categorical values (objects).",64d05394,0.2553191489361702
12122,73893f0467d5e3,02852e94,## For mean_perimeter,279787c6,0.2553191489361702
12123,04e6b0d3c70f46,38b660da,"### Feature Engineering
* Let us see what a mp3 file looks like. A wave will have values with time. Will be good to plot this.
* Let us see what a fourier transform adjusted to mel scale looks like. 
* Most literature uses melspectrogram for the sound file as the image feature set and then regular cnn layers based models are used for classification.
* One literature suggested using muliple features returned by librosa for the model and this we will use as our baseline model.
* So let us get a sense of these features by doing some EDA",56344f77,0.2553191489361702
12132,49ac6594c8f5cf,ce8b57a1,Let us now see which specialization gets more average salary,6f19f28a,0.25555555555555554
12137,2e40928927c0d4,d704683e,**Showing randomly chosen Mild DR image one at a time** ,b6385ef2,0.2558139534883721
12138,22bd95f4807a23,a0740e6d,"# Basic Analysis
## How many individuals are there for each division type and rating?",c05d356f,0.2558139534883721
12140,72d393488311b6,a569653e,# Preprocessing,80663df0,0.2558139534883721
12145,c09fac3c943d51,83f8766c,"Weird ""double"" sessions:",678d076d,0.2558139534883721
12146,6a80f915608fc2,31548c8d,"### C- and g- : ave, std, 5%-tile, 95%-tile and various color-coded scatter plots
For now the 100 c features for an id are digested to their average, standard deviation, 5%-tile and 95%-tile -- c-ave, c-std, c-5%, c-95%; this makes some sense since the c values seem correlated.  These c-ave, etc. features were added to df_aug_feats (above, right after the data are read in.)   For completeness, g-ave, g-std, g-5%, g-95% were also created and and used to create various 2D color-coded scatter plots.",636938eb,0.25595238095238093
12147,9c26c5dcd46a25,c3266610,L'analyse de ces pairplots montre déjà des relations linéaires entre certaines variables. On remarque également le regroupement par Nutrigrade sur la plupart des features. Regardons à présent la répartition de ces Nutri-score et Nutri-grades,1bbbb677,0.25609756097560976
12160,c8c4705cca1ebb,cdfc3cf9,For test.csv,6d9d7107,0.2564102564102564
12163,0a1fcda859252c,6fe5ec72,"As you can see the data is highly imbalanced. We have almost with thrice pneumonia cases here as compared to the normal cases. This situation is very normal when it comes to medical data. The data will always be imbalanced. either there will be too many normal cases or there will be too many cases with the disease. 

Let's look at how a normal case is different from that of a pneumonia case. We will look at somes samples from our training data itself.",13a38774,0.2564102564102564
12169,80ad12f326ab70,b07b27a1,"* Only about 2% of the data contains missing values
* Data contains only object datatypes",da404a16,0.2564102564102564
12177,e4525eb0c96f28,4256f5cc,"Because the points on the scatter plot are so grouped, it's hard to tell much from the data. We decided to use a violin plot to represent the same information except with data-point density also shown below. 

With the violin plot in place, we could now clearly see density of points.",2093a1f1,0.25675675675675674
12178,63b44c85e32c1f,6051252a,There might arise a requirement where you might need to check if a particular element is there in a predefined list. Consider the below list.,fb9b9562,0.25675675675675674
12184,7454fdc444df16,656066b7,"As we expect, the shape of our dataframe matches our expectations, containing a total of 5 features, and about 280,000 patches.",a7818ef5,0.2571428571428571
12185,3a6274ed72cc00,77243980,## <a id='3.'> 3. Variable Description</a> ,51369a2a,0.2571428571428571
12197,2b36742b49c7bc,b0820e65,"2. Дараалалсан label-үүдээс үүссэн алдааг засах (reverse-engineering)

![Reverse Engineering](https://raw.githubusercontent.com/bayartsogt-ya/mlub-muis-soril/main/images/ss0.png)",c8f8a96d,0.2571428571428571
12206,0fa9979b5690e9,01d1ac2b,"No trecho de código anterior, foi avaliado o desempenho do método de aprendizagem usando 5 parametrizações diferentes: k = 1, 2, 3, 4 e 5. Em vez de fazer essa verificação diretamente no conjunto de teste, essa verificação deve ser feita no conjunto de validação quando houver dados suficiente. No exemplo anterior, o melhor resultado foi obtido com k = 1. Assim, o modelo salvo como *melhor modelo* foi k = 1, e foi esse modelo o escolhido para ser utilizado na verificação de performance com o conjunto de teste.",c26eea94,0.2571428571428571
12212,675b60eaf415a6,52176353,### **Visualize random image from each of the 101 classes**,68c0b725,0.2571428571428571
12213,6b65d81a5743dd,980eb9f8,Check for missing values,4080a2d2,0.2571428571428571
12215,ca73f3d2e25b47,325e470c,Display the last column (label),4cd11efe,0.2571428571428571
12216,b01ee6cb674fa3,425ec985,"The location_country is Barents Sea and the launches were carried by submarines

Doing a search by the rockets names it is found that they are SLBM carring satelites into their payloads

Belongs to Russia, because they are after 1991",a8ffd35e,0.2572463768115942
12217,5ce12be6e7b90e,86df3d6c,## **Exercise**: Operators,c0ab62dd,0.2573099415204678
12218,30fdc4a6e3c1db,bb9f2999,"We have 7 departments in total (2 hobbies ,2 household and 3 food departments)",6111ddee,0.2573099415204678
12223,2a123b4e8f9433,f915415e,# Train and run a Logistic Regression model,0a082218,0.25773195876288657
12225,8ec771f5600a61,66948e18,### here we see that the the change of survival min size is max then alone travler and famaly size more than 7,48364c1f,0.25773195876288657
12227,0932046e1f485d,83920c45,I like to create a separate dataset with numerical values when I need to log() some columns. In the lines below we are going to look into the numerical columns of the dataset.,218cc7a3,0.2578125
12230,0cb456a5456cf9,f8ac5333,# **Q2**<br>**When is peak hotel occupancy?** <br> 入住酒店高峰期,5701729c,0.25806451612903225
12232,0925f172b5eb74,de886023,"# Removing duplicates

To remove duplicated images, we use duplicates.csv file, which contains 62 sequences of duplicates found with image_hash, this list has been taken [from this notebook](https://www.kaggle.com/nickuzmenkov/pp2021-duplicates-revealing). For each duplicate sequence:

We leave only one sample if all duplicates share the same labels, and we will delete all duplicates if at least one of them is labeled differently, because in that case we can't know which one is the correct label.",ec34cd72,0.25806451612903225
12234,ad26c020235dfc,192fff06,"## River Arno
The target which we want to predict is Hydrometry_Nave_di_Rosano",bf766e48,0.25806451612903225
12236,0d9a2067267ba1,9a062898,### Missing Values,abc194fb,0.25806451612903225
12245,57070ad5e0f94f,039f5fa3,# **Checking For (Linear) Correlations**,d97edc41,0.25806451612903225
12249,78998e078eaaa1,a9a60a51,# oversampling the data,2b29364c,0.25806451612903225
12253,c0ddb77bf32e2b,2dfd6e55,"If we make the NA threshold higher, it will be more abundant  but less clean, vice versa. It's a trade-off.  Judging by the slope, it will be a nice a balance of abundance and cleanliness  if we set the threshold be 3, but no more less further.",a0cb45f7,0.25806451612903225
12255,62487bcd70b199,d99c2c9c,## <a id='4.3.'>4.3. Age and Experience in Personal Loan</a>,f6ae50af,0.25833333333333336
12260,f3c6048d1058e3,dc1f5f50,# Word Cloud,1d9056b0,0.25862068965517243
12261,1dd9c6aa74d289,b0e9d945,### Climber Composition,5ef9a1be,0.25862068965517243
12262,84127ade6fde87,1cd5aefd,"**NOTE** 128 characters are clearly not enough to account for all the glyphs, accents, ligatures, and so on that are needed to properly represent written text in languages other than English. To this end, a number of encodings have been developed that use a larger number of bits as code for a wider range of characters. That wider range of characters was standardized as Unicode, which maps all known characters to numbers, with the representation in bits of those numbers provided by a specific encoding. Popular encodings are UTF-8, UTF-16, and UTF-32, in which the numbers are a sequence of 8-, 16-, or 32-bit integers, respectively. Strings in Python 3.x are Unicode strings.",f55d05b6,0.25862068965517243
12263,d42518f6cb0995,44aa24bc,"Look's noisy, let's clear it a little bit",26913a9b,0.25862068965517243
12264,00001756c60be8,aaad8355,*Тип данных обучающего сета*,945aea18,0.25862068965517243
12266,20e1ba19eb9b5e,2f1e543e,## 2.1 Outliers,4569bfc1,0.25862068965517243
12277,869a39a3d4dea2,40f267b8,"## Drawing <a id=""drawing""></a>",9020daf8,0.25882352941176473
12288,ddcdecdd6a3b6d,96bb6658,### 训练,90831448,0.25925925925925924
12292,c6f8ff61a5fa87,b7da68f7,"## <span style=""color:blue;""><strong>3.Feature Engineering</strong></span>",3eea586b,0.25925925925925924
12299,ab6da5994949a3,fa45ebcb,## Initializing ANN,fae6b91d,0.25925925925925924
12301,aa7db7b023d0a2,c188bd93,"<h1 style=""background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%"">Factors Predicting One-year Mortality in Amyotrophic Lateral Sclerosis Patients – Data From a Population-based Registry</h1>

Authors: Joachim Wolf; Anton Safer; Johannes C Wöhrle; Frederick Palm; Wilfred A Nix; Matthias Maschke; Armin J Grau

BMC Neurol. 2014;14(197)

"" Survival in amyotrophic lateral sclerosis varies considerably. About one third of the patients die within 12 months after first diagnosis. The early recognition of fast progression is essential for patients and neurologists to weigh up invasive therapeutic interventions. In a prospective, population-based cohort of ALS patients in Rhineland-Palatinate, Germany, the authors identified significant prognostic factors at time of diagnosis that allow prediction of early death within first 12 months.""

""Methods: Univariate analysis utilized the Log-Rank Test to identify association between candidate demographic and disease variables and one-year mortality. In a second step theye investigated a multiple logistic regression model for the optimal prediction of one-year mortality rate.""

""In the cohort of 176 ALS patients (mean age 66.2 years; follow-up 100%) one-year mortality rate from diagnosis was 34.1%. Multivariate analysis revealed that age over 75 years, interval between symptom onset and diagnosis below 7 months, decline of body weight before diagnosis exceeding 2 BMI units and Functional Rating Score below 31 points were independent factors predicting early death.""

""Probability of early death within 12 months from diagnosis is predicted by advanced age, short interval between symptom onset and first diagnosis, rapid decline of body weight before diagnosis and advanced functional impairment.""

https://www.medscape.com/viewarticle/834905",ec912af3,0.25925925925925924
12303,ac04ba639d1c93,81b87474,"The importante things to know is that the scalar coupling constants in train.csv are a sum of four terms. 
```
* fc is the Fermi Contact contribution
* sd is the Spin-dipolar contribution
* pso is the Paramagnetic spin-orbit contribution
* dso is the Diamagnetic spin-orbit contribution. 
```
Let's merge this into train",748059d5,0.25925925925925924
12304,db5a369894fef6,0a9ac990,"
### Compare top five countries with highest COVID count and compare it to Australia (I'm from OZ, that's why!)
- Examine confirmed, active, recorded, deaths in SIX plots",065aaf61,0.25925925925925924
12307,d128317750d689,76e38e9e,"The  dataset consists of 1D arrays for each image. If we want it to work correctly with CNN, we will need to reshape it into 2D format. In this specific case, we have 28\*28 images.

I've created a primive function that gets the job done, it takes the dataset and dimention of images, and returnes the reshaped dataset:",d87f7428,0.25925925925925924
12308,7baeb0ffc6659e,a3fba51c,**Survival rate based on Pclass**,8cbebba9,0.25925925925925924
12310,2b39f4ff896f97,05fa3f65,Get Size of Processed Image,3ddfe182,0.25925925925925924
12313,b9328fe3b0cefc,5fbc2823,### Count of Champion and second(冠军、亚军的次数可视化),3a35eb23,0.25925925925925924
12314,f4b9042e693b6c,8a2a580f,"## Definitions

Now let's define the necessary functions and variables needed for training.",676cacc9,0.25925925925925924
12316,e0f03003a69819,4dfdaf1a,"# Conclusions

* more males have raised claim
* women seem to have a wider range than men in raising claims
* see signs of skewedness in claims charges towards the lower end .
* men claim accounts have peaks in two points (but nithing major in comparision to women)",609ad1f4,0.25925925925925924
12322,ee23a565163388,3dd25231,"**Inference**
- 35% of the anaemic patients have faced heart failure while the other 65% did not.",88aacbc4,0.2595419847328244
12324,44f6a002ecd033,04bcbbdd,"For cleaning the data there are a couple of tasks that will need to be done in order to make sure that the model is running the best it can.
* Clean up the missing values
* Check for any duplicates
* Make a separate train dataset where the columns that needed a logistic regression earlier are modified.",70bbe106,0.25961538461538464
12326,663bbc9eaf267b,42e2ec9f,"Let's see the price boxplot for ""2 Series"" model",32445529,0.2597402597402597
12329,241cf32abb22d8,7a6d26e8,"After performing one-hot-enconding on those 4 nominal features, the number of columns with descriptive features in the dataset extend from 32 to 45. ",47157066,0.2597402597402597
12334,ce9ed5e2d601d7,a0e8238e,"## Group by labels, how many predictors are obvious?",f58a2f43,0.25984251968503935
12335,2bd6c370695ea7,accb0882,## Image size features,cbe6aec8,0.26
12336,5e11e9a02c1c96,53ec49ce,"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')",965a1e37,0.26
12337,2ada0305b68956,e34fd9e2,### 42. Palette = 'PuRd_r,133e26f4,0.26
12338,4cd25e50c7e007,bb44ae98,"# Month

* 1:Jan
* 2:Feb
* 3:Mar
* 4:Apr
* 5:May
* 6:June
* 7:July
* 8:Aug
* 9:Sep
* 10:Oct
* 11:Nov
* 12:Dec",ceb0c525,0.26
12339,91eaec994e0c6f,10d319e1,"- A simple observation: from the first plot we can see that the biggest part of sales of this item (<i>FOODS_3_586</i>) in this store (<i>TX_3</i>) is during Month 8 (August).<br>
- This very detailed level (<i>Item level</i>) won't generate many insights, aggregated levels will do such as <i>State</i>, <i>Store</i>, <i>Category</i> and <i>Department</i> levels.",376aef10,0.26
12340,83df814455f06c,1389cd37,"### Rename column names

We can see that the dataset does not have proper column names. The columns are merely labelled as 0,1,2.... and so on. We should give proper names to the columns. I will do it as follows:-",c9cff71a,0.26
12345,7dd46c750653eb,c3791373,"**Inference**

* The Month of January has the most number of Death over the years

* For the year 2010 , July and August has the highest number of Deaths",c2644713,0.26
12346,e19e307b3fd188,dc7996e8,"Disregarding outliers:
- **São Paulo**: rent amount around **500,0**0 to **12.000,00**
- **Porto Alegre**: rent amount around **500,00** to **4.500,00**
- **Rio de Janeiro**: rent amount around **500,00** to **7.500,00**
- **Campinas**: rent amount around **500,00** to **5.500,00**
- **Belo Horizonte**: rent amount around **500,00** to **9.500,00**",2173955b,0.2601626016260163
12352,fdc9f4863744b1,2347a595,"Since there are no duplicated data, I can look at the values and make sure they have correct data types.",b4529365,0.2602739726027397
12354,c4386b8a01d66e,5be282a5,There are no correlated columns presebt in the data,dc732bf5,0.2605042016806723
12355,4ae6a182abac64,67a175cc,* **Survived by Age**,418676c5,0.2605042016806723
12357,90ead00a8ee283,bc2b1eaf,"**Exercise 1**: Create a `DataFrame` that looks like this:

![](https://i.imgur.com/Ax3pp2A.png)",612efa48,0.2608695652173913
12362,2409b2d74a9871,a5630f37,"The price data is somewhat positively skewed. Therefore to make the distribtion balanced, prices above 1000 are removed from dataset.",b0a6c313,0.2608695652173913
12363,2b434130adf886,2c0d74f8,# Extracting feature using various pretrained CNN,0c4afeca,0.2608695652173913
12369,ea4e559a86d613,48a34000,**2. AGE COLUMN**,eff47843,0.2608695652173913
12370,71c3c1eab0377d,e011adca,#### Column: Cabin: The Nan values in the Cabin column means that the passangers didnt had the cabin.,52b4e360,0.2608695652173913
12371,0e2a23fbe41ca9,d97d7452,"Observations:

- Value 3 (most frequent category in the whole feature_1 column) is at all time high across all the years except 2018
- Value 2 is another value which is 2nd highest after 2015

### 7. Year vs feature_2",64e4762c,0.2608695652173913
12377,9b5de3823ad5ab,f2f3b600,"### Removing images without a label

We can see that we have a few unlabeled images, so we're going to remove them using Pandas' `dropna()`.",33e48774,0.2608695652173913
12378,72d528df923403,3c61b17e,# Exploratory data analysis,d51c8e8e,0.2608695652173913
12384,3319c5c562f607,4240019f,# Text Preprocesing,f298250a,0.2608695652173913
12398,d1ff7e10ee0102,9662a006,"*Like all the pretty girls, 'SalePrice' enjoys 'OverallQual'. Note to self: consider whether McDonald's is suitable for the first date.*",2cc71c3c,0.26136363636363635
12399,917957c6c4065f,6278e66c,동영상에 포함된 태그의 개수로 tag_count 열을 생성했습니다.,55b8ed68,0.26143790849673204
12410,d07915a6e6992e,5597075e,"**Parch**

Parch indicates number of parents / children aboard the Titanic. Note that Parch = 3 and Parch = 1 shows higher survival probabilities. ",2b912140,0.26153846153846155
12411,f2f2db16a2f86c,c1462469,**Plots**,ffc6a115,0.26153846153846155
12412,c84925c8171900,a02124b8,"<div class=""alert alert-block alert-info"">
    <span style='font-family:Georgia'>
        <b>Insight: </b><br>
        This is an anomoly as the downloaded data is for Year 2017 only. We will remove the row(s) with wrong or null year or we will try to find the real year for those columns.
    </span>
</div>",e21ff7ec,0.2616822429906542
12413,726833f92fb87a,889977f2,"This variable presents a long tail distribtion, as the previous ones.",7dc5e1b6,0.26174496644295303
12414,5f32117bcd5255,b4f96975,#### PHOTOMETRY KEYWORDS,85882abf,0.26174496644295303
12415,e16860fce156b0,2789e3a3,"#<b><mark style=""background-color: #9B59B6""><font color=""white"">Understand a column with plot(df, x)</font></mark></b>

After getting an overview of the dataset, we can thoroughly investigate a column of interest x using plot(df, x). The output is of plot(df, x) is different for numerical and categorical columns.

When x is a numerical column, it computes column statistics, and generates a histogram, kde plot, box plot and qq-normal plot:

https://sfu-db.github.io/dataprep/user_guide/eda/plot.html",2054f1ce,0.2619047619047619
12418,916ccf243827f1,5d80b177,## 3. Normalising the dataset,5147f4d2,0.2619047619047619
12429,2d40f383473fa4,ab6f0b48,Drop the two `NaN` samples in `Embarked`.,1da1eff0,0.2619047619047619
12439,98a6794067932a,0f8d7a33,"Le code ci-dessous permet de générer le top 10 des meilleurs clients en fonctions du montant total des ventes effectuées auprès de ces clients. Dans un premier temps, le code permet d'évaluer le montant total des ventes pour chacun des clients. Dans un deuxième temps, un tableau représentant seulement les 10 clients ayant les ventes totales les plus élevées est créé. Cette analyse sera pertinente dans notre prise de décision stratégique étant donné que les dirigeants voudront fort probablement favoriser de bonnes relations d'affaires avec ces clients.",08600fe2,0.2621359223300971
12441,0858e1bb3cbaca,3aab86ee,"What if we only want to focus on one product?
We can display data with specific aspects by using

**.loc[]**

where in the sqaure brackets could be the name of the row(s) you want to see",78548374,0.26229508196721313
12442,918040fad252ec,0e82adad,Membuat nama dari isi tabel yang digunakan untuk test dan train,966fcd8f,0.26229508196721313
12446,957e035ba5b9d5,dbf05db4,# Preprocessing,778ab3d3,0.2624113475177305
12452,fdbbd573ba31c2,2ae8bed1,### gearbox_temperature(°C),f7c28d74,0.2625
12454,1294fb4c86f993,3cd7e315,Dropping irrelevant column and row,4471e513,0.2627118644067797
12457,c80939c7c626cf,51f00096,#  *Now I am going to combine train and test data set and create a Title Feature/Column and extract whether Mr/Mrs using regular Expression.*,b9ac31e2,0.26277372262773724
12461,e03eb63c1f725d,f1af8862,"<a id=""2""></a>
<font size=""+2"" color=""blue""><b>Title/Text Length of True/Fake news</b></font><br>",e204b7e3,0.2631578947368421
12463,0d8df2c2983694,88287c6a,## Load the data,9bf7fa4e,0.2631578947368421
12465,31b564f11ef638,402c4404,### Create new features,424f9692,0.2631578947368421
12466,9e27af2600925c,fe60aaaa,"<font color='blue'>
**What you need to remember:**

Common steps for pre-processing a new dataset are:
- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
- Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)
- ""Standardize"" the data",9b556435,0.2631578947368421
12468,a1dcd92986bc84,d59f8df9,### Create `tf.data.Dataset` for training and evaluation,730acaaa,0.2631578947368421
12469,bef2347846e476,302fa0f8,"Here in the last line we use data.describe(),it returns the result of just the numerical data.And below you can see the name data2.We use it to hold the content of the second csv file which inludes the sentiment polarity and subjectivity values.",cb93bf51,0.2631578947368421
12470,6b955982396c14,d6e03bdb,"## 문제2
- 데이터셋(basic1.csv)의 앞에서 순서대로 70% 데이터만 활용해서,
- 'f1'컬럼 결측치를 중앙값으로 채우기 전후의 표준편차를 구하고
- 두 표준편차 차이 계산하기",2b4cb71b,0.2631578947368421
12472,9f3710be6aea65,61538fa8,Now we can visualize if there is a difference on the fill values:,ae9bda88,0.2631578947368421
12473,038abade89e59f,b88a33a5,# Dataset,cb32a3fe,0.2631578947368421
12474,c2a9f2fb3e1594,ecdb900d,"## 3.23 Convert Formats

We will convert categorical data to dummy variables for mathematical analysis. There are multiple ways to encode categorical variables; we will use the sklearn and pandas functions.

In this step, we will also define our x (independent/features/explanatory/predictor/etc.) and y (dependent/target/outcome/response/etc.) variables for data modeling.

** Developer Documentation: **
* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)
* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)
* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)
* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)
* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)",53411c04,0.2631578947368421
12475,30fdc4a6e3c1db,ef792aea,### Plotting sales ditribution across departments,6111ddee,0.2631578947368421
12477,d81d3830152f88,e5d56571,"Creation of treatment and control group:

- **Treatment** group includes data where a team won given it has higher 3pt field goal percentage (3pt-fg) than the other team
- **Control** group includes data where a team won given it has lower or equal 3pt field goal percentage (3pt-fg) than the other team",9551eac9,0.2631578947368421
12481,c3498779cda661,39f3a351,# Comparar con las clases originales,0f531b65,0.2631578947368421
12482,f35ee6e9fab592,1b7f0150,A tidy barplot demonstrating the top 10 popular platforms ,b15f7073,0.2631578947368421
12486,169177b6e9edea,cb47d46f,<p> substituir faltantes com S que representa mais de 50% dos dados em todas as classes,ca42152f,0.2631578947368421
12487,fe7360cddc13e5,808e2c9f,"Her müşterinin davranışları birbirinden farklıdır, dolayısıyla birbirlerinden bağımsızdır.",8979e423,0.2631578947368421
12489,5ce12be6e7b90e,1a6cc659,"Define two variables, `a` and `b`, set 'a' to 2019 and 'b' to 6. <br>
* Find the operation for which, `a` ? `b` = 336.5<br>",c0ab62dd,0.2631578947368421
12492,1f3295ed0d4e4a,c178a645,# Image Generator,b0aeb172,0.2631578947368421
12493,ba4b3bd184acbb,873ea2cb,### Iloc Method,0f5de724,0.2631578947368421
12494,826ccb616bd2a8,36768c5a,"## One-Hot Encode Sex, Blood Pressure, and Cholesterol
This will denote 1 or 0 for each value inside each column.",4d7df2ec,0.2631578947368421
12497,29437539745aa5,611891f0,"<h3 style=""text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;"">1.1 APPROACH OVERVIEW</h3>

---

**TRAINING**

1. Identify slide-level images containing only one label
2. Segment slide-level images (get RLEs for all cells in all applicable slide-level images)
3. Crop RGBY image around each cell
4. Pad each RGBY tile to be square
5. Resize each RGBY tile to be (256px by 256px)
6. TBD ---> Filter the images based on certain additional factors to obtain a better training dataset
7. Seperate the channels and store as seperate datasets
8. Augment the dataset (rotation, flipping (horizontal and vertical), minor-skew)
9. Train a model to classify these tile-level images accurately

---

**INFERENCE**

1. Use CellSegmentator to do instance segmentation on images in test-dataset
2. Record this mask in the appropriate format for later submission
3. Crop RGBY image around each cell
4. Pad each RGBY tile to be square
5. Resize each RGBY tile to be (256px by 256px)
6. Infer on each slide 
7. Combine cell-level classification with segmentation as RLE when submitting",c17b490a,0.2631578947368421
12498,caa0ce2715bf34,4710756a,## Checking for Outliers,78a5dc51,0.2631578947368421
12504,f14f6708035916,6c4b25d5,### Distribution of Spending,ca22d04b,0.2631578947368421
12511,5f4ae633cfd090,ca5208d3,Now that the baseline model is built let's focus on building an actual model for our prediction,a30a16e2,0.26373626373626374
12512,69ac33d79f5130,51625077,#### Remove columns that you don't use,9d760d2a,0.2638888888888889
12515,593d1d3d1df05a,87266d22,# Imposing Boxes on Original Image of Car,bc682ffe,0.2638888888888889
12520,510b8303776bb6,21693d5e,As we can see from the box plot there are a lot of outliers so we set a threshold for the price. Here I have taken it to be $450000,18080db8,0.2641509433962264
12521,a070fd03ae8ed2,8b8d72ad,"# 4. Анализ первой модели (1)
---
## 4.1 Расчет прогноза дефолта ",c0ec4138,0.2641509433962264
12523,23df07a474aaae,930b31fa,# Prediction on Streams,0ea40276,0.2641509433962264
12526,f3c8651cb08234,7dcbf3ec,# Dealing with categorical variables,37f86e36,0.2641509433962264
12527,614ba9f0c62677,06f3aa73,"<a id=""2""></a>
## Normalization, Reshape and Label Encoding 
* Normalization
    * We perform a grayscale normalization to reduce the effect of illumination's differences.
    * If we perform normalization, CNN works faster.
* Reshape
    * Train and test images (28 x 28) 
    * We reshape all data to 28x28x1 3D matrices.
    * Keras needs an extra dimension in the end which correspond to channels. Our images are gray scaled so it use only one channel. 
* Label Encoding  
    * Encode labels to one hot vectors 
        * 2 => [0,0,1,0,0,0,0,0,0,0]
        * 4 => [0,0,0,0,1,0,0,0,0,0]",b8551335,0.2641509433962264
12528,f015d0147e8fbf,69f92d73,### 3. Bureau Balance Data Table `bureau_balance.csv`,518954fb,0.2641509433962264
12529,ee9ddc756b2d4a,63850f42,"Ho studiato e modificato la funzione di preprocessing da questo lavoro, poichè si adattava bene al mio progetto https://www.kaggle.com/ruslankl/brain-tumor-detection-v1-0-cnn-vgg-16",e367eab3,0.26436781609195403
12533,e9b9663777db82,d395bf1a,#### 2.Quantitative analysis for Number of Rooms feature,648e8507,0.2644628099173554
12534,2f47abddfd1928,be88edeb,"The age of this two passengers are not filled because there is no more passengers with its combination of Pclass, SibSp, Parch and Fare. Therefore I will fill them just using Pclass and Fare to group and extract the data.",ae33cc0b,0.2644628099173554
12540,842547b2def18c,811883f3,"### 相関関係 (カテゴリカル変数-カテゴリカル変数)

カテゴリカル変数と目標変数(Survived)との相関関係をみることができます．

**観察**

- 女性の乗客は，男性の乗客よりも良い生存率をもっていた． (classifying #1)
- Embarked=Cの中には，高い生存率をもつ男性という例外もいた．これはPclassとEmbarkedの間の相関になるし，PclassとSurvivedとの相関関係につながる．EmbarkedとSurvivedとの直接的な相関関係は必ずしも必要ない．
- Pclass=3の女性は，Pclass=2 (C, Qポート) と比較して，良い生存率をもっていた． (completing #2)
- Pclass=3の男性の中で，Embarked(乗船ポート)は生存率を変化させていた． (Correlating #1)

**決定**

- Sexをモデルの訓練に追加する
- Embarkedの欠損値を補完し，モデルの訓練に追加する．",b8efde6d,0.2647058823529412
12541,02b7e38902069e,895941ae,"#Tokenization with iNLTK

The first step we do to solve any NLP task is to break down the text into its smallest units or tokens. iNLTK supports tokenization of all the 12 languages I showed earlier:

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.2647058823529412
12542,a0a5baa6c7e12a,d9e6fcc5,The pair scatter plots above further detail the insights on the feature variables-to-target class label relations.,551d41de,0.2647058823529412
12543,1d1598b6fa2aa7,32a0dce6,"### Line Charts

We can also plot not only points, but also lines. Let's plot function below:

$$y = x \cos(\frac{1}{x})$$",e066accf,0.2647058823529412
12550,7cfd96218dd933,5a764c5a,### GROUPBY MEANING,7c34d96c,0.2647058823529412
12559,52cfd66e9ec908,938e0d64,"So this is a demonstration of the movement of the other vehicles and (in relation to the movement and placement of the other vehicles) the movement of the AV. The AV is currently taking only a straight path in its motion, and a straight path seems logical with the movement and placement of other vehicles.",c74adcdf,0.2647058823529412
12560,410285582f4f7e,e024223f,"**Data Cleanup**

1. Replace missing age and first_browser to NAN
3. Age will be filtered between 18-100 and set to NAN
4. Date time data of 'time_first_active' and 'timestamp_first_active' to be split into day, month and year columns
5. Categorization of all non-numeric data such as gender, language, device type, browser type etc. 
6. Drop 'time_first_active', 'date_of_boking' and timestamp_first_active' 
7. Store IDs separately
8. Save 'country_destination' as target variable",d026266b,0.2647058823529412
12562,49ee86d074de69,accbe062,"* 0: 'Unknown',
* 1: 'Certain infectious and parasitic diseases',
* 2: 'Neoplasms',
* 3: 'Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism',
* 4: 'Endocrine, nutritional and metabolic diseases',
* 5: 'Mental and behavioural disorders',
* 6: 'Diseases of the nervous system',
* 7: 'Diseases of the eye and adnexa',
* 8: 'Diseases of the ear and mastoid process',
* 9: 'Diseases of the circulatory system',
* 10: 'Diseases of the respiratory system',
* 11: 'Diseases of the digestive system',
* 12: 'Diseases of the skin and subcutaneous tissue',
* 13: 'Diseases of the musculoskeletal system and connective tissue',
* 14: 'Diseases of the genitourinary system',
* 15: 'Pregnancy, childbirth and the puerperium',
* 16: 'Certain conditions originating in the perinatal period',
* 17: 'Congenital malformations, deformations and chromosomal abnormalities',
* 18: 'Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified',
* 19: 'Injury, poisoning and certain other consequences of external causes',
* 20: 'External causes of morbidity and mortality',
* 21: 'Factors influencing health status and contact with health services',
* 22: 'Patient follow-up',
* 23: 'Medical consultation',
* 24: 'Blood donation',
* 25: 'Laboratory examination',
* 26: 'Unjustified absence',
* 27: 'Physiotherapy',
* 28: 'Dental consultation'",71ccc6d3,0.26495726495726496
12563,b86bda7afe3ac3,18cf364c,Train test split. Not used/usefull. ,16197934,0.26495726495726496
12564,4daf6153275cbf,423c0081,"I am using country for the analysis because it would be better to use for the localization analysis. Also, that way I can combine multiple cities in a country.",51db1961,0.26506024096385544
12567,12f4d16fc21645,1161ac7a,<h1 style='color:blue'>Data Visualization and analysis</h1>,c7752038,0.2653061224489796
12570,f35bf4df70d310,fa3ace92,## 2. PCA on MNIST Digit Recognition,10bb859a,0.2653061224489796
12571,f1e162ddd14f11,66e90569,## One Hot Encoding to convert categorical data to numeric data,cdb2e771,0.2653061224489796
12582,ff3a8ce61fab6a,6a37454e,"<hr>

## Exampel 2 ",9afe1654,0.265625
12583,2ada0305b68956,1544c538,### 43. Palette = 'Purples',133e26f4,0.26571428571428574
12590,bd380b97b5c894,49f584db,Select only those values that have gaps,66f2562a,0.26605504587155965
12591,9ceb7278784462,3ec09dd0, ## <a id='11'> 8.Number Of Pages </a>,3768a567,0.2661290322580645
12598,d96e03a9e7c030,9a44aa82,"### OK, let's call all of these functions within our features_controller()
This will take a few minutes as well. The two arguments we pass in are the input data directory (i.e., where the DoE and NYT data reside), along with an output directory where we can put the transformed data frame.",d2b72ced,0.26666666666666666
12605,ea1ba5e7ba436c,4cb12426,"# LINE PLOT

* :PARAMETERS

    * x = x axis
    * y = y axis
    * mode = type of plot like marker, line or line + markers
    * name = name of the plots
    * marker = marker is used with dictionary.
    * color = color of lines. It takes RGB (red, green, blue) and opacity (alpha)
    * text = The hover text (hover is curser)
    * data = is a list that we add traces into it
    * layout = it is dictionary.
    * title = title of layout
    * x axis = it is dictionary
    * title = label of x axis
    * ticklen = length of x axis ticks
    * zeroline = showing zero line or not
    * fig = it includes data and layout
    * iplot() = plots the figure(fig) that is created by data and layout",8dd6985c,0.26666666666666666
12611,04bac111ffbe9c,71393c18,"##### 2nd METHOD : BASED ON PCLASS
1. Group the dataset on the basis of Pclass and for every Pclass, find the mean of the ages. Store them orderwise in a list.
2. Now, loop through the Pclass values ->
    -  For every Pclass, say i:
        -  Pick all rows under the 'Age' column for that Pclass and replace the NaN values with the corresponding mean from the previously created list.
        (For example, for Pclass 1, we'll replace NaN values with the 1st value of the list.)
       end loop.
   Done.

###### FINALLY, PLOT A DISTPLOT TO CHECK NEW DISTRIBUTION OF AGES. DOES OUR PRIMARY OBSERVATION DEVIATE AFTER PROCESSING ?",82576b17,0.26666666666666666
12618,864302b10e7730,bb92997d,We have data for 7 different regions.,e9dd1d2d,0.26666666666666666
12619,4fd4b6a80d40e3,e6c566f0,"## Sigmoid Function

![image.png](attachment:image.png)",f6913cc3,0.26666666666666666
12623,7454fdc444df16,b5411ed3,"# Explarotory Data Analysis

## Let's create a visual summary of our data",a7818ef5,0.26666666666666666
12624,cb6f349e54c2a1,eb940301,"# Sequential API

The Sequential model API is a way of creating deep learning models where an instance of the Sequential class is created and model layers are created and added to it. Our model is a linear stack of layers, we start by calling the Sequential function. We then add each layer one after the other.

The Sequential model API is great for developing deep learning models in most situations, but it also has some limitations.

For example, it is not straightforward to define models that may have multiple different input sources, produce multiple output destinations or models that re-use layers.",962bade8,0.26666666666666666
12625,7e1da639035ac5,ac9baad4,"Schools in Rockaway, Bronx and Arverne are in dire need of financial help while Whitestone, Little Neck and Douglaston are financially stable.",120b6c23,0.26666666666666666
12626,f89f8540df580e,2ffccb0f,# Generating Response for a given Dialogue,83579ee7,0.26666666666666666
12629,d905cde3391d2b,0877d616,"### Geometric Mean

Another type of mean is **geometric mean**. It is calculated as **Nth root** of **product** of all the numbers, where N is the total number of values in the dataset

$$
\begin{align}
Geometric\,mean = \sqrt[n]{product\,of\,all\,numbers}
\end{align}
$$

$$
\begin{align}
\bar{x}_{geom} = \sqrt[n]{\prod_{i=1}^n x_i}
\end{align}
$$

Geometric mean of our data is calculated as,

`geometric_mean = 315thRoot(35 x 15 x 97 x 17 x ...)`",067dba39,0.26666666666666666
12630,ff9142eb631dd5,506dafac,## Install LOFO,453131ac,0.26666666666666666
12632,be616f0785c32d,b0982cfa,"[Go Top](#top)

<div id=""PartAcat"">
</div>
<div id=""PartAeff"">
</div>


###### A.2.4 Literature summary

After hand-removing the irrelevant ones, the drugs can be roughly categorized by their effective mechanisms into:

| Group and Mechanism | Popular Drugs in Trials |
| --- | --- |
| RNA mutagens that stop the copying of the virus | Remdesivir, Favipiravir, Fluorouracil, Ribavirin, Acyclovir  |
| Protease inhibitors that block the multiplication of the virus | Ritonavir, Lopinavir, Kaletra, Darunavir |
| Stopping the entry of the virus into the host cell | Arbidol, Hydroxychloroquine, Chloroquine phosphate |
| Stopping the release of the virus from the host cell | Oseltamivir |
| Monoclonal antibodies targeting a virus protein/epitope | IL-6 monoclonal antibody, Spike (S) protein antibody |


<div id=""PartAcat1""></div>
**A.2.4.1 RNA mutagens**

Viruses need to copy themselves in order to invade the host and transmit (like cancer cells), thus it makes sense that mutagens that block the copying can be used as drugs. 

   **Remdesivir**: It was studied in many publications related to coronavirus. It was suggested to be highly effective in the control of 2019-nCoV infection in vitro, while their cytotoxicity remains in control (0562f70516579d557cd1486000bb7aac5ccec2a1.json, 95cc4248c19a3cc9a54ebcfa09fc7c80518dac5d.json). It was also reported to significantly reduce lung viral load in mice and with successful clinical cases (0562f70516579d557cd1486000bb7aac5ccec2a1.json, 49ac69f362c27acbc6de0c5cbb640267e7a1e797.json). In clinical settings, it has been used as compassionate treatment. Other papers, e.g.,  3e9ae5329eecab16d7c39f1f6dc778cf4a53ee0d.json, suggest the effect is still to be verified.

   **Favipiravir**: It was suggested to be a good candidate (58be092086c74c58e9067121a6ba4836468e7ec3.json). It has been used in trials to treat SARS-CoV-2 infections, while the scores of favipiravir docking with the targets in some virtual screenings are relatively low (based on a computation study 95cc4248c19a3cc9a54ebcfa09fc7c80518dac5d.json)

   **Fluorouracil**: The RNA mutagen 5-fluorouracil (5-FU) treatment will also increase the U:C and A:G transitions. 

   **Ribavirin**: It was suggested to be useful for MERS (e5f19b6daf956e815c779228cc0cad1293d65bbb.json). It has been reported to reduce death rate in COVID-19 patients: f294f0df7468a8ac9e27776cc15fa20297a9f040.json. 

   **Acyclovir**: No statistical difference in treatment effect (baabfb35a321ea12028160e0d2c1552a2fda2dd5.json)

[Go Top](#top)

<div id=""PartAcat2""></div>
**A.2.4.2 Protease inhibitors **

   **Ritonavir**: It was suggested to inhibit proteases and thus block multiplication of the virus. It was reported to deliver a substantial clinical benefit for COVID-19 patients (0562f70516579d557cd1486000bb7aac5ccec2a1.json, and its effectiveness is suggested by *computational* docking studies (9e94f9379fd74fcacc4f3a57e03cbe9035efee8e.json), while others clinical studies showed no effect at all or 'failed' treatment (24e17488d399c436305c819953beae2961214771.json, 8349823092836fe397a59e38615d1491423dbe70.json,8349823092836fe397a59e38615d1491423dbe70.json, ). Previously, it was shown to be beneficial for treating SARS and MERS (3afd5fba7dc182ddfa769c0d766134b525581005.json ).

   **Lopinavir**: Lopinavir is a protease inhibitor. It was reported with substantial benefit for treating COVID-10 patients (0562f70516579d557cd1486000bb7aac5ccec2a1.json). Most studies consider Lopinavir as a potential candidate. 

   **Kaletra**: It is the combination of Ritonavir and Lopinavar.

   **Darunavir**: The drug was suggested to be potentially beneficial by *computational* docking experiments (9e94f9379fd74fcacc4f3a57e03cbe9035efee8e.json), and in vitro studies (95cc4248c19a3cc9a54ebcfa09fc7c80518dac5d.json). 

[Go Top](#top)

<div id=""PartAcat3""></div>
**A.2.4.3 By stopping the entry of the virus into the host cell**

   **Arbidol**: It inhibits membrane fusion between virus particles and plasma membranes, but it shows no statistical difference in treating COVID-19 patients (baabfb35a321ea12028160e0d2c1552a2fda2dd5.json) 
   
   **Hydroxychloroquine, Chloroquine phosphate**: Some studies also suggest that hydroxycholoroquine is working by blocking the entry of the virus, though the exact mechanism is unknown (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7102587/). Chloroquine effectively inhibited SARS-CoV-2 in vitro (58be092086c74c58e9067121a6ba4836468e7ec3.json). Chloroquine phosphate was reported to have apparent efficacy and acceptable safety against COVID-19 in a multicenter clinical trials (462cbb326ccd8587cae7a3538c8c6712d9013698.json, b70d27459fd8143edf76721da40cdbca399c9fb1.json).Chloroquine has been recently written into official recommendation for empirical therapy of COVID-19 for its adequate safety data in human (0562f70516579d557cd1486000bb7aac5ccec2a1.json)

<div id=""PartAcat4""></div>
**A.2.4.4 By stopping the release of the virus from the host cell**

   **Oseltamivir**: Tamiflu, inhibitors of the neuraminidase enzyme, no statistical difference in treating COVID-19 (baabfb35a321ea12028160e0d2c1552a2fda2dd5.json) 

The other drugs in the list are irrelevant in this context of effectiveness. Some are related to test of toxicity

<div id=""PartAcat5""></div>
**A.2.4.5 By generating monoclonal antibodies targeting certain proteins of the virus**

   **IL-6 monoclonal antibody**: the IL-6 monoclonal antibody-directed COVID-19 therapy has been used in clinical trial in China (No.ChiCTR2000029765) (7852aafdfb9e59e6af78a47af796325434f8922a.json, c8d206a4f9af0709b6e9ee90c4d854d482cb0784.json), and IL-6 level was suggested to serve as an indicator of poor prognosis, and was suggest to be used for these patients (c8437a45bfb84fb206fe03fd18d28858bae32651.json).
  
   **Spike (S) protein antibody**: It was suggested that monoclonal antibody against the S protein may 231 efficiently block the virus from entering the host (c8437a45bfb84fb206fe03fd18d28858bae32651.json). 
   
Note: some other drugs, though used to treat COVID-19, are not relevant to the discussion. For example, broad-spectrum antibiotics or fever reducers are often used in the control arm. 

[Go Top](#top)

##### A.3. Limitations

The above analysis has the following limitations: 

  1. We used a rather earlier version of the literature set (because the searching step took quite a long time), and some popular drugs, e.g. hydroxychloroquine are only discussed but without clear clinical conclusion yet. 

  2. Literature could be substantially biased towards positive results and by computational methods (discussed below).",b78e18aa,0.26666666666666666
12634,91eaec994e0c6f,4579c562,Plotly offers some interesting plots and visualizations ! Let's try some of them at our time series.,376aef10,0.26666666666666666
12635,e5dd725b8fa422,4eb6bb81,"we can see that time is changeing by 15 min with respect to that our Fature 2 is going upwards.

now lets try to plot joint plot for both fetures using seaborn library",14675d8b,0.26666666666666666
12640,07d6ca51d43510,b1fbf57f,"> In order to cluster the Papers, here K-Means clustering is used as it's simplest and well known algorithm and best suit in this case. In order to define the number of cluster, I'm using Silhouette Method among various others to achieve that. 

> <U>__Silhouette__</U> value measures how similar a point is to its own cluster (cohesion) compared to other clusters. The range of the Silhouette value is between +1 and -1. A high value is desirable and indicates that the point is placed in the correct cluster. <U>_The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k._</U>",38e74a14,0.26666666666666666
12641,5b92c712910a11,530f0645,# Text Preprocessing,e1d17100,0.26666666666666666
12645,c18267b203f28a,93459ccf,## Adding in augmentations ,09ca8efb,0.26666666666666666
12650,f7436bc492474c,18db85d0,The length of the comments varies a lot.,328fd235,0.26666666666666666
12652,3597174a998d4d,58534356,## 2.2 Variables about customers,276892ed,0.26666666666666666
12655,b547f0f38f7744,f4aef2a5,"
Distribution of size of train images:",b6ba66b3,0.26666666666666666
12656,d58491f2896fc1,24009a8f,**Gerekli kütüphanelerin projeye eklenmesi**,514bfdff,0.26666666666666666
12657,5be39e4e35cec7,d79cf034,"<a id = ""4""></a><br>
## Categorical Variable ",14d617c9,0.26666666666666666
12659,67b7354e96113a,3b36cd4f,**Using the titles obtained we'll fill the age**,dca94250,0.26666666666666666
12660,06e0990b9b0dcc,12fe520d,# Q_learning,b36f10d2,0.26666666666666666
12661,55a5e31d03df9f,0b32b331,"As we saw earlier our flatten effectively outputs 150528 as shape and we have over 15M parameters! Wow!


Nevertheless, before we train our model you can see that activation function are a crucial step for our Deep Learning Model. Let's quickly wrap some of these activation functions, see what they do with a TOY tensor. ",06dce00f,0.26666666666666666
12663,396bc36edb95d3,4e4578df,"#### 2.2 Data Split: Split the data into test and train to build classification model CART, Random Forest, Artificial Neural Network. Feature importance for each model.",965e4f8f,0.26666666666666666
12670,80729c2598eb26,df553f57,It seems that most projects have failed while a lot have been successfull ,153e0b56,0.26666666666666666
12671,6998861ff6ff01,43c756f8,"You may have to check the [numpy documentation](https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.dtype.kind.html#numpy.dtype.kind) to match the letter code to the dtype of the object. ""O"" is the code for ""object"", so we can see that these two methods give us the same information.",ea9e72cf,0.26666666666666666
12672,e0e19e91579432,3d1ab920,"1. Keras Tuner- Decide Number of Hidden Layers And Neuron In Neural Network
2. How many number of hidden layers we should have?
3. How many number of neurons we should have in hidden layers?
4. Learning Rate",0c8a0755,0.26666666666666666
12673,738bfced935b69,2e74ba2c,"The distribution price & year is skewed to left, in this data set most data between 2010 to 2020.",2d3c592d,0.2671232876712329
12675,ee23a565163388,f7ad1805,## **Influence of CPK Enzyme in Heart Failure**,88aacbc4,0.26717557251908397
12677,d96642860ab3dd,86782fe4,### 1.7 Handaling Name column feature,98419d48,0.26744186046511625
12679,3d77c1560bd16e,cac097f5,"<a id='2'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Demographic analysis</div>",87c141ca,0.2676056338028169
12680,bddd799cdbbae8,48b54c2b,**Top 20 in type of cyberbullying tweet**,b44e3c08,0.2676056338028169
12693,917957c6c4065f,cbd71a0a,"#### Ratio (likes/views, dislikes/views, comment_count/views, dislikes/likes)",55b8ed68,0.2679738562091503
12701,8d70dcae7f40a3,ba319b78,# Model Building,472c71ce,0.2682926829268293
12705,fd4017c1514157,e21165cf,"* It contain recordings of 397 different primary labels(species).
* As we can see from the graph, it is highly imbalanced training data kind of 'Multitailed Classification'.
* Out of 397 species only 39 species have label count of more than 300",fd8f0896,0.2682926829268293
12710,514d8de15cb7ef,d851594c,### Splitting the data into training and test data,cfe111b2,0.2682926829268293
12714,dbccf99c49570f,c9eae441,"## Delete the columns ""Passenger Id"" and ""Ticket"" because those wont contribute anything as features for classification. Along with them ""Cabin"" is also removed as too many missing values prevails in it.",c20fc09e,0.2682926829268293
12715,e19e307b3fd188,8b609097,## Correlations,2173955b,0.2682926829268293
12717,726833f92fb87a,b54c2d22,## pdays Analysis,7dc5e1b6,0.2684563758389262
12721,fdc3afd309b850,6949bed7,"<a id=""dars""></a>
### 6.1.3  Defining Administrative Regions (AR)",966bde38,0.26851851851851855
12724,e58e68e4eeefe5,bdc23719,### Romove Outliers,a87662ce,0.26865671641791045
12729,21413205980558,348d3959,"* # Management is the most common job type;
#  管理是最常见的工作类型；
* # Retired people are older than other jobs and students are the youngest;
# 退休人员比其他工作年龄大，学生年龄最小；
* # Retired people have the most balance。
# 退休的人有最多的盈余
# //the better chart is stacked column chart，we can cut the balance as high,mid,low three type,and see the proportion that every type of job have<p>
# //比较好的图表是堆积柱形图，我们可以把平衡分为高、中、低三种类型，看每种工作所占的比例",84197de0,0.26865671641791045
12731,0caaec057f7184,ebd8531e,Shop 31 has the most item sales in 2 years.,b875533e,0.26881720430107525
12736,e1a69c71c2c282,0e1b9675,# EDA,b2ca7d3a,0.2692307692307692
12738,a915263bc207da,ad39fded,"Evidently  from the boxplot its seen that outliers are those who gave away more than 320 ratings. So will drop the user IDs of those who rated more than
320 movies.",b17ebcda,0.2692307692307692
12739,af6556ced704f6,418a8c18,"***Look count,mean,std,min,quartiles***
   * We use **describe()** for look this values. **describe()** shows only numeric values 
   * We find outliers with looking quartiles (25%,50%,75%)
   
   Outliers: Data that is out of range. (higher or lower)      ",881577c0,0.2692307692307692
12741,a4f8ad33c823c5,644a7c0b,"### APACHE SCORES
APACHE scores help to measure during the first 24 hours which results in the highest APACHE III score. Apache scores are also known as Acute Physiology and Chronic Health Evaluation Score (APACHE) and it is used in critical care medicine to predict the mortality upon admission to an intensive care unit.

APACHE measures two things
- Assesses the severity of the acute illness
- assess the pre-illness chronic medical status of the patient

The worst values achieved by the patient in the first 24 hours of admission to the intensive care unit should be used

https://radiopaedia.org/articles/apache-score-2 

The d1_glucose_max value is the highest value during the first 24 scores. 

Hence, the apache scores and d1_glucose_max values can be comparable. So, we can take just take one of the variables for analysis.
",fcd48307,0.2692307692307692
12743,5c6fcd59adac6e,1f0a0efa,"After initial columns with info, there is a long list of drugs with values indicating total number of prescriptions written for the year by that individual 

For Opioid.Prescriber, 1 indicates the individual has prescribed opioids more than 10 times that year",f365ba13,0.2692307692307692
12744,95efc1ad1d3e26,023a913d,## 1.3. Train,79de1120,0.2692307692307692
12745,aae204e78a48d1,bc48835d,"The attrition base is slightly older but this is not significant in this dataset.

**Hypothesis 1: not proven**",53ab6133,0.2692307692307692
12747,6f1481148352e9,fec3aacc,"**Short info about Mato Grosso:**

![](https://avatars.mds.yandex.net/i?id=77139b3708668e3e627cfa67e952c6f8-4589829-images-thumbs&n=13)

**Mato Grosso** - the third largest by area, located in the Central-West region. The state has 1.66% of the Brazilian population and is responsible for 1.9% of the Brazilian GDP. A state with a flat landscape that alternates between vast chapadas and plain areas, Mato Grosso contains three main ecosystems: the Cerrado, the Pantanal and the Amazon rainforest. A state with a flat landscape that alternates between vast chapadas and plain areas, Mato Grosso contains three main ecosystems: the Cerrado, the Pantanal and the Amazon rainforest.",7cfbdb8f,0.2692307692307692
12748,163ceeb80d6923,1280f21f,## Define Datasets,4adfbb90,0.2692307692307692
12749,44f6a002ecd033,c30d30af,### Cleaning Missing Values,70bbe106,0.2692307692307692
12750,2facf256353117,c1199e72,# Work with ISBI Data Set,18f579be,0.2692307692307692
12751,71d3e4aee86e3e,0c57146a,### Statewise Comparison of Confirmed/Recovered/Deceased cases reported over time,69706f0b,0.2692307692307692
12752,09751c520b0616,97299369,#### (iv) Dealing with numrical features,a4d0c7e9,0.2692307692307692
12760,897ca904b74a98,a9feb5f2,### Target/Variables relations,c5844ad4,0.2692307692307692
12761,669ce946943d60,cd9a7526,### Dataload,0f63c4ce,0.2692307692307692
12765,3f451680b1857b,8d911738,# Pre-Processing,56c45a1b,0.2692307692307692
12768,be9597c72542a2,7c0ae7b3,"# **Which inverter (source_key) has produced maximum DC/AC power?
**",6f29c6d8,0.2692307692307692
12769,4d91e84c564cbe,e2b850f8,"i.e. the expression above means ""give me all the planets from index 3 onward"".

We can also use negative indices when slicing:",355a43e3,0.2692307692307692
12774,957e035ba5b9d5,43d205b6,"Let's prepare the data using `flow_from_directory` to generate batches of image data (and labels) and resize all images to 128x128.

**Question**: should we preprocess the images for the pre-trained VGG network?",778ab3d3,0.2695035460992908
12777,312135b445bd23,33b4a9f7,"## Training bi-gram Model
Some words on its own doesn't give a lot of information, but when coming together, the meaning is changing to something else. Our goal was to transform meaningful bi-gram phrases to one token, for example: `fake news` to `fake_news`. For that, we used [Gensim](https://radimrehurek.com/gensim/)'s Phrases package, that has two implementations: 1. Data-Driven approach and 2. NPMI (Normalized Pointwise Mutual Information) score. We won't show here the training, since it takes times, but we will load our trained model and see some examples. The trainind code is in our [notebook](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb).
We used a threshold of 10 and minimum count of 5 that worked best for our use case, to build a search engine. One can play with the hyper-parameters for his own use-case, depending on the tradeoff between large number (and less meaningful) of phrases to a smaller (but more meaningful) amount of phrases.",8ced381f,0.2696629213483146
12778,04ff2af52f147b,0c8c0fa7,"We see that *Age* is correlated with *Pclass*.  *Sex* together with *Pclass* provides yet better predictions seen by the varied medians below.  We see two distinct trends, women having a lower median age than men for a given class, as well as median age increasing as class increases (class 1 is 'higher' than class 3 ie. more expensive).  As a result, it is necessary to take these relations into account when filling null values.",d5f37be9,0.2696629213483146
12783,06ecf7a304c309,c92f2b4a,조기 학습 종료를 이용하여 훈련을 해보겠습니다. (`early stopping callback`),714de627,0.2698412698412698
12784,f3d5d8917ce5df,336f4e96,"### Free up some RAM
The free Kaggle notebook only gives 16GB of RAM. So we'll delete the initial DFs that were merged into larger DFs and run the ""reduce_mem"" function on all DFs that we are going to be using in the next section of the notebook.

I do a lot of stuff like this throughout. Obviously, feel free to skip these blocks if you're running this notebook in a more powerful environment.",e45112f8,0.2698412698412698
12785,8985a124d4b657,3f0db330,"It says 0 because after running the below cell, I ran the above cell. I had to this because I forgot to add a print statement. There were 291 null values and we had to get rid of this. Else, it's gonna be a problem while training.",586d1846,0.2698412698412698
12789,3c2033cc99c12c,730e874a,"**Note:** *The above part is about the data cleaning and preprocessing, I have dealt with the null values and drop the outliers in the normal class*",dfa22a54,0.27007299270072993
12790,c80939c7c626cf,2f9d6290,# 4 Name,b9ac31e2,0.27007299270072993
12794,2dda7facf3c1e0,be2d000c,###(Optional) Download and Unzip pre-trained model,45552d2b,0.2702702702702703
12796,d76896b30cebd3,8d119a2e,Distribution of Main Categories,1b4e8f34,0.2702702702702703
12797,a6c34cd514e30e,b26be651,### Let's check 1st file: ../input/calendar_summary.csv,bf603ddd,0.2702702702702703
12799,ac9b48d531bad9,7cfc4ca6,Changing Yes and No values into 0 & 1,95965e35,0.2702702702702703
12801,63b44c85e32c1f,54e087a6,To check if 'Fire' and 'Renton' is present in the list names. A conventional approach would be to use a for loop and iterate over the list and use the if condition. But in python you can use 'a in b' concept which would return 'True' if a is present in b and 'False' if not.,fb9b9562,0.2702702702702703
12803,8276973853faa1,c64fbfca,> Heatmap view of the data,88da542b,0.2702702702702703
12806,b7b1057764fa02,57f557f5,"With our helper function now ready, we begin to print the images. The code lines above the print command: 

`y_train_in = y_train.argsort()
y_train = y_train[y_train_in]
X_train = X_train[y_train_in]` 

sort the data according to the symbols, making it easier for us to deal with it, and ensures that we do not run into any mismatches of data image and its symbol.",5053a192,0.2702702702702703
12807,27d5291d6365ba,2685479b,# Transaction Volume by merchant state,96b30229,0.2702702702702703
12808,62037c5832129c,0da3e681,"## K-fold cross-validation
* In k-fold cross-validation you randomly split the training set into k-folds without replacement, where k-1 folds are used for the model training and one fold is used for testing. 
* Since this is repeated k times we obtain k models and performance metrics. 
* We can use the k-folds for model tuning. 
* Once we found the right hyperparameters values we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set. 
* **k=10** works for most applications unless you have a smaller training set. 
* For smaller training sets increase the number of folds. When k = the number of observations in your dataset, then that's LOOCV",61474350,0.2702702702702703
12810,979f1e99f1b309,27813096,***The plot showing that most number of kills are between 0 and 1 and between 2 and 3***,d1bfebbf,0.27049180327868855
12811,869a39a3d4dea2,faa5ce15,"Draw shapes on top of image using cv2.line, cv2.circle and cv2.rectangle",9020daf8,0.27058823529411763
12814,ba4b3bd184acbb,e0f9d6e2,"If we do not want to use the index labels, we can also access the index position using the `iloc` method.",0f5de724,0.2706766917293233
12819,eb0854a6601407,79f51f17,# Target analysis,6d107747,0.2708333333333333
12820,2a377ced98d67a,078fdf89,### 3.1. Preprocessing,262231a8,0.2708333333333333
12821,a3ae04b78e45b5,cb30905d,**PERCENTAGE OF STATE'S POPULATION ACCORDONG TO THE RACES**,4195da8b,0.2708333333333333
12822,3b5903412fe741,3a1fa364,"Both `loc` and `iloc` are row-first, column-second. This is the opposite of what we do in native Python, which is column-first, row-second.

This means that it's marginally easier to retrieve rows, and marginally harder to get retrieve columns. To get a column with `iloc`, we can do the following:",ad231969,0.2708333333333333
12823,64a336ac34d95c,f8f23f0c,"## Average Monthly Charges vs Number of Services
",be73a990,0.2708333333333333
12828,b9bc7dc9f582e5,3f21478e,"**LotFrontage** has more than 17% null values but it affects the Target variable with 0.35 correlation score.So, we will keep this column and try to impute it.Also,columns having over 50% of **correlation** can introduce bias in dataset. Hence we will remove those.",15cc4d28,0.2711864406779661
12832,a81661cc35d8d2,843d3f8f,"<b><font size=""4"">1. Correlation Matrix</font></b>",3331f113,0.2711864406779661
12833,149cb8d3489224,d1121fe4,### Most frequent values,116858e7,0.2711864406779661
12836,2a56d6b0e153f2,2f7fbb51,"AS DEMONSTRATED, LARGE NUMBER OF PEOPLE DO NOT HAVE WORK EXPERIENCE",8dc315e6,0.2711864406779661
12838,dac3c8204a2d1b,7f68b3ca,Grouping the data by the year ,b0d2d0dc,0.2711864406779661
12839,f2e5e9fb9eaaf7,3c440c1c,"<a id=""4.1.2""></a>
### 4.1.2 Individual features
Count how many missing values in each features on `train` and `test` dataset to see if there any similiarity between them.

**Observations:**
- Every features in `train` and `test` dataset has a missing value of around `1.6%`.
- `train` dataset has a missing value of around `15,000` for each feature.
- There are around `7,000 - 8,000` missing values for each feature in `test` dataset.",048e0d08,0.2711864406779661
12843,2ada0305b68956,893e3163,### 44. Palette = 'Purples_r',133e26f4,0.2714285714285714
12849,fe6750354fb64f,2d899363,## Bar Graph of Active Cases,271741f0,0.2714285714285714
12850,9c044fa3072552,c8d8a525,The Dataset consists registered accidents from over 11790 cities in the US,1362842e,0.2714285714285714
12851,38b79494ac749e,e6356bb1,### Fit,39162a40,0.2714285714285714
12858,3fb15e6e48aec2,ce88a129,"# Cabin
* Suspected that your cabin letter or the cabin number will be at a certain location on the ship therefore you could be closer to the boats
* A quick look on wikipedia shows this is true",9d1f4358,0.2719298245614035
12859,fe7360cddc13e5,69a19f1a,------------------------------------------------------------------------------------------------------------------------,8979e423,0.2719298245614035
12862,7f74a04ae75792,b8390549,"## Explore Numerical Variables
",d01e91da,0.27205882352941174
12863,396bc36edb95d3,5cc1e312,"#### 2.3 Performance Metrics: Check the performance of Predictions on Train and Test sets using Accuracy, Confusion Matrix, Plot ROC curve and get ROC_AUC score for each model, Make classification reports for each model. Calculate Train and Test Accuracies for each model. Build confusion matrix for each model. Calculate roc_auc_score for each model. Build classification reports for each model. ",965e4f8f,0.2722222222222222
12865,eb3aeb8ad87ea0,67476208,### **Creating Dataset **,2301252e,0.2727272727272727
12873,be2f4d8a6b73ca,3d2582d7,**Cardinality of the data**,5d8ce40a,0.2727272727272727
12875,1691c9f2c2f656,e5429211,# Read data,70433e76,0.2727272727272727
12876,dd02a9b545f742,44b58482,"""""""
Turn to code when testing
""""""
try:
    print(""Set the system mode (1) Test mode (2) Commissioning mode "")
    systemModeOption = int(input())

except StdinNotImplementedError or NameError:
    print(""System runs without support of input requests due to Kaggle competition rules."")
    systemModeOption = 2

if(systemModeOption == 1): systemMode = 'test'
if(systemModeOption == 2): systemMode = 'commissioning'",7116cd2d,0.2727272727272727
12882,2cb457b60dd246,1e037289,### Transfer the Images into the Folders,339367df,0.2727272727272727
12885,4b64dc653fb7eb,811fd04e,"<br>
###   <a id=""jump"">Basic Text Preprocessing Steps - Cleaning </a>
<br>
Now that we have comments, its time to process them to convert them into a form that can be fed to classifier.

To do so following basic steps are performed and to get a better idea of what these steps do, an example is added as well. 

**“You are annoying!!! goJumpOff4Cliff pleaseeeeeeee”**
* Step 1 - [Remove punctuation](#1) →** You are annoying goJumpOff4Cliff pleaseeeeeeee**
* Step 2 - [Remove digits](#2)→ ** You are annoying goJumpOffCliff please**
* Step 3 - [Split combined words](#3) → **You are annoying go Jump Off Cliff please**
* Step 4 - [Convert to lowercase](#4) →   ** your are annoying go jump off cliff please**
* Step 5 - [Split each sentence using delimiter](#5) →   ** your, are, annoying, go, jump, off, cliff, please**
* Step 6 - [Remove stop words](#6) →       **annoying, jump, cliff **
* Step 7 - [Convert Word to Base Form](#7) →                      **annoy, jump, cliff** 

Please note that order of steps matter here, if step number 4 is performed before Step 3, we wont be able to split the Combined words like **goJumpOffCliff**.",57675cc2,0.2727272727272727
12886,eda49464dd6d1b,75032080,"### Only 158 people with previous insurance switched to our company.  The number is so small they do not even show up on the bar chart.  This is very useful to the model, because it will tend to rule out everyone with previous insurance, which is about half of the customers.",8421f81f,0.2727272727272727
12888,241cf32abb22d8,28f49bc9,"## Feature Scaling

Scaling descriptive features is beneficial as it can normalise the numeric values among different variables within a specific range and can help speed up the processing time in the algorithm. 
Min-Max Scaling is applied to scale the descriptive features between 0 and 1. Each binary feature can be still kept as binary after scaling.",47157066,0.2727272727272727
12889,132fa9714f2046,c855b9ce,"## Exercise 2
** Create a figure object and put two axes on it, ax1 and ax2. Located at [0,0,1,1] and [0.2,0.5,.2,.2] respectively.**",3bb1775f,0.2727272727272727
12893,75adb7945ef9bd,7a920c12,"As location is free text, the data is not clean, you can see both 'USA' and 'United States' in top locations. We than have a look at % of disaster tweets for common locations.",785c5095,0.2727272727272727
12896,347c7b0f48c53f,4f76d067,"**** * Fetching Articles from Wikipedia
 Before we could summarize Wikipedia articles, we need to fetch them from the web. To do so we will use a couple of libraries. The first library that we need to download is the beautiful soup which is very useful Python utility for web scraping. Execute the following command at the command prompt to download the Beautiful Soup utility.**********",c58305ba,0.2727272727272727
12902,0cb9adc158b705,48a9c65d,"hmm, just image names and their label. Looks simple ? Not so soon. The labels are space-delimited strings. Its a multi-label problem. 

We will use Fastai's datablock API to load our data. DataBlock API is simply amazing. Infinitely  flexibility and incredibly powerful. To use the datablock API, you need to define some functions.",3abf056e,0.2727272727272727
12904,b82610a9364f75,6acf1fb2,# Load data,22c70e69,0.2727272727272727
12906,c13f73168789c2,f39faa0c,"### 1.7 Accessing values from multiple rows but same columns<a id='9'></a>
Syntax : `df.loc[[row_label1, row_label2], 'column_name']`",16175052,0.2727272727272727
12911,adf419444a59df,f1eff34d,## Utility Functions,3a275e7f,0.2727272727272727
12912,1466e61d45b718,bcbe5794,"There is 0 csv file in the current version of the dataset:
",b062d92b,0.2727272727272727
12916,b066ab2167199c,0e29bf61,#### Checking for Numerical and Categorical features,18a1753d,0.2727272727272727
12925,450fda47b03baa,d8033393,"Veri çerçevesindeki sayısal değişken için temel istatistik değerlerini görüntüleyelim.

Standart sapma ve ortalama değerden çıkarımda bulunarak değişkenin ne kadar varyansa sahip olduğu hakkında fikir yürütelim.",62c04adb,0.2727272727272727
12926,2f964d08c25d93,574c8935,Now you're ready to read in the data and use the plotting functions to visualize the data.,1f2e4468,0.2727272727272727
12931,a0b321057e7402,26342677,"To further highlight the differences between the opening and closing reads, I am going to create a column that represents the differences between these prices on the hourly dataset. The modification will allow the data to be clearly visually presented.",5f73fb91,0.2727272727272727
12932,722cd844dfbe8f,00e4871a,"We are now going to check the number of DCM files to check if their number is the same for each category and for each patient. For that, we will complete a copy of train.csv with the calculated informations:",0cedb385,0.2727272727272727
12946,fdc3afd309b850,7dcb1bb9,"The Administrative Regions of the Federal District are divisions very similar to the regular cities with their particular administration, address system and neighborhoods.

Here we are going searching on the web to create a list for each AR containing the neighborhoods of each one of them.
",966bde38,0.27314814814814814
12953,49ee86d074de69,0ac70e8a,"<a id = ""11""></a><br>
## Feature Engineering Part-2",71ccc6d3,0.27350427350427353
12956,510b8303776bb6,7834820b,### Removing of outliers in Sale Price,18080db8,0.27358490566037735
12957,f91f58d488d4af,c794a9b3,"#### Now, let's do the same thing for 7.",5df1bbf3,0.2736842105263158
12961,565ad413cd802f,28a4f77d,Let's check how many samples the dataset contains,397b074e,0.27380952380952384
12963,3d08ca7656dec0,14fd162f,# slp,bd3f87e3,0.273972602739726
12966,1eb62c5782f2d7,8e6d62cc,## 3. Area diantara 2 point z-score,bb69f147,0.273972602739726
12975,81712ee7510ac5,1ab35c66,**How to grab more than one element?**,c4685e79,0.2742857142857143
12978,ac1abfe1dfe815,31798830,"**As we saw, the mean and median of number of words in positive tweets are the same, and the plot is close to normal distribution, so will check using `normaltest`**",6529dbcb,0.2743362831858407
12981,fa02c409161192,16491da5,### Reading in the processed .csv files ,e97077f7,0.27450980392156865
12982,d0080e3a39bc5c,d93affaf,"* I used the library named ""[**Albumentations**](https://github.com/albu/albumentations)"" for the image transformation shown above. 


( Since I was unable to install the library on Kaggle, the image transformations were done locally. )

Have a look at the below images to get an idea about the transformed images.",2fcde4cf,0.27450980392156865
12985,71b75664517244,a7fef3c6,"On 2004, Arsenal come out as the winner of Premier League with 0 lost",fc905af5,0.27450980392156865
12986,64169805aacf17,47751e50,## These are the actions that defines each stroke,1f12ded0,0.27450980392156865
12988,917957c6c4065f,99905d44,"조회수 대비 좋아요 비율  
조회수 대비 싫어요 비율  
조회수 대비 댓글수 비율  
좋아요 대비 싫어요 비율  
을 알아보기 위하여 변수 생성",55b8ed68,0.27450980392156865
12991,1a0bd2f72bbe36,6d4bc5ea,#### Also there are no null values present in the data set:,2fa311dc,0.27450980392156865
12993,5f4ae633cfd090,2b3f7dc5,"# 2. Data Visualization
Feature visualization on ufc_master_ds to gain some insight into our data ",a30a16e2,0.27472527472527475
12995,5ce12be6e7b90e,f2534a5e,"* Find the operation for which, `a` ? `b` = 336 ",c0ab62dd,0.27485380116959063
12996,30fdc4a6e3c1db,4a28f1f8,"What we see:
* We have almost 50% sales coming from FOODS-3 department which is the highest (out of the total ~70% sales coming from foods)
* In household, houshold_1 contributes to 17.5% to the total sles (out of the 22 % sales coming from household)
* Of the 9.3% sales coming from hobbies 8.5% comes from hobbies_1",6111ddee,0.27485380116959063
12998,62487bcd70b199,6285dccf,"## Inference
Age and Experience are alomost linear and they dont give much info on personal loans",f6ae50af,0.275
13001,9a040a4f21091e,7991c774,"# Vectorizing Text
We'll use scikit-learn's CountVectorizer to vectorize each comment, whereby each word gets assigned to a numerical index. We'll throw away any word that occurs in more than 25% of the comments as well as any word that occurs less than 5 times",f591b57d,0.275
13002,3dd4294f903768,65c1a0b2,"We can see that there are only 34 restaurants who make deliveries. Because of this imbalance in the data, we will not use this feature.",0d89d098,0.275
13003,1005ca950e8a81,20689e25,"**Question 3: Clusters that include samples from totally different classes totally destroy the *homogeneity* of the labelling, hence:**",52570331,0.275
13007,37b09262279764,fc59a076,"***SibSp -> Number of siblings / spouses aboard the Titanic*** <br>
Most of the people who <b>survived had no Siblings/Spouses</b><br>
People with more Siblings/Spouses were not able to survive<br>",37c4c417,0.275
13009,5ffe6aa38958a1,3203a835,"**Correlation between different features**

The following code draws a nice correlation plot. Ideally, this should go into a helper function. ",11f5412e,0.275
13013,fdbbd573ba31c2,146a3d29,### engine_temperature(°C),f7c28d74,0.275
13015,5f32117bcd5255,f2244e1d,#### WORLD COORDINATE SYSTEM,85882abf,0.2751677852348993
13020,598b6228760590,b1028c43,"- SibSp
-  of siblings / spouses aboard the Titanic
- Indicates the number of siblings and spouses",be30ab66,0.2753623188405797
13021,0e2a23fbe41ca9,e000908b,"Observations:

- 1 and 3 are present for all the years
- 2 started showing up with good numbers from the year 2015",64e4762c,0.2753623188405797
13022,b01ee6cb674fa3,747ffde4,"Russia and Kazakhstan pose the present situation: The launches from Russia happen at Baikonur, but the site has been used by non russian companies, such as Arianespace.

So, the next lines of code are to set the location_regionas Kazakh and location_country as the launch company",a8ffd35e,0.2753623188405797
13025,9d9da6c439b96b,121324f0,it seems there is no duplicated data,361cc7d9,0.2753623188405797
13032,bb8f5d7807718b,fe718b76,"# 3. Table Demo

Displaying Table within a plot using the [table function](https://matplotlib.org/3.2.1/gallery/misc/table_demo.html)",181ec286,0.27586206896551724
13038,6903d3f38c6a66,4edcc44f,"The above both Code line works the same.

And for every plot set figsize. The graph gives a very different feeling depending on the ratio.",6067ce5e,0.27586206896551724
13045,1750367e54f407,76470e41,"I also prepare a special dataset that will be fed to the Normalization layer. The EfficientnetB3 provided by `tf.keras` includes an out-of-the-box Normalization layer fit onto the `imagenet` dataset. Therefore, we can pull that layer and use the `adapt` function to refit it to the Cassava Disease dataset.",a8e655b2,0.27586206896551724
13046,84127ade6fde87,f3992c51,"We are going to one-hot encode our characters. It is instrumental to limit the one-hot encoding to a character set that is useful for the text being analyzed. In our case, since we loaded text in English, it is safe to use ASCII and deal with a small encoding. We could also make all of the characters lowercase, to reduce the number of different characters in our encoding. Similarly, we could screen out punctuation, numbers, or other characters that aren’t relevant to our expected kinds of text. This may or may not make a practical difference to a neural network, depending on the task at hand.",f55d05b6,0.27586206896551724
13047,a1ba5ffd30dbde,38894a4e,"- Target column ie status is imbalanced as 75% is for 1 and rest 25% is for 0
",48e57546,0.27586206896551724
13048,5ea840754577e3,3ac4571f,"The total passenger count predominantly consists of male passengers. Where **35.24%** are women and the rest **64.76%** are men. The percent of women survived is far greater than that of men (**74.24% > 18.89%**). Gender plays a huge role in deciding who get saved and who are left behind. The crew must have considered the women and children to be saved first and then, the men.
> Second Officer Lightoller insisted on excluding men, while First Officer Murdoch, on the other side of the ship, permitted men and women to board the lifeboats.

source: https://en.wikipedia.org/wiki/Titanic#cite_note-10",9cf9b73f,0.27586206896551724
13051,eb800c50fcfbb2,c626342b,# Load the dataset,e7173f4d,0.27586206896551724
13058,45921c50ac56fa,d60ec7bc,"After punctuation tokens removal and word tokenization, our list of words looks like this.",465973eb,0.27586206896551724
13059,20e1ba19eb9b5e,7d63f63a,"Check the outliers on chosen variables:
* GrLivArea
* TotalBsmtSF",4569bfc1,0.27586206896551724
13062,00d295edcd117e,f14977cb,"torchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1,1]的张量。",f5810f4b,0.27586206896551724
13067,fb5c6021d127ef,6b1b6fa0,Write your query below:,dd05cbd3,0.27586206896551724
13073,6b54e39f86bdb5,b8465ae9,"## Split the data 

We want to split the data we have into a training set and a crossing validation set. I will not use a test set in this application as we do not have a lot of data and we want to maximise the potential of what we have.

The split ratio is determine by the input test_size.
",198084bc,0.27586206896551724
13076,7a058705183598,30a71c31,standardization of features,b0ead917,0.2761904761904762
13077,bbaa07ad21cf4e,b904c367,### N-gram analysis,3ab6b254,0.2761904761904762
13079,7454fdc444df16,83315544,"Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.",a7818ef5,0.2761904761904762
13081,52ee792e228d54,3a2bf0a2,"### The observations from above are:
#### &emsp;&emsp;-> People having more income tend to spend more on their credit card.
#### &emsp;&emsp;-> From the 1st figure it is clear that people with higher income opted for the Personal Loan.
#### &emsp;&emsp;-> Also, people don't tend to spend more than their income on the credit card. (Scatterplot triangular)

### Let us now see how the income varies with the Education and Experience",5096094e,0.27631578947368424
13083,b61ab8f81dc03d,75fd1476,"The result above means that we can easly treat the fields ""Sex"" and ""Embarked"" via mapping and the fields with High cardinality we can remove it (""Ticket"", ""Name""), for ""Cabin"" we can use a different method, find out more about ""Cabin"" formatting below.",64d05394,0.2765957446808511
13086,56785caebaa256,079bcf31,"## 3.2. Additional dates of anomalies as holidays<a class=""anchor"" id=""3.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.2765957446808511
13087,c7e5f658090347,2277aa08, <a id='Feat_Distrib'></a>,43c78e7d,0.2765957446808511
13088,5f674175839b32,9c068a3e,<font color=pink>From the above table we get the idea of sales per year(in millions) over 40 years.IN year 2008 the maximum no. of sales happened with 678 millions.</font>,53a2e343,0.2765957446808511
13094,485de87c50af82,13b55003,### Text Preprocessing,a5bd438e,0.27692307692307694
13097,3cb96bd8eb364b,0831c83f,#### Dataframe Viz,3157af7e,0.27692307692307694
13098,c115e287523aab,3f1a737b,# TPU Configs,feb1288b,0.27692307692307694
13101,a8c042af6b7245,2f4ccce5,"#### reg variables
* only ps_reg_03 has missing values
* the range (min to max) differs between the variables. We could apply scaling (e.g. StandardScaler), but it depends on the classifier we will want to use.

#### car variables
* ps_car_12 and ps_car_15 have missing values
* again, the range differs and we could apply scaling.

#### calc variables
* no missing values
* this seems to be some kind of ratio as the maximum is 0.9
* all three _calc variables have very similar distributions

#### Overall, we can see that the range of the interval variables is rather small. Perhaps some transformation (e.g. log) is already applied in order to anonymize the data?",2487ac62,0.27692307692307694
13102,09751c520b0616,f808a2e2, - Calculate null values,a4d0c7e9,0.27692307692307694
13105,03048e86a6d806,83b928e9,"Generally, 3-5 years is the most frequent period for programming experience, but there are still some people (with big percentage from each job title) have less than 3 years of programming experience. The most frequent period of programming experience for Data Analyst is 1-2 years, while for business analyst is less than 1 year.",1285c231,0.27692307692307694
13107,835a7b4e660d23,414021dc,"### Scatter Plot
* İt is better when there is correlation between two variables.",53bc7a6e,0.27710843373493976
13109,2ada0305b68956,3410a75d,### 45. Palette = 'RdBu',133e26f4,0.27714285714285714
13110,4ae6a182abac64,4aab84e8,* **Age**,418676c5,0.2773109243697479
13115,caee5b3bdf65c1,eb669508,"In order with target: exang, cp, oldpeak, thalach, ca, slope, thal, sex, age",a46111dd,0.2777777777777778
13119,c01049afb6d307,ca1f6e80,"## Variables 2
**These variables have no direct connection with the ID. They can take different values for each ID.**",d37d3b5d,0.2777777777777778
13120,f18e737fcc4b06,4d2dc615,"1. PassengerId
2. Survived
3. Pclass 
4. Name
5. Sex
6. Age
7. SibSp
8. Parch
9. Ticket
10. Fare
11. Cabin
12. Embarked",087b8637,0.2777777777777778
13121,ab6da5994949a3,52d07c94,## Adding Layers,fae6b91d,0.2777777777777778
13122,cb4ad8ed4cb300,a4a9f9c2,# 3. Use spacy to Get a Vector Representation of Questions,7c0f3236,0.2777777777777778
13123,fbb1f9d3818830,d96e70ed,# Paths Setup,c7027f86,0.2777777777777778
13126,b0c2805cd5c087,05398f07,Image investors.com,0446f327,0.2777777777777778
13127,5d6d539f8e7121,04905716,### 연습 :,79340a85,0.2777777777777778
13128,9289395e9c480f,02802343,"# Get data from Prepared data. This data is created in another notebook in kaggle I made.

Cite:
https://www.kaggle.com/nike0good/population-studies-that-related-to-covid-19",55d03d67,0.2777777777777778
13133,c9dc8d00773da4,75606fa3,# Data size,d9aa2f85,0.2777777777777778
13134,3597174a998d4d,a63934eb,"For customers, the variables can be sorted into 2 parts:
* Basic information. It contains the number of customers, country information and customers' type.
* The behavior. It contains customers' previous booking, the need for meal, their deposit type, special requests, number of days that booking in advance and number of stay nights.",276892ed,0.2777777777777778
13135,4d1d6dbab10b20,50db14d0,we have lot of target varible. lets take 2 variable and fit multiple linear regression.,fe366345,0.2777777777777778
13136,892be0a523578c,2c0c529f,"#### 5. minuteSleep_merged.csv
**5.1** According to the [metadata](https://www.kaggle.com/arashnic/fitbit/discussion/281341), value = **3** means **awake**, value = **2** means **restless**, value = **1** means **asleep**",b0e8d7c0,0.2777777777777778
13139,9ad9a97e628bfa,fcdc40cf,"- 하나의 코드만 기재된 경우와 둘 이상의 코드가 기재되어있는 경우로 나눌 수 있다. 
- 둘 이상의 코드가 기재된 경우, / 혹은 . 를 기준으로 앞/뒤로 지명을 나타내는 것으로 추정되는 코드가 나뉘어 있으며,  / 와 . 가 혼용되어 있는 듯 보인다. 
- 같은 지명이라도 표기가 다르게 된 경우가 있는 듯 하다. SOTON, STON 등이 그 예이며, C.A.와 같은 경우 CA 와 같은 코드인지, 아니면 C와 A라는 두 개의 코드를 '.' 구분자를 사용해 나눈것인지 알기 어렵다. 

섣부른 판단을 하기에 정보가 부족하니 일단 명확하게 **일치하는 코드명이라 여겨지는 것들을 처리. 우선 코드명의 맨 뒤에 붙는 '.'는 제거해도 될 것으로 보인다. **",0a7e1136,0.2777777777777778
13140,e0f03003a69819,d87bdd17,# Smoker-Charges relatioship,609ad1f4,0.2777777777777778
13141,4ba67fa2de9e6e,51f157ca,#### Regions,8e0dd482,0.2777777777777778
13148,cf39cde80e66b7,09200876,The model,aed4bc9b,0.2777777777777778
13154,49ac6594c8f5cf,2ef2f371,"We can clearly see that average salary in Finance is more than HR.


We will now see which stream in hsc will get you a better chance of placement as well as salary",6f19f28a,0.2777777777777778
13161,b3e0b7e9ff6849,a4d6a1c4,"<p> When we look at the descriptive statistics of numerical variables, we can see that there are negative values in the Quantity and Price columns. These transactions are canceled orders. In the next step (data preprocessing), we'll eliminate these observations.",f6e4bb0d,0.2777777777777778
13162,3ac432b2cac29c,795b61e5,"## Step 2: Specify and Fit the Model

Create a `DecisionTreeRegressor` model and fit it to the relevant data.
Set `random_state` to 1 again when creating the model.",a358669e,0.2777777777777778
13164,268a610bbc64b4,28f290c7,New User distribution,8a16f301,0.2777777777777778
13165,593d1d3d1df05a,c9862e38,> Drawing Boxes on the characters of license plate(To see progress),bc682ffe,0.2777777777777778
13167,166a62ebb4fc3a,ab8a16c8,"From above sample of code, we can understand that, ""diagnosis"" column is having 2 unique categorical fields. Where 'B' stands for Benign and 'M' stands for Malignant. Let's quickly convert them into values.",db48a079,0.2777777777777778
13168,c349ee5a821411,49697b23,"Having the standard error of the happiness score we can see ordered wich countries present the biggest standard error. 
If this score were greater, that could mean that the social differences in these countries are very large, so the overall Happiness Score wouldnt be reliable. But in our datafram, the standard error looks normal",572b269d,0.2777777777777778
13169,df2a7968c08ee4,1bf40938,"### Creating Tensors and Pytorch Datasets


In the following cell we are converting our numpy array's into Torch Tensors. 

""Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators""

Read more here --> [Tensors Tutorial](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)
",a2ba0a72,0.2777777777777778
13170,30c8dc87ce52ca,9f621ce1,MODELING,805e9d67,0.2777777777777778
13178,69ac33d79f5130,45e5dbf4,## Exploratory Analysis and Visualization,9d760d2a,0.2777777777777778
13183,225b4fe5d3894a,51dd778e,"<a id=""5b""></a>
### b. Looking for Correlations",4b4197b3,0.27835051546391754
13186,5d2a3e82679cf3,5c4fcc9d,# 3) ONE HOT ENCODING,9e60b1e3,0.27848101265822783
13190,2105f2c5132866,36b2eb05,"Correlating numerical and ordinal features

We can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.

Observations.

Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption 

Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption

Most passengers in Pclass=1 survived. Confirms our classifying assumption 

Pclass varies in terms of Age distribution of passengers.

Decisions.

Consider Pclass for model training.",bfe8023d,0.2786885245901639
13192,601e18072783b4,38675c8f,# Netflix dataset,36b2b1fa,0.2786885245901639
13194,99bf357eaf61f1,ccc073c3,"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.",9d92fafe,0.27884615384615385
13196,44f6a002ecd033,83fcd6b2,"So the process does not have to be done twice for train and test, the datasets will be merged while it is cleaned and then split back up after the data is put together like wanted.",70bbe106,0.27884615384615385
13199,743ae010f5e875,99032c4c,#### Additional Govt Datasets,02c54445,0.27906976744186046
13201,8539260444e6b5,167f7794,# Lemmatization Function,0369463f,0.27906976744186046
13202,c09fac3c943d51,ee6b4f18,VisitStartTime seems to be same thing as visitId... yet not always!,678d076d,0.27906976744186046
13208,1660daf8867980,5e5eb93a,# 2.1 Monte Carlo Control,42d7cffc,0.27906976744186046
13210,e4c6dd957eb5ce,69e10b6a,"Cool! <br>At Users Achievement table we have the total of registrations multiplied by 3; <br>
This table shows the tier of each Kaggle category and the total medals for each one, date of achievement and points for each category type. <br>
We will explore it furthuer later. ",2e383665,0.27941176470588236
13211,eb0ecd6bebeb15,0258f2f3,"Veri çerçevemizin hedef değişkeninin ""variety"" benzersiz değerlerini görüntüleyelim.",d7b93a60,0.27941176470588236
13212,c65a65d4041018,1899eb6b,"People come from various backgrounds, though most come from math and CS, which isn't surprising. On the other hand, we can say, that those who come from non-relevant background could have higher interest and drive in growing as a DS.

In is quite interesting that in India most kagglers have CS or engineering degree and business disciplines as well as maths aren't as popular as in other countries.",824fb229,0.27941176470588236
13214,99821bc6a45be6,084cb396,###  Train - Validation Split:,b9d59346,0.27941176470588236
13232,7dd46c750653eb,a887b4dd,**Comparing Birth Rate and Death Rate**,c2644713,0.28
13243,cee088a6840708,ecec8b23,"You can try with any of the three datasets available from dgl
 (available dataset: ""cora"", ""citeseer"", ""pubmed"")
## Cora dataset


## Citeseer dataset


    CiteSeer for Document Classification
        The CiteSeer dataset consists of 3312 scientific publications classified into one of six classes. The citation network consists of 4732 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 3703 unique words. The README file in the dataset provides more details.
        Download link:
            https://linqs-data.soe.ucsc.edu/public/lbc/citeseer.tgz  
        Related papers:
            Qing Lu, and Lise Getoor. ""Link-based classification."" ICML, 2003.
            Prithviraj Sen, et al. ""Collective classification in network data."" AI Magazine, 2008.  source -> https://linqs.soe.ucsc.edu/data


## Pubmed dataset

    PubMed Diabetes
        The Pubmed Diabetes dataset consists of 19717 scientific publications from PubMed database pertaining to diabetes classified into one of three classes. The citation network consists of 44338 links. Each publication in the dataset is described by a TF/IDF weighted word vector from a dictionary which consists of 500 unique words. The README file in the dataset provides more details.
        Download Link:
            https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz
        Related Papers:
            Galileo Namata, et. al. ""Query-driven Active Surveying for Collective Classification."" MLG. 2012. source -> https://linqs.soe.ucsc.edu/data
",55463e1c,0.28
13244,0687cd5c8597db,a060f84f,### **Displaying Random images as a grid**,4edec76a,0.28
13245,7e1da639035ac5,600b19b9,# <a id='7'>7. School Income Estimate</a>,120b6c23,0.28
13249,4945eab98d7d39,cc0e1523,After droping the null values and unwanted columns,46258ffb,0.28
13251,bbad077c274022,c46afe51,*Dividing the Date column into the Weekday and the date*,3c2e3dea,0.28
13253,2bd6c370695ea7,d7744067,## Sanitize,cbe6aec8,0.28
13261,c84925c8171900,638db434,"<p>
    <span style='font-family:Georgia'>
        Imagine: Makeup Artist game was launched in 16th April, 2009. Thus we will change 2020 to 2009
    </span>
</p>",e21ff7ec,0.2803738317757009
13262,9c26c5dcd46a25,ef7cef2c,La répartition des Nutriscores est quasi équitable avec tout de même une prépondérance pour la classe D. On peut également projeter la **répartition des scores nutriscore par catégorie de produits** (`pnns_groups_1`) ,1bbbb677,0.2804878048780488
13263,0b01138ad120fc,00e21f7c,**Split ok**,0b4b72e6,0.2804878048780488
13264,fd4017c1514157,697ab832,"### <font color = 'red'>Secondary Labels</font>
* It contains list of eBird codes (i.e., primary labels) that recordists annotated.
* Can be used for Multi-label training.",fd8f0896,0.2804878048780488
13266,54004b32784b68,6a9ac171,# 2-2 Transforming the Categorical Columns,27213ca9,0.2807017543859649
13267,30fdc4a6e3c1db,43b555cf,### Plotting sales distribution of stores across departments,6111ddee,0.2807017543859649
13272,fe7360cddc13e5,ca5633bb,### Modelin Çıktıları,8979e423,0.2807017543859649
13274,9d27afa9ca3f96,7a278f11,enhancement functions,2d86a18d,0.2807017543859649
13276,9e27af2600925c,dfb84127,"## 3 - General Architecture of the learning algorithm ##

It's time to design a simple algorithm to distinguish cat images from non-cat images.

You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**

<img src=""images/LogReg_kiank.png"" style=""width:650px;height:400px;"">

**Mathematical expression of the algorithm**:

For one example $x^{(i)}$:
$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$ 
$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$

The cost is then computed by summing over all training examples:
$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$

**Key steps**:
In this exercise, you will carry out the following steps: 
    - Initialize the parameters of the model
    - Learn the parameters for the model by minimizing the cost  
    - Use the learned parameters to make predictions (on the test set)
    - Analyse the results and conclude",9b556435,0.2807017543859649
13280,738bfced935b69,92088511,Six clusters by tax with year.,2d3c592d,0.2808219178082192
13281,fdc9f4863744b1,8e67df0f,"""SALE PRICE', ""YEAR BUILT"", ""LAND SQUARE FEET"", ""GROSS SQUARE FEET"" have 'object' values but should all be numeric. ""SALE DATE"" has an 'object' value but should be datatime, ""TAX CLASS AT THE TIME OF SALE"" has 'integer' value but should be categorical. ""TAX CLASS AT PRESENT"" has 'object' value and should also have categorical value.",b4529365,0.2808219178082192
13284,e67925694c07d3,7cb42b3e,## **NUMERICAL EDA**,83af4c4a,0.2808988764044944
13286,2f47abddfd1928,617b9d50,Now age feature shows no more missing values.,ae33cc0b,0.2809917355371901
13295,2a724fb7835cdc,b573b37f,**TF-IDF**,c38ac61d,0.28125
13299,117fc0956643d0,40d952b6,"## Step 1. Filter for Only Question-Related Articles Using LDA

We use the filtered articles extracted by **Latent Dirichlet Allocation** (LDA), a well-know topic modeling algorithm. Receiving words as an input vector LDA performs a generative process to provide topics that are probability distribution over words. The aim is to employ LDA to find the most important topics (a bunch of keywords) that are relevant to the description of task-4 (which is related to the question) and use them to find articles from the Cord-19 dataset (a dataset with 40K pieces) that have one of those topics in their titles. 
Since the output of topic modeling only contains the articles that have been proposed for the first round of submission, we have not included the articles added to the second round of introduction. 
The filtered articles are joined with ""metadata.csv"" to get detailed information on each article. 
After joining two data frames, the number of articles has been reduced to 15170 from 138794. 
Note: Due to the lack of suitable computational power, we used the Cord-19 dataset with 40K articles. ",68cef9fd,0.28125
13303,0932046e1f485d,cadc1d11,Let us see if there is any difference to the distribution of rating between free and paid apps.,218cc7a3,0.28125
13309,98a6794067932a,1f404a2e,"Le code ci-dessous permet de générer le top 10 des meilleurs clients en fonctions du nombre total d'expéditions effectuées auprès de ces clients. Dans un premier temps le code permet d'évaluer le nombre total d'expéditions pour chacun des clients. Dans un deuxième temps, un tableau représentant seulement les 10 clients ayant le nombre total d'expéditions le plus élevé est créé. Cette analyse sera pertinente dans notre prise de décision stratégique étant donné que les dirigeants pourront comparer s'ils veulent favoriser leurs clients en fonction du nombre d'expéditions ou plutôt en fonction de la valeur des ventes.",08600fe2,0.2815533980582524
13313,d8ff894670d506,15574a3b,**Creating New Data**,eb0fb7de,0.28169014084507044
13317,631cd434fc3aa2,78f46bf1,"Ok, let's go through them one by one:
* _PoolQC_: documentation (see above) state that NA means there's no pool (make sense, right?) so we can be pretty confident by replacing those with a None.**",2b74febb,0.28169014084507044
13318,726833f92fb87a,a8e9adda,The great majority of customers in the dataset have pdays=-1 (not previously contacted). We could create a new binary feature to indentify customers who have previosuly been contacted or not.,7dc5e1b6,0.28187919463087246
13324,e424c111c44669,bc537302,**Classes**,d9fccfba,0.28205128205128205
13329,4bbe953f82d29b,4486cba2,"В нашем случае временные данные не монотонны, но уникальный. Попробуем отсортировать данные и вновь проверить на монотонность.",772301f2,0.28205128205128205
13333,49ee86d074de69,4e3bef18,* One Hot Encoding,71ccc6d3,0.28205128205128205
13338,9b42412e75d640,0f5da6b8,Duplicates are evenly distributed between both classes so I can remove them,b616570a,0.28205128205128205
13341,9ceb7278784462,d0b53e3e, ## <a id='12'> 9.Word Cloud </a>,3768a567,0.28225806451612906
13344,869a39a3d4dea2,9d80745e,"now we are going to create our image, because we already know how image is being represented by numpy matrix or tensor of numbers",9020daf8,0.2823529411764706
13346,ee23a565163388,6e2f3ab2,"**Inference**
- The CPK enzyme level in the blood of affected patients is more than the others.
- Eventhough the difference in enzyme level between both the groups tend to be less, this may also be one of the factors.",88aacbc4,0.2824427480916031
13349,0e2a23fbe41ca9,1ed962a9,# Merchant Data,64e4762c,0.2826086956521739
13352,72d528df923403,be7c2224,"## Categories and department behavior
- FOODS category is the most sold.
- Most of the items sold are in the FOODS_3 department, second place HOUSEHOLD_1 and third FOODS_2. 
- Each department has a product that sells a lot more than the rest. These are FOODS_3, HOUSEHOLD_1 and HOBBIES_1",d51c8e8e,0.2826086956521739
13354,a6b9837940ee38,5547e48e," * We will use a LeNet-style neural network for training, which requires input dimensions (None,32,32,1). So we need to add a dimension at the end of the array. But the image is 28*28, and to convert it to 32*32, we just need to fill in two rows or two columns with zeros around the image.",52d2acc7,0.2826086956521739
13356,7e89d387feb9f5,9f2c5728,"### Инсайт!!!
На гистограмме видно, что у большинства ресторанов с отсутствующими данными по диапазону цен количество отзывов меньше 40.
Из этого можно сделать вывод, что такие рестораны появились относительно недавно и данных по ним недостаточно.
Будем исходить из предположения, что в новых ресторанах уровень цен минимальный, так как им нужно сначала приобрести клиентуру.
Заполним недостающие данные по уровню цен таким образом:
- При 0-5 отзывов уровень цен будет минимальным '\\$'
- При 5+ отзывов уровень цен будет средним '\\$\\$ - \\$\\$\\$'

# Забегая вперед... Как ни странно, это не сработало. Точность лучше, если просто заполнить пропуски значением '\\$\\$ - \\$\\$\\$'. Дружно кекаем!!!",989e3a1b,0.2826086956521739
13358,2ada0305b68956,d781cc11,### 46. Palette = 'RdBu_r',133e26f4,0.28285714285714286
13363,23df07a474aaae,6295c692,**Preparation for the Modelling**,0ea40276,0.2830188679245283
13367,0ad8d416b89b78,6c5a8a8c,"# Relationship Status:

The relationship attribute was identified in the exploration as providing insight into the solution to the classification problem, further exploration of the attribute and its relationship between income confirmed this analysis. There is a clear increased proportion of higher income earners amongst those who are 'Married' either Husband or Wife, when compared to those who are unmarried, Own-Child, or Other-Relative. This may potentially be due to the increased age of those married when compared to those unmarried or classified as an 'Own-Child'.",0b0562f0,0.2830188679245283
13374,62487bcd70b199,ad342184,## <a id='4.4.'>4.4. Family and Income in Personal Loan</a>,f6ae50af,0.2833333333333333
13376,63d0d9b9a8c7d2,29206025,**For the categorical variables only**,e32e5933,0.2833333333333333
13379,b10bd75889dad9,fd289ca1,#### drop date columns with >=70% mising values,ee00ceee,0.2833333333333333
13380,712198370d5521,19d40b00,Now we will be exploring the unique values in the categorical features to get a clear idea of the data.  ,5882e04c,0.2833333333333333
13383,5b92c712910a11,cdc0c3dc,# 1.1 Lower Case,e1d17100,0.2833333333333333
13384,f6488772605bb5,14795bb2,## **Training and Validation Datasets**,068d4697,0.2833333333333333
13387,1a222fee3089d2,8dcea840,## **Embarked**,59ab8894,0.2835820895522388
13390,20b372b6e4e276,e5535f7a,"## 4. Submission <a class=""anchor"" id=""4""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.2835820895522388
13392,21413205980558,e2cd9f5e,# 3.2  Balance,84197de0,0.2835820895522388
13396,e4525eb0c96f28,f09bdd59,"It's hard to tell any specific ordering or correlation from both the ESRB_Rating and Genre categories. We decided to plot each graph according to year to examine how well their sales data performed.

From this point on, we will be doing more exploration on how sales change over time for games in specific categories. To do this, we will use the function below that lets us plot each category of a specified variable over year.",2093a1f1,0.28378378378378377
13405,d1ff7e10ee0102,849e686f,"*Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.*

<b>Note</b>: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years.",2cc71c3c,0.2840909090909091
13408,840534f2908a9c,1f6026ba,"Series.dt.date
Returns numpy array of python datetime.date objects (namely, the date part of Timestamps without timezone information).",8081c3cc,0.28421052631578947
13409,842547b2def18c,af141bb5,"### 相関関係 (カテゴリカル変数-数値変数)

カテゴリカル変数と数値変数の相関関係を知りたいでしょう．<br>
Embarked(カテゴリカル変数)，Sex(カテゴリカル変数)，Fare(数値変数)とSurvived(カテゴリカル変数)との相関関係を考慮できます．

**観察**

- 高い運賃を払っていた乗客は，良い生存率だった． (creating #4)
- 乗客ポート(Embarked)は生存率と相関がある． (correlating #1 / completing #2) 

**決定**

- Fareをまとめた特徴量を考慮する．",b8efde6d,0.28431372549019607
13410,71b75664517244,e660efb6,"## Points Master

Acumulation of each team point in total",fc905af5,0.28431372549019607
13413,52cfd66e9ec908,553bf668,We're also able to take a more low-level move by using the semantic option in the Lyft level 5 kit. Or... we can go even deeper and use the box rasterizer.,c74adcdf,0.28431372549019607
13414,bd380b97b5c894,05be9d0e,"Missing gender and age values are observed only for one group, some of the patients have been observed for several years, we select the values without gaps from train_info (patient index indicators, gender and age) and see if patient indices are observed in train_info_NanSEX, if yes, we can restore the gender value and age",66f2562a,0.28440366972477066
13415,f3c6048d1058e3,3259f9df,"- From these word clouds, we are not able to judge any starling differences in both the sentiments by looking at words. We don’t see usage of extreme negative connotation or abusive language used while writing negative reviews.",1d9056b0,0.28448275862068967
13417,a4f8ad33c823c5,1bc9e700,"From the above check, it is observed that over 50% of the non NaN values of the '_max', and '_apache' variables are the same except for temp and resprate.
",fcd48307,0.2846153846153846
13418,be9597c72542a2,dcc6216e,"# Rank the inverters based on the DC/AC power they produce
",6f29c6d8,0.2846153846153846
13420,d07915a6e6992e,3815c798,"**Ticket**

This variable has alphanumeric value which might not be related to Survival directly but we can use this variable to create some additional features.",2b912140,0.2846153846153846
13421,3c2033cc99c12c,6ecdaa86,## Data Visualization ,dfa22a54,0.2846715328467153
13425,98fd05fcc5c3e3,e5eef95c,## Removing Null Values,55fe7ece,0.2857142857142857
13426,55339ceb40d5e9,13b00dbc,"# Missing Value
* isnull().sum() generates pandas Series object 
* changing Series datatype into DataFrame
",d0f687dd,0.2857142857142857
13431,55a5e31d03df9f,6425e7ea,"This will be the our toy tensor, we will see how different activation functions affect the tensor.",06dce00f,0.2857142857142857
13433,0dd3ac2d55efd7,b4585d8e,"spaCy provides 96,200,300-dimensional word embeddings for several languages, which have been learned from large corpora. In other words, each word in the model's vocabulary is represented by a list of 96/200/300 floating point numbers – a vector – and these vectors are embedded into a 96/200/300-dimensional space.
1. en_core_web_sm: Here en is for english and sm is for small i.e 96 dimensional word embeddings
2. en_core_web_md: md is for medium i.e 200 dimensional word embeddings
3. en_core_web_lg: lg is for large i.e 300 dimensional word embeddings",e9aa2cc2,0.2857142857142857
13434,585c280865b46e,f6e29ab9,# Look at duplicates,4d6056f1,0.2857142857142857
13435,bbaa07ad21cf4e,017d4a0f,##### We will do a bigram (n=2) analysis over the medical text. Let's check the most common bigrams in text.,3ab6b254,0.2857142857142857
13436,b8849a04581d32,d4a54f15,"#### We will consider only those features that are related to the ball (position, the distance at which it was kicked, etc.). All others, including those from other datasets, are not considered.  
> **specialTeamsPlayType**: Formation of play: Extra Point, Field Goal, Kickoff or Punt (text)  
> **specialTeamsResult**: Special Teams outcome of play dependent on play type: Blocked Kick Attempt, Blocked Punt, Downed, Fair Catch, Kick Attempt Good, Kick Attempt No Good, Kickoff Team Recovery, Muffed, Non-Special Teams Result, Out of Bounds, Return or Touchback (text)  
> **yardlineNumber**: Yard line at line-of-scrimmage (numeric)  
> **kickLength**: Kick length in air of kickoff, field goal or punt (numeric)  
> **kickReturnYardage**: Yards gained by return team if there was a return on a kickoff or punt (numeric)  
> **playResult**: Net yards gained by the kicking team, including penalty yardage (numeric)  
> **absoluteYardlineNumber**: Location of ball downfield in tracking data coordinates (numeric)  
> **snapDetail**: On Punts, whether the snap was on target and if not, provides detail (H: High, L: Low, <: Left, >: Right, OK: Accurate Snap, text)  
> **kickType**: Kickoff or Punt Type (text). Possible values for punt plays:   
> > N: Normal - standard punt style  
> > R: Rugby style punt  
> > A: Nose down or Aussie-style punts  

> **kickContactType**: Detail on how a punt was fielded, or what happened when it wasn't fielded (text). Possible values:  
> > BB: Bounced Backwards  
> > BC: Bobbled Catch from Air  
> > BF: Bounced Forwards  
> > BOG: Bobbled on Ground  
> > CC: Clean Catch from Air  
> > CFFG: Clean Field From Ground  
> > DEZ: Direct to Endzone  
> > ICC: Incidental Coverage Team Contact  
> > KTB: Kick Team Knocked Back  
> > KTC: Kick Team Catch  
> > KTF: Kick Team Knocked Forward  
> > MBC: Muffed by Contact with Non-Designated Returner  
> > MBDR: Muffed by Designated Returner  
> > OOB: Directly Out Of Bounds  

> **operationTime**: Timing from snap to kick on punt plays in seconds: (numeric)  
> **hangTime**: Hangtime of player's punt or kickoff attempt in seconds. Timing is taken from impact with foot to impact with the ground or a player. (numeric)

",b8a568cd,0.2857142857142857
13445,62ae2b200f6b36,9700b7a0,"Co-relation coefficient measures the strength of the relationship between two variables. For this analysis, features having co-relation coefficient less than 0.2 are dropped or not considered while modelling.(Since, their relationship strength is too weak).

Note: The number of casual bikes have four features (weekend, humidity, workingday, atemp) which have co-relational coefficient greater than 0.2, while number of registered bikes have only two feature with co-relational coefficient greater than 0.2 (atemp and humidty).",7da4ea31,0.2857142857142857
13456,ba4b3bd184acbb,ac3d2e70,"
However, this cannot be used to add new rows to the DataFrame",0f5de724,0.2857142857142857
13457,8dd655515e7d18,62331781,### Openness Analysis,895f41cf,0.2857142857142857
13459,867a9f977fa945,8a60bbc7,# **Lemmatization**,2740fcca,0.2857142857142857
13461,be357c1e2c975d,574aa0e3,### Task 2: Plot Sample Images,2486a061,0.2857142857142857
13462,663bbc9eaf267b,6853e098,* Interesting! There is a noticeable difference between that specific car and the other cars.,32445529,0.2857142857142857
13463,a76e0e8770b7a0,38cc86c2,Some Entries is just duplicate entries. İt is interesting. ,02863d3b,0.2857142857142857
13465,a758983a68c014,639c201e,Here we are creating a vocabulary which gives id for each word appearing in corpus. ,ab89f181,0.2857142857142857
13478,c65f7b375af4ef,59b8b406,"# Kullanıcıların verdikleri rating(degerlendirme) sonuçlarını inceleyelim 

Bir kaç kullanıcıların degerlendirmelerinin ortalamasına bakalım kim positiv kim negativ degerlerme yapıyor görelim ",871d53ca,0.2857142857142857
13480,c54ea4523bd49c,4a566dbc,Encode x data as numpy stack,097ccba2,0.2857142857142857
13483,90691864eb68c7,1a0b12b4,"We got all the values of n_hot_rooms greater than percentile value.

replace the value of 101.12 and 81.12 by values close to 15.399519999999",3555ef9b,0.2857142857142857
13489,675b60eaf415a6,d6ae8297,### **Split the image data into train and test using train.txt and test.txt**,68c0b725,0.2857142857142857
13506,84d1ef55b89e17,c1d4629d,**Variable**,2d0b9d51,0.2857142857142857
13508,c4386b8a01d66e,0c6ad301,# Hardness,dc732bf5,0.2857142857142857
13509,0fa9979b5690e9,d21a972d,"Agora que o conjunto de teste está totalmente isolado e não é usado para definir melhores parâmetros, é importante se atentar para outra questão: suponha que, por acaso, as amostras mais fáceis tenham caído no conjunto de teste. Isso vai influenciar o resultado positivamente, gerando uma falsa impressão de que o método de aprendizagem trouxe bons resultados. Aqui duas saídas são possíveis: embaralhar os dados e rodar o mesmo experimento várias vezes ou utilizar validação cruzada.

**Os experimentos feitos até aqui usaram a estratégia de validação hold-out**, onde uma parcela dos dados é isolada e utilizada para verificar o desempenho do modelo. Na primeira proposta para contornar o problema de amostras muito fáceis caírem no conjunto de teste, sugeriu-se usar várias vezes o hold-out com embaralhamento das amostras. Embora aplicável, estatisticamente existe um grau de cetismo da viabilidade disso. Isso porque não há garantias de que todas as amostras em algum momento estarão no conjunto de teste. *A solução de garantia é a validação cruzada ou cross-validation.*",c26eea94,0.2857142857142857
13512,b7298d6aaff625,0d1b1719,### Segregating Target & Features,bdf24bf7,0.2857142857142857
13514,87e94f864d74be,fd201e22,Since there are only 7 rows so I decided to find there replacements from internet.,294bfe9f,0.2857142857142857
13517,ffdb3fe29f4755,311e6b65,"# Part 1
**General wisualizations**",019b2e69,0.2857142857142857
13519,1084376bc4897c,484da139,#### We see that our data is not skewed,1b598487,0.2857142857142857
13521,1823d096209b96,c2506d0e,## Prepare The Data preprocessing ,cb2a79e0,0.2857142857142857
13522,eb33e05704d647,358ec324,Calculate IoU (Intersection of Union),cd80436d,0.2857142857142857
13523,f4b603905215b7,20b9d277,Distribution of different columns.,efe1d587,0.2857142857142857
13533,e69a496109e7d8,b3ef619f,# Univariate Analysis,1c640591,0.2857142857142857
13536,324c699253abc2,af97276c,Preprocessing the database in order to fill the missing values.,7e81e44e,0.2857142857142857
13537,12f4d16fc21645,26e94af0,<h3 style='color:red'>Nitrogen Analysis</h3>,c7752038,0.2857142857142857
13554,4ae464582bac51,991693a3,# FEATURE ENGINEERING,ca6a52ce,0.2857142857142857
13557,e78e7edae89049,e0173fe5,# Train-Test Split,9cef1d94,0.2857142857142857
13558,3cea0f929a2035,b81f0a99,"One can notice that about one third of the number of all suicides was done the Boomers' generation.

Below we can see rough distribution of each feature in our data.",04cfbade,0.2857142857142857
13559,f35bf4df70d310,40858e6d,Step 0 : Import the PCA model from the sklearn library and do dimentional reduction on the data,10bb859a,0.2857142857142857
13560,20c9a2456e494a,b24e4f2a,# Importing the dataset,3e487f55,0.2857142857142857
13562,3879ef16f5eb28,450d28ba,"# Line,scatter,box,violin plots",784b8d8e,0.2857142857142857
13567,38b79494ac749e,b67a85b0,Let's fit a model and plot the hypothesis it learns:,39162a40,0.2857142857142857
13568,3cd78d8d6d56e4,6f637fdd,# Building a Deep Neural Network based on RandomizedSearch,9f632e94,0.2857142857142857
13570,f13534449a3750,f8b24d05,"### Run-length encoding 

The encoded string looks like this: start, length, start, length, ... , where each pair of (start, length) draws a line of length pixeles starting from position start. The start position, in turn, is not a (x, y) coordinate but an index of the 1-d array resulting of flattening the 2-d image into a rows-after-row 1-d sequence of pixels. Knowing the shape of the images we can just unfold this 1-d representating into a 2-dimensions mask using // and %.

We load this .cvs file and write some useful functions to better use the econding.",8b7f3332,0.2857142857142857
13571,3a6274ed72cc00,f6a54c27,"There are 5 independent variables.
* CustomerID: ID number of the customers (categorical) 
* Gender: Gender of the customer (categorical)              
* Age: Age of the customer (numerical)                    
* Annual Income(k$): Annual income of the customer (numerical)  
* Spending Score(1-100): Spending score of the customers (numerical)

dtypes: int64(4), object(1)",51369a2a,0.2857142857142857
13573,f4514ec092a771,d6fa64ee,Create wav.scp file with format: utterance_id -> audio path,3739ab1e,0.2857142857142857
13580,5ce12be6e7b90e,f207e306,"* Find the operation for which, `a` ? `b` =  3",c0ab62dd,0.28654970760233917
13581,91eaec994e0c6f,8a391540,"An interesting pattern to observe is that, for this item example, sales decrease to the lowest level at the end of each year !",376aef10,0.2866666666666667
13584,eda49464dd6d1b,6a77444a,"## Response and Vehicle age
* Response rate appears to be highest with older cars, lowest with new cars
* A large majory of the cars are <2 years old, making this an interesting sample set.  The average vehicle age in most countries is around 10 years old.",8421f81f,0.2867132867132867
13586,c65a65d4041018,cf394d87,### Industry and profession,824fb229,0.2867647058823529
13587,979f1e99f1b309,c42e1977,***THE Disturbution shows that most of match took time from 1300s to 1450s and also between 1750s to 2000s***,d1bfebbf,0.28688524590163933
13600,fdbbd573ba31c2,b0de5fe1,### motor_torque(N-m),f7c28d74,0.2875
13602,254cccd5145725,be748240,As you can tell we have one column less than the training dataset. This is because of the absence of the 'Target' column which is what we are gonna be predicting.,a49b4037,0.2875
13606,1eb62c5782f2d7,173fc75c,### A. z-score = 1.56 dan 0.54,bb69f147,0.2876712328767123
13610,a566b5b7c374e7,aa76474f,"### Initial Impressions:
- The following metrics appear to be the best candidates for investigating further:
     - Restfulness Score - This is my benchmark. While imperfect, it is my best estimate of how I personally feel.
     - Deep Sleep Time - This metric appears to correlate with more other metrics than any other measure.
     - Average HRV - This is my best measure of ""readiness"" and appears to have some moderate correlations. This is also a very reliable measurement that should not be heavily impacted by Oura algorithms.
     - Average Resting Heart Rate - This is another good measure of readiness and recovery, and appears to have some moderate correlations. This is also a very reliable measurement that should not be heavily impacted by Oura algorithms.
     - Start v 12AM, End v 12AM - There appears to be moderate correlation with heart rate and heart rate variability.",b3dc5545,0.28776978417266186
13612,f166950fa915f8,dffdc272,### Split train and test,a7f6ca5e,0.2878787878787879
13617,bb0905d33ae417,7be7dcd6,# Model creation,25fd1965,0.288135593220339
13618,c4bca5d86a38c3,1380a01d,Se puede apreciar que si se tiene título de Nobleza (Royal) o The Countess se tiene probabilidad de salvarse de 1.0,e23d297c,0.288135593220339
13620,a44368590e878a,47b6e78f,### Age,77743ba8,0.288135593220339
13621,a077820f7ab459,250a57f0,### Prepare img_generator_flow,05a43104,0.288135593220339
13623,1294fb4c86f993,955d7f4a,Dropping NaN,4471e513,0.288135593220339
13628,6f1481148352e9,2f050f80,**Let;s check the value of fires in the stete.**,7cfbdb8f,0.28846153846153844
13629,582cb872d19026,b41795fb,"
******************************************************************************************************************************
      ** Now, the ""installs"" column in our dataset has been brought to the desired format and is ready to be analyzed. **
******************************************************************************************************************************
",8d966d69,0.28846153846153844
13635,71d3e4aee86e3e,cecf819a,> # Line Plot Comparison,69706f0b,0.28846153846153844
13637,2ada0305b68956,819951a9,### 47. Palette = 'RdGy',133e26f4,0.2885714285714286
13639,5f32117bcd5255,ef787e50,### HST OPTICAL ONE,85882abf,0.28859060402684567
13642,8ec771f5600a61,8ec6f65c,"Above output gives us some insights that if you are a child then you've really had better chances of survival than aged people so now while impute Age we have to care somewhat about Age as well other than Pclass and sex. 
Now the points comes how to can we get to know whether the person is child or not if Age is NULL. we've seen that in name column we've Titles like Master, Miss and etc... so we will take that title and make a column to know whether a person is child or not ",48364c1f,0.28865979381443296
13647,c8bf959b9608cf,b508ed49,"Note that, style transfer is different than training a CNN where we feed the input images during training process and the weights get updated in each iteration. Here, we are training the image itself. There is no native functionality in Keras to do style transfer. Therefore, we need to create a custom network where the image gets updated. Keras, however, does provide functionality to create customer networks by directly manipulating the tensorflow backend. You're already familiar with the steps involved in style transfer. In this notebook, you'll look at how to implement that in code. Before moving, there are certain tensorflow functions that you need to know about before proceeding. Till now, you didn't need to use these functions as these were taken care of by Keras. But you need to directly use these in order to create your custom network tailored according to the network.


### K.variable()
K.variable(), as the name suggest, can be used to store a variable. 


### K.placeholder()
With placeholders, we can insert values into the model in runtime. These can be used for training input(X) and target(Y) values.  The difference is that with <b>K.Variable() you have to provide an initial value when you declare it. With K.placeholder() we don't have to provide an initial value and can provide it at runtime.</b> 

Since, content and style image are contant and does not require to feed during runtime, we are using K.variable() to store that. Generated image is fed during runtime after each iteration, so we use placeholder for that. Note that when using backend as tensorflow(tf) for Keras, K.variable is same as tf.variable and K.placeholder is same as tf.placeholder. 

We combine the 3 images into a single Keras tensor along the axis of batch size(axis = 0) which we feed to the model. So the batch size will become 3. Along the dimension of batch size, the first is content image, second is style image and third the generated image get the content image feature

Additional reading: 
1. https://keras.io/backend/
2. https://learningtensorflow.com/lesson4/",155e3672,0.28888888888888886
13650,3597174a998d4d,30ba76d6,### 2.2.1 Basic information,276892ed,0.28888888888888886
13654,d77e6d61ad2e8b,0be155b8,# Create a Model,03fd0e96,0.28888888888888886
13661,e25c0f830df3f4,6a0a4ec8,# Adding the Sentiment Polarity & Subjectivity columns to the data,fdcf7189,0.28888888888888886
13668,2f47abddfd1928,434ea8ec,"#### 2.1.5. Embarked

There are just two missing values in embarked feature, initially I would use a similar approach as with Age, but being just two, let's inspect the lines to see if we can do it better.",ae33cc0b,0.2892561983471074
13670,f05342aabe2b59,73eb42c8,### My submission(s) and a random agent for comparison,cfbb391f,0.2894736842105263
13674,fe7360cddc13e5,14213963,"Tüm müşterilerin geçmiş verilerini modelin varsayımlarındaki dağılımlara uydurarak, her müşteri özelinde şunları bulabiliriz:

**1- E(X(t) | λ, p)** t uzunluğundaki bir dönemdeki transaction sayısının beklenen değeri

**2- P(X(t) = x | λ, p)** t uzunluğundaki bir dönemde x kadar transaction gerçekleşme olasılığı

**3- P(teta>t)** teta anında müşterinin pasif hale gelme olasılığı",8979e423,0.2894736842105263
13682,d93a87fdbdb3d2,4b03ea89,#### Find missing data,30d079c3,0.2894736842105263
13685,7e89d387feb9f5,952f26fd,### Добавленный числовой признак №3. Отсутствующие данные в столбце 'Number of Reviews',989e3a1b,0.2898550724637681
13686,7e275c8d5ff2a0,313a585d,"# ICMR TEST LAB DATA
",b3afcc98,0.2898550724637681
13689,9d9da6c439b96b,7c4365c5,"Trying converting ""Year"" column type into integer data type",361cc7d9,0.2898550724637681
13690,8336d84cf3ff6b,9896500a,"From the above chart we see that crop damage has a decent corelation with the numerical variables 

Estimated_Insects_Count , Number_Weeks_quit and Number weeks used ,Pesticide_Use_Catergory 

So lets magnify this effects by using SKleanr polynomal feature extractor ",b96b58a0,0.2898550724637681
13692,a1a31459abf078,da1110be,"Few things are visible from the charts above - 
* The mean of correct answer rate across questions is quite high, **the distribution of percentage correct across questions has a peak between 0.7-0.8**
* Majority of the questions in the dataset appear less than 5k times, however there are small number of questions which have very high unique occurences, sometimes higher than 10k occurences
* When we look at distribution of percentage correct answers across users, we see that it has a skew towards lower values with median at 0.56. Q1 is at 0.42 and Q3 is at 0.66
* **The total user scores are lower for users in initial stages, as the number of questions answered increases the average scores increase and show lower variance and vary between 0.6 - 0.8**
* After 4k questions, there is no increase in average user scores
* There is no linear relationship between total questions answered and average user scores.
",66fc0f54,0.2898550724637681
13693,e3fb4c6300cb56,7b6fdd7f,"<a id=""2""></a>
## Point Plot",8ebbdf89,0.2898550724637681
13696,17a24d566ffa59,a8218071,##### Recreate PCA through SVD,89049e56,0.2898550724637681
13697,4cd25e50c7e007,60eab68a,"# weekday
* 0:Sun
* 1:Mon
* 2:Tue
* 3:Wed
* 4:Thu
* 5:Fri
* 6:Sat",ceb0c525,0.29
13698,83df814455f06c,8f42e553,"We can see that the column names are renamed. Now, the columns have meaningful names.",c9cff71a,0.29
13699,b10bd75889dad9,01e90bab,#### impute missing dates with mode,ee00ceee,0.29
13700,ee23a565163388,77f407b3,## **Diabetes and Heart Failure**,88aacbc4,0.2900763358778626
13703,0caaec057f7184,2908085a,"For each shop, the best selling categories varies, but there are category 40 and category 30 are having around 75% being the best category sellers in the 60 shops. 

For each category, the sales are different in each shops. While category 31, 55 and 25 are containing more of the best selling conditions of the category.",b875533e,0.2903225806451613
13705,b59b5aaeedb1fb,d8c4d749,"> #### Must be ""Male"", ""Female"" or ""Infant"".",1ad63faf,0.2903225806451613
13706,ad26c020235dfc,17aad1b5,Create new features based on the timestamp index:,bf766e48,0.2903225806451613
13707,098fedfcd07456,dfbc2e23,"## Reshaping the image for CNN for gray scale image
1. The images are added with added 1 byte for CNN .
1. For Colour Image this steps is not required . ",052ece26,0.2903225806451613
13709,e8c6480a3122b3,7371ca49,Sex is an important categorical feature because more of female survived than male.,40dc4cca,0.2903225806451613
13711,16862cb02d73d5,2047ba17,"Define **isolation forest** and specify parameters. 

Isolation forest tries to **separate each point** in the data.In case of 2D it randomly creates a line and tries to single out a point. Here an **anomalous point could be separated in few steps while normal points which are closer could take significantly more steps to be segregated.**

I am not going deep into each parameter.**Contamination** is an important parameter here and I have not specified the any value for it as its unsupervised and we dont have information on percentage of outliers.You can also specify it by trial and error on validating its results with outliers in 2D plot or if your data is supervised use that information to specify it. It stands for **percentage of outlier points** in the data.


I am using **sklearn's Isolation Forest** here as it is a small dataset with few months of data, while recently **h2o's **isolation forest is also available which is more scalable on high volume datasets would be worth exploring.

*More details of the algorithm can be found here : https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf

More details on H2O Isolation forest : https://github.com/h2oai/h2o-tutorials/tree/master/tutorials/isolation-forest
        *",d7ffa1a6,0.2903225806451613
13715,b90ef792fd07c2,b30f77c9,# Missing Values,5e2762eb,0.2903225806451613
13719,a3e8d6ef4c5188,f4f4fff3,"All of the features in this dataset are categorical. We need to transform to values. The target variable is 'e' (Edible) and 'p' (Poisonous). The Veil type has only one category. So, we will drop that columns has it doesnot give any value addition to the model. 
Some of the variables such as Veil-color, ring number, gill-attachment have higher percentage with respect to one category. We will see how to deal with that kind of variable.",7c8212dd,0.2903225806451613
13721,57070ad5e0f94f,b48ba96e,# **Checking For Rates of Survive for Males and Females discretely**,d97edc41,0.2903225806451613
13723,56e58d53ac9c57,3c2d2e74,"we see that there are only few NaN values in some columns, but in Review title column, there are quite a lot of empty places. Still this column is very likely to give us useful information so we will keep it.",90e2ab8e,0.2903225806451613
13724,c0ddb77bf32e2b,e575d90a,"We got some annotation in the numeric column like, 1.7#,9.6#,2.3*,2.5x, we want to clean that.",a0cb45f7,0.2903225806451613
13727,63b44c85e32c1f,8abdd787,"In a list with elements as string, **max( )** and **min( )** is applicable. **max( )** would return a string element whose ASCII value is the highest and the lowest when **min( )** is used. Note that only the first index of each element is considered each time and if they value is the same then second index considered so on and so forth.",fb9b9562,0.2905405405405405
13733,957e035ba5b9d5,69e916c3,## Create keras model,778ab3d3,0.2907801418439716
13736,f0fab078f8533b,d6bd9f7b,"## c. Replacing 'nan' values in ['director', 'cast', 'country', 'rating'] col with 'Other'",bdb5ea32,0.2909090909090909
13739,5a8c553e21c70f,0c1b0675,"## Outlier Check with Isolation Forest

Isolation Forest is used to detect outliers. The idea is to isolate each sample with random trees. Each tree is trained on a subset of training data. Since outliers are different from the general trend (in terms of feature values), it is easy to isolate them. The path from root node to a sample in a random tree is expected to be shorter for an outlier. The path length is averaged over a forest of random trees.

Since the number of Class 1 samples is very low, outlier analysis is done only on Class 0 samples. Divide train_X based on Class values.",9ebd9d8f,0.2909090909090909
13740,20b372b6e4e276,429d6e3c,Code from notebook https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34448972,ec8b0860,0.291044776119403
13742,08f845750d026a,831c21db,"
### Select all the data points from India, group by region and compute the total loan amount for each region.
",1c54de30,0.2911392405063291
13746,ce9ed5e2d601d7,dcd486c9,### Top10 predictors,f58a2f43,0.29133858267716534
13749,fc8e0042411c46,3b607798,Now Data is free from all missing value  and we can start with the analysis ,af476c2a,0.29153605015673983
13757,923e97b05be00b,a28b4d73,"How does that look? If there are a sufficient amount of training examples (at least 100), then we can move onto the next step! 

If not, let's re-type something into the <code>finding = """"</code> field, run that cell again, and then run the above cell again.",3a4a22dd,0.2916666666666667
13759,1d5daeca89f48d,5892b1dc,## Calculation using Python,48d478bc,0.2916666666666667
13769,37b09262279764,bc11d5bc,"***Parch -> Number of parents / children aboard the Titanic***<br>
People with <b>no Parents/Children</b> Survived the most<br>",37c4c417,0.2916666666666667
13779,2c8119a4061997,bf16abfa,"The starter model is based on DenseNet121, which I found to work quite well for such kind of problems. The first conv is replaced to accommodate for 1 channel input, and the corresponding pretrained weights are summed. ReLU activation in the head is replaced by Mish, which works noticeably better for all tasks I checked it so far. Since each portion of the prediction (grapheme_root, vowel_diacritic, and consonant_diacritic) is quite independent concept, I create a separate head for each of them (though I didn't do a comparison with one head solution).",1836a79c,0.2916666666666667
13786,386c42a7fb27a4,8f8a68f1,### Numeric Columns,9e9f6974,0.2916666666666667
13790,57740be713cf12,c6f766d5,## ***3. Feature Engineering***,ac122df5,0.2916666666666667
13792,69ac33d79f5130,5203ebd5,"### Data we are going to analysis
- City
- Start Time
- Latitude and longitude
- Temperature
- Weather condition",9d760d2a,0.2916666666666667
13794,3c2033cc99c12c,f154cf82,"*After the data preprocessing and cleaning part, the data distribution of the dataset become relatively smooth, so in this data visualization part, i will aggregate the data and visualize the distribution.*",dfa22a54,0.291970802919708
13795,a5a419dc7245b0,7f0a64df,### Min-Max Scaling,4279726e,0.2920353982300885
13796,ac1abfe1dfe815,4e7e01fc,-----,6529dbcb,0.2920353982300885
13797,c9b4e282e4e2c1,71562504,"These three attributes are categorical attributes. We are going to fill these NaN values with the mode of each column.
",f44d339f,0.2920353982300885
13801,04ff2af52f147b,0de9b070,We will now take this information and use it to fill the null values for *Age* using the median of their class and sex combinations (possible since no null values for either *Pclass* or *Sex*).,d5f37be9,0.29213483146067415
13810,03048e86a6d806,c3657c57,### Experience Using ML Methods,1285c231,0.2923076923076923
13812,09751c520b0616,4aad4f77,"- Filling null value<br>
'LotFrontage'and 'GarageYrBlt' has most null values<br>",a4d0c7e9,0.2923076923076923
13816,510b8303776bb6,981bfa8e,"Since we  do not want to remove too many data ponts, we will keep a track of how many data points have been removed up till any step.",18080db8,0.29245283018867924
13820,8d70dcae7f40a3,118a7e21,"### **Get X, y data from dataframe**",472c71ce,0.2926829268292683
13821,8cefb86a675e5d,d2cfb4c5,# Data Cleaning.,79f9e69b,0.2926829268292683
13829,0e09587faffa8f,ce1c6962,Agency with code **T** was issued the maximum number of summons,0d563d61,0.2926829268292683
13830,47b2c9be5e31cb,5cd71bcb,Let's take a quick look at what the data looks like:,7d4afe56,0.2926829268292683
13834,e19e307b3fd188,0a71da73,"The features that most positively influence the **rent amount** (correlation> = 0.5) are:
* rooms
* bathroom
* parking spaces
* fire insurance (R$)

I will analyze them better below:",2173955b,0.2926829268292683
13840,1dd9c6aa74d289,fad33c8a,## Process time intervals between grades,5ef9a1be,0.29310344827586204
13842,434f930cb58aee,6f974981,We will use a function to split the Labels into separate columns of the dataframe.,0e1d3554,0.29310344827586204
13843,f3c6048d1058e3,0b7912e9,### Utility Function,1d9056b0,0.29310344827586204
13845,84127ade6fde87,764c68ed,"At this point, we need to parse through the characters in the text and provide a one-hot encoding for each of them. Each character will be represented by a vector of length equal to the number of different characters in the encoding. This vector will contain all zeros except a one at the index corresponding to the location of the character in the encoding.",f55d05b6,0.29310344827586204
13849,00001756c60be8,21616367,*Деление признаков на числовые и текстовые*,945aea18,0.29310344827586204
13851,7e1da639035ac5,826f93fe,### <a id='7.1'>7.1 School Income distribution for community schools</a>,120b6c23,0.29333333333333333
13854,67b7354e96113a,e71a2936,"**Explanation:**

1) Subgrouping age based on title.

2) Filling the missing values based on median so that distribution of data remains the same after filling Na's",dca94250,0.29333333333333333
13858,e5dd725b8fa422,4675cee9,"it seeames there are some outliers are present. but only one is present so i am leaveing this as it is.

our data is yearly distributed or only one year we can check it by ploting",14675d8b,0.29333333333333333
13860,b01ee6cb674fa3,5b394e51,"Land Launch is a service product of Sea Launch, an international consortium of companies",a8ffd35e,0.29347826086956524
13867,fa02c409161192,4d0353b1,Extracts the labels and the images,e97077f7,0.29411764705882354
13871,e170d33ee1da8c,023dd8e0,## Training on first fold,253cee3c,0.29411764705882354
13873,629f2918807a9b,72e2a02d,"#### Observations:::

44 orders got cancelled, While 2396 are returned",be56dc84,0.29411764705882354
13874,917957c6c4065f,1b288162,"dislikes/likes 에 결측값이 있는데, likes의 값 중에 0이 있어서 그렇습니다.  
결측값을 0으로 대체합니다.",55b8ed68,0.29411764705882354
13875,02b7e38902069e,52ebf8f0,"#Generate similar sentences from a given text input

Since iNLTK is internally based on a Language Model for each of the languages it supports, we can do interesting stuff like generate similar sentences given a piece of text!

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.29411764705882354
13877,16ca1123840e9f,a4567280,"#### RVSN USSR has the maximum launches. It is a very old company.
##### Strategic Missile Troops [ex-Raketnyye Voyska Strategicheskogo Naznacheniya (RVSN) are a military branch of the Russian Armed Forces that controls Russia's land-based intercontinental ballistic missiles (ICBMs). The RVSN was first formed in the Soviet Armed Forces, and when the USSR collapsed in December 1991, it effectively changed its name from the Soviet to the Russian Strategic Rocket Forces or Strategic Missile Troops.",e8b8f086,0.29411764705882354
13880,a0a5baa6c7e12a,c1b5fce7,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Pair Associations Between Features by Target</div>",551d41de,0.29411764705882354
13883,a871419285588a,1c9937a0,"### Load images, masks and labels",5e08e15f,0.29411764705882354
13884,1a0bd2f72bbe36,22f6a191,### Lets's check the Outliers:,2fa311dc,0.29411764705882354
13885,e4c6dd957eb5ce,c4fae36f,"# Kaggle is a Trend?

Let's explore the total of registrations for each day since the launch of the platform.
What we will see:
- Count
- Mean
- Cumulative total Registers

Them try to analyze and understand the patterns, peaks, differences and so on",2e383665,0.29411764705882354
13892,6014a2010722f2,97e54350,"## Getting tweets data

In this steps, i want to create a dataset that contain tweets of a topic.",abd38dca,0.29411764705882354
13904,d0080e3a39bc5c,dd04ec4a,**ORIGINAL IMAGE**,2fcde4cf,0.29411764705882354
13906,0e662a463309e7,6969c277,Stemming,84c57e51,0.29411764705882354
13911,523123dad03177,76e44d56,Lets have a look their parent's education status. ,48a5e4e6,0.29411764705882354
13919,2ada0305b68956,ce00e1ed,### 48. Palette = 'RdGy_r',133e26f4,0.29428571428571426
13920,9ceb7278784462,2387caa5, ## <a id='13'> 10.Data Preprocessing </a>,3768a567,0.29435483870967744
13922,738bfced935b69,b16041cb,"There is a positive relationship between price and engine size, the larger the engine size the price increases for cars.",2d3c592d,0.2945205479452055
13924,fc8e0042411c46,63544872,# Exploratory Data Analytics,af476c2a,0.2946708463949843
13926,f91f58d488d4af,01461091,"* Now, Pick an arbritrary 3 and let's measure it's distance from our ""ideal digits""

#### There are two main ways to measure distance.

* Take the mean of the *absolute value* of differences. This is called *mean absolute difference* or *L1 norm*.

* Take the mean of *square* of differences and then take the square root. This is called *root mean squared error* (RMSE) or *L2 norm*.",5df1bbf3,0.29473684210526313
13931,897ca904b74a98,d519e20a,#### Target vs blood variables,c5844ad4,0.2948717948717949
13932,80ad12f326ab70,91c460f8,All missing values are below 30%,da404a16,0.2948717948717949
13933,a566b5b7c374e7,ab34c71a,## Binary Activities - Frequency and Correlation,b3dc5545,0.2949640287769784
13935,0858e1bb3cbaca,2b445fa7,"To view the return of all baskets sold, we can combine the two codes above and display",78548374,0.29508196721311475
13936,918040fad252ec,b8cc6886,Mengambil data secara acak yang digunakan pada train dan test data,966fcd8f,0.29508196721311475
13938,601e18072783b4,a639322d,## Preprocessing on Netflix dataset,36b2b1fa,0.29508196721311475
13943,7454fdc444df16,ae4ab728,"### Insights
1. The number of image patches per patient varies a lot! This leads to the questions whether all images show the same resolution of tissue cells if this varies between patients.
2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. Does a tissue slice per patient cover the whole region of interest?
3. The classes of IDC versus no IDC are imbalanced. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (in case we would like to rebalance the classes).",a7818ef5,0.29523809523809524
13946,5f32117bcd5255,d6ab8f91,#### TARGET INFORMATIONS,85882abf,0.2953020134228188
13951,da199f8fb59439,6022f853,"> Since we have released year ,there is no use of date added.
Therefore we can drop date added.",baaa665d,0.29545454545454547
13952,6d66ced0028dea,be05165c,**Rooms**,f50aae52,0.29545454545454547
13956,d83e5b44d1b80d,83b64103,## Top 15 countries survey data as sample,62845930,0.29545454545454547
13957,d1ff7e10ee0102,9986289f,"### In summary

Stories aside, we can conclude that:

* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.
* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.

We just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).

That said, let's separate the wheat from the chaff.",2cc71c3c,0.29545454545454547
13959,32e04b08ff52eb,ea7314e0,Token Calculation,8d5b86e0,0.29545454545454547
13963,a35cdce61f4059,d707104f,the plot above shows the relationship between the features in our dataset. as it can be seen almost all of our features are independant of each other.,acc8eab6,0.29545454545454547
13971,087e21401d7dfc,bcc859e5,## Linear Regressor Model,42000489,0.29591836734693877
13976,8106640e2f9c7e,e059d55b,## Преобразование данных,faea9b8e,0.2962962962962963
13978,c1984e64b35234,8cda6edb,For more info on the dataset and how it is obtained I read here: https://customercare.23andme.com/hc/en-us/articles/115004459928-Raw-Genotype-Data-Technical-Details,1811225b,0.2962962962962963
13981,c6f8ff61a5fa87,8184f962,"# <span style=""color:blue;""><strong>4.Sanity Check</strong></span>",3eea586b,0.2962962962962963
13983,55ce731a138ca7,0cd961e0,# Data Augmentations,4996250b,0.2962962962962963
13987,f4b9042e693b6c,547e79b1,"These are the flags for training. When you fork (after you upvote of course, 😉), feel free to play around with these flags!",676cacc9,0.2962962962962963
13995,c968dbd8d49ae6,f20bfc5e,# **Use of mapping functions and fix of TotalCharges column**,dfb2684d,0.2962962962962963
13998,24e550b8226932,6910046e,##### item_categories:,0caee953,0.2962962962962963
14004,0dfa3e758551c8,3d079f6a,"549 dead, 342 alive passengers",7adbb44c,0.2962962962962963
14005,613bf7bfdcb9e3,8384b086,### axis = 0 -> top to bottom most common values,32beb65d,0.2962962962962963
14006,135122550b6483,ac75aceb,"Since this is a Time Series Forecasting Question the Test Dates would be dates in the 
future from 2019.",6592d6d8,0.2962962962962963
14009,ab6da5994949a3,9b38328b,## Fitting ANN to Training Set,fae6b91d,0.2962962962962963
14013,faa8e6c8ab9246,90dbb7dd,"There are no null values in train dataset
",2bea1419,0.2962962962962963
14015,9169c4e9c33c90,2136e5b3,"<a id=""Title""></a>",725bf880,0.2966101694915254
14017,b10bd75889dad9,7711b75d,"#### binnig the date values to week1, week2, week3, week4 etc",ee00ceee,0.2966666666666667
14019,3cc097a5859dc1,9a184d77,# **Number of Nan values in each column**,14380d73,0.296875
14020,c85c94076e9c3a,7a1fd65a," Will impute null values with median value, to avoid effects of outliers on imputation value",3ea0c443,0.296875
14021,ff3a8ce61fab6a,237bbc52,"**We can change variable value by equal operator (=) or assign keyword**
<br>

## Exampel 1 ",9afe1654,0.296875
14022,0932046e1f485d,496fde0f,"The ratings of the free apps look to be more evenly distributed with their rating, but there is no major difference between them.",218cc7a3,0.296875
14024,0e2a23fbe41ca9,cbe0c189,"## Sanity Check

- Unique ```merchant_id```s
- Nulls",64e4762c,0.2971014492753623
14028,bbb3f4b76a4559,24f532a6,"Oh... In this case, we need only 2 features to explane 9 original cols. So let's take 2 cols like bellow.   
(Usually only few cols can't explane all of origins. We shoud consider balance of Explained variance ratio and feature dims. For example, ""take 32 cols to explane 80% of origins"".)",75185823,0.2972972972972973
14032,ccabe7a86825ce,4e1b2241,"**Using custom order for ORD_1, ODR_2, ORD_3, ORD_4**",d766cbf9,0.2972972972972973
14037,8276973853faa1,80465564,# Let's take a look into Veg and Non-Veg data by stateswise,88da542b,0.2972972972972973
14046,e9b9663777db82,fda41119,#### 3.Quantitative analysis for Floor Location feature,648e8507,0.2975206611570248
14050,565ad413cd802f,2efc66c9,"Let's take a look at a sample image from the dataset. We'll define a function `show_sample` to help us. We will also include the option to invert the image before showing it, because the original images are quite dark.",397b074e,0.2976190476190476
14052,fc8e0042411c46,bf0a9d00,## Univariate Analysis,af476c2a,0.29780564263322884
14061,c7e5f658090347,21a3edd3,"## Comparing the Distributions of each Feature to the Target Classes
Below, violin plots are used to compare the distributions of each feature to the target dataset. 
Beyond showing that there are differences in the distributions between each class (good!), an argument can be made towards performing some kind of outlier removal prior to training the model. Depending on the model used, outliers can have a relatively large impact on the model (sometimes referred to as ""high leverage"").",43c78e7d,0.2978723404255319
14063,44f6a002ecd033,da1cfd70,Now we have a dataset with the combination of rows for train and test and the loan_status column dropped.,70bbe106,0.2980769230769231
14066,30fdc4a6e3c1db,ffd3ba45,"What we see:
* Store WI_3 has the most skewed distribution of sales where 54% of sales come from food-3, ~7% sales in hobbies
* Store CA_2 has lowest sales in FOODS_3 when compared to other stores(42%) and the highest in FOODS_1 and HOUSEHOLD_2 when compared to other stores(12% and 8%) where the averages are quite low",6111ddee,0.2982456140350877
14067,5ce12be6e7b90e,91a1d7b0,"## Comparison operators
These operators are used to compare values. They always return boolean values: `True` or `False`.

| Symbol | Operator          | Use    |
|--------|-------------------|--------|
| ==     | Equals            | x == y |
| !=     | Not equals        | x != y |
| <      | Smaller than      | x < y  |
| >      | Larger than       | x > y  |
| <=     | Smaller or equals | x <= y |
| >=     | Larger or equals  | x >= y |",c0ab62dd,0.2982456140350877
14073,9e27af2600925c,802a3db8,"## 4 - Building the parts of our algorithm ## 

The main steps for building a Neural Network are:
1. Define the model structure (such as number of input features) 
2. Initialize the model's parameters
3. Loop:
    - Calculate current loss (forward propagation)
    - Calculate current gradient (backward propagation)
    - Update parameters (gradient descent)

You often build 1-3 separately and integrate them into one function we call `model()`.

### 4.1 - Helper functions

**Exercise**: Using your code from ""Python Basics"", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().",9b556435,0.2982456140350877
14074,c2a9f2fb3e1594,391ee263,"## 3.24 Da-Double Check Cleaned Data

Now that we've cleaned our data, let's do a discount da-double check!",53411c04,0.2982456140350877
14075,9f3710be6aea65,4181cfb2,"We can say that there is no significative difference after we fill the nan values. The distribution is almost the same. But have two peaks. Later we will deal with that.

Now we have to deal with nan values in embarked column on train dataframe and with nan ones in Fare column on test dataframe. ",ae9bda88,0.2982456140350877
14079,9d27afa9ca3f96,2793e7a8,experiment image enhancement,2d86a18d,0.2982456140350877
14081,ba655a261cc09e,ef65b2d8,"The graph shows that most of the survivors are children
Let's fill in the missing values of age, taking into account the class of the passenger, sex and category (child/adult)

По графику видно, что среди выживших большая часть детей,
заполним пропущенные значения возраста с учетом класса пассажира, пола и категории(ребенок/взрослый)",48cc549a,0.29850746268656714
14091,75adb7945ef9bd,518f2442,"The top 3 locations with highest % of disaster tweets are **Mumbai, Inida, and Nigeria**. As the location data is not clean, we see some interesting cases, such as **'London, UK' saw a higher-than-average % of disaster tweets, but 'London' is below average**. We try to clean up the location and see if there is any difference:",785c5095,0.2987012987012987
14093,c13f73168789c2,d30e87a0,"### 1.8 Accessing values from multiple rows and multiple columns<a id='10'></a>
Syntax : `df.loc[[row_label1, row_label2, ...], ['column_name1, column_name2, ...']]`",16175052,0.2987012987012987
14096,14defffcd250f3,0b4b4de5,# Categorical Features,3a683b94,0.2988505747126437
14097,d5f78aa381f58d,be95636b,"Most of the Heart Disease patients are found to have asymptomatic chest pain. These group of people might show atypical symptoms like indigestion, flu or a strained chest muscle.",d60f358f,0.2988505747126437
14101,2a123b4e8f9433,1e8ec620,Transform columns for Logistic Regression use,0a082218,0.29896907216494845
14103,c84925c8171900,0482f668,"<p>
    <span style='font-family:Georgia'>
        Let's check the rest of the rows with missing year
    </span>
</p>",e21ff7ec,0.29906542056074764
14108,3c2033cc99c12c,74ced391,#### The heap map of the data ,dfa22a54,0.29927007299270075
14109,c80939c7c626cf,189e26c6,"# Here there are different types of titles but I am going to use Mr ,Miss Mrs and other categories to Others",b9ac31e2,0.29927007299270075
14111,a78d363403fce2,3e7e799b,"# Setup

With the dicom reader in place, the normal flow of fitting from fastai should work. We choose a modest file size, and just do a few epochs of fitting.",0f824cb6,0.3
14118,86ce0702951761,7d7fdec6,"

---


DenMune detects noise and outlier automatically, no need to any further work from your side.
*   It plots pre-identified noise in black 
*   It plots post-identified noise in light grey 

You can set show_noise parameter to False to show clean data as identified by the algorithm",392dc913,0.3
14119,8d575f495686ab,3c7b9fd1,"As seen from the abpve plot, there is a sudden surge in the values of the opening price, closing price, high price, low price, volume and the adjusted closing price.",0fc16499,0.3
14123,5626e84c4e6bf8,b5255f5c,"Now, we will plot the map. First, we will manually define labels with their markers.
* 0 -> Light blue circle - T-shirt/top
* 1 -> Caramel square- Trouser
* 2 -> Blue pentagon - Pullover
* 3 -> Orange star - Dress
* 4 -> Tomato red triangle - Coat
* 5 -> Bright cyan tri_down - Sandal
* 6 -> Electric indigo hexagon - Shirt
* 7 -> Light orange x - Sneaker
* 8 -> Raspberry plus - Bag
* 9 -> Purple diamond - Ankle Boot",e2ecb669,0.3
14126,3dd4294f903768,8e2a8eb0,"In this column there is only one option, so we will not use this column also.",0d89d098,0.3
14129,09bac0c221388e,84cf9592,# Summarization Methods and Implementation,bea4aa2e,0.3
14130,5ffe6aa38958a1,3f117a46,Plotting again using the helper function. ,11f5412e,0.3
14135,3597174a998d4d,12332e74,#### 2.2.1.1 the number of customers,276892ed,0.3
14136,9c044fa3072552,43370aa3,"Interesting to note that these cities are also one of the most populated cities in the US. Although, it doesn't include the most populated city of all, New York.",1362842e,0.3
14139,49ac6594c8f5cf,6f7757b7,Commerce students get placed more and next comes Science followed by Arts,6f19f28a,0.3
14140,3ab4bbd4c4212b,0ee31aa9,"My GM, reusing Hoxosh likelihood, with some modification.
",4b25fe80,0.3
14141,91eaec994e0c6f,1f016756,We can easily notice that the item's Average sale is at its maximum at Father's Day.,376aef10,0.3
14143,fdbbd573ba31c2,8ffbedf6,### generator_temperature(°C),f7c28d74,0.3
14144,d07915a6e6992e,e1876504,"**Fare**

Let's check the distribution first.",2b912140,0.3
14151,d6cbd7160961dc,163e3ad1,## 4.2. Running the script on the data set,36d74664,0.3
14153,be616f0785c32d,56040a2f,Groups of drugs in clinical trials by working mechanisms,b78e18aa,0.3
14155,7dd46c750653eb,01d3ffb3,"**The crude birth rate (CBR) is equal to the number of live births (b) in a year divided by the total midyear population (p), with the ratio multiplied by 1,000 to arrive at the number of births per 1,000 people.**",c2644713,0.3
14156,312d2a3c7547f1,a7a1881c,Data Cleaning i.e. Removing the null vaues,8fd1efcd,0.3
14157,2ada0305b68956,4e2cfb16,### 49. Palette = 'RdPu',133e26f4,0.3
14159,09751c520b0616,3fb3dce8,Finding median of 'LotFrontage',a4d0c7e9,0.3
14161,892be0a523578c,c3325b60,"**5.2** By inspecting how long the participants sleep each day, I found that:
* Only 24 participants have sleep data, and few of them have full records (31 records), which indicates they are not used to wearing device during sleep
* The total minutes of sleeping in some records are abnormally small",b0e8d7c0,0.3
14169,061d6757dfbce0,4c6c04e4,> #### weather_test_df data,c0c2915a,0.3
14170,e78f177ca86768,b504d797,## Tuning Gamma,120e25c1,0.3
14171,8cd6656a65e6e7,40b89dc7,## Dimension reduction,c8e1697a,0.3
14173,cf4d1c1ad1476c,d0c87e97,Target feature is equally distributed,768c1a59,0.3
14174,83df814455f06c,3d24ebfc,### View summary of dataset,c9cff71a,0.3
14175,62487bcd70b199,2342aa2d,"## Inference:
its very evident that person with a family greater than 2 and income greater than 100 are very much likey to be taking a personal loan and they can be targeted for campaign",f6ae50af,0.3
14177,9f0ccf5b9e8f03,fe144440,"Although the Article suggests a possible emerging inflammatory syndrome associated with COVID-19, it is crucial to reiterate—for parents and health-care workers alike—that children remain minimally affected by SARS-CoV-2 infection overall. Understanding this inflammatory phenomenon in children might provide vital information about immune responses to SARS-CoV-2 and possible correlates of immune protection that might have relevance both for adults and children. In particular, if this is an antibody-mediated phenomenon, there might be implications for vaccine studies, and this might also explain why some children become very ill with COVID-19, while the majority are unaffected or asymptomatic.
https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31129-6/fulltext",66691203,0.3
14178,bfe6c7096b1ad0,9f9101ee,## Корреляция признаков,fffd95e0,0.3
14179,bc058fe14d3d1b,e8ac60e8,### 1.2 Checking outlier,d0273670,0.3
14183,ad121e0531afa4,c7922f84,"<center><img src=""https://i.imgur.com/gb6B4ig.png"" width=""400"" alt=""Weights & Biases""/></center><br>
<p style=""text-align:center"">WandB is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.
We will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br></p>

![img](https://i.imgur.com/BGgfZj3.png)",a3492905,0.3
14186,b547f0f38f7744,7008d02b,Distribution of number of bounding boxes:,b6ba66b3,0.3
14190,4bada947d597ac,6f665eb4,# Checking a sample image,eab5094a,0.3
14196,36c35f0a9f70f7,a8cc6fe6,"## Logistic Regression
",67358bc7,0.3
14197,2bace980aeb34c,6adf24a3,The next code cell uses code from the tutorial to preprocess the data and train a model.  Run this code without changes.,dc05ef6c,0.3
14208,ba4b3bd184acbb,bf3805d6,***,0f5de724,0.3007518796992481
14209,e19e307b3fd188,9467f36c,## Analysis of important features,2173955b,0.3008130081300813
14212,ac1abfe1dfe815,8aef4de1,**Convert the date from object to datetime**,6529dbcb,0.3008849557522124
14214,fc8e0042411c46,07ad2ecc,### Converted,af476c2a,0.30094043887147337
14215,98a6794067932a,422f93d1,"L'analyse ci-dessous permet de constater le type de livraison favorisé par les différents segments de clients qu'on retrouve au sein de l'entreprise. Le code permet donc d'additionner le nombre de commandes ayant été livrées selon chaque type de transport et ce, pour chaque segment de client. Cette analyse permettra aux dirigeants de concentrer leurs efforts au niveau de certains types de transport en fonction des clients majoritairement visés ou même de revoir la catégorisation des types de transport à l'avenir.",08600fe2,0.30097087378640774
14218,0caaec057f7184,5cb11b87,## Price,b875533e,0.3010752688172043
14219,835a7b4e660d23,97e7cef5,"### Histogram Plot
* İt is better when we need see distribution of numerical data.    
",53bc7a6e,0.30120481927710846
14220,4daf6153275cbf,d5185339,"I decided not to use **Slovakia, Finland and Luxembourg** for the analysis, because of their low volume and also there's no Finnish restaurants ",51db1961,0.30120481927710846
14224,3d08ca7656dec0,0b168a73,# caa,bd3f87e3,0.3013698630136986
14226,91473a39b85068,fb6e42bb,"### Observations:

* Maximum number of tags in a question: 5
* Minimum number of tags in a question: 1
* Average number of tags per question: 2.92
* Most of the questions have either 2 or 3 tags",6e3d91c2,0.3013698630136986
14227,7f74a04ae75792,5a4dda4d,"In the last 3 years, most same customers often repeat purchase twice",d01e91da,0.3014705882352941
14228,c65a65d4041018,acb6d1d6,"I know that many people has shown this graph, but still let's look at it again. I think that most of these titles can be joined into several groups:
- Students can be a separate group,
- Let's leave DS also by themselves;
- People in research;
- Next we have analysts - DA, BA and others, who need a different set of skills, but could be considered a level before DS;
- DE and SE who build production systems;
- Managers to lead the products;
- And others;

This grouping is arbitrate and could be wrong, but let's see what will be the result.",824fb229,0.3014705882352941
14230,06ecf7a304c309,b71fe493,검증 데이터에 예측을 생성합니다.,714de627,0.30158730158730157
14237,23df07a474aaae,dfa42310,"First we are going to drop some data columns such as Artist Name, Song Name and Release Date as they are not relevant for prediction.",0ea40276,0.3018867924528302
14243,510b8303776bb6,bfb383fe,## Categorical data fields:,18080db8,0.3018867924528302
14244,f3c8651cb08234,2b0d93ca,# Checking the correlation coefficient,37f86e36,0.3018867924528302
14246,726833f92fb87a,513f94e2,# Exploratory Data Analysis,7dc5e1b6,0.30201342281879195
14247,a566b5b7c374e7,04bffc5b,### Frequency,b3dc5545,0.302158273381295
14248,c09fac3c943d51,34c7b3b1,Suspicious simultaneous visitors with same visitorId and same visitStartTime:,678d076d,0.3023255813953488
14253,d96642860ab3dd,44ee7a66,### 1.8 Ticket column,98419d48,0.3023255813953488
14256,2e40928927c0d4,d9ae6a69,**Showing randomly chosen Moderate DR image one at a time** ,b6385ef2,0.3023255813953488
14257,1660daf8867980,38a995c8,"**Theory**  
The basic intuition is:
* We do not know the environment, so we sample an episode from beginning to end by running our current policy
* We try to estimate the action-values rather than the state values. This is because we are working model-free so just knowning state values won't help us select the best actions. 
* The value of a state-action value is defined as the future returns from the first visit of that state-action
* Based on this we can improve our policy and repeat the process untill the algorithm converges

![](http://incompleteideas.net/book/first/ebook/pseudotmp5.png)",42d7cffc,0.3023255813953488
14261,4ae6a182abac64,e84d7fa8,* **Fare**,418676c5,0.3025210084033613
14263,bd380b97b5c894,8c650f64,"No, there are no intersections",66f2562a,0.30275229357798167
14269,4b64dc653fb7eb,0e2b7749,"<a id=""1"">Step 1 - Remove Punctuation</a>",57675cc2,0.30303030303030304
14270,70193f0c034b98,ed681b62,# Cutmix,f8cacd26,0.30303030303030304
14272,b241b847319d13,bd43d35b,# **Helpers Functions**,0fb698f0,0.30303030303030304
14274,efd44ce2c08541,7406daeb,"# Similarity: Bert (Indonesian-BERT, Multilingual-BERT, and Paraphrase-XLM embeddings)",ebc2d00c,0.30303030303030304
14278,2f964d08c25d93,0593f163,### Let's check 1st file: /kaggle/input/data/review/Austin_Animal_Center_Intakes.csv,1f2e4468,0.30303030303030304
14280,979f1e99f1b309,d06484cb,***THE plots showing that most of players play in squad or duo while a few of them play a solo game***,d1bfebbf,0.30327868852459017
14286,df51d4c54fbb91,d6d14d73,We split the data into training and validation set.,4226dd72,0.30357142857142855
14287,3cd78d8d6d56e4,e07f8276,## Preparing Model Visualization with Tensorboard (not for Kaggle),9f632e94,0.30357142857142855
14294,5d2a3e82679cf3,3269c2dd,#  4) DETECT THE OUTLIERS,9e60b1e3,0.3037974683544304
14295,71b75664517244,7a34cefb,"## Total Win Rate

Accumulation of win rate on each team for all season in total",fc905af5,0.30392156862745096
14296,52cfd66e9ec908,682c5b6f,"The semantic view is good for a less clustered view but if we want a more detailed, more high-level overview of the data we should perhaps try to use the satellite view voer semantic.",c74adcdf,0.30392156862745096
14297,842547b2def18c,463b8cde,"## データを飼いならす

データセットと要求される問題解決について，いくつかの想定と決定を集計しました．

今までのところ1つの特徴量/変数を変えなければいけないことはなかったのです．
ここで，データの修正(correcting)・作成(creating)と目標達成に対する想定と決定を実行しましょう．

### データ修正 (特徴量を削除する)

少ないデータ点で処理する特徴量を排除することは，実行する最初の目標です．ノートブックを加速化させ，分析を軽減させます．

想定と決定に基づいて，我々はCabin (correcting #2) とTicket (correcting #1) を削除したいと思います．

※ train/test両方のデータセットを一貫して同時に処理することが可能か？に留意する．",b8efde6d,0.30392156862745096
14298,629f2918807a9b,556be18f,### Find a correlation between city and order status,be56dc84,0.30392156862745096
14309,b01ee6cb674fa3,eb5b966e,"From all the data analyzed, location_country does not means that the country's space agency did the launch.

For illustration:
- Arianespace, a private co. launched from Baikonur and French Guiana.

- Sea Launch is an international Co. with hq in Switzerland

- Northrop is an american aerospace company

---

location country is just where the launch site bellongs. 

Company name is a more representative information than location country. Further engineering can be done, to relate the company to a country or at least where its HQ is located",a8ffd35e,0.30434782608695654
14310,73ca9abcc2034e,29255c6b,# Normalize the text to be lowercase,cec3446c,0.30434782608695654
14311,33d736abb432d0,90b645af,## 去除非中文字符,d64052a2,0.30434782608695654
14315,2b434130adf886,2fc583bb,"downloading trained CNNs as feature extractor: inception, resnet50",0c4afeca,0.30434782608695654
14316,69d50f5e1373f1,121644ba,"In each task, there are two dictionary keys, `train` and `test`. You learn the pattern from the train input-output pairs, and then apply the pattern to the `test` input, to predict an output.",ec7545ee,0.30434782608695654
14318,8336d84cf3ff6b,3371c64a,"# Let do some Feature Engineering 

",b96b58a0,0.30434782608695654
14319,598b6228760590,96d17e28,"- Parch
-  of parents / children aboard the Titanic
-  The upper column represents the number of siblings and spouses, and now this column represents the number of parents and children. We can combine these two columns into one column to indicate the number of accompanying persons.
- We can also see that people without parents or children have the largest number and the highest survival rate.",be30ab66,0.30434782608695654
14320,9b5de3823ad5ab,da3a692e,### Check number of classes and data distribuition,33e48774,0.30434782608695654
14325,548f961125248d,fe0fbd69,* ROLE_TITLE and ROLE_CODE represent the same data. One of the two features can be dropped. ,d8c5e8b8,0.30434782608695654
14330,a1a31459abf078,bc25aac2,### Lectures,66fc0f54,0.30434782608695654
14331,fe118026267a88,d4846368,"## 2.

Create a dataframe `fruit_sales` that matches the diagram below:

![](https://i.imgur.com/CHPn7ZF.png)",612efa48,0.30434782608695654
14338,db7890856ec28b,7364d4b5,"# NLTK's Naive Bayes Classifier
### with custom text pre-processor using NLTK's word_tokenize and WordNetLemmatizer",106f6dca,0.30434782608695654
14341,0932046e1f485d,4f1cf019,"Now, let us see if there is a difference of ratings between different categories using a box plot.",218cc7a3,0.3046875
14345,7454fdc444df16,1cad1a7a,"## Healthy Tissue Patches Vs Cancerous Tissue Patches
Let us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. Usually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.",a7818ef5,0.3047619047619048
14347,fd4017c1514157,d9b7e7a4,"* Around in 41358 rows, secondary label is not present.
* Majority of recordings do not have an annotation of background species. 
* Yet, it is highly likely that most of them actually contain one or more additional species. ",fd8f0896,0.3048780487804878
14349,9c26c5dcd46a25,7dffbb83,"Afin de vérifier si la catégorie `pnns_groups_1` ou `pnns_groups_2` influence rééllement le Nutriscore, nous pouvons réaliser **une ANOVA (Analyse de la variance)**. Le choix de ce test est dû au fait que nous étudions 1 variable qualitative comparativement à une variable quantitative.

Les hypothèses posées seront donc les suivantes :
- ***H0*** : La distribution des échantillons est similaire *(et donc la catégorie n'a aucune influence sur le Nutriscore)*.
- ***H1*** : Une ou plusieurs distributions sont inégales.

Pour commencer, nous pouvons projeter les boxplots de la répartition des nutriscores par catégorie `pnns_groups_1` pour également vérifier les hypothèses de départ liées à l'ANOVA à savoir :
- Les observations dans chaque échantillon sont indépendantes et distribuées de manière identique (iid). 
- Les observations dans chaque échantillon ont la même variance.
- Les observations de chaque échantillon sont normalement distribuées. ",1bbbb677,0.3048780487804878
14350,56785caebaa256,bb455ef3,"### 3.2.1. The weakening of quarantine<a class=""anchor"" id=""3.2.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.3049645390070922
14351,b61ab8f81dc03d,b03ff740,"Replacing missing values for '**Embarked**' column with the most common Embarked, in this case ""S"".",64d05394,0.3049645390070922
14354,f2e5e9fb9eaaf7,14c3f29c,"<a id=""4.1.3""></a>
### 4.1.3 Individual rows
Count how many missing values in each rows on `train` and `test` dataset to see if there any similiarity between them.

**Observations:**
- The maximum of missing value in an observation is `14` and the lowest is `no missing value`.
- Interestingly, the missing value distribution (row basis) is quite the same between `train` and `test` dataset.
- Though there is around 2% of missing value in each features, there are around `38%` of the observations (row basis) that has no missing values.
- In reverse, there are `62%` observations that has missing value.
- `1 to 3` missing values in the a observations constitute around `41%` of total observations. ",048e0d08,0.3050847457627119
14357,bb0905d33ae417,63b6e308,"Submissions into the competition are [evaluated on the area under the ROC curve](https://www.kaggle.com/c/histopathologic-cancer-detection#evaluation) between the predicted probability and the observed target. Since we have a limited number of submissions per day, implementing a metric for the ROC AUC (which is non-standard in the fast.ai v1 library) allows us to run as many experiments we want.

At this point, I am not sure if changing the metric changes the loss function in the `Learner` to optimize the metric. I will be doing more reading up in that area. If anyone knows the answer to this, leave something in the comments below!",25fd1965,0.3050847457627119
14358,1294fb4c86f993,6d36bfa1,Replace commas and replace `Value Flags` with 0,4471e513,0.3050847457627119
14359,9169c4e9c33c90,ff2e4f52,"# Title

[Back to top](#Top)",725bf880,0.3050847457627119
14360,c4bca5d86a38c3,69eb6fe4,Luego se pasa a hacer OneHotEncoding de los titulos,e23d297c,0.3050847457627119
14361,2a56d6b0e153f2,3b7527f4,"AS DEMONSTRATED, A DECENT AMOUNT OF INDIVIUALS ARE SPECIALIZED IN MARKETING & FINANCE",8dc315e6,0.3050847457627119
14362,a81661cc35d8d2,278e9d42,"<font size=""3"">The above heatmap gives us a sense of what variables could possibly help us predicting the incidence of a cardiac arrest. We can see what these variables mean.</font>",3331f113,0.3050847457627119
14364,dac3c8204a2d1b,c116b2f2,"Here we group every year based on the user rating, reviews and price. ",b0d2d0dc,0.3050847457627119
14374,fdc3afd309b850,c3679a9f,"<a id=""nanv""></a>
### 6.1.4 New Address Null Values",966bde38,0.3055555555555556
14376,593d1d3d1df05a,1676f742,# Rotate Plate Images,bc682ffe,0.3055555555555556
14377,0f5085b162bd9f,45123d14,"# Assigning a label to each cluster
* As there's no relation between a cluster number and the true label we need to map a cluster to the one label which appears most in that cluster

* These corrected predicted labels are needed below to calculate model performance vs the the true labels",a3d989ee,0.3055555555555556
14378,2f0f808765fc67,2155c3de,# Find Correlation for NEW FEATURES,fd1f6494,0.3055555555555556
14384,10b5af05d804ff,ed773d54,Now our model can predict our test target easy!,4a9b1705,0.3055555555555556
14387,1d73d04c3aaae8,6da30626,"There have been 315 tie games since the start of the NFL. This has fallen off dramatically in the last 45 years, with the introduction of overtime in 1974. The overtime rules have been tweaked several times in the last 10 years. ",cd43d0aa,0.3055555555555556
14394,6dcfe6a610d86b,2ea76cb2,## Special chars,d05c59da,0.3055555555555556
14397,396bc36edb95d3,0d7d9e8d,### Building CART / Decision Tree Model,965e4f8f,0.3055555555555556
14399,2ada0305b68956,f60d36ac,### 50. Palette = 'RdPu_r',133e26f4,0.3057142857142857
14401,2f47abddfd1928,0c136a25,"They happen to have the same ticket number, therefore they should have embarked from the same place. Looking for the names in internet, they did from Southampton, therefore I can replace NaN with a 0.",ae33cc0b,0.30578512396694213
14403,e93a41c03638fe,15eb1ac7,# 2. Converting the text to a numerical vector format using tensorflow TextVectorizer:,7363527b,0.3058823529411765
14404,869a39a3d4dea2,3533b153,Now we draw shapes on the black board,9020daf8,0.3058823529411765
14409,ffd1df95ca5289,32821dc0,I will keep it as it is because if i change that to median and mean it will create a biasing like -1 as almost 90% data is missing ,db00c338,0.30612244897959184
14411,2343dc02ffb96a,e0c1b0ca,# Convert to dataframes.,29aa95a4,0.30612244897959184
14414,0e7ac281a19feb,067c97d0,## Create Detectron2 dataset dict ,5b84d10f,0.30612244897959184
14422,3c2033cc99c12c,05cbb20d,"*The Heap map could describe the density, distribution and changes of the data group on the page, we can use different colors to correspond to different data intervals, visually present the amount of data, and clearly describe the information of correlation.*",dfa22a54,0.30656934306569344
14423,c80939c7c626cf,7a26ec15,"Assign using these values
Mr : 0
Miss : 1
Mrs: 2
Others: 3 ",b9ac31e2,0.30656934306569344
14425,37e461081e47c5,ff63791b,"We can see that movie, music, PC games and cinema are highly correlated, new carriers (piece/spire) are correalted, accessories are generally well-correlated with other variables. On the negative correlation side, PC & Android games are negatively correlated, which makes sense. Android games also do not appear to be interested in movies, music or cinema. ",b3e6549e,0.30666666666666664
14426,2bd6c370695ea7,1b0e0e45,## Drop columns,cbe6aec8,0.30666666666666664
14427,67b7354e96113a,cb59890d,"**Imputing Fare**

There is a single value missing from fare in test data let's impute that value with mean",dca94250,0.30666666666666664
14430,cb570c7b7f0501,b5d616c6,"<a id='eda'></a>
## Exploratory Data Analysis
Now we have a cleaned data that we can work on.
Lets invistgate what is hidden behind non-shown patients.

what is the actual precent of No-show patient ?
",a200a0ec,0.30666666666666664
14431,91eaec994e0c6f,675b7070,## 2.3 Aggregated Level Analysis,376aef10,0.30666666666666664
14432,d6ddbe57f59cf7,20c038e7,# Line plot,504a3cda,0.30666666666666664
14433,73d8e56bc709b1,e1babaaa,"From this plot[1], we could find that players may reach the peak at the age of 28.  
So we made a definition that talented players are **'Age < 28,sort_values(by=['Potential','Overall'],desc)'**",78ec3cce,0.3068181818181818
14434,d1ff7e10ee0102,58d0317a,# 3. Keep calm and work smart,2cc71c3c,0.3068181818181818
14436,fe7360cddc13e5,eb144c8c,"T = **Gözlemlenen datanın son günü/ Müşterinin yaşı**

t = **Tahminlemeyi yapacağımız zaman diliminin uzunluğu**

tx = **Müşterinin son işleminin zamanı**

X(t) = **t zamanda gerçekleşen işlem sayısı**",8979e423,0.30701754385964913
14438,6cade0b6a41ba2,fd18e8f2,## 3.4. HyperTension,e6110293,0.30701754385964913
14439,ce9ed5e2d601d7,b309ca42,### Worst 10 predictors,f58a2f43,0.30708661417322836
14440,917957c6c4065f,358f4ecb,#### title & title_length,55b8ed68,0.30718954248366015
14447,f2f2db16a2f86c,a494dbf7,"It can be inferred that most of the places have **Population<5000**.

There are very few places with **Population>15000**.",ffc6a115,0.3076923076923077
14449,9eed0fae1c7958,96fdb0e0,# validation data,3fb1438e,0.3076923076923077
14453,0a1fcda859252c,80bbc421,"If you look carefully, then there are some cases where you won't be able to differentiate between a normal case and a pneumonia case with the naked eye. There is one case in the above plot, at least for me ,which is too much confusing. If we can build a robust classifier, it would be a great assist to the doctor too.",13a38774,0.3076923076923077
14454,e169603b62be56,e1f67b97,"for other columns, we will change by the most **frequent** if the type of data **object** and with the **mean** if the type of data does **not object (int float...)**",8c311ec1,0.3076923076923077
14456,eda49464dd6d1b,83649479,"## Damage Vehicle and Response
* Customers with damage are far more interested in vehicle insurance.  This will also help the model rule out about half of customers.  Don't have vehicle damage?  Then we won't bother asking you if you want vehicle insurance.",8421f81f,0.3076923076923077
14458,3cb96bd8eb364b,2496b92e,##### Correlation Heatmap,3157af7e,0.3076923076923077
14460,020c28a360b0cd,aec96ff4,### Rubric,2ba397f0,0.3076923076923077
14461,be9597c72542a2,48697ede,# Is there any missing data?,6f29c6d8,0.3076923076923077
14463,a915263bc207da,3f8b90d7,### Removing outliers & creating a sparse matrix using pivot table,b17ebcda,0.3076923076923077
14469,d0f6276d5b628c,31b38e87,The new dataframe has the same shape as the previous dataframe which indicates that every single movie has atleast one ratings by this far.,c64f5ce5,0.3076923076923077
14473,3e325daf577158,d338a695,## Data Preprocessing,c873dfec,0.3076923076923077
14478,4d91e84c564cbe,5f6f6adf,"## Changing lists

Lists are ""mutable"", meaning they can be modified ""in place"".

One way to modify a list is to assign to an index or slice expression.

For example, let's say we want to rename Mars:",355a43e3,0.3076923076923077
14479,cf08b03b002c13,d7603e36,NSFW content on reddit is only half a percent of all reddits posts.,104d416f,0.3076923076923077
14480,866b157128a09c,1a9ebb07,"## Create Siamese net

Save sample from: https://github.com/keras-team/keras/issues/8612

    #Save model
    model_json = model.to_json()
    with open(""model.json"", ""w"") as json_file:
        json_file.write(model_json)
    model.save_weights(""color_tensorflow_real_mode_2_imgs.h5"")

    ##### Load the model

    with open('model.json','r') as f:
        json = f.read()
    model = model_from_json(json)
    model.load_weights(""color_tensorflow_real_mode.h5"")",0909f459,0.3076923076923077
14489,d4c5aaa4b36810,1e583295,Lets find the the average year for the data points if we select the most recent non-null data point for each country. ,65441f28,0.3076923076923077
14497,d78988cb5a1b02,c44e62cf,**We are going to create models using some algorithms**,233f3a92,0.3076923076923077
14498,d8c3936820c920,0b7cac74,# Ручное добавление новых оценок,9d95f15d,0.3076923076923077
14503,c970849d1f6da2,60fad92b,"# Creating text training file
* Clean data - Remove special characters and stopwords
* Can consider keeping special characters - Will be useful for retrieving percentages, dollars etc 
",056e3955,0.3076923076923077
14509,aae204e78a48d1,66aeb194,"# Hypothesis 2: does the card type make a difference?
Next we want to understand whether the card type makes a difference to whether a customer is likely to attrite",53ab6133,0.3076923076923077
14510,5f4ae633cfd090,e5ff63bb,"Now, let's see the correlation between variables and target and then select the appropriate variable to visualize",a30a16e2,0.3076923076923077
14512,8e9d63e1f6319e,9c5e4b72,"## Naive Bot

The next bot is convenient to determine **y** boundaries.",4743b346,0.3076923076923077
14515,c115e287523aab,9f12256c,"# GCS Path for TPU
* TPU requires **GCS** path. Luckily Kaggle Provides that for us :)",feb1288b,0.3076923076923077
14516,a8c042af6b7245,34db7e6e,"* Only one missing variable: ps_car_11
* We could apply scaling to deal with the different ranges",2487ac62,0.3076923076923077
14520,8ac70416723897,1501f1bd,Checking the train data for missing values,d32fd8f6,0.3076923076923077
14521,0fc0cbf884acd6,63f6a58a,# XG Boost by Month,064949f1,0.3076923076923077
14526,738bfced935b69,d0a535b4,"As we see No relationship between price and tax, even if the price increase the tax is constant.",2d3c592d,0.3082191780821918
14527,ba4b3bd184acbb,5fe4d264,"# Exploring DataFrames

With a basic understanding of DataFrames, we can start to explore them more in depth.

### Overview

The first exploritory task when working with new data is to get an overall picture of it.

The `.dtypes` property shows us the datatype for each column of a DataFrame

The `.shape` property of a DataFrame returns the size as a tuple of (# of rows, # of columns)",0f5de724,0.3082706766917293
14528,37b09262279764,ec43b3d7,"***Embarked -> Port of Embarkation C = Cherbourg(France), Q = Queenstown(New Zealand), S = Southampton(England)***<br>
Most of the people who boarded from <b>Cherbourg</b> survived the most",37c4c417,0.30833333333333335
14529,62487bcd70b199,ee53d4c8,## <a id='4.5.'>4.5. Family and Mortgage in Personal Loan</a>,f6ae50af,0.30833333333333335
14539,5f32117bcd5255,b763c227,#### WORLD COORDINATE SYSTEM,85882abf,0.3087248322147651
14544,dc0b0e1cb46c6f,f74bf8fd,Now re-order the dataframe based on '**date**' so we can plot correctly later,47b17a7b,0.3088235294117647
14546,eb0ecd6bebeb15,78a40cd0,"Veri çerçevemizin hedef değişkeninin ""variety"" benzersiz kaç adet değer içerdiğini görüntüleyelim.",d7b93a60,0.3088235294117647
14550,e19e307b3fd188,b1954fe3,### rooms,2173955b,0.3089430894308943
14560,a76e0e8770b7a0,60c345d1,What about Year by Year evaluation?,02863d3b,0.30952380952380953
14564,916ccf243827f1,6528e0c7,## 4. Dataset,5147f4d2,0.30952380952380953
14565,0d59a3e0130db0,dc9e855f,"It's time for lemmatization. Stamming is too harsh with words, in cases of profanity and jargon, it can worsen the situation, so I use lemmatization.



I want to thank the author of this article: https://habr.com/ru/post/503420/, his life hack helped speed up the lemmatization, applying the method not to every cell, but to the whole text, separated by separators",285f04b2,0.30952380952380953
14567,2d40f383473fa4,736838e8,"Thinking about `NaN` values, it's good to check it on test set.",1da1eff0,0.30952380952380953
14568,87e94f864d74be,263a3e64,"""rating"" for missing values has been fixed.",294bfe9f,0.30952380952380953
14569,1084376bc4897c,4f9eda4a,"### 3.2 We look at each feature now, starting with continuous features. We analyze their significance using stacked histogram with kernel density estimation and joint plot. ",1b598487,0.30952380952380953
14572,31268b33de97b5,128f6d08,# Percentage of Popularity,1e6f7d14,0.30952380952380953
14573,27778055896e17,8a5c3755,# Logistic Regression,1dbe0165,0.30952380952380953
14574,2b97b399158701,6285b6e6,"## Model Setup and Training
A lot of code below are not used in this particular notebook. But code previously used in other notebooks were not deleted intentionally. It should give an indication on the different kinds of experiments one could run for this competition.",04bd0060,0.30952380952380953
14575,0c57e3132ae184,73b59592,"### II. Numeric Columns that are read in as strings

These are the columns that include strings such as ""%"" or ""$"". While such representation is convenient for viewing, it is not appropriate for quantitative analysis. We should convert them into numerical columns.",f6bac298,0.30952380952380953
14579,066c5ee1ef39e6,40779044,### Ruddit data,0f394e1b,0.30952380952380953
14580,b4ecd6e4277e3c,2851aab6,"## Normalization

Borrowed from:
* How to: Preprocessing when using embeddings
https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings
* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing",94d79d5f,0.30952380952380953
14584,631cd434fc3aa2,a5236d91,"* _MiscFeature_: NA means ""No misc feature"". Same as before.",2b74febb,0.30985915492957744
14588,9bcfa825c8b2e6,8e73c9d7,Veri seti küçük olduğundan aykırı değerlere herhangi bir işlem yapılmadı.,220f36e4,0.30985915492957744
14590,30fdc4a6e3c1db,051edb88,### 1.2 Price Data ,6111ddee,0.30994152046783624
14596,858da4bb312f67,1b22090b,"## Build Data Pipeline
Cycle GAN requires two inputs:
- **Images of Source Domain**
- **Images of Target Domain**

First, I choose ""CBSD"" as source and ""Healthy"" as target.  ",9cca4391,0.3103448275862069
14598,18a96bb5711ed9,0403e567,"With this map, we can check where all the reported incidents in our dataset happened. The majority of reported incidents in the dataset are from Mediterranean,US-Mexico Border,and North Africa regions. ",e79768db,0.3103448275862069
14599,656185a18260be,894f9188,"# Sequential Trainer for Data Recipes

I'm preparing and shuffling my datasets in the data recipe stage, so I don't want any shuffling in my training loop. For that reason, I'm subclassing the HF QA Trainer and replace the RandomSampler with SequentialSampler below. ",0318cab5,0.3103448275862069
14601,3a23b3984eba32,98c64adf,"# Convolutional Neural Network - CNN 

'''model = Sequential()
model.add(Conv2D(32,(3,3),input_shape = (224,224,3)))
model.add(Activation(""relu""))
model.add(MaxPooling2D())

model.add(Conv2D(64,(3,3)))  
model.add(Activation(""relu""))
model.add(MaxPooling2D())

model.add(Flatten())
model.add(Conv2D(128,(3,3)))
model.add(Activation(""relu""))
model.add(MaxPooling2D())
model.add(Dense(numberOfClass)) # output
model.add(Activation(""sigmoid""))

model.compile(loss = ""binary_crossentropy"",
              optimizer = ""adam"",
              metrics = [""accuracy""]) 
batch_size = 128 '''

model = Sequential()
model.add(Conv2D(16,kernel_size=(3, 3),activation='relu',input_shape=(224,224,3)))
model.add(MaxPooling2D())

model.add(Conv2D(32, kernel_size=(3, 3),activation='relu'))
model.add(MaxPooling2D())

model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

model.compile(loss=""sparse_categorical_crossentropy"",optimizer='adam',metrics=['accuracy']) 
",7fb376f1,0.3103448275862069
14603,d42518f6cb0995,c4c9697e,54% of users answered less than 50 questions. Let's divide all users into novices and active users,26913a9b,0.3103448275862069
14608,6903d3f38c6a66,87cbb1a9,"# 1. Alignments

Two or more graphs are much more visually and semantically better than just one.

The easiest way to do this is to place the rectangles of the same shape.

Usually you can start with the initial size with subplots.",6067ce5e,0.3103448275862069
14610,6a1d04e8153df3,cf273b04,"**We Observe-**
- From 30 - 83 in this age group only women's are suffering from breast cancer.
- Above 72 years only some has breast cancer or it decreasig too.
- Mostly women of age 52 are suffering from breast cancer.
- Age between 50 and 65 womens are suffering from breast cancer.",38572b05,0.3103448275862069
14614,a1ba5ffd30dbde,1ab7e2cb,- We have 48 healthy patients and 147 parkinson disease affected patients ,48e57546,0.3103448275862069
14615,a4f0a3e1316ff9,b6e88862,# Look at One Country - Australia,53bf0160,0.3103448275862069
14616,5fc2f23dfbeeb1,ea68be5d,"### Porter Stemmer & Stopwords

PorterStemmer is a process for removing the commoner morphological and inflexional endings from words in English.",f37b4110,0.3103448275862069
14618,1750367e54f407,d301b752,The data augmentation preprocessing layers below will be used when training the model but disabled in inference mode.,a8e655b2,0.3103448275862069
14621,fc8e0042411c46,7bb943e7,### Lead Origin,af476c2a,0.3103448275862069
14622,cd10f3afd970b3,9dd746be,Now you're ready to read in the data and use the plotting functions to visualize the data.,2db3c8e4,0.3103448275862069
14623,1cd8be6e679620,481a9cd7,## Test Data Overview,3ce15a43,0.3103448275862069
14625,f3c6048d1058e3,73960644,# Model based on Indirect Features,1d9056b0,0.3103448275862069
14630,b91c9eef23d284,b62986a6,#Text by https://www.russianforfree.com/text-russian-bear.php,8fb353fa,0.3103448275862069
14631,20e1ba19eb9b5e,b89711ff,We can see that two houses have large GrLivArea values (right side of the plot) while the prices are relatively low. We can define them as outliers and delete them. Outliers removal is note always safe.  We decided to delete these two as they are very huge and really bad.,4569bfc1,0.3103448275862069
14633,5ea840754577e3,c25d9d98,### Feature: Pclass,9cf9b73f,0.3103448275862069
14635,84127ade6fde87,3ecedfe7,We first split our text into a list of lines and pick an arbitrary line to focus on:,f55d05b6,0.3103448275862069
14639,9ceb7278784462,7fce0da8,* I will drop it in the data set because there was a low number of data,3768a567,0.31048387096774194
14643,63b44c85e32c1f,82ef554b,Here the first index of each element is considered and thus z has the highest ASCII value thus it is returned and minimum ASCII is a. But what if numbers are declared as strings?,fb9b9562,0.3108108108108108
14644,27d5291d6365ba,be915fed,# Account Balance (mean of all customers) over the dates,96b30229,0.3108108108108108
14646,e1fff2f67cbe32,2c4fcf37,## Distribution of Target Variable,c6dfde64,0.3108108108108108
14647,e4525eb0c96f28,2ee6b3bc,"### ESRB Sales data over time

Now looking at the data individually, we explore ESRB sales over time.

From looking at the plots below, we can now see that the best performing ESRB Rating games over time were M and E10. More importantly, we can also see the distribution of data points over the years. This helps us more accurately understand the data. For example, not shown in the violin/scatter plots above is the low number of data points in the EC and RP categories. From that we can also conclude that the regression line has less meaning in those contexts.

Other observations:
- E and T ratings are fairly consistent in sales over the years.
- M rating over the years has not only grown in quantity but also average sales.
- E10 rating has risen in average sales since 2004.",2093a1f1,0.3108108108108108
14650,396bc36edb95d3,bfc6a61e,#### Extracting the target column into separate vectors for training set and test set,965e4f8f,0.3111111111111111
14651,3597174a998d4d,68cc8d40,"I find many unreasonable records and delete them.
",276892ed,0.3111111111111111
14660,d905cde3391d2b,f1a1c78e,"## Median

**Median** is the middle value, when the data is sorted in ascending order. Half of the data points are smaller and half of data points are larger than the median.

For example purpose, let's take first 10 entries of the data.",067dba39,0.3111111111111111
14665,b0c2805cd5c087,44199466,Image eciaplatform.eu,0446f327,0.3111111111111111
14668,d58491f2896fc1,5736e7c3,# Sklearn Ne İşe Yarar? ,514bfdff,0.3111111111111111
14670,43e60eb1362f5c,a81fed4e,The departure time above is not very much informative so we are going to change it in the datetime format so that we get a better idea of the time.,87934234,0.3113207547169811
14671,510b8303776bb6,a35b6128,### Bar Plot of all categorical data fields,18080db8,0.3113207547169811
14672,2ada0305b68956,a23dac3d,### 51. Palette = 'RdYlBu',133e26f4,0.31142857142857144
14676,2105f2c5132866,583a02c5,"Correlating categorical features
Now we can correlate categorical features with our solution goal.

Observations.

Female passengers had much better survival rate than males. Confirms classifying

Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily 
direct correlation between Embarked and Survived.

Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing

Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating

Decisions.

Add Sex feature to model training.
Complete and add Embarked feature to model trainin",bfe8023d,0.3114754098360656
14679,7e89d387feb9f5,d8360a25,### Добавленный числовой признак №4. Отсутствующие данные в столбце 'Price Range',989e3a1b,0.3115942028985507
14681,0e2a23fbe41ca9,c46bdefc,Few ```merchant_id```s have more than one rows. Lets investigate some of them.,64e4762c,0.3115942028985507
14682,663bbc9eaf267b,3f1a02db,"* By comparing the first 5 cars in this particular model, we can see car No. 3638 is not even as good as other 4 cars in the specified features.
* This difference could be related to having very special customizations which are not reflected in our features in this dataset.
* Lastly, the ""123456"" in order as a car price seems a bit odd!!",32445529,0.3116883116883117
14686,241cf32abb22d8,0457d92b,"## Feature Selection & Ranking

The 1-nearest neighbor classifier is used as a wrapper to compare the performance of feature selection methods: F-Score and Random Forest Importance. I also use stratified 5-fold cross validation with 3 repetitions for assessment.",47157066,0.3116883116883117
14688,722cd844dfbe8f,25983308,"Note that some values for each scan category are over-represented. On the other hand, the span ranges of the counters are important. This may be due, for example, to the use of different X-ray machines ...

However, if we only consider patients with maximum values, the amount of data available may be too low for a complex machine learning algorithm.",0cedb385,0.3116883116883117
14691,bd380b97b5c894,2c052bb5,fill in missing gender values with median value,66f2562a,0.3119266055045872
14694,56785caebaa256,903f8fe7,#### Thanks to [Oxford COVID-19 government response tracker](https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker),a792961a,0.3120567375886525
14696,28a1ff0f223da9,8e520e5e,"**From this plot we can see ""software engineering"" and ""machine learning"" are the top areas of interest which are specifically mentioned**",c945b27d,0.3125
14698,eb0854a6601407,dea61eeb,"The target values look quite normal without any outliers or long tails. We should not have any problems working with it. 


Assuming an uniform investment (all investment have the same weight), the overall investment is in loss ",6d107747,0.3125
14700,51a46d0a7597f5,3f3b9f85,"Usually as the Market Value increase Wages increases, but we also have higher wages and close to zero Market Value",e9e25b17,0.3125
14701,254cccd5145725,f71d69cc,From the above plot we can conclude that the data is unbalanced in nature.,a49b4037,0.3125
14703,ff029d7b52ae1d,fcbf3bd5,"Next we import the autoviml library and send in the train, test and sample submission data frames",c987f868,0.3125
14707,2a377ced98d67a,b2f3b0c0,### 3.2. Boxplot for showing distribution and spread of all features,262231a8,0.3125
14711,49f2274c1dd516,859a62e6,Columns most commonly found,06b0ffee,0.3125
14712,3b5903412fe741,226551df,"On its own the `:` operator, which also comes from native Python, means ""everything"". When combined with other selectors, however, it can be used to indicate a range of values. For example, to select the `country` column from just the first, second, and third row, we would do:",ad231969,0.3125
14714,e82462cdc998a7,e17a9a9e,"### 4.2 Seed<a class=""anchor"" id=""4.2""></a>",b39bf244,0.3125
14715,5e02999ca74e7e,73dc656f,"> Great job!, the dataset not contain null value!",b69da28e,0.3125
14718,204a60bace6fdb,17b1f8cd,## LightGBM Without Outlier Elimination,5cb3de8f,0.3125
14722,64a336ac34d95c,5059752e,"## Tenure(in months) vs churn
",be73a990,0.3125
14724,96c4c0e36b8ec0,b142b296,"What the graph tells us:
* Most common age is around 25
* Both median and mean age are around 30",4dd6de8c,0.3125
14728,6f05f4ea9addbf,7b85467e,Let's check the missing data again,dfb04c84,0.3125
14731,0635781991a885,6bd6405d,"**The Model with CatBoost Got the score 0.77076 on Public LB. As it is very naive model, so it is expected.**

*Now we can test the new Framework lightGBM *",13ab3e33,0.3125
14735,fdbbd573ba31c2,1f78d9a9,### atmospheric_pressure(Pascal),f7c28d74,0.3125
14736,ffc9490c4f6c38,48ab2e63,"Some plots show different from other. It looks like the range or variance is different.

Plot these range and variance.",ae7bbbb3,0.3125
14739,9ec2fb131cf677,94a0ad1c,"#### **Top 5 TED TALKS Most Viewed Events**
  1. TED2013
  2. TED2014
  3. TED 2015
  4. TED 2012
  5. TED2011
  
#### **Top 5 TED GLOBAL Most Viewed Events**
1. TEDGlobal2013
2. TEDGlobal2012
3. TEDGlobal2011
4. TEDGlobal2010
5. TEDGlobal2009

#### **Top 3 TEDx Most Viewed Events**
1. TEDxMidAtlantic
2. TEDxPugetSount
3. TEDxHouston

#### **Top 5 TEDxIndia Most Viewed Events**
1. The thrilling potential of SixthSense technology | **Pranav Mistry**
2. The fight against sex slavery |** Sunitha Krishnan**
3. Weird, or just different? | **Derek Sivers**
4. How to make the world suck less: **Alexis Ohanian** at TEDxMidwest
5. The neurons that shaped civilization | **VS Ramachandran**

",211ea6bd,0.3125
14744,bd0e173abb7b52,728a11bd,**3. What is the percentage of German citizens (*native-country* feature)?**,9bce3b0d,0.3125
14745,9d561aa4a298f3,3b2dcd6a,Let's visualize few of the rows in the dataset.,f56bdd1c,0.3125
14747,e7ab5703594800,de92641d,"Hence, the highest score of the question is 89491 and the lowest is 0. 

## Hypothesis:
## 1. Can a good question depend on number of words asked?
## 2. A reddit ask can have multiple sub questions. An ask having multiple questions affect the score?",78546b1b,0.3125
14748,f5ca8fb6a465f3,ec591143,# YOLOv5 Stuff,56c45a1b,0.3125
14749,fae5023faa435f,330d6b82,# Visualizations,b37c893b,0.3125
14750,c5fef7cc592736,1d532b69,With all transforms defined we can create the lists of tranforms for inputs and labels and prepare the splits between training and validation set,d21dc2c1,0.3125
14757,4daf6153275cbf,29fcfa2b,### Making of the master DataFrame,51db1961,0.3132530120481928
14759,91eaec994e0c6f,daa1ecb5,- Now we'll go through more aggregated analysis.,376aef10,0.31333333333333335
14766,20b372b6e4e276,be1259e0,"## 5. Outlier analysis <a class=""anchor"" id=""5""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.31343283582089554
14769,9169c4e9c33c90,40a79417,"None of the books are included in all of the Top 50 charts across the years.

The closest is the *APA Publication Manual*, which appears in 10 of the 11 years.",725bf880,0.3135593220338983
14771,64169805aacf17,34e3c813,## Checking the CLIP models available and picking one ,1f12ded0,0.3137254901960784
14777,523123dad03177,ab6a1542,# 3. Parent's Education,48a5e4e6,0.3137254901960784
14781,52cfd66e9ec908,02d44a4d,"Now, how about from the agent perspective? This would be quite interesting to consider, as we're modeling from principally the agent perspective in most public notebooks so far.",c74adcdf,0.3137254901960784
14783,d0080e3a39bc5c,402c186f,![Original Image](https://i.imgur.com/08A6bJx.jpg),2fcde4cf,0.3137254901960784
14792,b3e48999ed0d00,b7515ca6,## Trellis plots,fe9ada0f,0.3142857142857143
14793,2730840089c8eb,de0b5588,But a major way in which they differ from lists is that they are *immutable*. We can't modify them.,34d27dac,0.3142857142857143
14796,ca73f3d2e25b47,461ba502,Display the features columns,4cd11efe,0.3142857142857143
14801,55a5e31d03df9f,d1d748d5,"Wow! Now finally see some of the activation functions and how they transform a input tensor. So the first thing we noticed is that **activation functions are fundamental for the non-linearity power from neural networks** without activation functions we would be working on the linear space.

Now let's review these graphs:
* **Sigmoid**: Transform our data between the range of 0 and 1, all negatives get almost 0 as value and all positives get almost 1 as value. This is the why when we have a binary classifier instead of softmax we used a sigmoid, is either a thing or the other.
* **ReLu**: For the relu all the negatives get 0 as value else it returns there current value.
* **Tah-H**: Is similar to Sigmoid however values are between -1 and 1 range. 
* **Softmax**: Softmax is the hardest to see in graph, but it normalize the output of a network to a probability distribution over predicted output classes. In other words it transform whatever the input to the range of probabilities where the summ of all the tensor should be 1 (or very very close due to how computers calculate). 

",06dce00f,0.3142857142857143
14802,f50dc95483c98f,02cfe3c5,"## **Splitting the dataset into the Training set and Test set**

The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model. It is a fast and easy procedure to perform, the results of which allow you to compare the performance of machine learning algorithms for your predictive modeling problem. Although simple to use and interpret, there are times when the procedure should not be used, such as when you have a small dataset and situations where additional configuration is required, such as when it is used for classification and the dataset is not balanced.

For more study on this method, you can go on the following [`Blog of Machine learning Mastery`](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)",cd9e9621,0.3142857142857143
14803,6b65d81a5743dd,a66bd52a,"We have 11 missing values in 3PointPercent,

After checking the dataframe entries where there is missing data it appeared it's missing because it's a zero divide so we replace it with zero instead",4080a2d2,0.3142857142857143
14804,2b36742b49c7bc,1de5f650,3. Цөөхөн засаж чадахааргүй edge case-үүдийг энд дүрмээр өөрчлөв.,c8f8a96d,0.3142857142857143
14807,04bac111ffbe9c,b05d6f21,"##### Replacing the two missing values in 'Embarked' with the most common value under this feature(handling missing categorical data)
Let's find out the most popular 'Embarked' type",82576b17,0.3142857142857143
14809,38b79494ac749e,b9e5c112,### From underfitting to overfitting,39162a40,0.3142857142857143
14811,3a6274ed72cc00,fc5455c6,## <a id='4.'> 4. Data Analysis</a>,51369a2a,0.3142857142857143
14815,7a058705183598,f75fd651,Checking top 5 values of new dataframe,b0ead917,0.3142857142857143
14818,72e098fe5b2a04,c027eac6,# Inference Roberta base 465,5399eebd,0.3142857142857143
14820,fe6750354fb64f,d9d75b60,## Scatter Plot,271741f0,0.3142857142857143
14825,04ff2af52f147b,5e82995c,"**Create Deck Feature:**

*Cabin* is the feature with the most null values, a total of 879/1130 between the train and test sets.  This is too large of a proportion to impute, and the data would likely not be very valuable even if we did.  Instead, we will create a new feature, *Deck*, using the first letter of the *Cabin* values which indicates the deck where the cabin is.  We will denote all of the null values for *Cabin* with 'N' for *Deck*.",d5f37be9,0.3146067415730337
14826,e67925694c07d3,9dcef33d,"every ""DAYS"" feature are negative because they are recorded relative to the current loan application

see if there are any anomality in DAYS, as for others it might be hard to detect just by looking at its description above",83af4c4a,0.3146067415730337
14827,312135b445bd23,4cef31c1,"## Training tri-gram model
We created a tri-gram model, in addition to the bi-gram model, in order to catch more meaningful phrases, like `Epidemic Preparedness Innovations` for example. We transformed the cleaned sentences (from all articles) with bi-grams using the model above and trained a Phrases model with Gensim again, but with a lower threshold this time. Please note that this method can also crerate 4-grams if the model connects between two bi-grams.",8ced381f,0.3146067415730337
14833,ab6da5994949a3,5d8d439c,## Predicting the Test Set Results,fae6b91d,0.3148148148148148
14836,9ad9a97e628bfa,5dff85eb,"다음으로, '/'와 '.' 각각, 또 둘 모두를 기준으로 코드를 나누었을 때 개별적으로 존재하는 코드가 무엇이 있는지 살펴보겠다. (Ex: SC/A.3  -> SC와 A.3, SC와 A 와 3, 등)
**이 부분을 더 효율적이고 정확하게 처리할 수 있는 방법에 대한 아이디어가 있다면 공유바랍니다**",0a7e1136,0.3148148148148148
14845,91473a39b85068,684b6e7e,#### Total number of unique tags,6e3d91c2,0.3150684931506849
14846,b01ee6cb674fa3,86c914b3,"## Datum
dia_da_semana  mês dia, ano hora:minuto UTC
 - ano
 - mês
 - dia_da_semana
 - hora:minuto",a8ffd35e,0.31521739130434784
14848,09751c520b0616,811ba8f7,Finding median of diffrance in 'YrSold' and 'YearBuilt',a4d0c7e9,0.3153846153846154
14850,d07915a6e6992e,846d8343,The Fare variable is right skewed. We need to transform this variable using log function and make it more normally distributed. We will do this during feature engineering process.,2b912140,0.3153846153846154
14852,726833f92fb87a,9d407e2f,"To allow a better visualization, different palette will be chosen for the following plots.",7dc5e1b6,0.31543624161073824
14854,26b93b6f4dc148,b47fde6e,### Converting Categorical Features into Numerical By One Hot Encoding,6f667d22,0.3157894736842105
14860,840534f2908a9c,7bd18e64,**Distribution of Trip Fare**,8081c3cc,0.3157894736842105
14863,caa0ce2715bf34,ed2b6841,"### Observations
1. The Outliers are present in all the fields except : PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY.

## Removing Outliers",78a5dc51,0.3157894736842105
14864,29437539745aa5,b014d48c,"<h3 style=""text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;"">1.2 VISUAL HELPER</h3>

---",c17b490a,0.3157894736842105
14868,bef2347846e476,0af7d458,"The term ""**correlation**"" refers to a mutual relationship or association between quantities. In almost any business, it is useful to express one quantity in terms of its relationship with others. For example, sales might increase when the marketing department spends more on TV advertisements, or a customer's average purchase amount on an e-commerce website might depend on a number of factors related to that customer.In the first dataset we just have one numerical data colum so let's investigate it in the second data set.",cb93bf51,0.3157894736842105
14870,fe7360cddc13e5,61dc4683,"Yukarıdaki görselde bir müşterinin (0,T] zaman aralığında x işlemi olduğunu görebiliyoruz.",8979e423,0.3157894736842105
14882,52ee792e228d54,f6de3c48,"### That's Interesting!! People with lower education are earning more income. And Income does'nt vary based on one's Experience. Also, Personal loan neither strongly depend on Education nor Experience.  
### Personal loan is dependent on the income though. It is evident from the higher band boxplots for Personal Loan, people with higher income tend to opt for Personal Loan (supports our earlier stament).",5096094e,0.3157894736842105
14883,f35ee6e9fab592,cc9d66e2,"The next 3 cells perform the same as the the last 3; however, the column in question here, is ```genres```",b15f7073,0.3157894736842105
14886,31b564f11ef638,6bdf53a1,### Change categorical data type for memory reduction,424f9692,0.3157894736842105
14887,0d8df2c2983694,a089145b,Load the ‘Example_bank_data.csv’ dataset.,9bf7fa4e,0.3157894736842105
14891,a1dcd92986bc84,87ddfcee,"## Implement the projection head

The projection head is used to transform the image and the text embeddings to
the same embedding space with the same dimensionality.",730acaaa,0.3157894736842105
14893,f91f58d488d4af,539d7a58,"Measuring distance between our sample image and ideal images. And we'll be doing this using *mean squared error*
and *mean absolute error*
",5df1bbf3,0.3157894736842105
14896,e03eb63c1f725d,317a3b44,"<a id=""3""></a>
<font size=""+2"" color=""blue""><b>Ngrams of True/Fake news titles</b></font><br>",e204b7e3,0.3157894736842105
14898,169177b6e9edea,6c305ae6,<h3><b>Cabin,ca42152f,0.3157894736842105
14900,99afe9f3af6dbc,2134b19d,dist didefinisikan sebagai 1 - persamaan cosinus dari setiap dokumen. Kesamaan cosine diukur terhadap matriks tf-idf dan dapat digunakan untuk menghasilkan ukuran kesamaan antara setiap dokumen dan dokumen lainnya dalam korpus (setiap description di antara description). Mengurangkannya dari 1 memberikan jarak kosinus yang akan saya gunakan untuk memplot pada bidang euclidean (2-dimensi).,cdec9b3a,0.3157894736842105
14902,7f74a04ae75792,d4fd30bd,"However there are outliers, same customers that repeat purchase up until 14",d01e91da,0.3161764705882353
14903,c65a65d4041018,63c0b301,"Now we see that the number of DE and DS is almost equal and the number of DA isn't far behind.
It is worth noticing that different companies could have very different titles. For example in Facebook DS could work as DA; in some companies situation could be opposite.",824fb229,0.3161764705882353
14905,b86bda7afe3ac3,e7cded16,"Simple model 0.6745383286834064 

* validation score with second conv block and 3 epoch:
0.6651720472963997
",16197934,0.3162393162393162
14911,fc8e0042411c46,3366822b,"- API and Landing Page Submission have 30-35% conversion rate but count of lead originated from them are considerable.
- Lead Add Form has more than 90% conversion rate but count of lead are not very high.
- Lead Import are very less in count.",af476c2a,0.3166144200626959
14914,712198370d5521,a052b3a5,"**In the next bit, I will be performing the following steps to engineer some new features:**

* Extract the **""Age""** of a customer by the **""Year_Birth""** indicating the birth year of the respective person.
* Create another feature **""Spent""** indicating the total amount spent by the customer in various categories over the span of two years.
* Create another feature **""Living_With""** out of **""Marital_Status""** to extract the living situation of couples.
* Create a feature **""Children""** to indicate total children in a household that is, kids and teenagers.
* To get further clarity of household, Creating feature indicating **""Family_Size""**
* Create a feature **""Is_Parent""** to indicate parenthood status
* Lastly, I will create three categories in the **""Education""** by simplifying its value counts.
* Dropping some of the redundant features",5882e04c,0.31666666666666665
14919,63d0d9b9a8c7d2,e0a9a109,**Inserting these columns in the training dataset**,e32e5933,0.31666666666666665
14922,c18267b203f28a,10064639,"## Define data loading methods
The following functions will be used to load our `training`, `validation`, and `test` datasets, as well as print out the number of images in each dataset.",09ca8efb,0.31666666666666665
14925,5b92c712910a11,8ae3a566,# Removal of punctuation,e1d17100,0.31666666666666665
14926,786475feda0190,4aa0c870,## Getting into DATA.,e4663d97,0.3170731707317073
14931,dbccf99c49570f,b19408b2,## Dealing with NaN values in Age Column,c20fc09e,0.3170731707317073
14934,8d0aebab1e5914,7d3006da,# 2D Visualization using PCA,084e671f,0.3170731707317073
14939,fd4017c1514157,6bd31443,### <font color = 'red'>Author</font>,fd8f0896,0.3170731707317073
14942,2ada0305b68956,3c705c81,### 52. Palette = 'RdYlBu_r',133e26f4,0.3171428571428571
14945,44f6a002ecd033,39476037,"Because we have such a low percentage of missing values in each row it can be seen that none of these rows will need to be dropped, just cleaned up.",70bbe106,0.3173076923076923
14948,4b4117cf42ef8d,ab0e1236,# Split the Data to Train & Test,457cd6f4,0.31746031746031744
14949,8985a124d4b657,36d00115,Let's repeat the same procedure for test_norm.,586d1846,0.31746031746031744
14955,c84925c8171900,eb02cc30,"<p>
    <span style='font-family:Georgia'>
        We can impute the years for these games from other rows where the data may exists
    </span>
</p>",e21ff7ec,0.3177570093457944
14968,450fda47b03baa,360bd73c,varyans değeri=288832079761,62c04adb,0.3181818181818182
14981,d1ff7e10ee0102,327b63ff,"Until now we just followed our intuition and analysed the variables we thought were important. In spite of our efforts to give an objective character to our analysis, we must say that our starting point was subjective. 

As an engineer, I don't feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. There's a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.

So, let's overcome inertia and do a more objective analysis.",2cc71c3c,0.3181818181818182
14983,be2f4d8a6b73ca,debf85fe,"<div class=""alert alert-block alert-info"" style=""text-align:center""> 📌<b>Insights: </b>Here we have different types of data, such as categorical, numerical (discrette and continuous)<br>categorical features: Sex, Chest Pain Type, Resting ECG, Exercise Angina, ST_Slope<br>Numerical features:
    Age, RestingBP, cholesterol, FastingBS, MaxHR, Oldpeak, Heart Disease(Target)<br>Discrette Features: Sex, ChestPainType, FastingBS, RestingECG, ExerciseAngina, ST_Slope<br> Continuous features: Age, RestingBP, Cholesterol, MaxHR, Oldpeak
</div>",5d8ce40a,0.3181818181818182
14987,e323e594ef918f,4a793f88,"# Pytorch Hook: get access to layer activations!

To implement CAM, we need to get access to the activations of the last convolutional layer - we will use Pytorch Hook for this. We use Hook as context manager to avoid leaking memory - for that reason we define `__enter__` and `__exit__` methods. ",6e829ab6,0.3181818181818182
14988,5083d7a61f2426,64dd420c,"For data visualizations purpose, we gonna keep the dates into separate variables",541a0fec,0.3181818181818182
14990,f166950fa915f8,5845ec1e,### Word2Vec ,a7f6ca5e,0.3181818181818182
14992,930cd79ca51204,592e688d,"**Can we draw any conclutions?**

The higher GDP usually corresponds to a higher life expectancy. In other words, there is a positive correlation.",5506779a,0.3181818181818182
15003,9d9da6c439b96b,5c9c434f,## Data Distribution by Boxplot,361cc7d9,0.3188405797101449
15004,17a24d566ffa59,05f96596,"### Singular Value Decomposition

In linear algebra, the singular-value decomposition (SVD) is a factorization of a real or complex matrix.

Let A be an m × n term-document frequency matrix with rank r, r ≤ n. Without loss of generality let m ≥ n hold so that there are more terms than documents. The singular value decomposition of A can be stated succinctly as A = U*Σ*VT,

- U is an m × r orthogonal matrix whose columns make up the left singular vectors
- Σ is an r × r dimensional diagonal matrix whose diagonal elements are termed singular values
- V is an r × n orthogonal matrix whose columns form the right singular vectors of A.",89049e56,0.3188405797101449
15006,548f961125248d,660229b9,"## Preprocessing <a class=""anchor"" id=""third""></a>",d8c5e8b8,0.3188405797101449
15011,598b6228760590,2cbd882a,# Dealing with missing values,be30ab66,0.3188405797101449
15017,04e6b0d3c70f46,c91078cd,### Spectrogram Features,56344f77,0.3191489361702128
15019,5f674175839b32,1a53fdd7,<font color=pink>From the above table we get the idea of sales over different platforms. PS2 platform has the maximum sales all over the globe.</font>,53a2e343,0.3191489361702128
15025,4ae6a182abac64,58e6ea93,* **Sex feature vs Survived feature**,418676c5,0.31932773109243695
15029,69ac33d79f5130,0a30ec95,### Cities,9d760d2a,0.3194444444444444
15030,166a62ebb4fc3a,fbf60d10,"So here we have successfully converted data type of ""diagnosis"" column into numeric.

For our instance now, 
Malignant = 'M' = 1 & Benign = 'B' = 0",db48a079,0.3194444444444444
15038,979f1e99f1b309,46af4cf8,***THE plot showing that most of rankpoints are 0 and there is too many between 1500 and 1700***,d1bfebbf,0.319672131147541
15039,fc8e0042411c46,4ff2556b,"**To improve overall lead conversion rate, we need to focus more on improving lead converion of API and Landing Page Submission origin and generate more leads from Lead Add Form.**",af476c2a,0.31974921630094044
15044,4cd25e50c7e007,a6666f11,**During weekends the bike rental count is high and seems people go out during weekends and use the bikes.**,ceb0c525,0.32
15051,81712ee7510ac5,64653e2b,"**LIST: The sequence of elemens in a set of squre brackets separeted by commas**
",c4685e79,0.32
15054,8854f72e7e9be0,da4d78c3,**Define Hyperparameters**,2a1031b7,0.32
15056,916a9275d9326e,cb58531b,"# データの可視化

### まずは基礎的な可視化から",cbe4b24b,0.32
15057,7e1da639035ac5,c798331b,### <a id='7.2'>7.2 Bubble chart depicting estimated income and economic need index on map of school locations</a>,120b6c23,0.32
15059,519e936017c30a,c874e97b,"Ejecutado nuestro código, nos encontramos ante una extraña situación donde no se puede percibir con claridad dicho estudio, debido a que se tiene en cuenta con una amplia cantidad de datos. Para solventar este problema, vamos a examinar la evolución mediante la suma de todas las ventas de videojuegos obtenidas por cada año.",dc34915d,0.32
15060,19aae4a6ede288,dbb7bf44,## Creating and Evaluating the Wide and Deep NN,56934674,0.32
15061,0687cd5c8597db,b617cf28,### **Converting the Fetures into a format that is accepted by the Nerual network for training**,4edec76a,0.32
15062,10c5a39a87c47e,df311dcb,## Step 5: Train- Test Split<a id='step-5'></a>,09c7337a,0.32
15063,21c1e34efd71b8,3ba488dd,"I just averaged out the scores from the 5 runs above. Interesting numbers are Accuracy, who would not want a model that is 99.9 % accurate. But is the model really good?
Look the the Recall Score its too low, which means the fraud records that were ideally supposed to be predicted as Fraud are not done in a accurate way. 
Sooo.. K stratified Cross Validation does not work for me.",23b2cdd6,0.32
15065,50d4ddf1953997,ce6f79c6,"Using the same approach, we can separate each genre and look at how many entries each genre has every year between movies and TV shows. We are only considering the first 10 most popular genres.",90bdddd6,0.32
15066,9395559895004f,2c9cc22a,## Data Generators,b5a0494b,0.32
15067,91eaec994e0c6f,00ecd64b," - The Data hierarchy is:<br>
 <b>State</b> ==>  <b>Store</b> ==>  <b>Category</b> ==>  <b>Department</b> ==>  <b>Item</b>",376aef10,0.32
15070,e5dd725b8fa422,af0cffe5,"only one year data given that is 2019

now let's see how our data is distributed based on time hours",14675d8b,0.32
15071,83df814455f06c,e14d399a,"### Frequency distribution of values in variables

Now, I will check the frequency counts of categorical variables.",c9cff71a,0.32
15076,917957c6c4065f,ebc0c771,title의 길이로 title_lenght 열을 생성했습니다.,55b8ed68,0.3202614379084967
15077,0932046e1f485d,14d427f8,"Generally, the mean of the ratings is close. There are many extream values on the lower end. It's a bit hard to determine what the best category is, so we are going to calculate the mean using groupby and print out the best and the worst below.",218cc7a3,0.3203125
15078,98a6794067932a,286d3c8f,"La suite des trois cellules de code suivantes a été créée de la même manière pour chacune des cellules, cependant elles permettent de représenter des informations différentes. De manière générale, ces trois graphiques permettent de représenter sous forme de pourcentage la proportion des expéditions qui sont effectuées par chacun des types de transport pour un segment de client précis. Le premier graphique présente l'information pour le segment ""Consumer"", le deuxième graphique pour le segment ""Corporate"" et finalement le troisième graphique représente le segment ""Home office"". Dans un premier temps, le code permet de sélectionner seulement le segment de client désiré. Ensuite, dans un deuxième temps, une colonne représentant le pourcentage des expéditions selon le type de transport par rapport au total des expéditions de ce segment de client est ajoutée. Dans un troisième temps, le code permet de trier les pourcentages sous forme décroissante. Finalement, la dernière étape consiste à la création du graphique afin de représenter visuellement les résultats. Ces graphiques permettront donc aux dirigeants d'observer le comportement des différents segments de clients face aux types de livraisons qu'ils favorisent et permettront donc d'orienter leurs décisions pour le futur de l'entreprise encore une fois face aux segments de clients favorisés.",08600fe2,0.32038834951456313
15082,897ca904b74a98,df50ef91,#### Relation target / Age,c5844ad4,0.32051282051282054
15090,a070fd03ae8ed2,76832d3f,## 4.3 Расчет бинарного вектора прогноза (targets_pred) из вероятностного прогноза (pred_proba),c0ec4138,0.32075471698113206
15096,f015d0147e8fbf,a43b47bf,### 4. Previous Application Data Table `previous_application.csv`,518954fb,0.32075471698113206
15097,20b372b6e4e276,01b3fc92,"## 5.1. Training prediction result visualization <a class=""anchor"" id=""5.1""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.3208955223880597
15098,4883314a96dc34,59696950,## Understand data with visualization,50d36836,0.32098765432098764
15102,3c2033cc99c12c,48ce8160,"*From the heap map, we could find that the color of the normal part of the dataset seems much smooth while the fraud dataset seems much chaotic, especially between the column of V16-V18, as such we can refer that the column between V10-V12 may have a greater importance in the detection of credit fraud, which in means in PCA, those columns may contribute to a larger variance.*",dfa22a54,0.32116788321167883
15104,7686f42e1f28d2,592214eb,"# EDA and FE
Inspired by Andrew Lukyanenko, I want to experiment with OOP in machine learning to improve reusability, so we'll write a class for adding/processing features. The class `FeatureTransformer` will to the following:
* remove unecessary columns (`u`, `i`, `user_id`, `recipe_id`, `contributor_id`).
* `year`, `month`, `day_of_week`: three features extracted from `date`.
* `upload_year`: the year when the recipe is uploaded
* `calories`: I assumed that all the numbers in the `nutrition` column represent heat contained in the food, so I created this column as the sum of all the numbers.
* four `CountVectorizer`s for the columns `[""review"", ""description"", ""steps"", ""ingredients""]`. To learn more about `CountVectorizer`, check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).
* There are many other things that one can do here. Play around and see what changes new features can make if you'd like to.
<br><br>\*Note: In `PP_recipes.csv` there are data that can be used to calculate the averge rating of a recipe, but I didn't use it since it will become the dominating feature and ruin the fun.",6c128859,0.32142857142857145
15105,87e94f864d74be,6c4cf463,"### Drop ""director"" and ""cast"" columns",294bfe9f,0.32142857142857145
15108,f13534449a3750,93b31344,"<a id=""subsection-one-""></a>

## Statistics about data",8b7f3332,0.32142857142857145
15110,1c381451c17150,f0be9d93,"# Create Sequences
In a nutshell, this model will look at the last 75 characters in the script and attempt to predict the 76th. Our X variable will be a 75 character sequence and our Y variable will be the 76th character. This block chops the text data into such sequences of characters. 

Note that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is important to save a copy of the charindex with your model just in case. We will need it to decode our predictions later.
",e79b530f,0.32142857142857145
15111,62ae2b200f6b36,ce360dcf,Data_Visualisation for number of casual bikes.,7da4ea31,0.32142857142857145
15116,ff653816a337fb,8ca76d54,both start and end dates are same so its of no use to us,e5373696,0.32142857142857145
15117,8dd655515e7d18,d820f793,"**Analysis: ** 
The Openness is uniformly distributed between min-30 & max-72",895f41cf,0.32142857142857145
15120,585c280865b46e,dae19229,# Histone genes info,4d6056f1,0.32142857142857145
15122,9c33d1955302bf,4c829b3b,# **Pairplot**,0d9cfc89,0.32142857142857145
15127,99f84fa59cb1da,bae37b59,"So, we are dealing with a binary classification here",41e95f63,0.32142857142857145
15128,565ad413cd802f,32a41269,Here's a sample image without the colors inverted.,397b074e,0.32142857142857145
15134,2e4b36a0bd7613,c0ab8e95,#Text by http://mylanguages.org/slovak_reading.php,cd4d333e,0.32142857142857145
15136,30fdc4a6e3c1db,04636565,Let's first look at the distribution of prices across Categories,6111ddee,0.3216374269005848
15143,738bfced935b69,d5d5ef8d,"There is a positive relationship between tax and engine size, the larger the engine size the tax increases for cars.",2d3c592d,0.3219178082191781
15148,ed8009f482b380,e274efbe,## Turn Categorical Columns into Numerical Values,e99941fa,0.3220338983050847
15150,1294fb4c86f993,7d912e89,<b> Percent values in Census is not consistant (some is written in `'%'` and some in decimal) so we need to remove `%` and divide by 100 for those values,4471e513,0.3220338983050847
15153,a81661cc35d8d2,763fdf91,"<font size=""3"">Observations:</font>

1. Age of patient - The risk of developing heart disease increases with age, as pointed out in the following article - https://www.nia.nih.gov/health/heart-health-and-aging#:~:text=Adults%20age%2065%20and%20older,risk%20of%20developing%20cardiovascular%20disease


2. Percentage of blood leaving the heart at each contraction - Heart failure with reduced ejection fraction happens when the muscle of the left ventricle is not pumping as well as normal (ejection fraction is 40% or less), and hence the negative correlation makes sense - https://www.uofmhealth.org/health-library/tx4090abc#:~:text=A%20normal%20ejection%20fraction%20is,fraction%20is%2040%25%20or%20less


3. Level of serum creatinine in the blood (mg/dL) - High serum creatinine levels are associated with heart failure - https://www.onlinejcf.com/article/S1071-9164(02)25410-X/fulltext#:~:text=A%20prior%20study%20from%20this,of%20stay%20and%20mortality%20risk


4. Level of serum sodium in the blood (mEq/L) - Hyponatremia (serum sodium concentration of <135 mEq/L) is one of the crucial factors in the clinical prognosis of heart failure patients - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6224129/


5. Follow-up period (days) - This is an interesting feature, since it has a high correlation with DEATH_EVENT. However, upon closer inspection this feature essentially reports the days after taking measurements the patient died (1) or lost contact (0). In a scenario where we want to predict a patient outcome, we will not have this variable, and hence it makes sense to drop this variable altogether.",3331f113,0.3220338983050847
15155,a077820f7ab459,15ec6772,### Visualize a batch of images,05a43104,0.3220338983050847
15157,149cb8d3489224,dd393db5,## Visualize the data distribution,116858e7,0.3220338983050847
15158,a44368590e878a,3414f214,### Country,77743ba8,0.3220338983050847
15159,5f32117bcd5255,f7d2c0d2,#### EXPOSURE INFORMATION,85882abf,0.3221476510067114
15163,396bc36edb95d3,82ead489,#### Splitting data into training and test set,965e4f8f,0.32222222222222224
15164,49ac6594c8f5cf,1a65668c,But Science students earn more salary than anyone else according to this data.,6f19f28a,0.32222222222222224
15167,2f47abddfd1928,69001c39,"#### 2.1.6. Cabin

The next feature to correct is embarked but it shows a 77% of missing values. First we should identify if it is worth to spend time on it.

I will start substituting NaN values with Other.

Also the cabin is compound by a letter and a number. The letter is likely to show the area in the boat and then the number should say specifically the room.

They area where you have the room seems that can be important to survive. Depends on how close it is from the deck or if it is closer to the sinking part of the boat, etc.df_all_cabin_count = df_all.groupby(['Cabin_Letter']).count()
df_all_cabin_count = df_all_cabin_count['Cabin'].apply(lambda x: 100*x/df_all.shape[0])

df_all_cabin_count.index

plt.figure(figsize = (10,8))
sns.set(font_scale = 1.2)
sns.set_style('ticks')
plt.title('% of People per Cabin Area',
         fontdict = {'fontsize': 25})

sns.barplot(x = df_all_cabin_count.index,
           y = df_all_cabin_count)df_all_cabin_count = df_all.groupby(['Cabin_Letter']).count()
df_all_cabin_count = df_all_cabin_count['Cabin'].apply(lambda x: 100*x/df_all.shape[0])

df_all_cabin_count.index

plt.figure(figsize = (10,8))
sns.set(font_scale = 1.2)
sns.set_style('ticks')
plt.title('% of People per Cabin Area',
         fontdict = {'fontsize': 25})

sns.barplot(x = df_all_cabin_count.index,
           y = df_all_cabin_count)",ae33cc0b,0.32231404958677684
15170,0d9a2067267ba1,e222f543,The datset contains only 5 features having missing values we need to impute the missing data especially for those with big propotion of missing values.,abc194fb,0.3225806451612903
15175,45568f3ca94aca,2ac0c5af,## SVM Model,2c6ea8e4,0.3225806451612903
15179,56e58d53ac9c57,af92f6a8,"Since we are not sure what to write on Review and review_title, let us fill them with empty space and for the rest we fill them with most common values.",90e2ab8e,0.3225806451612903
15183,0cb456a5456cf9,85e27eca,"# **Q2 Answer**<br>From the plot we can know that during 2015-2017, 9-10 is the first peak time and 4-5 is second.<br> 结合三张图我们可以知道第一个入住酒店高峰期是9-10月，第二个入住高峰期是4-5月。",5701729c,0.3225806451612903
15186,ad26c020235dfc,92d71c9d,Plot the target over the time:,bf766e48,0.3225806451612903
15187,b59b5aaeedb1fb,527708ed,#### Adding 1.5 to Rings to get the Age,1ad63faf,0.3225806451612903
15188,0925f172b5eb74,4c3c1089,"# Normalising the dataset

The following functions are also taken from *utils.py*, they are used to change the labels representation in the pandas dataframe. 

Initially the dataset looked like this:

>|***image***    |***labels***   |  
|---|---|  
|e88d1bbd624e9c34.jpg   |powdery_mildew   |  
|8002cb321f8bfcdf.jpg   |scab frog_eye_leaf_spot complex   |  
| ...  | ...  |  


## Disjoint normalisation

The first type of normalisation has been named as *disjoint* because it keeps the original 12 labels as 12 unique classes.

Normalising it this way transforms it to:

> |***image***    |***scab***    |***healthy***  |***frog_eye_leaf_spot***    |  ***rust***    |***complex***    |***powdery_mildew***    |***scab frog_eye_leaf_spot***    |***scab frog_eye_leaf_spot complex***    |***frog_eye_leaf_spot complex***    |***rust frog_eye_leaf_spot***   |***rust complex***    |***powdery_mildew complex***    | 
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|e88d1bbd624e9c34.jpg|0|0|0|0|0|1|0|0|0|0|0|0|
|8002cb321f8bfcdf.jpg|0|0|0|0|0|0|0|1|0|0|0|0|
|...|...|...|...|...|...|...|...|...|...|...|...|...|

## Joint normalisation

This normalisation joins the labels into 6 basic labels, removing the possibility of having spaces to separate different diseases.

According to the competition's information, different diseases are separated by spaces, that's why this normalisation is also considered and seen as the most accurate one.

This normalisation would transform it into the following way:

> |***image***    |***scab***    |***healthy***  |***frog_eye_leaf_spot***    |  ***rust***    |***complex***    |***powdery_mildew***    |
|---|---|---|---|---|---|---|
|e88d1bbd624e9c34.jpg|0|0|0|0|0|1|
|8002cb321f8bfcdf.jpg|1|0|1|0|1|0|
|...|...|...|...|...|...|...|

",ec34cd72,0.3225806451612903
15192,57070ad5e0f94f,899a6830,# **Checking If Anything Changed Unintentionally**,d97edc41,0.3225806451612903
15193,9ceb7278784462,1d967cfc,* I will drop it in the data set because there was a low number of data,3768a567,0.3225806451612903
15197,2ada0305b68956,876ca3ee,### 53. Palette = 'RdYlGn',133e26f4,0.32285714285714284
15198,fc8e0042411c46,6756630f,## Lead Source,af476c2a,0.322884012539185
15199,d07915a6e6992e,88b2d29a,"**Cabin**

Alphanumeric variable. 

687 missing values in train & 327 missing values in test data - which needs to be treated. We can create more features using this Cabin variable. ",2b912140,0.3230769230769231
15206,03048e86a6d806,e0d9a4f7,"In general, less than 1 year is the most frequent value of experience in using ML Methods, except for Data Scientist, Data Engineer, Machine Learning Engineer and Database Engineer. Most of the respondents with the first three job titles have 1-2 years of experience using ML methods, while 30% Database Engineers have not used any ML methods yet.",1285c231,0.3230769230769231
15214,21bce4ec54b3fa,f2b180c4,"Separate X and y, add random feature for importance reference, change dtypes to float32 to speedup processing",35546e30,0.3235294117647059
15215,7f74a04ae75792,697d3bbc,"###  Check the skewness and kurtosis of the variables? Is it normal? Document your process in the notebook of handling skewness and kurtosis
",d01e91da,0.3235294117647059
15217,71b75664517244,ece26389,"## Performance Charts

My favorite section, here we can see all performance charts by the best teams.",fc905af5,0.3235294117647059
15221,e170d33ee1da8c,df1c9abb,"For demonstration purposes I'm using `distilroberta-base`. It is a lightweight distilled vertion of RoBERTa, which performes considerably worse. You can easily switch to other models by changing `model_name`. The `TransformersTextBlock` uses pretrained huggingface tokenizer internally and is set up by providing path to pretrained model. ",253cee3c,0.3235294117647059
15223,842547b2def18c,045cf1af,"### 既存の特徴量から選抜し，新しい特徴量を作る

NameとPassesngerIdを削除する前に，
We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.

In the following code we extract Title feature using regular expressions. The RegEx pattern `(\w+\.)` matches the first word which ends with a dot character within Name feature. The `expand=False` flag returns a DataFrame.

**観察**

When we plot Title, Age, and Survived, we note the following observations.

- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.
- Survival among Title Age bands varies slightly.
- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).

**決定**

- We decide to retain the new Title feature for model training.",b8efde6d,0.3235294117647059
15227,395ed8e0b4fd17,e35efcd2,# <center>DATA DISTRIBUTION</center> ,7573ea31,0.3235294117647059
15229,8f50c9c16db95f,c77f9b51,"| Outcome      | Definition |
| ----------- | ----------- |
| **Touchback**      | When a player downs the ball after a free kick behind his team’s own goal line,or the ball is kicked  <br>through the back of the end zone, the play is dead and the ball is spotted on the 25-yard line.       |
| **Return**   | Once the receiving team possess the ball, the objective of them is to score a touchdown, <br>i.e. returning the ball to the end zone of the kicking team, though that is very unlikely on return plays.        |
|**Muffed**|When a player touches a loose ball while unsuccessfully attempting to gain possession. <br>Muffs most frequently occur when a kick or punt returner fails to successfully execute a catch on a free kick or a punt.|
|**Out of Bounds**|A player is out of bounds when he touches any boundary line or touches anything — except a player, <br> an official, or a pylon — that is on or outside a boundary line.|
|**Kickoff Team Recovery**|The kickoff team gains the possession of the ball.|
|**Fair Catch**|A player in position to receive a punt can signal for a fair catch by raising one arm above his head and waving it <br> from side to side. Once the receiver signals for a fair catch, he cannot advance the ball and the play is over when he <br>catches the ball <br>and the opponent may not interfere with or tackle him.|
|**Downed**|The ball is caused to be out of play.|",26cc763a,0.3235294117647059
15231,1d1598b6fa2aa7,76b748f6,Let's plot two functions on an one plot.,e066accf,0.3235294117647059
15232,a0a5baa6c7e12a,c5c25aca,"<img src=""https://raw.githubusercontent.com/gvyshnya/tab-dec-21/main/AutoViz_Plots/Cover_Type/Pair_Scatter_Plots.png"">",551d41de,0.3235294117647059
15234,ab657da5329e3f,3fdcaa96,## Visualization utilities,021526f8,0.3235294117647059
15236,7cfd96218dd933,725defd9,"#### **ATTENTION**
* THE HIGHEST BRIGHTNESS VALUE WAS DETECTED IN ALL 11 DAYS.",7c34d96c,0.3235294117647059
15238,02b7e38902069e,dfcacbeb,"#Extract embedding vectors

""When we are training machine learning or deep learning-based models for NLP tasks, we usually represent the text data by an embedding like TF-IDF, Word2vec, GloVe, etc. These embedding vectors capture the semantic information of the text input and are easier to work with for the models (as they expect numerical input).""

""iNLTK under the hood utilizes the ULMFiT method of training language models and hence it can generate vector embeddings for a given input text. Here’s an example:""

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.3235294117647059
15239,a566b5b7c374e7,2e86024b,"### Initial Impressions:
- Some activities are completed too frequently to analyze correlation with sleep metrics. For example, I've had coffee almost (if not) every single day since tracking began. I've taken sleep drugs only once. In general, I won't spend time analyzing activities that don't have at least 5 occurrences of success and failure. Even that is pushing it.",b3dc5545,0.3237410071942446
15241,7454fdc444df16,5b2dadf3,For a quick refresher on the different libraries used to read images and how to use them check out this medium article by [TowardDataScience](https://towardsdatascience.com/what-library-can-load-image-in-python-and-what-are-their-difference-d1628c6623ad) ,a7818ef5,0.3238095238095238
15248,9bcfa825c8b2e6,e3bdc65c,FEATURE ENGINEERING,220f36e4,0.323943661971831
15257,62037c5832129c,23684547,# Fine-tuning machine learning models via grid search,61474350,0.32432432432432434
15259,a6c34cd514e30e,e064be4d,Let's take a quick look at what the data looks like:,bf603ddd,0.32432432432432434
15261,297cbe4a23c4bf,264224cb,"I read the description of each feature and accordingly i filled their missing values with either the most repetitive values(mode) for categorical features, average for numerical features and 'None' for some features.

For example, it would be ridiculous to add Alley feature to the houses whose Alley feature is missing so a new value i.e. 'none' is added for such houses. Similarly every house has a Kitchen so i didnt put 'none' for missing KitchenQual values, rather i filled with most common KitchenQual value. Hope this kinda makes sense.",a843e619,0.32432432432432434
15262,b7b1057764fa02,4279584f,"Notice how the training images are all in a similar environment, with a combination of lights and shadows. We will address these issues later.

Let us now print the testing images.",5053a192,0.32432432432432434
15265,ac9b48d531bad9,00d58db3,# MODEL COMPARISON,95965e35,0.32432432432432434
15268,2dda7facf3c1e0,346a2afa,# Preprocess and Parse Training DataSet,45552d2b,0.32432432432432434
15273,fe7360cddc13e5,76281a9a,-----------------------------------------------------------------------------------------------,8979e423,0.32456140350877194
15276,4ae464582bac51,41e7296f,# EXPLORATORY DATA ANALYSIS,ca6a52ce,0.3246753246753247
15277,c13f73168789c2,15ba4844,"## 2. Select by index position<a id='11'></a>
You can select data from a Pandas DataFrame by its location. Note, Pandas indexing starts from zero.",16175052,0.3246753246753247
15282,663bbc9eaf267b,cbd18eee,"Thus, we are going to consider this car as an outlier and remove it from the dataset.",32445529,0.3246753246753247
15285,b86bda7afe3ac3,7e0bd36c,Text regressor,16197934,0.3247863247863248
15288,fdbbd573ba31c2,186a625b,### area_temperature(°C),f7c28d74,0.325
15289,254cccd5145725,6b0331f2,We examined how education affected the poverty level of the household. We have a feature called “meaneduc” which is the average amount of education in the family. When we plot this feature against the Target variable we can see that the families the least at risk for poverty  tend to have higher education levels.,a49b4037,0.325
15290,5ffe6aa38958a1,ef0ded79,Plotting Distribution ( https://www.kaggle.com/helgejo/an-interactive-data-science-tutorial) ,11f5412e,0.325
15292,edc19e349fe80a,e8d53487,### 2. Preprare data for training,7882221a,0.325
15293,0d58c434c7db1e,5101bcc3,column Title has 5435 nan values.,517e01d3,0.325
15294,3dd4294f903768,60f37e74,"We can see that from all the price category, the low price category has most of the restaurants.",0d89d098,0.325
15295,62487bcd70b199,4dc812ce,"## Inference:
Mortgage and family size doesnt really explain much on personal loan",f6ae50af,0.325
15299,37b09262279764,a9fde098,### Removing redundant features,37c4c417,0.325
15302,9a040a4f21091e,7efcc1a7,"I feel pretty good about the words we eliminated for occurring too frequently, but had some trouble figuring out the threshold for words that occurr too infrequently. ",f591b57d,0.325
15304,6e28c4f557f736,e8ea2ea2,"## Data cleaning
remove urls, html, emoji, punctuation

spell checker

https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove",021fdf75,0.325
15305,e19e307b3fd188,60b74563,"The number of rooms usually varies between 1 and 4, and we noticed that the more rooms, the higher the rent, which is already expected. The value of 10 rooms are strange...",2173955b,0.3252032520325203
15308,22bd95f4807a23,c81bc797,"*  A majority of users purchased general wear.
*  A majority of users gave high ratings",c05d356f,0.32558139534883723
15313,8539260444e6b5,54f31bb9,# Converting all the texts back to sentences,0369463f,0.32558139534883723
15315,c09fac3c943d51,bb9ffa54,Seems to be a serious problem:,678d076d,0.32558139534883723
15317,1660daf8867980,59c36569,**Implementation**,42d7cffc,0.32558139534883723
15318,743ae010f5e875,883a9339,### Matching on test data,02c54445,0.32558139534883723
15326,7e89d387feb9f5,b281ec14,### Добавленный числовой признак №5. Числовой код диапазона цен,989e3a1b,0.32608695652173914
15328,72d528df923403,b0d16ab8,"# states and stores behavior
- CA is the top item seller.
- CA_3, TX_2, WI_2 are top item sellers of their respective states.",d51c8e8e,0.32608695652173914
15336,56785caebaa256,b6e2fdf1,"### 3.2.2. Very comfortable conditions for rest <a class=""anchor"" id=""3.2.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.3262411347517731
15337,957e035ba5b9d5,c3458876,## Save model,778ab3d3,0.3262411347517731
15338,b61ab8f81dc03d,97038ecf,Now we can transform the columns with low cardinality using an OrdinalEncoder,64d05394,0.3262411347517731
15346,eb33e05704d647,fefb2d25,"
Calculate the rpn for all anchors of all images
",cd80436d,0.32653061224489793
15349,f35bf4df70d310,4d59fc39,Fit the PCA on the training set and set the model to retain 95% of the variance,10bb859a,0.32653061224489793
15350,12f4d16fc21645,cd934209,<h3 style='color:red'>Phosphorus Analysis</h3>,c7752038,0.32653061224489793
15354,b10bd75889dad9,b0f3529f,"#### Out of sample imputation for columns 'night_pck_user_6','night_pck_user_7','night_pck_user_8','fb_user_6','fb_user_7','fb_user_8'",ee00ceee,0.32666666666666666
15356,917957c6c4065f,a49c1723,#### description & description_length,55b8ed68,0.32679738562091504
15357,582cb872d19026,3986fa1d,"
******************************************************************************************************************************
#### The count values of the games in all categories in our dataset are displayed in this Figure.
******************************************************************************************************************************",8d966d69,0.3269230769230769
15362,6f1481148352e9,b29f2009,"**High variability is observed. The minimum number of fires in this state was in 1998 - 2.4k. The maximum number of fires was in 2009 - 8.2k.**

**On average, there were 4.8k fires per year.**",7cfbdb8f,0.3269230769230769
15368,5a8c553e21c70f,8dec99f4,Fit IsolationForest to only Class 0 samples.,9ebd9d8f,0.32727272727272727
15369,016abae0483764,8032ed90,"Thus we get to knew that there is no null value, so the data is clean...",bc9f289b,0.32727272727272727
15373,c9b4e282e4e2c1,f1da5513,2-Visualizating the data. Let's make questions about this dataset:,f44d339f,0.3274336283185841
15374,ac1abfe1dfe815,88624741,-----,6529dbcb,0.3274336283185841
15383,f3c6048d1058e3,46d25782,- Selecting numerical features for training model,1d9056b0,0.3275862068965517
15386,00001756c60be8,c3ce0945,**Считываем тестовый набор данных**,945aea18,0.3275862068965517
15391,c4386b8a01d66e,85431569,# Solids,dc732bf5,0.3277310924369748
15396,0858e1bb3cbaca,817cebc8,or,78548374,0.32786885245901637
15397,918040fad252ec,265ffeff,Membuat model data CNN untuk sequential tabel atau summary data,966fcd8f,0.32786885245901637
15398,bcd7e398c4d0ec,1c033e40,## Color_Label,77a143f6,0.32786885245901637
15401,ff3a8ce61fab6a,ca5139ba,"<hr>
## Exampel 2 ",9afe1654,0.328125
15403,3cc097a5859dc1,378c5f84,"# **Getting rid of the columns with nulls more than 10% which will not be used in our model**
",14380d73,0.328125
15404,ee23a565163388,080a0dd2,"**Inference**
- 35% of the diabetic patients faced heart failure.
- Only 29% of the non-diabetic patients suffered heart failure.",88aacbc4,0.3282442748091603
15407,1a222fee3089d2,46222048,# Feature engineering,59ab8894,0.3283582089552239
15411,c80939c7c626cf,f00c2e3e,"# Here last column is created for Title and values are assigned
",b9ac31e2,0.3284671532846715
15417,38b79494ac749e,ecd11430,We can investigate the shape of the fitted curve for different values of `degree`:,39162a40,0.32857142857142857
15420,2ada0305b68956,83cb88b7,### 54. Palette = 'RdYlGn_r',133e26f4,0.32857142857142857
15424,1eb62c5782f2d7,3effcacb,### B. z-score = 0 dan -1.77,bb69f147,0.3287671232876712
15427,3d08ca7656dec0,5dd4808b,# thall,bd3f87e3,0.3287671232876712
15431,726833f92fb87a,358da256,## Job Vs deposit,7dc5e1b6,0.3288590604026846
15432,52ee792e228d54,7d48b886,"### From the ZIP Code box plot, there is an outlier. Seems like one of the customers is in Asgard. Let's try convincing Heimdall, the gatekeeper of the Bifröst bridge to bring him back to Earth.",5096094e,0.32894736842105265
15433,08f845750d026a,9b3c9687,"Jeypore,Odish is the largest borrower from Kiva in India",1c54de30,0.3291139240506329
15436,9c26c5dcd46a25,09b37d46,"On remarque ici ques les catégories semblent assez différentes, même si l'ordre de grandeur des écart est relativement faible. Dans les histogrammes ci-dessus, nous voyons cependant que les distributions ne semblent pas suivre la loi normale.      
La question sera à présent de savoir si ces écarts sont significatifs ou pas via l'analyse de variance :",1bbbb677,0.32926829268292684
15439,513ce405d7f6a3,dbc9c3b0,"# The graph shows that most of the sentiments in SentenceId 3189 are negative but most of the sentiment in the full data set is natural.
so the SentenceId can be used as a feature to improve the results.",8461e086,0.32941176470588235
15442,73d8e56bc709b1,e5088148,"Therefore, we got **top 10 talented players**.",78ec3cce,0.32954545454545453
15444,d1ff7e10ee0102,b4c7fc0e,"### The 'plasma soup'

'In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.' (source: http://umich.edu/~gs265/bigbang.htm) 

To explore the universe, we will start with some practical recipes to make sense of our 'plasma soup':
* Correlation matrix (heatmap style).
* 'SalePrice' correlation matrix (zoomed heatmap style).
* Scatter plots between the most correlated variables (move like Jagger style).",2cc71c3c,0.32954545454545453
15446,b01ee6cb674fa3,f527de03,"looking the columns rocket_cost and day, the datatype is an object and should be numbers for a better handling ",a8ffd35e,0.32971014492753625
15452,225b4fe5d3894a,b4883fc8,"<a id=""5c""></a>
### c. Experimenting with Feature Combinations
we ll try to create new features that are more relevant",4b4197b3,0.32989690721649484
15459,a2176d4653ef60,b127f598,# Data Preprocessing,ac908675,0.3300970873786408
15461,510b8303776bb6,5e8972e3,"Since non-numerical categorical data in a dataset will be displayed in an alphabetical order in the graphs, we need to provide a dictionary with orders in order to override the default order.",18080db8,0.330188679245283
15462,bd380b97b5c894,8fd06011,Now let's divide the data into 4 groups for age recovery.,66f2562a,0.3302752293577982
15463,71c3c1eab0377d,a5c2d48c,##### Column: Age: Missing Value Treatment,52b4e360,0.33043478260869563
15469,ce9ed5e2d601d7,3baf1295,Undersample if SAMPLE parameter < actual count.,f58a2f43,0.33070866141732286
15470,a4f8ad33c823c5,75e009a4,* Descriptive columns ,fcd48307,0.33076923076923076
15472,09751c520b0616,88c16425,<b>Filling 'LotFrontage' null values with  median of 'LotFrontage' (68).</b>,a4d0c7e9,0.33076923076923076
15474,ba4b3bd184acbb,f917f416,"We can see that there are 10,841 different apps and a total of 64,295 reviews.

The `app` DataFrame has 13 columns, the `app_reviews` DataFrame has 5 and all the columns have either an `object` or `float64` datatype.

However, some of the columns of the app data such as Reviews and Price were assigned to the `object` datatype when they appeared to be numerical.

Let's figure out why:",0f5de724,0.3308270676691729
15476,c65a65d4041018,4bf3013d,I think it is interesting to see which industries have the most developed ML practices. It isn't surprising that IT companies are far ahead.,824fb229,0.33088235294117646
15477,a566b5b7c374e7,93c0ec7f,### Correlation of Activities,b3dc5545,0.33093525179856115
15478,63b44c85e32c1f,7867129c,Even if the numbers are declared in a string the first index of each element is considered and the maximum and minimum values are returned accordingly.,fb9b9562,0.3310810810810811
15484,d0080e3a39bc5c,dae23564,**1ST TRANSFORMATION**,2fcde4cf,0.3333333333333333
15485,4fd4b6a80d40e3,c1b20bce,"## ReLu Function

![image.png](attachment:image.png)",f6913cc3,0.3333333333333333
15486,7ff97196d5db8c,1908a392,# Correlation Matrix,3d82be43,0.3333333333333333
15491,b42180a6a5b42f,3e5bde21,"**Conforme apresentado pelo site do Ministério da Saúde, nesta primeira análise confirmamos que o gráfico de número de óbitos por data de notificação, se assemelha a um modelo de Distribuição assimétrica à esquerda ou assimétrica negativa (left skewed ou negative skewed).**

**Passemos, então, à segunda análise para verificar se há incompatibilidade entre as datas de notificação e a data efetiva do óbito.**

**Lembrando que:**
1. #### ***Se não houver diferença entre as datas de notificação e as datas efetivas dos óbitos***, podemos concluir que o ***Brasil segue um padrão atípico*** em relação aos demais países que tiveram seu período de expensão da epidemia por um período médio de 40 dias.

1. #### ***Caso haja diferença entre as datas de notificação e as datas efetivas dos óbitos***, e esta diferença seja suficiente para demonstrar que há a formação de um ""bell-shaped curve"", podemos concluir que se torna ***injustificável todas as medidas de prorrogação das medidas restitrivas impostas pelos governos (""lockdown"").***
",987cea5f,0.3333333333333333
15493,0e2a23fbe41ca9,98741397,"Most of the columns have same values, the ```_lag``` columns are different. We can try deduping the rows by taking an average of those values.",64e4762c,0.3333333333333333
15494,864302b10e7730,be6bb945,# Beginning with Visualization,e9dd1d2d,0.3333333333333333
15495,28a1ff0f223da9,b5c15670,### Where we need more professors? ,c945b27d,0.3333333333333333
15496,e25c0f830df3f4,b7cc6ef6,# Displaying the reviews with Polarity & Subjectivity,fdcf7189,0.3333333333333333
15497,37b09262279764,61886642,"Features going to be removed
- PassengerId
- Name
- Ticket",37c4c417,0.3333333333333333
15498,95656e8d666b16,2b9a7462,"### Exploring feature selection techniques
Note: I know that there are already barely any features, I'm simply including this to practice and improve myself in this realm of ML",65e88599,0.3333333333333333
15499,c6f8ff61a5fa87,c0511e6d,"# <span style=""color:blue;""><strong>5.Data Transformation</strong></span>",3eea586b,0.3333333333333333
15500,b4153d15e36294,633530b9,Zip file of train dataset is held by a variable namely *files_in_zip*.,f05d3393,0.3333333333333333
15504,1c5aaf7bea6414,47bff5fa,# Drop NA,34d8f42d,0.3333333333333333
15509,98ea617d18c9cc,0e83e35f,# Implementing Stratified K-Fold method,e6316d11,0.3333333333333333
15515,593d1d3d1df05a,71c8dfc2,# Thresholding Again to Find Chars,bc682ffe,0.3333333333333333
15517,b6c0ad74f95b8c,53cf9718,Scale the data; the above cell converts the elements of each column to a float,5de5b241,0.3333333333333333
15518,e19e307b3fd188,05ac5196,"### Bathroom
",2173955b,0.3333333333333333
15520,55a5e31d03df9f,e5d84499,"### <a name=""mlpcallbfit"">MLP Training - Callbacks and Fit function</a>

Let's resume the training of the model, as we did before let me write code and later on explain.",06dce00f,0.3333333333333333
15525,0800c019d227f2,95cacec6,"### Standarization
- Before tuning, let's standardize explain variables.
- In this competition, all of the columns in given data are continuous.",9cd3ffa1,0.3333333333333333
15533,62487bcd70b199,141f7e33,## <a id='4.6.'>4.6. Income and Mortgage in Personal Loan</a>,f6ae50af,0.3333333333333333
15534,5cfb546af2b8ce,08e0e23e,****What can we learn about different hosts and areas?,e8730ad1,0.3333333333333333
15536,b809d07ddd17ed,ebbbbb33,## Visualise some examples,e32bf3b1,0.3333333333333333
15537,f18e737fcc4b06,f8c1f16b,"* float64 : Age, Fare
* object : Cabin, embarked, ticket, name and sex

* int64 : Pclass, sibsp, parch, passengerId and survived
",087b8637,0.3333333333333333
15538,0a1fcda859252c,f878e914,"### Preparing validation data
We will be defining a generator for the training dataset later in the notebook but as the validation data is small, so I can read the images and can load the data without the need of a generator.  This is exactly what the code block given below is doing.",13a38774,0.3333333333333333
15539,e0f03003a69819,34fd55cf,"# Conclusion :
* We can see a clear relation between smoker and charges 
* Non - Smokers all had low charges for them 
* And Smokers are spread across the range . This proves that the smoker population is moe likely to need health insurance.Which is not that suprising . 

<h4> If the world is asking you to stop smoking and you are not doing it , something clearly is wrong with you !!!!",609ad1f4,0.3333333333333333
15540,d96e03a9e7c030,338ed06b,"## Step 3: Exploratory Data Analysis

Now that we have a full dataset to play with, we can start to explore the effects of each predictor variable on the response, `perc_testtakers`. We'll do this by fitting a single linear regression model for each predictor, and examining certain goodness-of-fit metrics like the coefficient of determination (r-squared), adjusted r-squared, and the median absolute error of the model.

I decided to use the **median absolute error** instead of more common metrics in linear regression like mean-squared-error (MSE) because the response is a proportion between 0 and 1. For example, squaring an error of .25 would produce an MSE of .5. In my opinion, median absolute error is more interpretable because you can look at it and know that 50% of model predictions had an error at or below the median absolute error.

To fit the model, we'll use the `sklearn` library in Python",d2b72ced,0.3333333333333333
15542,42e0005bed28aa,b7cb4e00,><h3>Creating Dataset from images</h3>,5616d451,0.3333333333333333
15548,d128317750d689,29f88cb8,"Now we can apply it to the dataset, then divide it into features (x) and labels (y), and construct Tensors. Tensors are PyTorch data structures that work like arrays, but are little bit different. For example, they can work with GPU. [See this](https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html) for more details, if you want. Tensors in PyTorch also have data type, so watch out for that.",d87f7428,0.3333333333333333
15549,2e0fd6e937bf79,26495d25,preprocess library path : https://www.kaggle.com/rhythmcam/titanic-preprocess,6acb965d,0.3333333333333333
15550,06ecf7a304c309,31e4e3e0,"원본 이미지와 예측 이미지를 그려봅시다.

**Inputs : Actual Images**",714de627,0.3333333333333333
15552,6cade0b6a41ba2,9eb2a85d,##### Nothing needs to be done with this data column,e6110293,0.3333333333333333
15553,c2a9f2fb3e1594,6c7311dd,"## 3.25 Split Training and Testing Data

As mentioned previously, the test file provided is really validation data for competition submission. So, we will use *sklearn* function to split the training data in two datasets; 75/25 split. This is important, so we don't [overfit our model](https://www.coursera.org/learn/python-machine-learning/lecture/fVStr/overfitting-and-underfitting). Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't ""cheat"" by memorizing the answers. We will use [*sklearn's* train_test_split function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). In later sections we will also use [*sklearn's* cross validation functions](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), that splits our dataset into train and test for data modeling comparison.",53411c04,0.3333333333333333
15554,b547f0f38f7744,85beafb6,Distribution of size of bounding boxes:,b6ba66b3,0.3333333333333333
15558,1fac5edd4063ba,79733ce1,![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTpE45eSqrPsaHD-3nrvCnJIQxG7gt6otPapA&usqp=CAU)usnhistory.navy.live.dodlive.mil,04bc01e0,0.3333333333333333
15562,d4c5aaa4b36810,42fbcc7f,These data points are recent enough on average and there isn't many missing so I will use these instead. ,65441f28,0.3333333333333333
15565,6a1ae8234c7653,d7925fd1,I will split the train data set in two datala set for model building and validation before prediction the value for the test data set.,2d643c72,0.3333333333333333
15567,cf4d1c1ad1476c,89eabf52,"## What we found ?
* There are a total of 15 languages

* English is the most predominant language (56.68% from the total)

* The rest of the languages have a similar distribution between 3.4% and 2.82% (the total of these languages is almost the other half of the dataset, a 55.68%)
",768c1a59,0.3333333333333333
15568,15eb884262ba09,17ecc0d8,"It appears our optimal number of continents would be 3, but let's stick to 6 for now (Antartica has fallen off the map).",d703bdab,0.3333333333333333
15569,598b6228760590,dae23111,"- Through the distribution of age characteristics, you can consider filling in missing values from the mean or median of age.",be30ab66,0.3333333333333333
15570,051b118f751e77,6d7859ee,"# Quick EDA
Since, this notebook isn't about EDA, let's do some quick plotting only using dabl",9fad25fd,0.3333333333333333
15576,4fa553c2b837d4,eba2cf7a,# Random Forest,c65a23e9,0.3333333333333333
15577,9276fa5cc2fef6,2005b7dd,Seems like length doesn't explain insincerity but certainly has some information to add to the context. I am curious about the highest lenght question lets check it!,24aa6a52,0.3333333333333333
15583,80664f474fe2ef,334caf26,# Data preprocessing,bd82c7eb,0.3333333333333333
15587,3597174a998d4d,4a3fc83b,"It can be concluded that:
* Usually, the number of adults varies from 1 to 3, and the number of adults varies from 0 to 2. The number of babies varies from 0 to 1.
* When the number of adults is higher than a particular value, the ratio of cancelation is 1.
* The booking with babies has a lower ratio of cancelation than booking without a baby.",276892ed,0.3333333333333333
15588,1883198d6d8c3c,4d067d05,To find the name of the columns of the dataframe,69a1d458,0.3333333333333333
15595,71d5f1925c2b4b,01cf42ee,## Exploratory Data Analysis,63ee03ac,0.3333333333333333
15596,49ac6594c8f5cf,acf5b59c,Relationship between degree percentage and salary,6f19f28a,0.3333333333333333
15599,233cb23d9e01b9,5ca2fdd6,# Model Build,ffa56c19,0.3333333333333333
15601,ab6da5994949a3,a65af68e,## Confusion Matrix,fae6b91d,0.3333333333333333
15607,6a05614abce6d9,275f03cc,## EDA,c0c9da16,0.3333333333333333
15609,7a058705183598,0835de94,Train & Test Split,b0ead917,0.3333333333333333
15614,454672c0f11328,b80f1bb7,"### Lv1
**Input**: 118 Features

**Models**: [XGB, CATB1, CATB2, LGBM1, LGBM2]",bfa2868b,0.3333333333333333
15616,923e97b05be00b,30efc03b,# 2. Preparing the data,3a4a22dd,0.3333333333333333
15618,2f0f808765fc67,11f6f7ef,*COUNT VARIATION WITH DIFFERENT FEATURES*,fd1f6494,0.3333333333333333
15623,65245c6e88a2ee,7bba47dd,"### As there are no null values, so we can continue!",71d6e90e,0.3333333333333333
15625,7ba63a2d9abb58,3e61395a,"<h2>Analysis of patterns in cases, deaths, and recoveries of top and bottom 20 countries</h2>",821a261f,0.3333333333333333
15631,a758983a68c014,2c156564,"Below we can observe the logic. We are taking two neighbors from each side of center word. We can see many padding tokens, that is because maximal length of our sentences is 3, which is why each word will have at least two neighbors being padding. It's done just for presentation purposes. Below you can see some plots, which will help to understand the logic better.",ab89f181,0.3333333333333333
15632,c01049afb6d307,3f8728c0,"* **Events worth investigating**

    * More absentee people and their excuses
    
    * The most used excuses and who uses them
    
    * What is the excuse that causes this in the more absentee months?",d37d3b5d,0.3333333333333333
15633,7baeb0ffc6659e,e730255e,**Sex**,8cbebba9,0.3333333333333333
15636,52cfd66e9ec908,0a4676be,"So yes, I probably should save these as a GIF to visualize the agent movements. Let's try a simpler form of this and use the semantic view for the agent dataset.",c74adcdf,0.3333333333333333
15639,4d91e84c564cbe,1db48aa1,"Hm, that's quite a mouthful. Let's compensate by shortening the names of the first 3 planets.",355a43e3,0.3333333333333333
15642,166a62ebb4fc3a,871cfaec,"Now,let's explore the data",db48a079,0.3333333333333333
15645,cf39cde80e66b7,fc260ced,"# <div class=""h1"">Regression metrics summary </div>
<a id=""M""></a>
[Back to Table of Contents](#top)

[The End](#theend)

![](https://miro.medium.com/max/1308/1*lke9jk2uY-ppHO0h0xytQw.png)",aed4bc9b,0.3333333333333333
15655,892be0a523578c,bb3bb83e,"**5.3** By inspecting some abnormal samples, I found that:
* Some sleep records are splitted into 2 days, for example, 2016-04-12 11:50 pm - 2016-04-13 6:00 am, which causes the length of sleeping in one day is very small
* Some sleep records are generated only in the daytime, maybe are collected during a nap or are attributed to the life style of the participant (like taking a night shift and sleeping in the daytime)
* Some noises may exist, for example, some 2s or 3s suddenly appear in consecutive 1s in a specific day for a specific participant, like 1,1,1,1,1,1,2,2,3,3,1,1,1,1",b0e8d7c0,0.3333333333333333
15656,b6e698d389d0d3,8e9c798d,# scaling and transformation,f02f68b5,0.3333333333333333
15664,76c8afe761adc1,262c52e2,# Training,8258944b,0.3333333333333333
15665,20e523830aab51,3864679d,# Encoding Labels,810b8785,0.3333333333333333
15666,c73e07ad6d25c5,55e20613,## PClass,3ab391fb,0.3333333333333333
15669,f3d5d8917ce5df,a110ad66,"# Split!<img src=""https://res.cloudinary.com/sagacity/image/upload/c_crop,h_1486,w_2046,x_0,y_0/c_limit,dpr_auto,f_auto,fl_lossy,q_80,w_1080/Screen_Shot_2017-01-18_at_10.04.37_AM_qaaqch.png"" align = ""right"" width = 200>
I'm using a smaller subet of data because it'll be faster to read and predict and to be able to keep it all in the free Kaggle RAM. Note that because this is a time series problem I'm not using scikit-learn train test split. This is because we want to train with older data and test with new data to simulate actual conditions. You will have all historical data, and will try to predict the next few months... Train test split will randomly split the data.

",e45112f8,0.3333333333333333
15671,95d896e75f9a50,bad96fba,"**My take:** We can remove *a lot* of the most important features, and still be able to perfectly classify `feature_0` - it is not just a distribution of a single feature that is dependent on the value of `feature_0`; rather it is the distribution of a lot of the features that is linked to the value of `feature_0`.

## Experiment 3: Relation with each other feature
In experiment 2 I iteratively removed the most predictive feature for `feature_0` from the other features to get an idea about the minimal set of features that together are related to `feature_0`. In this experiment I'll do it the other way around and see which features by themself are enough to predict the value of `feature_0`.",2721b6f5,0.3333333333333333
15673,a827e04a19562c,e172807e,"### PageRank algorithm

Assume a random surfer who starts surfing from one of the webpages available and keeps surfing from one webpage to another by clicking hyperlinks, until he restarts from any of the webpages and follows the same process.

The idea behind PageRank algorithm is that people are more likely to end up on pages which are more important than others. And good links will have more of good/important links connecting to them than bad links/spams. Lastly, spams will have the least number of connections and people are least likely to end up there.

Now the random surfer model can be modelled as a Markov Chain where the next state of surfer depends only on the current state of surfer. This is also known as Random Walk. However, there are some nuances of our network that PageRank deals with:

1. **Absorbing nodes**: Nodes which have an incoming link but not outgoing link. In this case, we assume that if random surfer happens to land at such node, they can migrate to any of the all nodes with equal probability. This helps the algorithm to converge.

2. **Restart probability (alpha)**: Often a random surfer might not want to keep following hyperlinks but restart the search process. To model this, we say that there is a certain probability for random surfer to restart.",ba301c25,0.3333333333333333
15674,d6cbd7160961dc,7a09ae81,## 4.3.1. Results: First Digit,36d74664,0.3333333333333333
15680,caee5b3bdf65c1,732d2eea,# Decision Tree Method,a46111dd,0.3333333333333333
15681,cb570c7b7f0501,56998052,"
### Research Question 1 ( Is it about the age !?)
i mean, do we have a certain age with a high percent of non-shown !? 

first, plotting the number of attended and absent cases for every age.",a200a0ec,0.3333333333333333
15685,1375dae95a1962,24fc1e4d,"# Zelle 1 (Markdown)

FIXME: Korrigieren Sie diesen Tippfäähler !
* Doppelklicken Sie auf die Zelle
* Ändern Sie den Text
* Bestätigen Sie mit STRG+ENTER.",6c688a10,0.3333333333333333
15687,56785caebaa256,20e70e00,"#### Thanks to:
* [COVID-19 Open Data](https://github.com/GoogleCloudPlatform/covid-19-open-data)
* [NOAA](https://www.ncei.noaa.gov/)",a792961a,0.3333333333333333
15689,6116b13d4464d0,da2ee914,"# Creating the .txt file for ffmpeg to process

In order to automate the concat process, ffmpeg requires a .txt file with a path to each .wav file you wish to merge together. In order to create this .txt file, I create a list containing the name of each directory. Because the Cornell Birdcall Identification dataset is already sub-divided by eBirdcode (i.e., species), this makes it easy to create .wav files with the appropriate eBirdcode so that the data is more organized.

Each .txt file created contains a path to each .wav file in the species folder so that ffmpeg will concatenate  every .wav file in the folder to a single large file that contains all of the sounds for that species. I did this because it makes the data augmentation easier as some of the clips are less than 5 seconds but we need to pull 5 second clips for the labeling as part of the competition.

Below I'm using absolute pathnames because ffmpeg was giving me trouble with relative paths. You may also notice the ""file"" before the pathname. This is because ffmpeg requires the following format for .txt files it uses for concatenation:
    
    file 'file1.wav'
    file 'file2.wav'
    ...

Note: The ' ' are stated as not being necessary by ffmpeg but it's considered best practice to include them as some directories may have spaces.
",e2826468,0.3333333333333333
15692,870a7144fa75ad,7263b3fe,**Bag of words**,2a8c3427,0.3333333333333333
15693,80ecc4c67a9f54,adfd21d4,## First name frequency,4bbf546c,0.3333333333333333
15700,36efe086c3f23c,426bb945,"'''MANAGING A DATASET WITH SUCH ENORMOUS NUMBER OF POINTS CAN LEAD TO A DECEPTIVE INTERPRETATION
 To Demonstrate this fact let us look at The following three plots
 namely
 1.) Scatter and Line plot of Prices v/s date
 2.) Scatter plot with Cmap of Prices v/s date
 3.) Histogram of Avg.Price v/s Date_span
 
 ""APPEARANCES CAN BE DECEPTIVE""
 '''",d0676cd0,0.3333333333333333
15702,7454fdc444df16,e1adff5a,### Cancerous Patches,a7818ef5,0.3333333333333333
15704,c8bf959b9608cf,f6ee1623,### Load VGG19 model ,155e3672,0.3333333333333333
15705,df2a7968c08ee4,dd2e7d95,"### Data Augmentation

The following cell is optional. With the model structure I use, I found that I get better accuracy without data augmentation. 

See next section to enable/disable Data Augmentation.",a2ba0a72,0.3333333333333333
15716,a69d41047fdd3e,cc56cf83,"For a hint or the solution, uncomment the appropriate line below.",b1f28647,0.3333333333333333
15717,799d4de27045fa,d408a722,"* Carefully studying the difference between Speech and Music Spectrograms, one can observe that in music spectrogram there is wider range of frequancies across time axis(x), Hence a well-trained simple cnn architecture can easily distinguish the difference between Speech and Music with a spectrogram as its input.",b96ffe66,0.3333333333333333
15719,56cc8fb47bef6a,f8dec965,"<p><u><span style=""color:#a52a2a""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Iterate through all the messages and do the following:</span></span></u></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">a) Remove all the characters except a to z and A to Z</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">b) Convert all the characters to lower case so that there cant be any duplicates</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">c) Split the sentences to get the list of words&nbsp;</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">d) for each of the word in a Sentense, stem that (get the base word) and remove all the words if that exists in stop words &nbsp;</span></span></p>
<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">   (Stop words are like the, is, was, those, these,... e.t.c) &nbsp;</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">&nbsp;e) The final output will be the messages that contains the&nbsp;result of step a to d&nbsp;</span></span></p>

<p>&nbsp; &nbsp; &nbsp;&nbsp;</p>
",652d6670,0.3333333333333333
15721,a4a494c667c673,002e3aa6,# Preprocessing,397d12f8,0.3333333333333333
15737,10b5af05d804ff,006ef8ed,"## So, let's do it for our base features in Dota",4a9b1705,0.3333333333333333
15739,8106640e2f9c7e,12d0c04e,"Создадим датафрейм, содержащий все существующие пересечения вида<br>
**Товар из Data** - **Товар из Target**",faea9b8e,0.3333333333333333
15741,e3fb4c6300cb56,5907b9c0,"<a id=""3""></a> 
## Joint Plot",8ebbdf89,0.3333333333333333
15746,bbaa07ad21cf4e,bacc1c60,##### We will do a trigram (n=3) analysis over the medical text. Let's check the most common trigrams in text.,3ab6b254,0.3333333333333333
15748,b39684e6670dd7,648a1f96,# build RobustScaler,83de9873,0.3333333333333333
15750,268a610bbc64b4,21d484b8,We only have the June booking month data. So to get month-wise percentage we just need to filter data for June month booking and take the count.,8a16f301,0.3333333333333333
15751,c818250dd720eb,9748f46e,"Two other points of note: 
* During the challenge it was revealed by the organisers that the Radboud data contained a significant level of noise. This was a deciding factor in my decision to split up the examples by data provider in the last step of grading.

* Our EDA checked for mislabelled examples, i.e. isup grades which did not match the underlying gleason patterns, one mislabelled example was identified and removed.",68ee40de,0.3333333333333333
15753,6b383ec35229a2,6afd226a,"In this kernel, we will use the transfer learning aproach.

Instead of initializing the embedding weights randomly, we will use a set of pretrained weights that have been computed using a large corpus.

In this particular kernel, we will use GloVe embeddings, the GloVe algorithm has been trained of the Common Crawl Dataset (the web archive) https://nlp.stanford.edu/projects/glove/",0c41b61b,0.3333333333333333
15762,9ca9a30fc69d9b,06022331,## Average goal per game ,f715c2e5,0.3333333333333333
15765,6b2776f151ed9c,ca2fb02e,# Baseline,40406d5c,0.3333333333333333
15777,d58491f2896fc1,7a149770,"Scikit-learn veri bilimi, makine öğrenmesi ve yapay zeka alanlarında en sık kullanılan Python kütüphanalerinden biridir. Bu alanlarda pek çok işlemi gerçekleştirebilmenize imkan sağlarken, bu işlemler sırasında da sizlere büyük kolaylıklar sağlamaktadır. Sınıfları, yöntemleri ve işlevleriyle uygulamalarınızın arka plan işlemlerini kolaylaştırır.
Genel olarak;
* Veri işleme
* Boyutsal küçülme
* Model seçimi
* Regresyon
* Sınıflandırma
* Küme analizi
gibi işlemlerde kullanılıyor olsa da kendi içerisinde sık kullanılan test datalarını dahi barındırmaktadır. Numpy, SciPy ve Matplotlib üzerine oluşturulmuştur. Tamamen açık kaynaklıdır ve geliştirilmeye açıktır. ",514bfdff,0.3333333333333333
15778,6998861ff6ff01,f2a02da3,"# Convert our date columns to datetime
___

Now that we know that our date column isn't being recognized as a date, it's time to convert it so that it *is* recognized as a date. This is called ""parsing dates"" because we're taking in a string and identifying its component parts.

We can pandas what the format of our dates are with a guide called as [""strftime directive"", which you can find more information on at this link](http://strftime.org/). The basic idea is that you need to point out which parts of the date are where and what punctuation is between them. There are [lots of possible parts of a date](http://strftime.org/), but the most common are `%d` for day, `%m` for month, `%y` for a two-digit year and `%Y` for a four digit year.

Some examples:

 * 1/17/07 has the format ""%m/%d/%y""
 * 17-1-2007 has the format ""%d-%m-%Y""
 
 Looking back up at the head of the `date` column in the landslides dataset, we can see that it's in the format ""month/day/two-digit year"", so we can use the same syntax as the first example to parse in our dates: ",ea9e72cf,0.3333333333333333
15779,0c452d3a0b9339,47a1ecee,# Distributions,5d857385,0.3333333333333333
15780,30fdc4a6e3c1db,cec9be9c,"What we see:
* Most of the prices for food products lie between 1 dollars and 10 dollars. As we can see the high peak between 10^0 and 10^1
* Hobbies show a pretty wide range of prices
* Households are costlier than Food",6111ddee,0.3333333333333333
15781,db5a369894fef6,64dafd58,"Interpretation
- Confirmed and active cases are growing exponentially in US, UK, Russia
- The active cases have a downturn (decreasing COVID cases) in Spain, Italy, Australia
- The active curve for Australia is similar to the COVID world count website
https://www.worldometers.info/coronavirus/country/australia/
### Let's examine the active cases more closely




## Examine Active Cases using `plotly`
- Great for interactive graphs (line graphs, contour plots, 3D charts, etc.)
- Also available in R and you can access `plotly`, an online platform for visualizing data offer 
- Examining the nature of active cases

### Simple analysis: 
Which month (Feb, Mar, Apr, May)  does the the maximum value occurs?
- If max occurs in Feb, Mar then early flattening of active curve
- If max occurs Apr then COVID cases are beginning to decrease 
- If max occurs in May then COVID is still a problem

### Advanced analysis (if you're bothered):
- SIR epidemic models
- Polynomial fits 
- Regression prediction
- Use calculus find turning points 
",065aaf61,0.3333333333333333
15782,ddcdecdd6a3b6d,c51e47ce,"#  使用重复元素的网络（VGG）
VGG：通过重复使⽤简单的基础块来构建深度模型。  
Block:数个相同的填充为1、窗口形状为$3\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\times 2$的最大池化层。  
卷积层保持输入的高和宽不变，而池化层则对其减半。
![image.png](attachment:image.png)",90831448,0.3333333333333333
15785,6d29650083cbde,d2541a13,"**3.  RANDOM FOREST**

I use the RandomForestRegressor from the sklearn package to fit my data. ",e65fd993,0.3333333333333333
15786,2b39f4ff896f97,44f91a7e,Transform Image Labels uisng [Scikit Learn](http://scikit-learn.org/)'s LabelBinarizer,3ddfe182,0.3333333333333333
15788,548f961125248d,791c9954,### Set random seed,d8c5e8b8,0.3333333333333333
15793,b9328fe3b0cefc,3e68dcd0,"- Season from 2017 to 2020:
    - Round 1 = days 137/138 (Fri/Sat)
    - Round 2 = days 139/140 (Sun/Mon)
    - Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)
    - Round 4 = days 146/147 (Elite Eight, Sun/Mon)
    - National Seminfinal = day 151 (Fri)
    - National Final = day 153 (Sun)
- Season from 2015 to 2016:
    - Round 1 = days 137/138 (Fri/Sat)
    - Round 2 = days 139/140 (Sun/Mon)
    - Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)
    - Round 4 = days 146/147 (Elite Eight, Sun/Mon)
    - National Seminfinal = day 153 (Sun)
    - National Final = day 155 (Tue)
- Season from 2003 to 2014:
    - Round 1 = days 138/139 (Sat/Sun)
    - Round 2 = days 140/141 (Mon/Tue)
    - Round 3 = days 145/146 (Sweet Sixteen, Sat/Sun)
    - Round 4 = days 147/148 (Elite Eight, Mon/Tue)
    - National Seminfinal = day 153 (Sun)
    - National Final = day 155 (Tue)
- Season from 1998 to 2002:
    - Round 1 = days 137/138 (Fri/Sat)
    - Round 2 = days 139/140 (Sun/Mon)
    - Round 3 = day 145 only (Sweet Sixteen, Sat)
    - Round 4 = day 147 only (Elite Eight, Mon)
    - National Seminfinal = day 151 (Fri)
    - National Final = day 153 (Sun)",3a35eb23,0.3333333333333333
15796,245c89d02f3f5f,0ad854f9,"## Ορισμός παιχνιδιού

Βάλτε εδώ το string που αντιστοιχεί στο παιχνίδι σας. Για την ονοματολογία των περιβαλλόντων:
- v0 vs v4: v0 has repeat_action_probability of 0.25 (meaning 25% of the time the previous action will be used instead of the new action), while v4 has 0 (always follow your issued action)
- Deterministic: a fixed frameskip of 4, while for the env without Deterministic, frameskip is sampled from [2,4]
- There is also NoFrameskip-v4 with no frame skip and no action repeat stochasticity.

Αναλυτικότερα στην εκφώνηση της άσκησης.",61a1eacd,0.3333333333333333
15798,2259c048379b36,40324616,"### By visualizing the two time series you can tell which one is more variable. The answer is time series number two. Are you sure? The graphics are misleading. But how can you tell it in statistical terms?
",43558d11,0.3333333333333333
15799,dbd96dd275dc60,58ce54a6,# Thanks to pandas categories() we now have a way to access all of our data in form of numbers. But we still have a bunch of missing data,1ed493a8,0.3333333333333333
15803,ee9ddc756b2d4a,7ddf817c,# Features extraction,e367eab3,0.3333333333333333
15804,6e472c6c591c7d,26462f9e,"# Exercises

The value in the `indicator_code` column describes what type of data is shown in a given row.  

One interesting indicator code is `SE.XPD.TOTL.GD.ZS`, which corresponds to ""Government expenditure on education as % of GDP (%)"".

### 1) Government expenditure on education

Which countries spend the largest fraction of GDP on education?  

To answer this question, consider only the rows in the dataset corresponding to indicator code `SE.XPD.TOTL.GD.ZS`, and write a query that returns the average value in the `value` column for each country in the dataset between the years 2010-2017 (including 2010 and 2017 in the average). 

Requirements:
- Your results should have the country name rather than the country code. You will have one row for each country.
- The aggregate function for average is **AVG()**.  Use the name `avg_ed_spending_pct` for the column created by this aggregation.
- Order the results so the countries that spend the largest fraction of GDP on education show up first.

In case it's useful to see a sample query, here's a query you saw in the tutorial (using a different dataset):
```
# Query to find out the number of accidents for each day of the week
query = """"""
        SELECT COUNT(consecutive_number) AS num_accidents, 
               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week
        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`
        GROUP BY day_of_week
        ORDER BY num_accidents DESC
        """"""
```",65532a3d,0.3333333333333333
15806,08b5b53e94599f,787b322d,Let's look at the data.,31609f4d,0.3333333333333333
15808,4883314a96dc34,129f3515,### Univariate plots to understand each individual attribute,50d36836,0.3333333333333333
15811,a2444ab5d5f147,fcd31e5a,"### Let's remove punctuation marks as well
### There are multiple ways to do it 
### I am using RegexpTokenizer from nltk
### (Refer) https://www.kite.com/python/answers/how-to-remove-all-punctuation-marks-with-nltk-in-python",10617755,0.3333333333333333
15818,4c55891bcb068d,4431c7b8,# Modeling,01b9cd67,0.3333333333333333
15830,37e461081e47c5,3a3e2d18,"It seems like movies, music, PC games, cinema are all positively correlated. ",b3e6549e,0.3333333333333333
15832,c1984e64b35234,b276e0da,"**Sex**

",1811225b,0.3333333333333333
15833,e16860fce156b0,b873aef7,"#When x is a categorical column, it computes column statistics, and plots a bar chart and pie chart:",2054f1ce,0.3333333333333333
15834,999258a81ba32a,5b1710e8,# Ball 3 Frequency Chart,48cd3d21,0.3333333333333333
15836,de577c910a687d,7f60778e,The target `'claim'` has binary outcomes: `0` for no claim and `1` for claim.,c3ebadbc,0.3333333333333333
15838,91eaec994e0c6f,25a7f9d1,"- We will use <b>pandas.DataFrame.stack</b> function to transform our data.
- The result data should have as columns <b>['d', 'state_id','store_id','cat_id','dept_id','item_id', 'id', 'sales']</b> where ""d"" column has values in <br>[d_1,..., d_1913].
- Initially we have data with 10 stores and 3049 items per store so as a result we have 30490 time series !
- Each time series contains sales data of 1913 days. Final output data will have then 8 columns and 30490*1913 = 58327370 rows !",376aef10,0.3333333333333333
15842,dd3721cb49c1fd,0fc0011b,"<a id='4'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">4. <b>Forecaster</b></h1>
  </div>
  <div style=""color:white;text-align:center"">The core module which performs the forecasting of the values for the given time series</div>
</div>",1a53fdd9,0.3333333333333333
15843,fe7360cddc13e5,b865d271,## Hesaplamalar,8979e423,0.3333333333333333
15845,b3e0b7e9ff6849,62421d7b,"There are null observations in the dataset, especially in the Customer ID column. Since this analysis will be consumer-based, we need to remove these observations from the dataset. Therefore, we will  eliminate these observations in the next step (data preprocessing).",f6e4bb0d,0.3333333333333333
15848,2ada0305b68956,0de533f5,### 55. Palette = 'Reds',133e26f4,0.3342857142857143
15851,fc8e0042411c46,74bce8c8,"- Google and Direct traffic generates maximum number of leads.
- Conversion Rate of reference leads and leads through welingak website is high.",af476c2a,0.335423197492163
15853,5f32117bcd5255,c17b2f72,#### TARGET ACQUISITION PARAMETERS ,85882abf,0.33557046979865773
15854,738bfced935b69,46db3b2f,The relationship between price and mileage with engine size are relationship inverse.,2d3c592d,0.3356164383561644
15855,fdc9f4863744b1,c467a8aa,"There are NaN values in the ""SALE PRICE"" variable, I will replace them with average Sale Price",b4529365,0.3356164383561644
15856,eda49464dd6d1b,928333e3,"## Vintage

* Number of days customer has been associated with the company
* Has flat, even distribution for both total customers and customers who purchased vehicle insurance
* not expected to influence the predictive models",8421f81f,0.3356643356643357
15858,3c2033cc99c12c,21674d22,"**Findings:** *From the barchart and the heatmap V12 & V4 have highest correlation to Class, which conform to the result of heat map.*",dfa22a54,0.3357664233576642
15860,ee23a565163388,4b40896e,## **Does the fluctuation in percentage of blood leaving the heart makes it to fail**,88aacbc4,0.33587786259541985
15862,979f1e99f1b309,be182827,***THE plot showing that most of players didn't get revives and they get killed after they get knocked***,d1bfebbf,0.3360655737704918
15866,c9b4e282e4e2c1,e6390bbc,A)Was the roster position of the player equal to the position of the player in the play?,f44d339f,0.336283185840708
15867,ac1abfe1dfe815,ea33a75e,"The classes are unbalanced, and the negative calss is three times more than the neutral or the positive. There was more than 800 negative tweet about `American Airlines`, even though there's data about it only in the last three days.  
BUT if we considered all the days, `United` has the  highest number of negative tweets overall, 2633 and 492 positive. We can compare it with `Delta` (955 Neg, 544 Pos)   
I think one of the way to handle thi is to seperate the three`American`, `US Airways` and `United`. The other airlines are behaving differently.
",6529dbcb,0.336283185840708
15869,c84925c8171900,153f8b68,"<div class=""alert alert-block alert-info"">
    <span style='font-family:Georgia'>
        <b>Insight: </b><br>
        We have successfully imputed the Year column. Now let's change it to an integer column
    </span>
</div>",e21ff7ec,0.3364485981308411
15871,99bf357eaf61f1,502f5add,"To explore further we will start with the following visualisation methods to analyze the data better:

 - Correlation Heat Map
 - Zoomed Heat Map
 - Pair Plot 
 ",9d92fafe,0.33653846153846156
15876,f91f58d488d4af,68839f79,"In both cases, the distance between our 3 and the ""ideal"" 3 is less than distance between the ""ideal"" 7.

#### Let's calculate these loss functions with in-built PyTorch methods.

* You'll find these inside `torch.nn.functional`.",5df1bbf3,0.3368421052631579
15877,840534f2908a9c,b6c93a35,"*Since we saw above that fare amount is highly skewed, let us take log transformation of the fare amount and plot the distribution.*",8081c3cc,0.3368421052631579
15878,b01ee6cb674fa3,12c17801,"Comming back to the rocket_cost, there are 2 different values that need to be changed
- 5,000
- 1,160",a8ffd35e,0.33695652173913043
15879,312135b445bd23,28276e42,Let's add phrases model to the ETL process and change the clean function:,8ced381f,0.33707865168539325
15880,04ff2af52f147b,b8dedc29,"The previous cell's output provides some useful insight into how the decks are laid out.  However, since the number of cabins per deck is not constant, a proportion is more illustrative.  The following cell calculates the proportion of each passanger class by deck (training data) and then plots it.",d5f37be9,0.33707865168539325
15882,e67925694c07d3,46a5031f,"DAYS_BIRTH looks pretty normal, nothing fishy",83af4c4a,0.33707865168539325
15886,d96642860ab3dd,3dabeb68,data_train['Ticket'] contain Numerical N values more so it must be maintain,98419d48,0.3372093023255814
15891,fdbbd573ba31c2,617b94db,### windmill_body_temperature(°C),f7c28d74,0.3375
15895,241cf32abb22d8,530e5e02,"### Full Set of Features
Before applying the feature selection methods, I assess the perfomance using all descriptive features in the dataset.",47157066,0.33766233766233766
15896,722cd844dfbe8f,490e5c1c,There are only 151 patients whose scans contain the maximum of DCM images out of the 585 at the start. We therefore keep the entire dataset for the moment. Lets have a look to the **test dataset** :,0cedb385,0.33766233766233766
15897,2cb457b60dd246,e001f1d9,### Copy the train images  into aug_dir,339367df,0.33766233766233766
15899,c13f73168789c2,cca4500e,"### 2.1 Select a row by index location<a id='12'></a>
Syntax : `df.iloc[index]`",16175052,0.33766233766233766
15900,75adb7945ef9bd,e0efffca,"Mumbai and Nigeria are still on the top. Other than the strange 'ss', London and New York made the bottom of % of disaster tweets.",785c5095,0.33766233766233766
15902,63b44c85e32c1f,83797070,But if you want to find the **max( )** string element based on the length of the string then another parameter 'key=len' is declared inside the **max( )** and **min( )** function.,fb9b9562,0.33783783783783783
15903,e4525eb0c96f28,a7c37341,"### Genre Totals

Moving on into Genre, we wanted to be able to see which of the genres was the most popular. A new column 'Genre_Totals' was created by totaling the sales of each genre. This information could be used later as a feature in later analysis.

New Column:
- Genre_Totals: Total sales per genre",2093a1f1,0.33783783783783783
15904,62037c5832129c,6679797f,"## Tuning hyperparameters via grid search 
* In machine learning we can tune the parameters of a learning algorithm (for example - depth of a decision tree or regularization parameter in logistic regression. 
* This tuning is **hyperparameter tuning**. 
* Using **GridSearch** you can find the optimal combination of hyper parameters for your model.
* Brute force exhaustive search where you specify a parameter range and you find the model which gets the best result.
* Computationally very expensive
",61474350,0.33783783783783783
15908,631cd434fc3aa2,27f8565d,"* It's the same also for _Alley_, _Fence_, _FireplaceQu_.",2b74febb,0.3380281690140845
15910,06c7ba9203293f,f1dec8a3,"# ECDF on Distance
",1e1a2b48,0.3380281690140845
15915,9cec5ddf8b6f49,7f42d3b6,### Outlier Treatment,d39fc8e7,0.3382352941176471
15916,156bbcff05dcea,91f5a766,# Train Test Split,66ad1fe9,0.3382352941176471
15920,dc0b0e1cb46c6f,63667506,"<a id='2'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;"">2. Data visualization 📊 by Countries</p>

Special thanks to Sharlto (https://www.kaggle.com/dwin183287/covid-19-world-vaccination) for this amazing resume chart.",47b17a7b,0.3382352941176471
15921,e4c6dd957eb5ce,c77f1754,"This chart is amazing. It is enough to tell us a good history of Kaggle growth! <br>
It's interesting to note that:
- Firts peak was on November, 4 of 2011 with 808 user registers. 
- Second peak was on December, 26 of 2013 with 1590 user registers.
- Third peak on May, 23 of 2016 with 1728 registers isn't so evident because that epoch Kaggle already had almost 1000 registration by day;
- After 2017 we can note a clear improvement and consolidation of more than 1k of registers each day. 
- The peak at March, 9 of 2017 with 3931 registers I find that is the day that Google announced the Kaggle acquirement.
- After it we can see that the number of registrations are apparently cyclical, let's explore other informations of user registration. 

Summary: We can see a clearly tendency in Kaggle registers, the number of register is very consistent and achieves easy 3k people registers by day with the highest peak at Apr 2019 with 5764 new registers. <br>
Looking this timeseries of registers I bet that Kaggle will keep receiving so much new data scientists and enthusiasts to the platform. 

<i> OFF: An interesting that I discovered is that I joined at Kaggle 11 days before it gets 1M users! =D </i>",2e383665,0.3382352941176471
15922,eb0ecd6bebeb15,b1e71435,Veri çerçevesindeki sepal.width ve sepal.length değişkenlerinin sürekli olduğunu görüyoruz. Bu iki sürekli veriyi görselleştirmek için önce scatterplot kullanalım.,d7b93a60,0.3382352941176471
15928,3cb96bd8eb364b,f54f671f,## Preprocess Data,3157af7e,0.3384615384615385
15930,a8c042af6b7245,8e374e66,"* A priori in the train data is 3.645%, which is strongly imbalanced.
* From the means we can conclude that for most variables the value is zero in most cases.",2487ac62,0.3384615384615385
15931,03048e86a6d806,2b52a47a,"# Getting to Know Better Your Employers

",1285c231,0.3384615384615385
15932,c115e287523aab,c7c6b93a,# Meta Data,feb1288b,0.3384615384615385
15933,be9597c72542a2,a2c382fa,# Visualization ,6f29c6d8,0.3384615384615385
15934,d07915a6e6992e,b81363c3,"**Embarked**

C = Cherbourg, Q = Queenstown, S = Southampton

Let's explore the variable with Survival rate. Embarked represents port of embarkation. As the analysis output below suggests Emabrked C shows high probabilities of survival.",2b912140,0.3384615384615385
15936,f2f2db16a2f86c,51c585fd,The maximum number of houses are **52** years old.,ffc6a115,0.3384615384615385
15937,fc8e0042411c46,d67eddbf,"**To improve overall lead conversion rate, focus should be on improving lead converion of olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.**",af476c2a,0.3385579937304075
15946,396bc36edb95d3,fc4cb008,#### Checking the dimensions of the training and test data,965e4f8f,0.3388888888888889
15947,f2e5e9fb9eaaf7,a0c7c927,"<a id=""4.1.4""></a>
### 4.1.4 Dealing with missing value (reference)
Some references on how to deal with missing value:
- [Missing Values](https://www.kaggle.com/alexisbcook/missing-values) by [Alexis Cook](https://www.kaggle.com/alexisbcook)
- [Data Cleaning Challenge: Handling missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values) by [Rachael Tatman](https://www.kaggle.com/rtatman)
- [A Guide to Handling Missing values in Python ](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python) by [Parul Pandey](https://www.kaggle.com/parulpandey)

Some models that have capability to handle missing value by default are:
- XGBoost: https://xgboost.readthedocs.io/en/latest/faq.html
- LightGBM: https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html
- Catboost: https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html",048e0d08,0.3389830508474576
15950,2a56d6b0e153f2,05b55970,MARKETING & FINANCE ARE MORE LIKELY TO BE PLACED.,8dc315e6,0.3389830508474576
15952,dac3c8204a2d1b,f2d982c0,# Analysing By Genre,b0d2d0dc,0.3389830508474576
15956,c4bca5d86a38c3,ae5e8ad8,Haciendo drop de la columna Ticket,e23d297c,0.3389830508474576
15957,9169c4e9c33c90,49c83e76,"A vast majority of the books are unique to a single year's Top 50.

Let's see which titles made the top 3 appearance count:",725bf880,0.3389830508474576
15960,ed8009f482b380,ef79b72a,"Using One Hot encoding, we should transform categorical columns such as gender, ever_married, work_type, Residence_type and smoking_status into numerical columns",e99941fa,0.3389830508474576
15963,5ce12be6e7b90e,30028528,"# Conditional statements
## `if` statements

The `if` statement allows us to condition the program flow on its data.",c0ab62dd,0.3391812865497076
15964,df51d4c54fbb91,7a8e6926,We use Keras ImageDataGenerator to artificially increase our training set.,4226dd72,0.3392857142857143
15968,8dd655515e7d18,ccf593ae,### Conscientiousness Analysis,895f41cf,0.3392857142857143
15973,23df07a474aaae,b1ed0a13,"Now we must encode Genre into Numerical value, as Model cannot train having string value",0ea40276,0.33962264150943394
15974,0ad8d416b89b78,46f6130a,"# Gender:

A quick visualisation of the Sex attribute with respect to the proportion of income distributions between each shows the disparity between incomes between the genders. this clear distinction provides a useful attribute for assistance in the classification task.",0b0562f0,0.33962264150943394
15985,917957c6c4065f,990c0c23,description의 길이로 description_length 열을 생성했습니다.,55b8ed68,0.33986928104575165
15986,0687cd5c8597db,6b731947,![image.png](attachment:b76791f2-5484-456d-bc6c-2837661874e5.png),4edec76a,0.34
15991,2ada0305b68956,c75fd3c5,### 56. Palette = 'Reds_r',133e26f4,0.34
15993,4cd25e50c7e007,c6ef6721,**During working day there is observed increase in bike rental**,ceb0c525,0.34
15995,83df814455f06c,c8e35d9f,"We can see that the `doors` and `persons` are categorical in nature. So, I will treat them as categorical variables.",c9cff71a,0.34
15998,8ec771f5600a61,42d55a66,### so from above output it is further clear than min age for a female having Title 'Miss' can be 0.75 and max age can be 64,48364c1f,0.3402061855670103
16001,c7e5f658090347,1e7dfbf9, <a id='Correlations'></a>,43c78e7d,0.3404255319148936
16004,957e035ba5b9d5,1733be3a,"## Visualize training history

Let's display Loss and Accuravy",778ab3d3,0.3404255319148936
16008,3f25b363afec54,efb2a7fd,## Data Visualization,bbdaae25,0.3404255319148936
16009,4c47839b067546,6d95dc52,Цены на автомобили с пробегом в России выросли на 22% с начала 2021 года. https://avtonovostidnya.ru/avtorynok/256226#. Поэтому приведем цены 2021 к ценам 2020:  ,1f517b02,0.3404255319148936
16011,7e89d387feb9f5,e5fc2fe2,## 2. География,989e3a1b,0.34057971014492755
16014,5f4ae633cfd090,0f091927,"The biggest contributors i.e. columns with a greater than 0.25 correlation (in increasing order of importance) are:
* B_td_landed_bout,
* R_td_pct_bout,
* B_sig_str_landed_bout,
* B_tot_str_landed_bout,
* R_tot_str_landed_bout,
* R_pass_bout,
* R_kd_bout,
* B_sig_str_pct_bout,
* B_pass_bout,
* B_ev,
* R_ev,
* R_odds,
* B_odds,
* R_sig_str_pct_bout,
* B_kd_bout",a30a16e2,0.34065934065934067
16016,d1ff7e10ee0102,99df871c,#### Correlation matrix (heatmap style),2cc71c3c,0.3409090909090909
16018,da199f8fb59439,8a30970c,"## We have dealt with all the missing data , lets get started with our data visualization ",baaa665d,0.3409090909090909
16020,73d8e56bc709b1,62b16c5d,# 4. Players in top european teams,78ec3cce,0.3409090909090909
16021,90964081c7faab,eb477f23,"I'm using the same graph as above, but I wanted to zoom into the number of contacts 1-7. Clearly, the chances of the client subscribing to a deposit increases as you have more previous contacts.",b423b0c3,0.3409090909090909
16022,450fda47b03baa,e4b498bb,Veri çerçevesinde hangi öznitelikte kaç adet eksik değer olduğunu gözlemleyelim.,62c04adb,0.3409090909090909
16023,a0b321057e7402,13f40eba,"The metric that makes the most sense to predict would be the weighted prices of bitcoins. Again, I am going to use the hourly resampled dataset from now on and the prediction will be done using that dataset.",5f73fb91,0.3409090909090909
16027,be2f4d8a6b73ca,98d3cf55,**Seperating the data into discrette features and continuous feat**,5d8ce40a,0.3409090909090909
16028,d83e5b44d1b80d,70f2c9c3,"***Top 15 countires data covers almost 71% of overall data set data, so, let us use this as sample data for further analysis***",62845930,0.3409090909090909
16032,513ce405d7f6a3,af2c66dc,# Let's combined SentenceId and Phrase data.,8461e086,0.3411764705882353
16035,74a03887600114,f867217a,As noted here one user has rated one or more than one movie. This means that one movie has been rated by more than one user.,c0ffb2f0,0.34146341463414637
16038,dbccf99c49570f,217a5157,## 177 values out of 891 in train and 86 of 418 in test data ages are missing. This percentage is not small but it's rather not too big either. The NaN values have been replaced with the median of the age values present in the column with respect to the gender of a particular person. This is one of the ways to handle missing data.,c20fc09e,0.34146341463414637
16040,8d70dcae7f40a3,ef39e114,"### **Splitting trainset, testset**",472c71ce,0.34146341463414637
16041,47b2c9be5e31cb,4dce6e12,Distribution graphs (histogram/bar graph) of sampled columns:,7d4afe56,0.34146341463414637
16042,62582b8036fbfe,a9398a19,### Cleaning and Text Processing,6c2160db,0.34146341463414637
16043,8cefb86a675e5d,78e31ce9,**Extract Features and Target ('overall_rating') Values into Separate Dataframes**,79f9e69b,0.34146341463414637
16045,4246295d91a6c1,e76cba11,Finetune,9138104a,0.34146341463414637
16048,514d8de15cb7ef,8d7edf0e,### Checking the number of quality types of wine in training data,cfe111b2,0.34146341463414637
16049,0e09587faffa8f,263585e6,The vehicle registration state with the highest number of summons was **New York**,0d563d61,0.34146341463414637
16053,fc8e0042411c46,68bf12b4,## Do Not Email & Do Not Call,af476c2a,0.34169278996865204
16054,08f845750d026a,0c44cca5,"### Create a pivot table with region and sector as the columns and count as the aggregate function
",1c54de30,0.34177215189873417
16061,6cade0b6a41ba2,71f3b893,## 3.5 Heart Disease,e6110293,0.34210526315789475
16062,c950cff74e51ac,ed728ba1,"Make some category based on price
less than 50 = Cheap
50 - 99 = Medium
more than 99 = Expensive",d59bf323,0.34210526315789475
16071,fe7360cddc13e5,1121bd0e,<font color='red'> **1- Olabilirlik Fonksiyonunun Türetilmesi**,8979e423,0.34210526315789475
16075,726833f92fb87a,d65bf692,"**We can see that customers with job type as 'student' or 'retired' tends to accept the deposit, while 'blue collars' largely refused the deposit.**",7dc5e1b6,0.3422818791946309
16084,fdc3afd309b850,f276b3f8,"<a id=""ach""></a>
## 6.2 Apartament characteristics",966bde38,0.3425925925925926
16092,81712ee7510ac5,9d767ff6,"**How to add element to the list???**
**The answer is use the append method**",c4685e79,0.34285714285714286
16098,f4b603905215b7,a853fb6c,Split the data in training and test set. ,efe1d587,0.34285714285714286
16100,04bac111ffbe9c,115899b8,"##### ALL MISSING VALUES HAVE BEEN HANDLED.
##### QUALITY ISSUES
##### 1. Merge Sibsp and Parch into one column and drop the other two",82576b17,0.34285714285714286
16101,3cea0f929a2035,de1120ba,One can notice that most of the features are right skewed. So we'll make log-transformation on those features. Below we can see the comparison of the data distributions before and after log-transformation.,04cfbade,0.34285714285714286
16102,5d5c9480b5a0a3,89866d31,"There were 216 1st class passengers, 184 2nd class passengers and 491 3rd class passengers",04d82e2d,0.34285714285714286
16105,2730840089c8eb,1660268e,"## String methods

Like `list`, the type `str` has lots of very useful methods. I'll show just a few examples here.",34d27dac,0.34285714285714286
16110,0fa9979b5690e9,a2f90cea,"Aqui é importante perceber a diferença da acurácia entre essa validação (repetindo 5 vezes o experimento) e o bloco de código anterior. Na situação anterior, o modelo de k-NN com k = 1 chegou próximo a 78% de acurácia, enquanto que ao repetir o experimento, a média foi 71,11% com desvio de 8.89%, que é bastante coisa. **Pode-se dizer que é mais confiável afirmar que esse modelo tem acurácia em torno de 71% do que 78%.**",c26eea94,0.34285714285714286
16111,fe6750354fb64f,830758dc,# Days to Double Cases,271741f0,0.34285714285714286
16120,3c2033cc99c12c,19bc357d,#### Features distribution,dfa22a54,0.34306569343065696
16122,52cfd66e9ec908,4228c5b3,"Uh it seems the rasterizer renders rather well the satellite and semantic views, and both in conjunction help one to get a good sense of the positioning of each vehicle in relation to the road. You can easily understand the placement and motion of the vehicles and highway layout in satellite by taking a good look at the semantic view too.",c74adcdf,0.3431372549019608
16123,7cfd96218dd933,7ac31a67,"#### **ATTENTION**
* THE HIGHEST FRP VALUES WERE DETECTED IN JULY 28,29.
* THIS IS THE FIRST 2 DAYS OF START OF WILDFIRE IN TURKEY.",7c34d96c,0.3431372549019608
16124,71b75664517244,a30a1a25,### Manchester United,fc905af5,0.3431372549019608
16125,629f2918807a9b,aa8654f7,"#### Observations:

1. We need to convert ""?? ???"", and other these type of values to missing or others.

2. There are different city names like ""Karachi"", ""KARACHI"" and ""karachi"", we need to add them all to one karachi umbrella. This case is with lahore and islamabad too.

3. Lastly, we will set ""others"" for the cities having very low order rate. ",be56dc84,0.3431372549019608
16126,842547b2def18c,ffb481cf,We can replace many titles with a more common name or classify them as `Rare`.,b8efde6d,0.3431372549019608
16128,21413205980558,d908489c,"* # The balance of people with default record is obviously low, which indicates that their economic situation is not very good;
# 有违约记录的人员盈余明显偏低，说明他们的经济状况不是很好；
* # There are several jobs that are better off financially: retired,technician,blue-collar,self-employed
# 有几种工作在经济上比较好：retired,technician,blue-collar,self-employed
* # Different from the forecast, the level of education does not significantly affect the level of economic balance
# 与预测不同的是，受教育程度对balance水平的影响并不显著",84197de0,0.34328358208955223
16129,e58e68e4eeefe5,97648f5f,# Model Building without Sampling,a87662ce,0.34328358208955223
16130,1a222fee3089d2,7392eb28,## **Passenger title**,59ab8894,0.34328358208955223
16136,c85c94076e9c3a,f711420a,### - Create more variables,3ea0c443,0.34375
16141,93f5423667b9d5,3276073a,"### ↑ first = 4,7,12,17,26,30,34,49,55,67,71,73,81,93,95,98は全てtestになっている",55bdf071,0.34375
16142,2c3a6969252dc0,09817d71,"The result shows obviously the higher university rating is, the GRE and TOEFL scores and respectively hgiher. It make sense both tests  examine English skill, and students with higher scores apply for better Universities.",d30f10ce,0.34375
16145,69130a37583a06,0bda3b51,"### ProductCD Distribution :

ProductCD kind of Product codes when ",65a4de1c,0.34375
16146,51a46d0a7597f5,8c4479f0,"<br/>
<A name=""section1.2"">2. What is the preferred Foot among the players and how does it affect their positioning?</A>",e9e25b17,0.34375
16150,fae5023faa435f,fa167912,Historical data for Bank Nifty (NSEBANK),b37c893b,0.34375
16160,2105f2c5132866,6c7465da,"# Correlating categorical and numerical features

We may also want to correlate categorical features (with non-numeric values) and numeric features. We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric).

Observations.

Higher fare paying passengers had better survival. Confirms our assumption for creating fare ranges.

Port of embarkation correlates with survival rates. Confirms correlating and completing.

Decisions.

Consider banding Fare feature.",bfe8023d,0.3442622950819672
16165,b0c2805cd5c087,9eb293e8,Image youtube.com,0446f327,0.34444444444444444
16171,4ae6a182abac64,241a0029,"- According to this graph, we can notice that womens are more likely to survive.",418676c5,0.3445378151260504
16177,cd10f3afd970b3,94623463,### Let's check 1st file: ../input/boxes_split1.csv,2db3c8e4,0.3448275862068966
16178,bb8f5d7807718b,c9788abc,# 4.Watermarking Images with Matplotlib,181ec286,0.3448275862068966
16183,a1ba5ffd30dbde,a62566d0,- We can see some of the data is normally distributed and most of the attributes are right skewed,48e57546,0.3448275862068966
16187,45921c50ac56fa,133d651b,"# 1.2 Lemmatization of strings
Lemmatizaion is the task of reducing a string to its base form.  
This helps us in groups words that have the same base form or lemma and will provide more meaning in the coming steps.",465973eb,0.3448275862068966
16188,9535bb04ae042c,b0664988,## iv) Train-Test Split,165b6fae,0.3448275862068966
16191,d5f78aa381f58d,e27f4a5c,# Data Preprocessing,d60f358f,0.3448275862068966
16194,fb5c6021d127ef,19c5753d,"You'll want to inspect your data to ensure it looks like what you expect. Run the line below to get a quick view of the data, and feel free to explore it more if you'd like (if you don't know how to do that, the [Pandas micro-course](https://www.kaggle.com/learn/pandas)) might be helpful.",dd05cbd3,0.3448275862068966
16195,84127ade6fde87,c52a9509,Let’s create a tensor that can hold the total number of one-hot-encoded characters for the whole line:,f55d05b6,0.3448275862068966
16198,ee9ddc756b2d4a,6935b302,### Metodo 1: passo le immagini in efficientnet ed estraggo i features vectors,e367eab3,0.3448275862068966
16200,6b54e39f86bdb5,c7adaa80,"## Time to build the model v2

For this second version of my submission of the MNIST competition i will build a CNN similar to the LeNet5. Using tensorflow and Keras API. In the second model, I tweaked a bit the architecture to get better performances. I added dropout layers for example.",198084bc,0.3448275862068966
16206,ef6d1e959a873e,1764606d,## data cleaning,f11a1f43,0.3448275862068966
16207,1750367e54f407,bba38efc,Simply reusing some of the code from [this tutorial](https://www.tensorflow.org/tutorials/images/data_augmentation) to show what our augmentations look like. I add the image previously opened to a batch and pass it through the data augmentation layers.,a8e655b2,0.3448275862068966
16219,c9b4e282e4e2c1,7a413c1c,The attribute Position has these values:,f44d339f,0.34513274336283184
16221,87e94f864d74be,2557ea56,"### Fix ""date_added"" column",294bfe9f,0.34523809523809523
16222,6a80f915608fc2,2e402dd3,### The g features,636938eb,0.34523809523809523
16223,565ad413cd802f,4975006e,Here's the same image viewed with the colors inverted,397b074e,0.34523809523809523
16224,a566b5b7c374e7,a14b1d43,### Correlation of Sleep Metrics with Activities (5+ Hits and Misses),b3dc5545,0.34532374100719426
16227,f0fab078f8533b,b91fd124,"## d. Collecting a list of all directors, actors, genres and countries",bdb5ea32,0.34545454545454546
16233,2ada0305b68956,b602840b,### 57. Palette = 'Set1',133e26f4,0.3457142857142857
16238,6f1481148352e9,3aa5b127,# Period,7cfbdb8f,0.34615384615384615
16241,44f6a002ecd033,3d9c400b,"Above I just wanted to quickly confirm that zero was not being counted as an NaN value or that No's were considered NaN's. As seen with the describe for Loan Amount column the lowest amount that there is, is nine which means that zeros are not apart of our range of values.",70bbe106,0.34615384615384615
16249,af6556ced704f6,e7e735e0,"***Visualize outliers***
   * For visulize to outliers, we use box plot 
   * Outliers is circle shapes in plot",881577c0,0.34615384615384615
16256,1bd6cc83c02681,beb9f19f,# Visualization,17ff92f0,0.34615384615384615
16260,8ddaa0c6c395ec,4f3c4d4e,## Loading a custom Dataset,9fccabdc,0.34615384615384615
16262,d0f6276d5b628c,9b330da3,"After visualizing the average rating we can identify that most of the movies are rated in a range greater than 4 and less than 5.

still there are a number of movies (very less) which have recieved very low consumer rating.",c64f5ce5,0.34615384615384615
16264,09751c520b0616,ce2ca981,<b>Filling 'GarageYrBlt' null values with ('YrSold'-35),a4d0c7e9,0.34615384615384615
16265,897ca904b74a98,365c1bf8,#### Target vs viral variables,c5844ad4,0.34615384615384615
16271,99bf357eaf61f1,0180573d,### Correlation Heat Map,9d92fafe,0.34615384615384615
16273,917957c6c4065f,f7b97577,#### treTime-pubTime,55b8ed68,0.3464052287581699
16274,ce9ed5e2d601d7,910c2a83,Drop features with 0 MI,f58a2f43,0.3464566929133858
16278,e5dd725b8fa422,89f692a9,"we can see only 2 hours are present have no much diffrence in values. it wont give us much insights

now let's try to at understand data by grouping Month and minute",14675d8b,0.3466666666666667
16281,7e1da639035ac5,ce7edc1d,# <a id='8'>8. Racial distribution analysis</a>,120b6c23,0.3466666666666667
16282,67b7354e96113a,2b5b309f,**Imputing the Embarked**,dca94250,0.3466666666666667
16287,ffd1df95ca5289,b9e14e4c,There is not much change w.r.t age,db00c338,0.3469387755102041
16293,f1e162ddd14f11,1e43b1cc,"the pairplot visualizes the correlation table in a form and shows how strongly two variables are related, we'll use heatmap next to make it more visualizing.",cdb2e771,0.3469387755102041
16295,e9b9663777db82,27dde48c,#### 4.Quantitative analysis for Heating feature,648e8507,0.34710743801652894
16296,2f47abddfd1928,2ed4a903,"We can identify how A, B and C and partially D and F are 1st class cabins, and then F and part of the unknown cabins are 2nd class. G is full 3rd class as well as the unknown part of cabin feature.

The most likely is that the 3rd class passengers were assigned to very crowded cabin areas with lack of registration.

So far we can find some value in cabin feature as it helps to distribute the passengers per class in the ship.As expected most of the passenger are in the missing cabin letter, almost none in G and T and then the other categories are more balanced. Although B and C are very predominant.

The fact that we have so little known values in general and also specifically for some of the categories that are less than 5 makes very difficult to fill the missing values with meaningful data.

From now I will keep the other category and check if cabin letter still provides some meaningful information.",ae33cc0b,0.34710743801652894
16299,c01049afb6d307,d171d866,"## Variables 3
**These variables are related to the days of absenteeism.**",d37d3b5d,0.3472222222222222
16301,593d1d3d1df05a,bf02d9f1,> Making the Chars more distinctive,bc682ffe,0.3472222222222222
16302,fdc3afd309b850,02b27e0e,"Now that we got all the address information possibles, we will be able to find the more appropriate values for the Apartments characteristics looking for the median of the addresses, neighborhoods or RAs to fill the null values with a high precision. 
Is important to look for these values in this order, because we can prioritize the median of close apartments.",966bde38,0.3472222222222222
16303,1014e6be391084,f0e35250,Distribution of the Amount as per Class,46f9168f,0.3472222222222222
16307,1294fb4c86f993,ce862338,<b> Converting the numerical columns into float type,4471e513,0.3474576271186441
16309,b61ab8f81dc03d,8e01bad8,"<a id=""predicting_missing_data""></a>
## Predicting missing data using ML",64d05394,0.3475177304964539
16316,0e2a23fbe41ca9,e6519eb7,"Great!! No nulls in the id columns.

## Exploration

### 1. Anonymised measure
numerical_1, numerical_1",64e4762c,0.34782608695652173
16319,cfcb3bdee4f1e4,13b4f1ca,**Comments length**,eb4ed2ae,0.34782608695652173
16322,17a24d566ffa59,babe1636,![](https://s3.amazonaws.com/nlp.practicum/svd_graph.png),89049e56,0.34782608695652173
16324,ea4e559a86d613,a1a8288b,**3. Anaotmy Column**,eff47843,0.34782608695652173
16325,9b5de3823ad5ab,8a05d723,"We can see that we have a lot of classes (1108, to be precise) but the data is mostly balanced, with around 400 instances for each class.",33e48774,0.34782608695652173
16334,90ead00a8ee283,d53a4bb6,"**Exercise 2**: Create the following `DataFrame`:

![](https://i.imgur.com/CHPn7ZF.png)",612efa48,0.34782608695652173
16341,77f958b3f41a70,57a5fc74,"# Results

The topics include virus, cov, rna, protein, antibody, cells, infection which are research-based words in our data.",2ad9bb69,0.34782608695652173
16343,f6c1eb62cceb70,b98c916f,"The data looks good, we can now start choosing our features and target variables",90a1b790,0.34782608695652173
16346,ed5c03987493eb,e0b22a70,"We now have an autoencoder that knows how to reconstruct hand-written digits, even though it have not seen the digit before. However, the reconstructed image will always look the same as the original, making it useless as a generative model.  
Next, we will try to understand the core concept of variational autoencoder by writing one.",bea97744,0.34782608695652173
16348,a1a31459abf078,08bdb867,"As we can see above students who have seen lectures generally do better than students who haven't seen any lectures. When we dive deeper into the number of lectures vs correct answer rate we see that for number of lectures higher than 100 the correct answer rate is much higher than 70%. However such instances could be outliers and could be skewed by a few users due to low data volume. 

## Relationship between Task Containers, Bundle & Part and dependent variable",66fc0f54,0.34782608695652173
16349,fc8e0042411c46,3fd79044,## Total Visits,af476c2a,0.34796238244514105
16359,d96642860ab3dd,4c7f3d0d,## 1.9 Fare and Age Features,98419d48,0.3488372093023256
16360,806ce45c8fa303,7cfd8014,## Split the Dataset,3e5c34dc,0.3488372093023256
16365,22bd95f4807a23,1cf8cdbf,## How many individuals are there in each age group?,c05d356f,0.3488372093023256
16369,2e40928927c0d4,f0733e56,**Showing randomly chosen Severe DR image one at a time** ,b6385ef2,0.3488372093023256
16370,726833f92fb87a,a6f9641e,## Marital Status vs campaign success,7dc5e1b6,0.348993288590604
16371,5f32117bcd5255,4fa8b11d,### HST OPTICAL TWO,85882abf,0.348993288590604
16373,510b8303776bb6,2c393b82,In order to view the detailed plot of any just replace x with the column of choice in the line 2 of box below.,18080db8,0.3490566037735849
16375,8985a124d4b657,a4f92a0a,There are 86 null values and we have to get rid of them as below.,586d1846,0.3492063492063492
16379,738bfced935b69,4d2885d6,"Hybrid and Other are higher a car's MPG with lower mileage, Petrol and Diesel lower a car's MPG with higher mileage.",2d3c592d,0.3493150684931507
16380,fdc9f4863744b1,ee241bcb,"There are 26054 missing data for ""Land Square Feet"", 27385 for ""Gross Square Feet"" and 14176 missing values for ""SALE PRICE"". I will;
<ul>
    <li> Avg each missing data variable</li>
    <li> Replace the missing dat value with the avg</li>",b4529365,0.3493150684931507
16381,4daf6153275cbf,3ec1734b,"Below, I renamed 1st most common things to locals, because most of the countries has the 1st ones as locals except for Germany, and Belgium, which I manually handled. **One other assumption is United Kingdom's local cuisine is ""Bar"".**",51db1961,0.3493975903614458
16382,835a7b4e660d23,ad9c18d8,### User Defined Function,53bc7a6e,0.3493975903614458
16386,e19e307b3fd188,01d138fc,"The number of bathrooms usually varies between 1 and 6, and we noticed that the more bathrooms, the higher the rent, which is already expected. The value of 9 bathrooms are strange...",2173955b,0.34959349593495936
16387,eda49464dd6d1b,b4a56c52,"## Region and Sales Channel
* Response rate varies from about 4% to 19% with region
* Response rate varies from about 2% to 19% among the major sales channels, and 0% to 100% for minor ones
This should help the model a great deal to predict a buyer.",8421f81f,0.34965034965034963
16389,ad121e0531afa4,9c569cc1,"To login to W&B, you can use below snippet.

```python
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()
wb_key = user_secrets.get_secret(""WANDB_API_KEY"")

wandb.login(key=wb_key)
```
Make sure you have your W&B key stored as `WANDB_API_KEY` under Add-ons -> Secrets

You can view [this](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases) notebook to learn more about W&B tracking.

If you don't want to login to W&B, the kernel will still work and log everything to W&B in anonymous mode.",a3492905,0.35
16390,712198370d5521,d27984f2,Now that we have some new features let's have a look at the data's stats. ,5882e04c,0.35
16391,312d2a3c7547f1,1f303cf3,"Based upon Pclass and Age , I am going to replace the NaN values Age.",8fd1efcd,0.35
16393,5ffe6aa38958a1,255d3658,Survival rate of females seems to be higher as expected from the correlation plot,11f5412e,0.35
16394,f6488772605bb5,91fcb03d,### Data Loaders,068d4697,0.35
16397,bc058fe14d3d1b,6d7f6046,> ### 1.3 Feature Creation,d0273670,0.35
16401,fdbbd573ba31c2,6bded1fc,### wind_direction(°),f7c28d74,0.35
16408,1011899b959f44,86e8308b,"# Select
Since we've taken a closer look at our Dataset now, we can select and locate specific data within the larger set. The methods for select are:
* .iloc[] - selecting by row numbers
* .loc[] - selecting by labels
* conditional - specifying conditions within the selection []
",0b112382,0.35
16410,83df814455f06c,ddab4dfe,"### Summary of variables


- There are 7 variables in the dataset. All the variables are of categorical data type.


- These are given by `buying`, `maint`, `doors`, `persons`, `lug_boot`, `safety` and `class`.


- `class` is the target variable.",c9cff71a,0.35
16413,09bac0c221388e,184c3d12,"We have plenty of summarization algorithms today. Assessing which one is the best is a chance hit, though. First of all, there is no clear consensus on which metrics to utilize to evaluate these systems. Moreover, the best summarization technique is highly dependent on the domain and the type of text you are intrigued with summarizing.",bea4aa2e,0.35
16414,cf4d1c1ad1476c,6f11d9e9,## Test Data,768c1a59,0.35
16416,8bb432d338a70b,03f7b2d9,Random Forest,7aab1dfd,0.35
16420,62487bcd70b199,299f8fd2,"## Inference:

There seems to be a better cutoff of mortgage greater than 300 and income greater than 100, we can target these customers for loans campaign",f6ae50af,0.35
16423,63d0d9b9a8c7d2,09c74c7d,"**Excluding ""PassengerID**",e32e5933,0.35
16424,5ba4207c371899,3801f5e9,"Data profiling
First is a simple table of attack by protocol. In network traffic analysis protocol is a simple tool to create some initial buckets to categorize our data. 'normal' is left in the set at this point as a benchmark.",187b1451,0.35
16425,4cd25e50c7e007,ca07f565,"**Weathersit**
* 1: Clear, Few clouds, Partly cloudy
* 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
* 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
* 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog",ceb0c525,0.35
16428,254cccd5145725,f05030fd,The Household size and how it affected the poverty level of a household was also examined. There is a feature called “overcrowding” which is basically depicts high person per room ratio. This feature was plotted against the Target variable and the resulting plot established the fact that larger the household size the more susceptible it is to poverty.,a49b4037,0.35
16430,b10bd75889dad9,c28f7065,### Feature Engineering,ee00ceee,0.35
16431,f0faf9e7ac5abd,6d4263b5,daysoftheyear.com,794edb83,0.35
16432,3dd4294f903768,4d724aba,We can see that in this column the data is in a normal distribution.,0d89d098,0.35
16435,9f0ccf5b9e8f03,6122a6db,"#Once again thanks to rossinEndrew his SHAP VALUES Visualization.
https://www.kaggle.com/endrewrossin/fast-initial-lightgbm-model-to-detect-exam-result/comments",66691203,0.35
16437,c80939c7c626cf,bbeab188,# This chart cannot specify any data because other titles may be maleor female,b9ac31e2,0.35036496350364965
16438,3c2033cc99c12c,8ea30306,"In this part, I will plot the density graph of different fatures and compare them between different classes.",dfa22a54,0.35036496350364965
16451,75adb7945ef9bd,e7a02c18,"## 4. Spoiler Alert!

(Spoiler alert) You Can Get Perfect Score in (public) Leader Board!

You will see in the public leaderboard, many participants got a perfect score. It is because the whole dataset with label is available online (one copy available [on Kaggle](https://www.kaggle.com/jannesklaas/disasters-on-social-media)). You can find the correct label of our test set so you can achieve perfect score.

In such case, the ranking on public leaderboard is meaningless. The good news is, you can now focus on learning NLP and modelling skills with this dataset, instead of fighting for higher position on the leaderboard!

(Reference: [szelee's notebook](https://www.kaggle.com/szelee/a-real-disaster-leaked-label/notebook))",785c5095,0.35064935064935066
16456,c3498779cda661,03779325,"Dada  la comparación de cantidad de datos por clase, las clases predichas no son exactamente las mismas que las originales.",0f531b65,0.3508771929824561
16458,30fdc4a6e3c1db,b9beebdd,"What we get:
* We see that household items specially HOUSEHOLD_2 department has shown the maximum price changes specially in Wisconsin.
* The price changed the most for HOUSEHOLD_2_406 item in WI_3 store where the min price was just  \$3.26 and had rised 32 times to  \$107",6111ddee,0.3508771929824561
16460,fe7360cddc13e5,65133228,"t1 işleminin likelihood'u (olabilitesi), üstel likelihood bileşenine göre hesaplanır. alfa*e^(-alfa*t1)

t2 işleminin likelihood'u, ilkinden farklı olarak, t1 noktasında pasifleşmeme(hayatta kalma) olasılığıyla çarpımıdır.

tx işleminin likelihood'u ise, üstel likelihood bileşeniyle, t(x-1) notasında hayatta kalma olasığının çarpımıdır.

tx ile T arasındaki süreçte hiç işlem yapmamasının olabilirliği ise, pasifleşme olasılığı(p) + aktif kalıp hiç işlem yapmaması olasılığına eşittir.

Bu şekilde tüm müşterilerin belirli bir dönem aralığındaki olabilirlik fonksiyonları aşağıdaki gibi yazılabilir.",8979e423,0.3508771929824561
16461,3fb15e6e48aec2,4ed640c3,## Family Size and lone traveller,9d1f4358,0.3508771929824561
16465,9e27af2600925c,f923af2c,"**Expected Output**: 

<table>
  <tr>
    <td>**sigmoid([0, 2])**</td>
    <td> [ 0.5         0.88079708]</td> 
  </tr>
</table>",9b556435,0.3508771929824561
16472,73893f0467d5e3,afad1264,## For mean_area,279787c6,0.35106382978723405
16474,ee23a565163388,ef36e34c,"**Inference**
- The ejection percentage of blood in patients who suffered heart failure is more than others.",88aacbc4,0.3511450381679389
16486,63b44c85e32c1f,52abca24,"But even 'Water' has length 5. **max()** or **min()** function returns the first element when there are two or more elements with the same length.

Any other built in function can be used or lambda function (will be discussed later) in place of len.

A string can be converted into a list by using the **list()** function.",fb9b9562,0.35135135135135137
16487,ac9b48d531bad9,7b221ab9,Spliting Dataset into Training and Testing,95965e35,0.35135135135135137
16488,8276973853faa1,fa49bdfd,# Veg consumption states ,88da542b,0.35135135135135137
16494,2ada0305b68956,23c96d3f,### 58. Palette = 'Set1_r',133e26f4,0.3514285714285714
16496,0932046e1f485d,38efd93b,Events score the highest mean rating while dating apps perform the worst.,218cc7a3,0.3515625
16497,5f4ae633cfd090,3e7a711d,"Since none of the bout variables have any information on them, I'll have to do some inspection and visualization and see if I can find out the meaning of these.",a30a16e2,0.3516483516483517
16499,fdc3afd309b850,d6aec44b,"<a id=""rnv""></a>
### 6.2.1 Room Null Values
",966bde38,0.35185185185185186
16504,ac04ba639d1c93,24b859fe,"* Results:
* mol_type 1JHC (mean) 94.976 (std) 18.277: 
* mol_type 1JHN (mean) 47.480 (std) 10.922: 
* mol_type 2JHC (mean) -0.271 (std) 4.524: 
* mol_type 2JHH (mean) -10.287 (std) 3.980: 
* mol_type 3JHH (mean) 4.771 (std) 3.705: 
* mol_type 2JHN (mean) 3.125 (std) 3.673: 
* mol_type 3JHC (mean) 3.688 (std) 3.071: 
* mol_type 3JHN (mean) 0.991 (std) 1.315: ",748059d5,0.35185185185185186
16506,ab6da5994949a3,0bc5ce7a,## Classification Report,fae6b91d,0.35185185185185186
16516,7454fdc444df16,02c94652,### Non-Cancerous Patches,a7818ef5,0.3523809523809524
16518,7a058705183598,22412216,Assigning X feature columns & y - Target column,b0ead917,0.3523809523809524
16521,979f1e99f1b309,f4b92768,***THE plot show that most of players didn't ride viechals too much and they tend to walk***,d1bfebbf,0.3524590163934426
16528,50fcff6c0425fa,888134c5,# Using tags as one hot encoded feature: [incomplete],8d355348,0.35294117647058826
16530,99821bc6a45be6,6e630fae,"Here the files are copied to train and validation directories. This is necessary for the data generators. 
Additionally, all of the files are preprocessed while copying them. The datagenerators do have the ability to pass a preprocessing funtion, however it is faster to only do it once instead of on each epoch for each model so we do it here. Furthermore, one of the models applies vertical flips which would be incompatible with the preprocessing funtion's ability to find the shoulders.  ",b9d59346,0.35294117647058826
16532,513ce405d7f6a3,b72c7449,# EDA,8461e086,0.35294117647058826
16537,907f08f9a2c6cf,d4c2522d,### Define the Models,aa84c325,0.35294117647058826
16538,523123dad03177,086ae031,**Quite educated parents but very few parents have higher education. Cause it's boring!**,48a5e4e6,0.35294117647058826
16542,36d0d4cb9c7993,ef632574,## Formation of Model and helper functions,34b93e27,0.35294117647058826
16546,b779c3ce7b671a,56ed55f5,"# Visualisation

Now to put it all together and visualise a made up solar system.

## The star

Let us start by defining the **star** in the origin$^1$ of a space. Our planet $p$ will orbit the edge of space, starting at $(2, 2)$.

$1$ (Pertubated by an epsilon $\epsilon$ for plotting.)",ca778770,0.35294117647058826
16547,7f74a04ae75792,71d0f9e9,"### Insight
",d01e91da,0.35294117647058826
16550,c3dfa835621ac4,02bfef6f,"# Implementation of Filters

Now lets create a variety of kinds of filters. Here we got

- `I`: `FixedFilter`
- `H`: `HistFilter`
- `HN`: `NeighHistFilter`
- `HD`: `DirectNeighHistFilter`
- Rotations and flips: `TransFilter` working with a bunch of rotation and flip functions

Note the values in the filter and state is represented as string even it's a number in the dataset, i.e., it's string `""1""` instead of integer `1`.",0126bdad,0.35294117647058826
16552,02b7e38902069e,82398ee9,Notice that each word is denoted by an embedding of 400 dimensions.,726a03a0,0.35294117647058826
16553,16ca1123840e9f,7fedd914,## Status of the Rockets,e8b8f086,0.35294117647058826
16554,869a39a3d4dea2,3e9ffcd8,"## Image Processing <a id=""image_processing""></a>",9020daf8,0.35294117647058826
16566,e4c6dd957eb5ce,4db4a770,"## The Million Days
One interesting thing that I see is that:
- Jun, 11 of 2017 was the day that Kaggle achieved 1 Million registers. 
- Aug, 26 of 2018 achieved 2 Million Registers.
- May, 19 of 2019 achieved 3 Million Registers.

Cool! We are more than 3 million people all over the world. =D",2e383665,0.35294117647058826
16567,d0080e3a39bc5c,24f835f0,![Transformation 1](https://i.imgur.com/kzfHhVl.jpg),2fcde4cf,0.35294117647058826
16570,395ed8e0b4fd17,4ac70130,### Training Data Distribution among differnet Assets (Crypto Currencies),7573ea31,0.35294117647058826
16572,8f50c9c16db95f,6a0c518b,"# Gained yards by kicking teams <a id=""gainedyards""></a>

How to evaluate a kickoff is good or not? Considering the ultimate goal is to take the ball towards the end zone of the opponent side, one relevant metric could be how much yards the kickoff team gained after the play. In other words, it is the net yards gained by the kicking team, including penalty yardage, which is documented as the **playResult** in the **Play** data dataset.

On the contrary, for the receiving team, the goal of them is to limit the gained yard of the kick team as small as possible, or we can say, they should return the ball as further as they can once they possess it. 

If we break down the performance by kickoff outcome, we'll see the **Return** is the most exciting one due to its wide range of potential yards the kickoff team can gain. Unlike the other types of outcome, a kicking team could possibly lose yards in the play which makes the game a bit risky. But on the other hand, they could also gain yards over 40 yards in the return.",26cc763a,0.35294117647058826
16573,a0a5baa6c7e12a,14b1b751,"When we review the associations between the feature variables (factored by the distribution of the target class labels in the training set observations), we see *Elevation* to separate the target class labels quite solidly, in the interactions with other feature variables.",551d41de,0.35294117647058826
16575,32ddc45133f77b,4d9260f8,**Building the model**,3c0d6831,0.35294117647058826
16577,4ae6a182abac64,791b3a5f,* **PARCH feature vs Survived feature**,418676c5,0.35294117647058826
16578,b10bd75889dad9,a58adb9a,#### get total amount spend by customers in the 6th and 7th month,ee00ceee,0.35333333333333333
16581,ba4b3bd184acbb,f361f136,"When we try to cast the `Reviews` column to a float, we can see that `3.0M` is a value in the column.

For the `Price` column, the first non-float value we find is `$4.99`.

This is important to know and we will resolve these values before casting the columns.

Before we start the cleaning stage, we will look for columns with a low number of unique values so we can casting them to the `category` datatype. 

The `category` datatype is not always the right choice, but if used correctly it can reduce memory usage, increase performance and allow for custom sorting.",0f5de724,0.3533834586466165
16583,dbd96dd275dc60,2c984d89,## Save df_temp to a new csv ,1ed493a8,0.35353535353535354
16584,0b01138ad120fc,27e9b4af,"**OK! It works!  
Now, I will apply it on another sample.**",0b4b72e6,0.35365853658536583
16586,fd4017c1514157,09d3ef8c,Total 2129 authors are there.,fd8f0896,0.35365853658536583
16588,03048e86a6d806,7a85da25,"When pursuing an employment in data-related job, one might wonder about the expected compensation as data professionals. Not only that, but information about the state of machine learning/data science application of the prospective companies also can be insightful for some job seekers.",1285c231,0.35384615384615387
16589,a4f8ad33c823c5,90a95bed,"From the above correlation plot, bmi and weight are highly correlated and height and weight are slightly correlated with each other.",fcd48307,0.35384615384615387
16590,1645979263c148,07d082e5,"## Data Preprocessing
1. deal with unwanted text.
2. deal with missing values
3. histogram
4. heatmap
5. pivot_table",fa11663e,0.35384615384615387
16595,3cb96bd8eb364b,cf2d7687,### Data Clean,3157af7e,0.35384615384615387
16597,2d75fd881827b8,aa97316f,**Pipeline**,107b5299,0.35384615384615387
16598,a8c042af6b7245,60a0c824,"### Handling imbalanced classes

As we mentioned above the proportion of records with target=1 is far less than target=0. This can lead to a model that has great accuracy but does have any added value in practice. Two possible strategies to deal with this problem are:

* oversampling records with target=1
* undersampling records with target=0

There are many more strategies of course and MachineLearningMastery.com gives a nice overview. As we have a rather large training set, we can go for undersampling.",2487ac62,0.35384615384615387
16599,a5a419dc7245b0,4a03c3e7,### Feature Engineering,4279726e,0.35398230088495575
16604,95656e8d666b16,60c6508c,Splitting the data first,65e88599,0.3541666666666667
16606,64a336ac34d95c,05ca61a3,## Churn vs Service Type,be73a990,0.3541666666666667
16611,3b5903412fe741,b0b598c5,"Or, to select just the second and third entries, we would do:",ad231969,0.3541666666666667
16613,e82462cdc998a7,93426ed3,"### 4.3 PCA features<a class=""anchor"" id=""4.3""></a>",b39bf244,0.3541666666666667
16620,b61ab8f81dc03d,d2b5e5af,"Now we can solve the **""Age""** column, methods like: mean, median or imputation for this case are not the better approach, so we can use ML to predict the ages value missing.
We have to apply the predictions on train and test data.",64d05394,0.3546099290780142
16621,957e035ba5b9d5,e3ff05db,## Evalute test data,778ab3d3,0.3546099290780142
16625,ad26c020235dfc,1ae4f485,There are 187 values with a target value equal to 0. We want to predict this values and use them as test data.,bf766e48,0.3548387096774194
16629,f15eac23fbcc9d,db24ea0f,"Bindly fit RF and see how good we can do, without even considering valication set and overfitting. ",ea46d8af,0.3548387096774194
16630,9ceb7278784462,886cbd69," ## <a id='14'> 10.1 Local Outlier Factor</a>

![](http://upload.wikimedia.org/wikipedia/commons/4/4e/LOF-idea.svg)
*  The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially lower density than their neighbors. These are considered to be outliers.",3768a567,0.3548387096774194
16633,e8c6480a3122b3,3d158ddd,Most of the passengers embarked from port S. ,40dc4cca,0.3548387096774194
16634,7dec6bdea6d779,f544b9ec,focal loss taken from [this Kernel from KeepLearning](https://www.kaggle.com/mathormad/resnet50-v2-keras-focal-loss-mix-up),18be5949,0.3548387096774194
16644,0d9a2067267ba1,d865b45a,"### Discrete variables
Discrete features are varibles that show a finite number of values",abc194fb,0.3548387096774194
16645,0cb456a5456cf9,25f18052,# **Q3**<br> **What are average occupation time for both?** <br> 两家酒店的平均入住时间？,5701729c,0.3548387096774194
16648,098fedfcd07456,692d18a0,"# Sclaing down the Image in 0 -1 the max value of image is 255 so dividing it by 255
1. The output for Train and Test is would be between 0 -1 
1. The step is not applicable for ytest or ytrain as they just contains the label",052ece26,0.3548387096774194
16652,c84925c8171900,a2dfd489,"<a id=""pubimpute""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            4.2 Publisher Imputation
            </span>   
        </font>    
</h3>",e21ff7ec,0.35514018691588783
16653,52ee792e228d54,bfcdbf03,#### 1 zipcode is having only 4 digit which probably is a typo error. We will change it to 5 digits by appending a 0 to maintain the geographic area of atleast first 4 digits. ,5096094e,0.35526315789473684
16660,49ac6594c8f5cf,979d96aa,"As degree percentage increases salary generally increases.
we will see if there is any such relationship with high school and secondary school percentages.",6f19f28a,0.35555555555555557
16663,5be39e4e35cec7,d2641d58,"<a id = ""5""></a><br>
## Numerical Variable ",14d617c9,0.35555555555555557
16666,d905cde3391d2b,3d7a3bab,"To find median,
* Sort the data from smallest to largest (ascending order)
* If there are **odd** number of data points, median is the *middle* data point.
* If there are **even** number of data points, median is the *average of two* middle data points

```
[5, 14, 15, 15, 17, 21, 27, 35, 51, 97]
                ^^  ^^  
           (middle numbers)
                                  
Median = (17 + 21)/2 = 19
```

Let's verify,",067dba39,0.35555555555555557
16667,d77e6d61ad2e8b,9ceba98e,# Tune a Model,03fd0e96,0.35555555555555557
16671,7341f069d9b2ee,c0a1a22b,Visualizing Geographical Data,e0a49e62,0.35555555555555557
16673,3597174a998d4d,ff6f0ee9,"Thus, I construct a new variable named family.",276892ed,0.35555555555555557
16675,396bc36edb95d3,2c9d6070,#### Decision Tree Classifier Model,965e4f8f,0.35555555555555557
16677,5f32117bcd5255,b4acd761,#### TARGET INFORMATIONS,85882abf,0.35570469798657717
16680,44f6a002ecd033,f452adef,#### Cleaning Numeric Variables,70bbe106,0.3557692307692308
16682,bb0905d33ae417,02e62f96,# Model training,25fd1965,0.3559322033898305
16685,149cb8d3489224,7f73606b,### User name,116858e7,0.3559322033898305
16686,f2e5e9fb9eaaf7,c186080d,"[back to top](#table-of-contents)
<a id=""4.2""></a>
## 4.2 Distribution
Showing distribution on each feature that are available in train and test dataset. As there are 118 features, it will be broken down into 25 features for each sections. `Yellow` represents train dataset while `pink` will represent test dataset

**Observations:**
- All features distribution on train and test dataset are almost similar.

### 4.2.1 Features f1 - f25",048e0d08,0.3559322033898305
16691,a44368590e878a,13368d3a,### Infection reason,77743ba8,0.3559322033898305
16695,738bfced935b69,6b8d69d6,"* MPG, or miles per gallon, is the distance, measured in miles, that a car can travel per gallon of fuel. MPG is also the primary measurement of a car's fuel efficiency: The higher a car's MPG, the more fuel efficient it is. 
",2d3c592d,0.3561643835616438
16698,3d08ca7656dec0,cb6c97fa,# trtbps,bd3f87e3,0.3561643835616438
16704,d5f78aa381f58d,73f0f1b0,"Before providing the dataset to any model, it is essential to check outliers and transform it so that its distribution will have a mean of 0 and a standard deviation of 1.",d60f358f,0.3563218390804598
16713,b211c8c107f56d,c605b37d,"## Define a grid and train  <a name=""step2""></a>

With the RandomDiscrete strategy you test *n_models*, with parameters randomly chosen from the grid. 

2500 trees should be enough here to reach the early stopping *AUC* criteria on the validation frame.",805b90f3,0.35714285714285715
16718,0dd3ac2d55efd7,78714396,"We can use these word embeddings and fit any classification model to get predictions. If not spacy we have glove embeddings uploaded in kaggle as datasets which can be loaded in directly and be used for predctions with any classification model. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. To get sentence level embeddings we can directly average all the word embeddings of words in that particular sentence so that we can use these for predictions by a classifier model.",e9aa2cc2,0.35714285714285715
16719,b290039151fb39,af916b69,## Model,1836a79c,0.35714285714285715
16721,f4514ec092a771,6aaf816f,Create utt2spk file with format: utterance_id -> speaker,3739ab1e,0.35714285714285715
16728,b7298d6aaff625,2e2c9f86,#### Splitting Data in Train & Test Sets ,bdf24bf7,0.35714285714285715
16730,ffdb3fe29f4755,ea763421,"Firstly, let's show price of goold changed by time.",019b2e69,0.35714285714285715
16732,38b79494ac749e,279b24d9,### Fitting graph,39162a40,0.35714285714285715
16742,92e9fc3a0ff5c0,05d654f0,## **Finding polarity using TextBlob**,d53da425,0.35714285714285715
16745,400bbcc496138f,b8946acd,Predcictions with a lower threshold,191b86b8,0.35714285714285715
16746,67efe818cb2372,fe2f02bb,"The summary above tells us a lot about what the net is going to do. It comprises 4 kinds of layers.

1. Conv2D : this layer is a 'features finder'. It seeks translation invarient features in the input. Using the first layer as an example; it takes an input tensor (in the first layer this is a tensor of size 28x28x1) and passes a sliding window (3x3 in the first layer) over it, discovering features. The output is a new tensor of size 26x26x30. The number of filters (channels) 30 is indicitive of the fact that we have created 30 'views' of the input data showing the existence of particular features. Both the input and output tensors can be described as 'feature maps'. The X,Y size of the output feature map is smaller (28 -> 26) because not all (the outermost) pixels can be validly represented in the sliding windows. (See Keras docs for details {ref here}).
2. MaxPooling2D : this layer is an aggressive downscaling of the input tensor. It slides a 2x2 window over its input tensor with no overlap looking for max val. What we get is a new tensor with exactly half X,Y dimentions and the same number of  filters (channels). In our first MaxPooling2D layer we take a 26x26x30 input tensor and out put a new representation; a 13x13x30 tensor. It's all about taking what we've found so far and reducing its dimentionality.
3. Flatten : What we've got so far is OK but at some point we need to make a decision about things. The decision is going to be made by some feed-forward, back-propogation (of error) layers. These (Dense) layers take a 1D tensor (vector) as input and this is the job of the Flatten layer. In this case a 3x3x64 tensor is simply turned into a vector comprising 576 elements.
4. Dense : the 3 Dense layers take this 576 input vector and output a 10 element vector which can be used as input against an error function.

Let's just run the model, I'm setting GPU on! 30 epocs will run in under 3 mins on GPUs. On CPUs the same run would take much longer",f28a2a34,0.35714285714285715
16751,2d40f383473fa4,3089c59b,And let's check the `Fare` distribution to decide how to fill `NaN` values.,1da1eff0,0.35714285714285715
16756,76d94f2011a1cb,34fc3fd7,## Training Densenet,9820aca8,0.35714285714285715
16759,1084376bc4897c,9270d410,"#### There are three features that strongly correlate with the output: age of the patient,  maximum heart rate and old peak.  We can thus draw three conclusions. First, people who are younger than 50 have much higher risk of getting a heart attack compared to older people.  People around 40 are particularly vulnerable. Second, maximum heart rate beyond 150 is a strong indicator of heart attacks. Third, small values (smaller than one) of old peak are worrisome. 
#### We draw some joint plots to further confirm our observation about these features. We can see sharp slope in the figures below.",1b598487,0.35714285714285715
16763,c54ea4523bd49c,4b2400c8,"Since we use binary_crossentropy, the y category data just needs to be made as floats",097ccba2,0.35714285714285715
16775,e78e7edae89049,a9f086f9,# Mean value plot,9cef1d94,0.35714285714285715
16776,2ada0305b68956,69180f51,### 59. Palette = 'Set2',133e26f4,0.35714285714285715
16780,916ccf243827f1,091b1dd3,## 5. Defining the Neural Network,5147f4d2,0.35714285714285715
16781,a76e0e8770b7a0,ce3bd3b7,1992 and 2015 and 2016 is very bad years.,02863d3b,0.35714285714285715
16785,6e9b4020644836,91ae2811, ####  Summarizing Data,5ad41fc6,0.35714285714285715
16788,1fac5edd4063ba,79c59ff2,"#Grace Hopper – Computer programming

In 1944, US born Grace Hopper and Howard Aiken designed Harvard’s Mark I computer, a five-tonne, room-sized machine. Hopper invented the compiler that translated written language into computer code and coined the terms “bug” and “debugging” when she had to remove moths from the device (who knew?!). Now, close your eyes, and try to think what the world would be like without the invention of programming.http://www.takepart.com/article/2015/06/14/10-female-inventors-you-need-to-know",04bc01e0,0.35714285714285715
16794,c80939c7c626cf,f556878d,# Now we should delete unnecessary data,b9ac31e2,0.35766423357664234
16796,e19e307b3fd188,35f858ec,### Parking spaces,2173955b,0.35772357723577236
16797,bd380b97b5c894,a4c8a176,The age will be restored for the groups Sex_female_target0 and Sex_male_target0,66f2562a,0.3577981651376147
16798,840534f2908a9c,a2780ae6,**Distribution of Pickup and Dropoff**,8081c3cc,0.35789473684210527
16799,f91f58d488d4af,51fcc527,#### Our Simple Model gave us the right prediction in this case,5df1bbf3,0.35789473684210527
16801,faa8e6c8ab9246,2298f290, There are no null values in test dataset,2bea1419,0.35802469135802467
16804,21413205980558,95569af4,# 3.3  Marital,84197de0,0.3582089552238806
16810,37b09262279764,5285ae42,### Converting categorical features to numerical features,37c4c417,0.35833333333333334
16811,62487bcd70b199,477343eb,## <a id='4.7.'>4.7. CCAvg and Family in Personal Loan</a>,f6ae50af,0.35833333333333334
16813,4dd47072617594,f0f14b65,# 3. Sentiment Analysis,44ff1d11,0.3584905660377358
16820,614ba9f0c62677,489c5062,"<a id=""3""></a>
## Train Test Split
* We split the data into train and test sets.
* test size is 10%.
* train size is 90%.",b8551335,0.3584905660377358
16823,ee23a565163388,543ff0eb,## **Are BP patients more prone to heart failure**,88aacbc4,0.35877862595419846
16826,c8c4705cca1ebb,dbbca6d9,if item_price < 0 than equal to Zero,6d9d7107,0.358974358974359
16828,9b42412e75d640,be1a88ff,"# Part 2 

# Feature selection ",b616570a,0.358974358974359
16829,4bbe953f82d29b,f8d75673,"Мы можем исследовать данные за 1796 дней и 23 часа. Это чуть менее 5 лет. Хорошие данные для учебного примера.  
Наблюдения в примере приведены в виде суммы затраченной электроэнергии за каждый час.",772301f2,0.358974358974359
16835,9eed0fae1c7958,52a9a241,# test data,3fb1438e,0.358974358974359
16836,80ad12f326ab70,368481f0,* ### Engagements data,da404a16,0.358974358974359
16838,4d91e84c564cbe,93c3288a,"## List functions

Python has several useful functions for working with lists.

`len` gives the length of a list:",355a43e3,0.358974358974359
16841,34fff8ce731b03,b9d729e4,"Os valores nulos das colunas numéricas são substituídos por zero.
As colunas ""city"", ""state"", ""cnae_id"" são transformadas para valores numéricos utilizando a função [get_dummies](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.get_dummies.html) do Pandas. Essa Engenharia de Features é importante para que o classificador funcione corretamente.",6f9e5b2e,0.358974358974359
16844,98a6794067932a,cb134455,"La suite des trois cellules de code suivantes a été créée de la même manière pour chacune des cellules, cependant elles permettent de représenter des informations différentes. De manière générale, ces trois graphiques permettent de représenter sous forme de pourcentage la proportion des expéditions qui sont effectuées pour chacune des sous-catégories de produits et ce, pour un segment de client précis. Le premier graphique présente l'information pour le segment ""Consumer"", le deuxième graphique pour le segment ""Corporate"" et finalement le troisième graphique représente le segment ""Home office"". Dans un premier temps, le code permet de sélectionner seulement le segment de client désiré. Ensuite, dans un deuxième temps, une colonne représentant le pourcentage des expéditions selon la sous-catégorie de produit par rapport au total des expéditions de ce segment de client est ajoutée. Dans un troisième temps, le code permet de trier les pourcentages sous forme décroissante. Finalement, la dernière étape consiste à la création du graphique afin de représenter visuellement les résultats. Ces graphiques permettront donc aux dirigeants d'observer le comportement des différents segments de clients face aux sous-catégories de produits qu'ils favorisent et permettront donc d'orienter leurs décisions pour le futur de l'entreprise encore une fois face aux segments de clients favorisés.",08600fe2,0.3592233009708738
16847,c85c94076e9c3a,47edbf4f,* At what age customer enrolled in the company,3ea0c443,0.359375
16850,ff3a8ce61fab6a,fad47d53,"<hr>
# 5. Placeholders

<p>
A <b>placeholder</b> is simply a variable that we will assign data to 
at a later date.<br>It allows us to create our operations and build our computation graph, without needing the data.
<br>
    
In <b>TensorFlow</b> terminology,we then feed data into the graph through these <b>placeholders.</b> 
</p>

## Exampel 1 

<p>
    Assume we want to know a values of <b>C</b> for this equation 
    <code>C = 2πr</code> for this follwing values <br>For 
    <code>r = 1,2,3,4,5</code> to compute a circle perimeter for each         value of <b>r</b>.
</p>

<p>
You can try it using <a href='https://www.google.com/search?q=circle+perimeter+equation&oq=circle+perimeter+equation&aqs=chrome..69i57j0l6.1413j0j9&sourceid=chrome&ie=UTF-8'>this link</a> 
</p>",9afe1654,0.359375
16851,917957c6c4065f,9e6f80b0,동영상 게시 후 얼마 후에 인기동영상에 갔는지를 의미하는 treTime-pubTime입니다.,55b8ed68,0.35947712418300654
16852,04ff2af52f147b,6c6e8adc,"The 'T' deck shown above is the boat deck (top deck) and only has 1 passenger.  For reference, decks 'A' through 'G' start just below the boat deck and run down towards the bottom of the ship.",d5f37be9,0.3595505617977528
16861,37e461081e47c5,81452dc2,"Here, Android games are most negatively correlated with other variables of interest.",b3e6549e,0.36
16862,83df814455f06c,328b2da9,### Explore `class` variable,c9cff71a,0.36
16864,5cb7f999fd1ecb,8238383b,# Missingno - Missing Data,88b54f70,0.36
16869,cb570c7b7f0501,c1cde79d,"it's clear that the heighest number of non-shown and shown cases is for new born babies.

But we are more interested in the ratio of no-show cases with the total cases for every age.",a200a0ec,0.36
16870,7e1da639035ac5,e3676c26,### <a id='8.1'>8.1 Average racial distribution in schools of different cities</a>,120b6c23,0.36
16871,b10bd75889dad9,28a2d372,#### Filter high value customers,ee00ceee,0.36
16872,7dd46c750653eb,43301796,**Comparing Birth Rate of Male and Female Yearwise**,c2644713,0.36
16873,cee088a6840708,4fb61fff,# Step 5 -> Extract needed information from the dataset,55463e1c,0.36
16875,2bd6c370695ea7,cbf5d2a1,## GroupStratifiedKFold,cbe6aec8,0.36
16876,10c5a39a87c47e,c4356a33,## Step 6: Normalizing Data<a id='step-6'></a>,09c7337a,0.36
16877,bbad077c274022,0df7ca8e,**Let's see how are my steps across the day divided.**,3c2e3dea,0.36
16879,91eaec994e0c6f,2c3ff275,<i>The followoing are function that will be used for different analysis.</i>,376aef10,0.36
16883,81712ee7510ac5,551fad6d,**How to grab the item of the list??**,c4685e79,0.36
16890,d6ddbe57f59cf7,722bb0a9,# Histogram (distribution) plot,504a3cda,0.36
16896,c65a65d4041018,2bd43464,"We can see real differences between countries:
* In Russia most of Kagglers are DE or DS, who are most relevant for Kaggle;
* In USA there are more DA that DE among the Kagglers! I suppose this could be due to the fact that some DS are called DA. Also lot's of students;
* In India most of Kagglers are students, who aspire to become DS (or DE);
* Other countries are similar to Russia, but have a higher rate of students;",824fb229,0.3602941176470588
16898,c09fac3c943d51,842937b5,# Preprocessing,678d076d,0.36046511627906974
16901,0858e1bb3cbaca,6d0d3eb8,"To display multiple columns, we can combine them as a list",78548374,0.36065573770491804
16903,918040fad252ec,aa63c8fa,Membuat val akurasi pada saat epoch data,966fcd8f,0.36065573770491804
16910,225b4fe5d3894a,ebf8a9e3,"<a id=""6""></a>
## 6. Preparing Data for Machine Learning Algorithms

Lets start by separating labels and predictors of our orignal train dataset into copies that we can use
",4b4197b3,0.36082474226804123
16917,1d73d04c3aaae8,4f62d811,"## Check on Prediction

We first eliminate the tie predictions and actual ties. ",cd43d0aa,0.3611111111111111
16919,cf39cde80e66b7,3636a081,"# <div class=""h3"">MAE: Mean absolute error</div>
<a id=""m1""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.3611111111111111
16921,69ac33d79f5130,aa6c21c6,"
#### Top 20 cities by accidents numbers",9d760d2a,0.3611111111111111
16922,f18e737fcc4b06,c6285802,## Catorical Variable,087b8637,0.3611111111111111
16926,6dcfe6a610d86b,f7b068dd,"## Performance evaluation

",d05c59da,0.3611111111111111
16929,30c8dc87ce52ca,3e590657,SAMPLING,805e9d67,0.3611111111111111
16930,c349ee5a821411,3f000351,"In general, the highest correlation are between Happiness Score with Economy, Family and Health. 
We can also find some correlation between Happiness Score with Freedom and Dystopia.
I would like to recall that Trust and Generosity have the lowest correlation with the Happiness Score, this is something surprising for me.
Its also interesting to see the correlation between Health and Economy but this is a different topic",572b269d,0.3611111111111111
16931,b3e0b7e9ff6849,d0bb9779,## 2. Data Preprocessing,f6e4bb0d,0.3611111111111111
16932,2f0f808765fc67,74e02312,# **DATA MODELLING**,fd1f6494,0.3611111111111111
16933,268a610bbc64b4,946823df,All Users distribution,8a16f301,0.3611111111111111
16944,09751c520b0616,93da2192,<b>Filling remaining null values with 0.,a4d0c7e9,0.36153846153846153
16947,d07915a6e6992e,a7f856c0,"**Additional analysis**

Let's create few additional charts to see how different variables are related.",2b912140,0.36153846153846153
16948,56785caebaa256,764c3c6f,"### 3.2.3. Holidays as days of less efficient work of laboratories <a class=""anchor"" id=""3.2.3""></a>

[Back to Table of Contents](#0.1)",a792961a,0.3617021276595745
16950,c7e5f658090347,70a52e13,"## Linear and Non-Linear Correlations to the Target Class: Are All of The Features Important for Prediction?

* Below we will first generate a correlation matrix (that will look for only linear correlations). Given that features V1-V28 were generated from PCA it is not a surprise to see that all PCs have 0 linear correlation with each other. We can also see the largest absolute correlation value between two features (""V2"" and ""Amount"") is 0.53, meaning we do not need to worry about any potential issues from multicollinearity.  

* Perhaps a better question is do we need all these features? To help with this, below both the linear correlation and the mutual information (able to find non-linear relationships) for all features to the target is shown. These results clearly suggest that features such as ""V13"", ""V15"", ""V22"" to ""V26"" (and others) could be removed prior to machine learning. This will be investigated later.",43c78e7d,0.3617021276595745
16951,3f25b363afec54,2fa3260c,Before delving in to the competition my simple rule is to visulize the data and try to learn what data says.,bbdaae25,0.3617021276595745
16953,5f674175839b32,eaba1bb5,<font color=pink>This table shows the best selling games on each platforms.,53a2e343,0.3617021276595745
16958,55a5e31d03df9f,548d0f76,"We trained sucesfully our first model! 

We cover a few things so let me start explaining, first of all let's check the **Callbacks**

Callbacks are an extra functionality you can add to your models to be performed during or after training. Why we want to use callbacks? Well it can help you prevent overfitting, visualize training progress, debug your code, save checkpoints, generate logs, create a TensorBoard, etc.

In our case we used it for 2 purposes:
1. Create a tensorboard to later on check visually all our experiments.
2. Save the best model as a CheckPoint, this is not required for our current model since it trains fast enough but imagine if you have a model that takes 10 hours to train, you would want to save different checkpoints while training in case someone by error turns off the computer when it was missing 5 min.

You can check more callbacks from Tensorflow documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) 


Now let's check the arguments we used in the `fit` function:
- We used the `train_ds` TF will detect in the `train_ds` that we have our X and Y.
- We specify the number of epochs that we want to train for, an *epoch means training the neural network with all the training data for one complete pass. In an epoch, we use all of the data exactly once. A forward pass and a backward pass together are counted as one pass.*
- The steps_per_epoch is the number of batches we have in our training data. In this case we have 361 images and we divide them by batches of 32, this will give us around 11.28  (last batch isn't from 32 but instead from 9) then we have 11 batches from 32 and a last batch of 9 for a total of 12 batches.
- Validation data would be in this case our `test_ds` with this we will compute the metrics that we specify in the compilation of the model.
- Validation steps: Same as steps for epoch but for validation data. 
- Callbacks: This would be a *list* that contains all the callbacks we want to have during the training.

But what is the performance of our current model?
",06dce00f,0.3619047619047619
16961,04bac111ffbe9c,1dbae286,##### Plot to check distribution,82576b17,0.3619047619047619
16965,f3c6048d1058e3,4e4e6669,"- As expected, this model is giving us poor accuracy of 58% as we depicted in EDA. Indirect features have very similar trends and patterns across both the classes, we have seen in EDA portion.",1d9056b0,0.3620689655172414
16966,a1ba5ffd30dbde,58ed4313,### Feature Engineering,48e57546,0.3620689655172414
16970,1dd9c6aa74d289,d3608fe7,## Make plots for results,5ef9a1be,0.3620689655172414
16971,00001756c60be8,a6357f7e,*Выводим сколько строк в тесте и на трейне*,945aea18,0.3620689655172414
16973,e3f3f108cd3869,07395062,**Concatenating train and test data and doing one hot encoding for categorical data**,2b78de2d,0.3620689655172414
16978,598b6228760590,7a9e2889,# Dealing with category characteristics,be30ab66,0.36231884057971014
16980,7e275c8d5ff2a0,9a73552b,Statewise Analysis of Test Labs,b3afcc98,0.36231884057971014
16983,0e2a23fbe41ca9,6d660f99,"Observations:
- No nulls in both the columns
- ```numerical_1``` values ranges from -0.057 to 183.735
- ```numerical_2``` values ranges from -0.008 to 182.097
- 75th quantile is at -0.047 for both ```numerical_1``` and ```numerical_2```
- max value for both is very close",64e4762c,0.36231884057971014
16984,8336d84cf3ff6b,67e9380d,# Building a Linear Regression model to Predict Null Values in Number_Weeks_Used column,b96b58a0,0.36231884057971014
16989,9d9da6c439b96b,fe6d338d,the graph shows only in Rank and Year columns which has normal distribution,361cc7d9,0.36231884057971014
16990,726833f92fb87a,9a91e600,**We cannot draw any particular conclusion.**,7dc5e1b6,0.3624161073825503
16992,5ffe6aa38958a1,6b8dbb0a,"**Pandas Dataframe Query** 
There will be better ways to do this, but for now, I split the database into male and female to analyze the correlation matrix. ",11f5412e,0.3625
16993,fdbbd573ba31c2,8b7736f5,### resistance(ohm),f7c28d74,0.3625
16996,30fdc4a6e3c1db,024c7699,"What we get:
* FOODS_1_014 item hasnt changed prices over the years. Also, the price is fixed in all stores",6111ddee,0.36257309941520466
16999,52cfd66e9ec908,f57d576e,"Again, box and stub will also give a good representaton of the data albeit with less low-level detail than the semantic view, seeing as the highways are not into much consideration here. The box view helps to just take a low-level look at the vehicles and their projected path whereas the stub view functions similarly to semantic. We can now proceed to taking a good look at the metadata provided by kkiller and potentially train a good model. The ones to check now will be stub and satellite to check.",c74adcdf,0.3627450980392157
17001,7cfd96218dd933,019859e3,"#### **ATTENTION**
* THE HIGHEST FRP VALUE LATITUDE: 36.95595
* THE HIGHEST FRP VALUE LONGITUDE: 31.4881
* THE HIGHEST FRP VALUE DATE: 2021-07-29
* THE HIGHEST FRP VALUE TIME: 20.03 FOR TURKEY",7c34d96c,0.3627450980392157
17002,842547b2def18c,6236ab07,We can convert the categorical titles to ordinal.,b8efde6d,0.3627450980392157
17007,2ada0305b68956,75b62b5d,### 60. Palette = 'Set2_r',133e26f4,0.3628571428571429
17020,2d15e241e451e7,605a7550,# The image augmentations to use:,1cc249bf,0.36363636363636365
17022,4b64dc653fb7eb,53f388d7,"<a id=""2"">Step 2 - Remove Digits </a>

Removing \n and digits",57675cc2,0.36363636363636365
17028,0cb9adc158b705,34e2e2b1,"Don't worry a lot if the above code looks cryptic. You can read the [6th notebook (or chapter)](https://github.com/fastai/fastbook/blob/master/06_multicat.ipynb) in fastbook for details. It explains the topic in the most simplest way possible. And once you master datablock API, you will feel like a Ninja (trust me on this)!

You are amazing! Now lets create our dataloaders, & then take a look at some images.",3abf056e,0.36363636363636365
17033,25c2f1ef13b402,b6940f98,"**COLLABORATICE FILTERING FAST AI, TRAIN MODEL **",b028c35a,0.36363636363636365
17034,5a8c553e21c70f,10529eb2,Drop Class 0 outliers.,9ebd9d8f,0.36363636363636365
17035,a0b321057e7402,34599d09,"Most time-series data is composed of three elements:

* Season - a repeating cycle in the series
* Trend - an upwards or downwards movement in the series
* Residual or noise - random variation in the data

Some literature also adds ""level"" to the decomposition. A ""level"" can be described as the average value in the series. 

**Seasonal decomposition** can be a great way structured approach to a time series problem. The acquired information is useful when thinking about the specific problem and planing the future approach to the model. I am going to use the automatic seasonal decomposition tool and plot the results.",5f73fb91,0.36363636363636365
17038,37360278c19104,3f1cbb5d,# How many date we have in each category?,21473a41,0.36363636363636365
17039,d1ff7e10ee0102,ee9e1436,"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)

At first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.

Another thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.",2cc71c3c,0.36363636363636365
17040,73d8e56bc709b1,760facc9,There are 548 players in top teams of europe. ,78ec3cce,0.36363636363636365
17041,dd02a9b545f742,2b5cee56,# Phase 0 | Get raw dataset and customization,7116cd2d,0.36363636363636365
17042,c13f73168789c2,8b64e26a,"### 2.2 Select a column by index location<a id='13'></a>
Syntax : `df.iloc[:, index]`",16175052,0.36363636363636365
17044,4ae464582bac51,0592b3cd,## Numerical Variable,ca6a52ce,0.36363636363636365
17049,132fa9714f2046,c78b4c59,"** Now plot (x,y) on both axes. And call your figure object to show it.**",3bb1775f,0.36363636363636365
17050,930cd79ca51204,67f2a18f,Let's create a histogram of life expectancy.,5506779a,0.36363636363636365
17052,32e04b08ff52eb,081ed919,Creating train and test dataset,8d5b86e0,0.36363636363636365
17053,70193f0c034b98,e4439404,# Gridmask,f8cacd26,0.36363636363636365
17055,0475899eec1ffe,18547598,"After the data, we add a column where we calculate the opening and closing price differences in our data set, and we use the data to prepare the data set preparation",d825dc37,0.36363636363636365
17056,241cf32abb22d8,1c89cc30,"The AUC score for the full features is very low, at 0.556. There are probably some irrelevant features in the dataset that weaken the performance of the model.",47157066,0.36363636363636365
17060,f67c61c7d50810,00ebea93,"## Options
These features determin how to take features from the original txt files.<br>
<br>
We take Wi-Fi or beacon features within specific timespan from waypoints. <br>
I assume that if the time gap is too much, Wi-Fi signals or beacon signals are not trustworthy. <br>
I set it to 3000ms, but you can try other numbers. <br>
<br>
And we can determin how many signals to take into the result dataset. <br>
Actually beacon doesn't have much samples in the original txt files, or even doesn't have it. <br>
You can try other numbers as well. <br>",2bfe0598,0.36363636363636365
17064,2f964d08c25d93,7938e486,Let's take a quick look at what the data looks like:,1f2e4468,0.36363636363636365
17072,a35cdce61f4059,359d0089,* **Classification with Neural Network**,acc8eab6,0.36363636363636365
17076,b42180a6a5b42f,df8d5a78,## 2) Análise das mortes pela data do Óbito ,987cea5f,0.36363636363636365
17080,efd44ce2c08541,8cf881a2,# Similarity: Image (NFNet-F0 and ViT ),ebc2d00c,0.36363636363636365
17086,d83e5b44d1b80d,ad4f85b3,# Gender,62845930,0.36363636363636365
17089,663bbc9eaf267b,b33764bc,## fuelType,32445529,0.36363636363636365
17094,2f47abddfd1928,a035df31,"We can identify how A, B and C and partially D and F are 1st class cabins, and then F and part of the unknown cabins are 2nd class. G is full 3rd class as well as the unknown part of cabin feature.

The most likely is that the 3rd class passengers were assigned to very crowded cabin areas with lack of registration.

So far we can find some value in cabin feature as it helps to distribute the passengers per class in the ship.",ae33cc0b,0.36363636363636365
17095,9169c4e9c33c90,3bc8d8da,"It looks like the the *APA Publication Manual* made 10 total appearances over the years, making Top 50 every year except in 2019.",725bf880,0.3644067796610169
17099,869a39a3d4dea2,4fbfc67f,"Explore simple image transformations like translation, rotation, resizing, flipping, and cropping, and image processing like image arithmetic, bitwise operations and masking.....",9020daf8,0.36470588235294116
17102,e4525eb0c96f28,0c61eb8a,"### Plotting Sales by Genre

In plotting the top genre categories, we can now see which genres have grossed the highest sales regardless of time.

Racing in the earlier scatter plots was first, but here we see Racing at 6th in terms of overall sales.

Using this data for further analysis will help us put data into the context of the most popular genres.",2093a1f1,0.36486486486486486
17104,27d5291d6365ba,81938cf8,# Account Balance (mean of all customers) for various states (all days average),96b30229,0.36486486486486486
17105,63b44c85e32c1f,6ce34ff6,**append( )** is used to add a element at the end of the list.,fb9b9562,0.36486486486486486
17106,3c2033cc99c12c,1b073c8d,"<b>From the distribution plot, we could obviously found that the variance of the Fraud part of the dataset is much larger than the normal part, lefr or right skewed than the normal part. <b><br>",dfa22a54,0.36496350364963503
17109,4b4117cf42ef8d,9e2785bf,# Scaling the Features,457cd6f4,0.36507936507936506
17110,06ecf7a304c309,6da783d9,**Predicted : Autoencoder Output**,714de627,0.36507936507936506
17117,99bf357eaf61f1,d7f07dd4,"Heatmaps are great to detect this kind of multicollinearity situations and in problems related to feature selection like this project, it comes as an excellent exploratory tool.

one aspect I observed here is the 'SalePrice' correlations.As it is observed that 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hello !' to SalePrice, however we cannot exclude the fact that rest of the features have some level of correlation to the SalePrice. To observe this correlation closer let us see it in Zoomed Heat Map ",9d92fafe,0.36538461538461536
17119,44f6a002ecd033,67b1f319,Now that we are setup to start cleaning the missing data we will split up the data into numeric and categorical variables.,70bbe106,0.36538461538461536
17120,6f1481148352e9,3ca1bcfa,**Let's analyze the number of fires by year**,7cfbdb8f,0.36538461538461536
17121,71d3e4aee86e3e,4e58fa66,> # Geospatial Comparison,69706f0b,0.36538461538461536
17123,0caaec057f7184,6980eeda,## Sales information / item based ,b875533e,0.3655913978494624
17127,fdc3afd309b850,f60ba5ce,"<a id=""bnv""></a>
### 6.2.2 Bathrooms Null Values",966bde38,0.36574074074074076
17129,fd4017c1514157,3c1132fa,### <font color = 'red'>Location</font>,fd8f0896,0.36585365853658536
17132,62582b8036fbfe,86ebb7db,##### Removing Punctuations,6c2160db,0.36585365853658536
17133,9c26c5dcd46a25,d8af10b8,"Les résultats du **test de Fisher** nous indiquent ici une p-value de 0 pour l'ensemble des catégories, donc inferieur au niveau de test de 5%. Nous rejettons donc l'hypothèse *H0* selon laquelle les ditributions sont identiques.        
**La catégorie de produit a donc bien une influence sur le Nutriscore**.",1bbbb677,0.36585365853658536
17142,0b01138ad120fc,5d4f3c1a,"**Since RNNs can treat sequence problems let's supose this case:  
 My roomate always make the same dinner, based on the day of week.  
     Monday    - Pizza  
     Tuesday   - Burguer  
     Wednesday - Pancake  
     Thursday  - spaghetti  
     Friday    - Meat  
     Saturday  - Chicken  
     Sunday    - Sausage**",0b4b72e6,0.36585365853658536
17146,3d77c1560bd16e,8489d4be,"<a id='3'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Details</div>
",87c141ca,0.36619718309859156
17147,9bcfa825c8b2e6,a499bd94,Bir kişinin glikoz değeri 140'tan yüksek ise risk grubundadır.,220f36e4,0.36619718309859156
17149,631cd434fc3aa2,a3886469,"* _LotFrontage_: means ""Linear feet of street connected to property"". Since the missing ratio is not too high (so better not drop the entire column) we can fill the missing values as the median for the neighborhood: houses in the neighborhood are likely to have the area of each street connected to the house property.**",2b74febb,0.36619718309859156
17153,d6cbd7160961dc,df11972d,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 800.

* The center graph shows the result for city B, which had the worst chi-squared value. 

* The rightmost graph shows the result for city G, which had the best chi-squared value. 

Note that since our data was randomly generated, the frequency in which numbers 0 to 9 appears as first digit will not comply to Benford's Law. This theory doesn't work for random numbers specially when we are considering the first digit.",36d74664,0.36666666666666664
17157,5b92c712910a11,daa9e449,# Stop Words,e1d17100,0.36666666666666664
17158,bc058fe14d3d1b,73d088df,month,d0273670,0.36666666666666664
17163,e78f177ca86768,41d5f4c6,"## Tuning Subasmple and colsample by tree
",120e25c1,0.36666666666666664
17164,f7436bc492474c,94de9d27,"We will create a list of all the labels to predict, and also create a 'none' label  to see how many comments have no labels and then summarize the dataset.",328fd235,0.36666666666666664
17167,4bada947d597ac,72ed5982,# Split train data into train and validation sets,eab5094a,0.36666666666666664
17168,37b09262279764,b660a7ce,"<b>Most machine learning algorithms can't work on text data. So, we need to convert text to numbers.</b><br>",37c4c417,0.36666666666666664
17169,864302b10e7730,84e03246,"# To Learn the trend, we plot the ***Lineplot***",e9dd1d2d,0.36666666666666664
17173,8d575f495686ab,219fa346,There is a sudden surge in the open price on 27 january 2021. But the volume experienced a surge right before on 13 january 2021. ,0fc16499,0.36666666666666664
17176,892be0a523578c,2617cecf,"#### 6. sleepDay_merged.csv
This file seems as a sleep record in daily dimension, so I transformed the minuteSleep_merged.csv to the one in daily dimension and compared it with sleepDay_merged.csv to find potential problems.",b0e8d7c0,0.36666666666666664
17181,be616f0785c32d,f28f379d,"[Go Top](#top)


<div id=""PartB""></div>

## Part B Subtyping computational approaches that are used to propose drug candidates

We then subtyped computational methods developed to repurposing drugs for COVID-19.

##### B.1 Methods

During reading the literature curated in Part A, we came across computational studies that focus on predicting drugs suitable for repurposing for COVID-19. These works tend to propose many drugs. 


##### B.2 Results

<div id='PartBcat1'></div>

##### B.2.1 Gene-gene network-based approaches

**Example**: https://www.nature.com/articles/s41421-020-0153-3 repurposed drugs by network approaches based on homology analysis to other viruses. The authors proposed 16 potential drugs: Irbesartan, Torernifene, Camphor, Equilin, Mesalazine, Mercaptopurine, Paroxetine, Sirolimus, Carvedilol, Colchicine, Dactinomycin, Melatonin, Quinacrine, Eplerenone, Emodin, Oxymetholone. 

**Background**: Network-based drug response has been intensively used in the cancer area and was shown to excel in several benchmarks. 

<div id='PartBcat2'></div>

##### B.2.2 Expression-based approaches

**Example**: https://arxiv.org/abs/2003.14333 repurposed drugs for treating lung injury in COVID-19 by 'could best reverse abnormal gene expression caused by (SARS)-CoV-2-induced inhibition of ACE2 in lung cells,' an effective drug treatment is one that reverts the aberrant gene expression back
to the normal levels'. The authors proposed the following drugs': geldanamycin, panobinostat, trichostatin A, narciclasine, COL-3 and CGP-60474.


<div id='PartBcat3'></div>

##### B.2.3 Docking or structural-based approaches

<div id='PartBcat3.a'></div>

**B.2.3.1 Small molecule prediction**

**Example 1**: https://www.biorxiv.org/content/10.1101/2020.03.03.972133v1.full 'a novel advanced deep Q-learning network with the fragment-based drug design (ADQN-FBDD) for generating potential lead compounds targeting SARS-CoV-2 3CLpro' Prioritized 48 candidates by docking (supplement Table S1).

**Example 2**: https://www.sciencedirect.com/science/article/pii/S2211383520302999  studied the proteins encoded by SARS-CoV-2 genes, compared them with proteins from other coronaviruses, predicted their structures, and built 19 structures that could be done by homology modeling, Library of ZINC drug database, natural products, 78 anti-viral drugs were screened against these targets plus human ACE2. Prioritized the hundreds of drugs, ranked by docking scores: e.g., Ribavirin, alganciclovir, β-Thymidine, Platycodin D, Chrysin,Neohesperidin, Lymecycline, Chlorhexidine, Alfuzosin, Betulonal, Valganciclovir, Chlorhexidine, Betulonal, Gnidicin.



<div id='PartBcat3.b'></div>

**B.2.3.2 Monoclonal antibody prediction**

**Example 1: docking-based proposal of antibodies** https://www.biorxiv.org/content/10.1101/2020.02.22.951178v1.full.pdf The neutralizing antibodies are proposed by computationally docking to the S protein of COVID-19 by docking simulation.

**Example 2: ACE2 pathway-based proposal of antibodies** https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7079879/ Potential therapeutic approaches include a SARS-CoV-2 spike protein-based vaccine; a transmembrane protease serine 2 (TMPRSS2) inhibitor to block the priming of the spike protein; blocking the surface ACE2 receptor by using anti-ACE2 antibody or peptides; and a soluble form of ACE2 which should slow viral entry into cells through competitively binding with SARS-CoV-2 and hence decrease viral spread as well as protecting the lung from injury through its unique enzymatic function. MasR-mitochondrial assembly receptor, AT1R-Ang II type 1 receptor.

**Background**: Docking has been used intensively in drug discovery in areas such as cancers. 

##### B.3 Limitations

* Computationally proposed drugs tend to be a lot in a single piece of article, sometimes, hundreds of drugs in a single study.
* Most of the works adopted methods from other pharmacogenomics field that were previously developed for cancers. 
* We are not aware these approaches have generated hypotheses that are used in real-world clinical trials even in popular fields, e.g. cancer, Alzheimer's. Thus, use them with cautions.



<div id=""PartC""></div>

[Go Top](#top)


## Part C. Drugs proposed by in vitro experiments

#### C.1 Methods

###### C.1.1 Data curation

Other than the drugs used in clinical trials and computational methods, we found an interesting study that carried out genome-wide in vitro binding screening of the virus proteins and human proteins, and proposed 37 drugs that directly target these proteins in the supplementary table 6 of Gordon et al (https://www.biorxiv.org/content/10.1101/2020.03.22.002386v1.supplementary-material?versioned=true). These drugs are currently being screened by the authors: Loratadine, Daunorubicin, Midostaurin, Ponatinib, Silmitasertib, Valproic Acid, Haloperidol, Metformin, Migalastat, S-verapamil, Indomethacin, Ruxolitinib, Mycophenolic acid, Entacapone, Ribavirin, E-52862, Merimepodib, RVX-208, XL413, AC-55541, Apicidin, AZ3451, AZ8838, Bafilomycin A1, CCT365623, GB110, H-89, JQ1, PB28, PD-144418, RS-PPCC, TMCB, UCPH-101, ZINC1775962367, ZINC4326719, ZINC4511851, ZINC95559591.

<div id='PartCexp'></div>
###### C.1.2 Construction of training set

We carried out a machine learning exercise, with the hypothesis that the drugs that will be potentially effective should overlap globally in function of these drug targets. We could extract the chemical structure of 34 of the 37 drugs proposed by the authors, which are used as positive examples. The second positive set is the combination of the first positive set and four other drugs that are currently under clinical trial and whose chemical structure can be extracted: remdesivir, hydroxychloroquine, favipiravir and Vitamin C, and thus 38 in total.

The negative training set, which is also the candidate set, is constructed using the FDA approved list, which was downloaded in Oct 2019 from https://www.accessdata.fda.gov/scripts/cder/daf/index.cfm. This list has a total of 7305 drugs, 5596 of which we could obtain the fingerprinting structure.",b78e18aa,0.36666666666666664
17184,b547f0f38f7744,0c87a5bb,Split dataset to `train:valid=8:2`:,b6ba66b3,0.36666666666666664
17187,a566b5b7c374e7,00d6c2bf,## Regularity in Sleep and Other Metrics,b3dc5545,0.3669064748201439
17190,4c47839b067546,e09de546,# 3. Предварительная обработка данных и EDA,1f517b02,0.3670212765957447
17198,e69a496109e7d8,e89c2fde,"On the above univariate analysis of three columns **Age** and **OperationYear** doesn't show the significant relationship.

But the distplot with No.of.AxillaryNodes gives much information than other two.
It shows people with axillarynodecount < 5 are more likely to survive. As the count increases, they are less likely to survive",1c640591,0.3673469387755102
17199,12f4d16fc21645,b36c8c42,<h3 style='color:Red'>Potassium analysis</h3>,c7752038,0.3673469387755102
17200,0e7ac281a19feb,bfe0a622,## DatasetMapper,5b84d10f,0.3673469387755102
17203,eb33e05704d647,518dae8b,"Get new image size and augment the image
",cd80436d,0.3673469387755102
17209,dc0b0e1cb46c6f,0971b34a,"<a id='2.1'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;"">2.1 Distribution of vaccinations by country</p>

We have to fill the missing with forward filling method from pandas.

As ***people_fully_vaccinated*** has many missings and the values are only for low number of countries, we are note going to plot.",47b17a7b,0.36764705882352944
17210,3d905ce4828057,56355b09,"# **Preparation of Life Time Dataset**

* recency: The time elapsed since the last purchase. Weekly. (according to analysis day before, here is user specific)
* T: How long before the analysis date the first purchase was made.Weekly.
* frequency: total number of repeat purchases (frequency>1)
* monetary_value: average earnings per purchase",5b006cc3,0.36764705882352944
17211,e4c6dd957eb5ce,41cdabf5,"# Explorating Registration Dates details
As we can see a Seasonality, let's see if the data shows is  ",2e383665,0.36764705882352944
17212,eb0ecd6bebeb15,cebe7745,Aynı iki veriyi daha farklı bir açıdan frekanslarıyla incelemek için jointplot kullanarak görselleştirelim. ,d7b93a60,0.36764705882352944
17213,156bbcff05dcea,a62366dc,"Since this data doesn't have any class imbalance the randomSplit() function works well. However it doesn't seem to be as good as the train_test_split() method from scikit-learn which provides multiple options like stratify to handle class imbalance while spliting the data.


A stackoverflow question has also been raised regarding the same which is currently open.
https://stackoverflow.com/questions/70721631/sklearn-train-test-split-equivalent-in-pyspark",66ad1fe9,0.36764705882352944
17215,02b7e38902069e,9cf98b41,"#Finding similarity between two sentences

iNLTK provides an API to find semantic similarities between two pieces of text. This is a really useful feature! We can use the similarity score for feature engineering and even building sentiment analysis systems. Here’s how it works:

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.36764705882352944
17218,14defffcd250f3,85ca7c14,**Normalization**,3a683b94,0.367816091954023
17220,d5f78aa381f58d,c18ded67,### Outliers Detection,d60f358f,0.367816091954023
17224,fe7360cddc13e5,0e0b4091,-----------------------------------------------------------------------------------------------,8979e423,0.3684210526315789
17225,a1dcd92986bc84,d4cdfa3f,"## Implement the vision encoder

In this example, we use [Xception](https://keras.io/api/applications/xception/)
from [Keras Applications](https://keras.io/api/applications/) as the base for the
vision encoder.",730acaaa,0.3684210526315789
17232,bef2347846e476,da3b3f2d,"**Correlation** can have a value:

**1** is a perfect positive correlation,
**0** is no correlation (the values don't seem linked at all),
**-1** is a perfect negative correlation.
The value shows how good the correlation is (not how steep the line is), and if it is positive or negative.So when we observe the values of the dataframe2 we see there are two perfect positive correlation between the same groups( it must be ).And between Sentiment polarity and Sentiment Subjectivity the correlation value is weak,approximately 0.27 .",cb93bf51,0.3684210526315789
17233,d93a87fdbdb3d2,16f5624c,### Delete useless words using <code>nltk</code>,30d079c3,0.3684210526315789
17234,30fdc4a6e3c1db,9744adcf,### Plotting boxplot for price changes,6111ddee,0.3684210526315789
17235,757fa8de4edc4c,426c9471,Add the structure of Atom in Train and Test Set,87211008,0.3684210526315789
17236,f14f6708035916,6c733431,Distribution graphs (histogram/bar graph) of sampled columns:,ca22d04b,0.3684210526315789
17240,c3498779cda661,0b3236a1,"# Visualización

Usar PCA para reducir dimensiones y poder graficar.",0f531b65,0.3684210526315789
17241,c2a9f2fb3e1594,0b76a823,"<a id=""ch6""></a>
# Step 4: Perform Exploratory Analysis with Statistics
Now that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.",53411c04,0.3684210526315789
17245,d81d3830152f88,45cec900,"Treatment:
- there are 4826 instance in the treatment group
- Proportion of instance where a team that has a higher 3pt-fg than its opponent won is 56.75%",9551eac9,0.3684210526315789
17246,f91f58d488d4af,6929e654,"So, is our baseline model any good? To Quantify this, we must define a metric.


#### Metric:
It's a number that is calculated based on the predictions of our model and the correct labels in our dataset, in order to tell how good our model is. We normally use *accuracy* as the metric for classification models. We want to calculate our metric over a *validation set*.",5df1bbf3,0.3684210526315789
17247,9e27af2600925c,58b88c22,"### 4.2 - Initializing parameters

**Exercise:** Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation.",9b556435,0.3684210526315789
17251,54004b32784b68,fda0e30e,**Filling Null Values in Categorical Columns with Most Frequent Value**,27213ca9,0.3684210526315789
17253,5ce12be6e7b90e,207b4774,"Notice the colon and the indented block. The syntax is always:

```py
if condition:  
    statement1
    statement2
    statement3
    ...
```

The condition does not need to be surrounded by round brackets `(...)`.

**Whitespaces mark block code**: Only commands within the indented block are conditional. Other commands will be executed, no matter if the condition is met or not. There is no use of curly brackets or `end` command: unindenting will close the code block.

Also, the condition does not need to be surrounded by round brackets `(...)`.

__Note__: the condition expression is always converted to a boolean -- if it's not already a boolean, it will be implicitly converted into one. 
The indented commands only occur if the boolean has a `True` value.
Therefore, we can use logical operators to create more complex conditions.",c0ab62dd,0.3684210526315789
17259,9f3710be6aea65,704a7380,## We can create other features from the data,ae9bda88,0.3684210526315789
17262,6cade0b6a41ba2,f3049c2f,##### Nothing needs to be done with this data column,e6110293,0.3684210526315789
17265,29437539745aa5,6100fdbf,"

![basic_idea_graph](https://i.ibb.co/y6YfBzN/basic-idea.png)",c17b490a,0.3684210526315789
17269,840534f2908a9c,eb6e4114,Let us look at Geographical Features,8081c3cc,0.3684210526315789
17270,31b564f11ef638,d1209758,### Separate train and test data. Assign train target value(y),424f9692,0.3684210526315789
17271,2ada0305b68956,4e1763db,### 61. Palette = 'Set3',133e26f4,0.36857142857142855
17275,957e035ba5b9d5,ee507a9d,"# Pre-Trained Network Part 1

We can leverage a pre-trained network like the VGG19 architecture, which is pre-trained on the ImageNet dataset. Even so the ImageNet dataset contains only ""cats"" and ""dogs"" it can be used for a more generalized problem like the Art Images.

Here we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. In our case we will freeze the layers of the VGG19 model. And only fine-tune the added layers.

To further improve the model we could make the last five nodes trainable. But that is for another time.",778ab3d3,0.36879432624113473
17276,979f1e99f1b309,010eedb9,***THE plot show that almost all players didn't swim too much and they tend to walk***,d1bfebbf,0.36885245901639346
17280,565ad413cd802f,3abd6692,"### Training & Validation sets

As a good practice, we should split the data into training and validation datasets. Let's fix a seed for PyTorch (to ensure we always get the same validation set), and create the datasets using `random_split`.",397b074e,0.36904761904761907
17281,87e94f864d74be,4c130d00,Since it is difficult to find the date_added so I will drop these as they are only 10 rows.,294bfe9f,0.36904761904761907
17283,5f32117bcd5255,762edcf0,#### WORLD COORDINATE SYSTEM,85882abf,0.3691275167785235
17284,726833f92fb87a,73b41c68,## Education vs campaign success,7dc5e1b6,0.3691275167785235
17285,f2f2db16a2f86c,cab5cd69,"This shows most of the houses are in **<1H proximity** from the ocean.

**Island** has the least number of houses.",ffc6a115,0.36923076923076925
17286,a4f8ad33c823c5,2bf9fd35,"Missing values for the diagnosis codes were replaced with text values , 'Others'",fcd48307,0.36923076923076925
17291,03048e86a6d806,18a151c9,### Current State of ML Application in Enterprises,1285c231,0.36923076923076925
17299,73ca9abcc2034e,8c75903d,# Descriptive statistics - title,cec3446c,0.3695652173913043
17300,9b5de3823ad5ab,636b6364,### Save dataframe containing the images' path and label,33e48774,0.3695652173913043
17302,7e89d387feb9f5,b6914d5e,### Добавленный числовой признак №6. Код города,989e3a1b,0.3695652173913043
17305,72d528df923403,3030e509,"# Stores and departments behavior
- CA_3 is the best seller in FOODS_3 department, beating second place CA_1 by more than 1 million.
- CA_3 is the best seller in HOUSEHOLD_1.
",d51c8e8e,0.3695652173913043
17307,4ae6a182abac64,dcad5cb5,- People with less than four parents or childrens aboard more likely to survive.,418676c5,0.3697478991596639
17310,738bfced935b69,f3994156,"The Manual & Automatic transmissions are higer Mileage in the Diesel & Petrol fuel, while the Semi-Auto transmission are lower in the Petrol & Diesel & Hybrid fuel, & Other transmission are very low Mileage in the all fuel type.",2d3c592d,0.3698630136986301
17311,91473a39b85068,1678c7f7,### Frequency of each tag,6e3d91c2,0.3698630136986301
17313,1eb62c5782f2d7,8fd6eacf,# Mencari Probabiliti dari Normal Distribution,bb69f147,0.3698630136986301
17315,fc8e0042411c46,85716be4,- Median for converted and not converted leads are the same.,af476c2a,0.36990595611285265
17323,613bf7bfdcb9e3,e969f2fd,### axis = 0 ->top to bottom most common value count,32beb65d,0.37037037037037035
17328,2500c5fe8497ee,12e9278e,# Univariate Analysis ,855355f0,0.37037037037037035
17330,e0f03003a69819,cf90796b,"* We can see that the distibution is quite uniform for most cases .
* We can also see that the higher the charges value goes , the more likely it is associated with a higher value 
# We can say that BmI-Charges is a linear relationship",609ad1f4,0.37037037037037035
17335,ddcdecdd6a3b6d,f03be027,### VGG11的简单实现,90831448,0.37037037037037035
17337,ab6da5994949a3,a9507f93,## Visualizing ANN Training Set results,fae6b91d,0.37037037037037035
17338,c968dbd8d49ae6,7e9d1c7c,# **Data exploration**,dfb2684d,0.37037037037037035
17343,cebaf20167fb49,6c7625af,# Converting Dtype Object To Float64,702e4057,0.37037037037037035
17347,24e550b8226932,fc4d8f37,##### shops:,0caee953,0.37037037037037035
17350,4392956f62c040,907a0920,## TRAINING HINDI MODEL,c3ed519d,0.37037037037037035
17351,3ce385889c62df,d3a08427,"#### Time is constant, so using model which will take less time to execute",29513ba3,0.37037037037037035
17356,faa8e6c8ab9246,ae693b44,value_counts() gives count of unique values.,2bea1419,0.37037037037037035
17360,efbcfe95cd7fde,b32fd73c,![](http://)Target(sufrio un paro cardiaco no (0) o si(1)),54be281a,0.37037037037037035
17361,55ce731a138ca7,d600fd40,"# Read csv, split & create dataset",4996250b,0.37037037037037035
17363,eda49464dd6d1b,49796acd,### The NaN values are likely 0.  These sales channels likely will not make much difference because the total number of customers in them is very low as demonstrated below.,8421f81f,0.3706293706293706
17364,f3c6048d1058e3,69cd85a5,"## Unsupervised- Pre-trained model- TextBlob 
- TextBlob is a python library for Natural Language Processing (NLP). TextBlob actively used Natural Language ToolKit (NLTK) to achieve tasks related to sentiment analysis. NLTK is a library which gives an easy access to a lot of lexical resources and allows users to work with categorization, classification and many other tasks. TextBlob is a simple library which supports complex analysis and operations on textual data. For lexicon-based approaches, a sentiment is defined by its semantic orientation and the intensity of each word in the sentence. This requires a pre-defined dictionary classifying negative and positive words. Generally, a text message will be represented by bag of words. After assigning individual scores to all the words, final sentiment is calculated by some pooling operation like taking an average of all the sentiments.",1d9056b0,0.3706896551724138
17367,e67925694c07d3,3439e5b2,"DAYS_EMPLOYED and DAYS_REGISTRATION also  looks pretty normal, nothing fishy for now",83af4c4a,0.3707865168539326
17368,312135b445bd23,f0f34130,"## FastText Word Embeddings
We trained word embeddings model on the full corpus (without filtering out articles) using Facebook's [FastText](https://github.com/facebookresearch/fastText) library. This will serve us later in the Sentence Similarity model to find relevant answers for each question. After training, we also created word counts that serves us in the sentence encoder when calculating the smooth weighted average of the word embeddings of all words in the sentence.
The code for training word embeddings using FastText is availabe at [train_fasttext.py](https://github.com/Hazoom/covid19/blob/master/src/w2v/train_fasttext.py)

Let's visualize our word embeddings. ",8ced381f,0.3707865168539326
17369,57070ad5e0f94f,5ddf815a,# **Checking Which Classes Still Not Numeric**,d97edc41,0.3709677419354839
17375,2a123b4e8f9433,a224c033,Distribution of validation,0a082218,0.3711340206185567
17378,9c044fa3072552,8aa97f7d,This shows that less than 5% cities have more than 1000 yearly accidents.,1362842e,0.37142857142857144
17379,867a9f977fa945,39affdb3,# Parts of speech tagging,2740fcca,0.37142857142857144
17385,2b36742b49c7bc,1a591800,"## Гараас өгөгдөх параметрүүд (Hyperparameters)
base моделиор BERT-ийг сонгож ашиглав. [bert моделүүд](https://huggingface.co/tugstugi) [bert-large-mongolian-uncased](https://huggingface.co/tugstugi/bert-large-mongolian-uncased)

Энэхүү хэсэг `num_epochs=5` гэж тохируулж өгсөн байгаа ба `num_epochs=25` үед `+0.97` үр дүн гаргаж авч чадна. 

Kaggle notebook-ийн disk limit 73.1GB байдгийг анхаарна уу!",c8f8a96d,0.37142857142857144
17389,ca73f3d2e25b47,017f3f09,"Data PreProcessing: change datatype
",4cd11efe,0.37142857142857144
17390,171494b45650a2,2c47a096,## ***4. Data Visualization***,9c8cc578,0.37142857142857144
17392,55a5e31d03df9f,475daeb4,"### <a name=""mlpeval"">Evaluating MLP model performance</a>",06dce00f,0.37142857142857144
17396,bbaa07ad21cf4e,b7bf909c,### Missing Value imputation,3ab6b254,0.37142857142857144
17398,38b79494ac749e,d018c732,In the next step we calculate the training and the validation error for each `degree` and plot them in a single graph. The resulting graph is called the fitting graph.,39162a40,0.37142857142857144
17400,0fa9979b5690e9,6e6ef8fa,"A validação cruzada pode ser feita de várias formas no Scikit-Learn, mas é importante observar que agora não temos mais apenas um resultado para o desempenho do modelo. Teremos um resultado para cada execução, então é necessário olhar para essas informações pela perspectiva da média e desvio padrão. As funções do Scikit-Learn que auxiliarão na validação cruzada são: *KFold* e *cross_val_score*.",c26eea94,0.37142857142857144
17401,675b60eaf415a6,4ae004fc,### **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**,68c0b725,0.37142857142857144
17407,7454fdc444df16,0a259d5a,"### Insights¶
* Sometimes we can find artifacts or incomplete patches, some images are also less than 50 x 50 pxs. 
* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?
* Though some of the healthy patches are very violet colored too!
* Would be very interesting to hear what criteria are important for a [pathologist](https://en.wikipedia.org/wiki/Pathology).
",a7818ef5,0.37142857142857144
17408,6b65d81a5743dd,d19393ca,# EDA,4080a2d2,0.37142857142857144
17413,c9b4e282e4e2c1,575ff423,"We can't see anything in this plot, so I'm going to divide the dataset in smaller sets of data.",f44d339f,0.37168141592920356
17415,897ca904b74a98,bb4188bf,#### Variable / Variable relations ,c5844ad4,0.3717948717948718
17417,d4c5aaa4b36810,e66df080,"![](http://)<a href=""IndicatorData.csv""> Download File </a>",65441f28,0.3717948717948718
17426,c09fac3c943d51,f7ef57cc,Setting time as index and saving time as feature (for FE purposes only),678d076d,0.37209302325581395
17430,1660daf8867980,05f451d1,"**Demo**  
We do 100 iterations of monte carlo learning while maintaining a high exploration rate of 0.5:",42d7cffc,0.37209302325581395
17435,3c2033cc99c12c,ceb090e0,#### Plot the table of statistical values and barchart of specified features ,dfa22a54,0.3722627737226277
17436,4c47839b067546,761b7dc5,### 3.1. Объединим train и test датасеты:,1f517b02,0.3723404255319149
17439,52cfd66e9ec908,be3d7a6d,"Now as you can see with stub and box, under the hood the function uses the keys to create a rasterizer (for stub_debug for example StubRasterizer), which generates the AV surroundings and paths and passes it to an AgentDataset, with which we use to generate the predictions with our model. Also, there's a lot of meta-info about the rasterization that I want to have a look at, so let's look at metadata.",c74adcdf,0.37254901960784315
17440,64169805aacf17,b6646c38,## Define some useful image transformations for data augmentation,1f12ded0,0.37254901960784315
17441,71b75664517244,50de0d7b,"Manchester United won 13 times on premier league, most of it happen around 1992-2012. We can see here Manchester United didn't perform well lately.",fc905af5,0.37254901960784315
17445,fa02c409161192,af14befd,Plotting the numbers and their label to make sure that they look correct,e97077f7,0.37254901960784315
17446,d0080e3a39bc5c,a9d14875,**2ND TRANSFORMATION**,2fcde4cf,0.37254901960784315
17448,4fa553c2b837d4,74fdc02a,"# Submission
This is the way to submit your results in any of the kaggle competitions.",c65a23e9,0.37254901960784315
17449,917957c6c4065f,df67ed64,"여기까지가 전처리 과정입니다.
***********",55b8ed68,0.37254901960784315
17451,523123dad03177,9f91ff33,# 4. **What type of lunch are the students having?**,48a5e4e6,0.37254901960784315
17456,c4bca5d86a38c3,516eadf7,Visualizando la data,e23d297c,0.3728813559322034
17458,2a56d6b0e153f2,b1a05efc,"HERE, THE INDIVIUALS WHO HOLDS COMMERCE & MANAGEMENT DEGREE ARE LIKELY TO PLACED.",8dc315e6,0.3728813559322034
17459,a077820f7ab459,ab0b7dc0,## Transfer Learning ,05a43104,0.3728813559322034
17460,bb0905d33ae417,c133ffa3,"The most important hyperparameter in training neural networks in general is the **learning rate**. Unfortunately as of now, there is no way of finding a good learning rate without trial-and-error. 

The library has made it convenient to test different learning rates. We find a good learning rate using the method `lr_find`, then plotting the graph of learning rates against losses. As a rule of thumb, the learning rate is chosen from a part of the graph where it is **steepest** and **most consistent**.",25fd1965,0.3728813559322034
17462,a81661cc35d8d2,9cf70f58,"<font size=""3"">Observations:</font>

1. This is an unbalanced classification problem i.e. there are fewer cases of deaths due to heart failure as compared to survival (~31% of total)


2. Only high_blood_pressure variable seems to have a noticeable effect on the DEATH_EVENT variable


3. Other variables do not seem to impart much information. However, we will still keep these variables as we don't have an issue of dimensionality, and the variables are not highly correlated to each other as we observed in the correlation matrix above. Another consideration is that the classification accuracy may reduce if we drop these variables.",3331f113,0.3728813559322034
17463,ed8009f482b380,e7df68ce,Concat to the dataframe (df2) and make it into a new dataframe,e99941fa,0.3728813559322034
17465,9169c4e9c33c90,e34e49fc,"<a id=""Author""></a>",725bf880,0.3728813559322034
17466,1294fb4c86f993,98ce00ab,<b> Checking column `states` at both data files to be the same,4471e513,0.3728813559322034
17467,fc8e0042411c46,b832b2f3,**Nothng conclusive can be said on the basis of Total Visits.**,af476c2a,0.3730407523510972
17474,b01ee6cb674fa3,99b5f393," Lets check what these bellow have 
- weekday
- month",a8ffd35e,0.37318840579710144
17475,67b7354e96113a,c3cf9b95,Since age and sex seem's to be a good parameter we'll try to localize the Embarked based on those values and impute them,dca94250,0.37333333333333335
17478,e5dd725b8fa422,4767ab79,"This looks so messy. hence no insight hear also so it was getting harder to make assumptions.

",14675d8b,0.37333333333333335
17483,2bd6c370695ea7,9fe0ea9d,## Categorical Target Encoding,cbe6aec8,0.37333333333333335
17484,4daf6153275cbf,949d57a8,"That was quite a pandas usage :)

Finally, printing my whole master dataframe. I am not going to use all the featrues but some of them was needed, as shown in previous cells.",51db1961,0.37349397590361444
17485,835a7b4e660d23,280f298f,### Filtering Data,53bc7a6e,0.37349397590361444
17488,c84925c8171900,031644b1,"<div class=""alert alert-block alert-info"">
    <span style='font-family:Georgia'>
        <b>Insight: </b><br>
        We will delete these rows, i.e 39 game information as they are sold only once and sell value is insignicant in our analysis. 
    </span>
</div>",e21ff7ec,0.37383177570093457
17490,e19e307b3fd188,57165f06,"The number of parking spaces usually varies between 0 and 5, and we noticed that the more parking spaces, the higher the rent, which is already expected.The value starts to decrease from 7 parking spaces, something strange...",2173955b,0.37398373983739835
17493,5ce12be6e7b90e,0cf3afa7,"## ** Exercise ** if

print to the screen awesome if *grade* is greater then 65. <br>
At the end print 'finished' in any case.",c0ab62dd,0.3742690058479532
17495,2ada0305b68956,a50de27d,### 62. Palette = 'Set3_r',133e26f4,0.3742857142857143
17496,593d1d3d1df05a,ac539337,# Taking Negative Again,bc682ffe,0.375
17500,b4fe1e3b0414e7,a6729b33,"smv---Standard Minute Value, it is the allocated time for a task
wip --Work in progress. Includes the number of unfinished items for products
over_time--amount of overtime by each team in minutes",8d2bcffe,0.375
17507,5ba4207c371899,7acf8954,pd.crosstab - Compute a simple cross tabulation of two (or more) factors,187b1451,0.375
17508,62487bcd70b199,1879e4ef,"## Inference:
CCAVG > 3 there is higher chance of personal loan and there are better chances on family sizes 3 & 4.
They can be targetted for campain",f6ae50af,0.375
17512,1d5daeca89f48d,6d9504f8,## Using sklearn,48d478bc,0.375
17513,117fc0956643d0,d953608a,"<div id=""step2""></div>",68cef9fd,0.375
17516,3cd78d8d6d56e4,536c4b52,"### Keras Callbacks for Tensorboard
With Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)
",9f632e94,0.375
17519,7f74a04ae75792,2e04a635,## Check the data for missing values?,d01e91da,0.375
17522,e3c0b55ed519e2,04c29675,# Highest adult populations.,9f51352e,0.375
17524,13c7672da1b571,a6d912e6,"Below, I define the function which will preprocess data. It takes as arguments: 
- a train set `X` 
- a test/validation set `X_test` 
- the target output as `y`.  

It works on copies of the `X` and `X_test` data, and returns these two datasets after completing all preprocessing steps, which include:
- removing columns which have more than `missing_threshold` missing entries,
- imputing missing data in the rest of the numerical columns with missing data using the median value or 0,
- imputing missing data in categorical columns with a new value (often the NaN in categorical data has the meaning of a new category, e.g. ""the absence of a garage"") 
- removing features have mutual information with the target smaller than `mutual_inf_threshold`,
- creates new features by applying PCA to existing numerical features and then adding as new features the first `pca_components_to_include` components
- dropping categorical data that have cardinality larger than `low_cardinality_threshold`,
- performing One Hot Encoding on the low-cardinality categorical data.
",002d3ec0,0.375
17530,923e97b05be00b,44a01147,"Now, we've figured out what we want our model to take a look at. Behind the scenes, we just need to sort the data into two folders: one with negative cases and one with positive cases. We can click the code below to get this done for the purpose of this presentaion. If we're creating our own model, we would use another workflow to get it done (via montage, e.g.). Let's click the run button below to get this done.",3a4a22dd,0.375
17532,6f05f4ea9addbf,52b1fcfe,"We observe that the total number of missing rows to be 11, making the total number of rows being deleted less than 0.002% of the dataset.",dfb04c84,0.375
17538,e8e4447e99a463,bd218285,# Model,7256de70,0.375
17539,99bf357eaf61f1,e7ab397e,#### SalePrice Correlation matrix,9d92fafe,0.375
17542,56a583a039b57c,338d2ff0,### Aggregate on Median,c0526ea5,0.375
17543,edc19e349fe80a,f6fd08bb,Check the data to see if it is fit for training,7882221a,0.375
17544,5626e84c4e6bf8,17ee3ca2,"# How to interpret and evaluate a Self Organizing Map
* **Again, a Self Organizing Map creates a view that represents high dimensional data as low dimensional data preserving topological properties of the input space using a neighborhood function** 
* The heatmap in the background on which the clusters reside represent the topological properties of the input space. The colorbar() on the right represent the topological distance. The distance goes from **0(black) to 1(white) where lesser the distance, more is the correlation/similarity of the feature with its immediate neighboring features**.
* If the feature is white i.e., **topological distance close to 1, then they can be classified as anomalies**.
* The markers(colored shapes) represent different labels and are clustered on the topological space on the basis of their topological properties.
* Our goal is to have distinct clusters but that doesn't mean all the points of the cluster have to be close to each other because this is non-linear dimensionality reduction and not K-means clustering where points are located close to the centroids
* For better evaluation, we have to take care that any **given feature should be occupied by only one label/marker. We should optimize the map for the same.** 
* Overlap of mutiple labels on a feature means its uniqueness is compromised and there is a scope of improvement.",e2ecb669,0.375
17545,2c8119a4061997,c32fa949,# Loss,1836a79c,0.375
17549,96c4c0e36b8ec0,dc284a64,**Age distribution between Male and Female**,4dd6de8c,0.375
17551,c5fef7cc592736,8b914bcd,"With the numpy array, the transforms and the splits we create the training and validation `Datasets` object. 

With the datasets we can create the dataloader and we add some batch transforms to improve accuracy. 

* IntToFloatTensor() converts the tensor we had until now that was composed of 8bit values to float.
* aug_transform() helps generating larger training dataset from the current batch [more info about data augmentation in fastai](https://docs.fast.ai/vision.augment.html)
* Normalize() normalize the input it is very usefull when using pretrained models but even on not pretrained model like this it helps improve the performance",d21dc2c1,0.375
17552,0d58c434c7db1e,8cd229b6,"Woman FIDE master = WFM has the highest count.

Grand Master = GM is the highest title in chess game has only 37 count.",517e01d3,0.375
17554,d1ff7e10ee0102,0f335d2c,#### 'SalePrice' correlation matrix (zoomed heatmap style),2cc71c3c,0.375
17555,28a1ff0f223da9,b71ec81f,"**By analzing the data we get to see that there are many important fields in which we have very less number of professors such as ""data science"" which is mentioned by only 4 professors as thier area of interest and ""cyber security"" which is mentioned by only 1 professors as thier area of interest**",c945b27d,0.375
17556,9a040a4f21091e,985f76b2,"Although 93.6% correctly labeled might look good, keep in mind that the most naive classifier we could build (one that says all comments are non-toxic) would have 90% accuracy. So we've got a lot of work to do, evidenced by the f1 score for both classes.ds",f591b57d,0.375
17561,d46508f983e086,a2b4c3ba,"**Classes (Labels) are trained with many images. Usually around 100 images are sufficient to train a class.
Therefore I set the minimum amount of class as 100.**",454138b8,0.375
17564,a3ae04b78e45b5,9b75f6c6,**POINT PLOT**,4195da8b,0.375
17565,2ca509e51a6e4b,fbea102b,First we can use `.groupby` to count up co-occurences for any pair of features.,e63b0ba6,0.375
17567,fdbbd573ba31c2,bf526aa1,### rotor_torque(N-m),f7c28d74,0.375
17568,1011899b959f44,e7a9b6a9,4. Display the list of deaths ranging from rows seven through eleven (Hint: Use iloc[]),0b112382,0.375
17570,1005ca950e8a81,e632649e,"**Question 4: How many “misclusterings” do we have?**
Il y en a 3 d'après le graphique",52570331,0.375
17571,bd0e173abb7b52,6911afea,**4-5. What are the mean and standard deviation of age for those who earn more than 50K per year (*salary* feature) and those who earn less than 50K per year? **,9bce3b0d,0.375
17573,37b09262279764,fdbd6a5a,"There are many techniques, mostly used ones are described below:
- Use <b>Label Encoding</b> for ordinal(Which has order) columns
- Use <b>One-Hot Encoding</b> for nominal(which doesn't have any order in them) columns<br>
Here's a link to learn about ordinal and nominal columns:
https://sciencing.com/difference-between-nominal-ordinal-data-8088584.html",37c4c417,0.375
17574,02773bdc5d3c7a,4905a68c,"# **Noteworthy points**

1. The data has original columns hidden due to privacy issues and columns available with us are the **28 principal components** of the original columns. As we know that PCA requires data to be scaled before usage hence we can safely *assume* that the 28 columns are already scaled.
2. Amount column is not scaled and hence we will try to scale it.
3. Train test split must be done **before** applying any sort of resampling techniques. The reason is as follows:

Let's suppose that you perform oversampling of the minority class before the train test split. What oversampling will do is, it will replicate the minority class records and in our case the 492 records will be replicated to ensure that their count matches that of the majority class i.e. 284315.

> You see the problem here?

After oversampling when you perform the train test split, the model will have close to 90% (if not 100%) overlapping of minority class records in the training dataset and testing dataset. Thus, the model already has learnt how to handle the  testing dataset records (from the training dataset) and will pass all the evaluation metrics with flying colours. 

> How do we handle this situation?

Perform the train test split first and then apply any resampling techniques. This will ensure that that the minority class records in the training dataset are replicated to form a balanced training dataset. The model will learn to differentiate between the fraud and non-fraud cases using the balanced training dataset and its performance can be efficiently and unbiasedly be evaluated using the unseen testing dataset.",86245f35,0.375
17575,8447633e1d256c,788c0fa2,## Feature Engineering,60d593ce,0.375
17577,84e0e568316ba1,7a25d98c,## Create skewed data from chi2 distribution stride across ZERO,9ebad019,0.375
17584,7a75cba9317186,0cd84ec1,# HOMEWORK 2,3d7e3235,0.375
17591,254cccd5145725,bc210f02,Now we are gonna combine the test and train dataset as this way we can reduce the redundancy of performing the same operations of the train on the test dataset. We will separate them after we clean the data.,a49b4037,0.375
17599,49f2274c1dd516,1111e283,DataFrames that share the most features,06b0ffee,0.375
17603,7ba63a2d9abb58,1091f432,"<ul>
    <li>The data shows that US, Brazil and India are the top 3 countries with most confirmed cases.</li>
    <li>The total deaths are not following the same pattern of total cases.</li>
    </ul>",821a261f,0.375
17605,d8d227c158d883,b12d8e72,## Model,3391b4a7,0.375
17608,8c7e00ca3dc5a7,fd14b85e,## Correlation Study,c83346e4,0.375
17611,3dd4294f903768,76eca1f9,"It looks like this data here is just the same as the 'Rating text', so we will use the Rating text column.",0d89d098,0.375
17616,8dd655515e7d18,a4ce5a6d,"**Analysis:**
The 'Conscientiousness' is uniformly distributed between min-8 & max-50 ",895f41cf,0.375
17622,b61ab8f81dc03d,1a2bfd01,Let's check if there are any missing value on age column,64d05394,0.375886524822695
17623,ba4b3bd184acbb,b9076ecb,"From the output, we can see that Category, Installs, Type, Content Rating, and Android Ver are all `object` datatype and have less than 50 unique values.

These columns could be stored more efficiently as a `category` datatype.",0f5de724,0.37593984962406013
17628,fc8e0042411c46,fba617ec,## Total time spent on website,af476c2a,0.3761755485893417
17630,513ce405d7f6a3,1ef8bf25,# **wordcloud of Very negative review**,8461e086,0.3764705882352941
17631,869a39a3d4dea2,cdaa25ee,"#### Image Transformation <a id=""image_transformation""></a>
",9020daf8,0.3764705882352941
17633,75adb7945ef9bd,f97c89f8,"## 5. Clean up Text Column

Here we clean up the text column by:
- Making a 'clean' text column, removing links and unnecessary white spaces
- Creating separate columns containing lists of hashtags, mentions, and links",785c5095,0.37662337662337664
17635,241cf32abb22d8,a0e215f8,### Random Forest Importance,47157066,0.37662337662337664
17640,90691864eb68c7,0fdb1180,# 4. Preparing Data,3555ef9b,0.37662337662337664
17652,9d9da6c439b96b,434af703,## Matrix orrelation,361cc7d9,0.37681159420289856
17653,17a24d566ffa59,809c94af,"### Reduced SVD

The SVD comes in two forms, a full SVD, and a reduced SVD. In NLP, we tend to focus on the reduced SVD, using SVD for dimensionality reduction.

SVD can be viewed as a sum of rank one matrices.The matrix A can then be approximated by choosing any k ≤ r. This generates a rank k matrix, Ak, that is the best rank-k approximation to A in terms of least-squares best fit

SOURCE: ftp://ftp.sas.com/techsup/download/EMiner/TamingTextwiththeSVD.pdf",89049e56,0.37681159420289856
17655,0e2a23fbe41ca9,e1f89a01,"Observations:
- Big tail after IQR (many outliers) for both the columns",64e4762c,0.37681159420289856
17658,09751c520b0616,2f7e178b,###  Dealing with low variance feature,a4d0c7e9,0.3769230769230769
17665,2105f2c5132866,664e1a2c,"Wrangle data

We have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.

Correcting by dropping features

This is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.

Based on our assumptions and decisions we want to drop the Cabin and Ticket features.

Note that where applicable we perform operations on both training and testing datasets together to stay consistent",bfe8023d,0.3770491803278688
17668,fe7360cddc13e5,0877dfca,<font color='red'> **2- t Zamanda x Transaction Gerçekleşme Olasılığının Türetilmesi**,8979e423,0.37719298245614036
17669,6cade0b6a41ba2,5bdd824c,## 3.6. Ever Married,e6110293,0.37719298245614036
17670,3fb15e6e48aec2,91feebc7,"# Ticket
* cleanup ticket (miss-spellings etc)
* get start and end of ticket",9d1f4358,0.37719298245614036
17671,23df07a474aaae,fa2b104f,**Splitting the data into Training and Testing Sets**,0ea40276,0.37735849056603776
17672,510b8303776bb6,498359c6,"list_pure_categorical: This is the list of data fields which do not have any linear pattern and thus we cannot apply regression on these varibles directly in any manner and therefore these variables will go under one hot encoding.

categorical_ordered: It is the list of categorical data fields which have some king of order to it and therefore we can directly apply regression on them by converting these into numbers and do not need to apply one hot encoding on these.

list_continuous: It is the list of continuous data fields in the dataset.(except SalePrice)",18080db8,0.37735849056603776
17673,43e60eb1362f5c,10031fb5,# Merging of  3 data sets,87934234,0.37735849056603776
17677,f015d0147e8fbf,2ef377c8,### 5. POS CASH Balance Data Table `POS_CASH_balance.csv`,518954fb,0.37735849056603776
17679,0ad8d416b89b78,6120288b,"# Occupation:

Occupation another attribute that was identified as having potential to be a useful attribute for use within the classification task due to its correlation with the income attribute. There is a increase in the proportion of those earning over $50K with respect to the 'Prof-Speciality' and 'Exec-Managerial' categories when compared to the other categories within the attribute.",0b0562f0,0.37735849056603776
17681,087e21401d7dfc,9ff3a6f7,# Accuracy,42000489,0.37755102040816324
17688,b0c2805cd5c087,b2989b45,Image linkedin.com,0446f327,0.37777777777777777
17690,49ac6594c8f5cf,b703d245,Secondary school percentage also related to salary but can it be attributed to better stream selection following secondary school?,6f19f28a,0.37777777777777777
17691,e25c0f830df3f4,01fcbc40,# Displaying Positive comments,fdcf7189,0.37777777777777777
17695,d58491f2896fc1,fe99d1eb,**Split fonksiyonu veri setimizin training ve test seti olarak parçalanabilmesini sağlamaktır.**,514bfdff,0.37777777777777777
17696,188731d7fa0604,5ddb4a99,"## 풀이 (Baseline)
- 아래코드는 정답이 아닌 풀이 예시입니다.",7cc543d3,0.37777777777777777
17700,c8bf959b9608cf,c93472af,"### Giving name to the layers
We can give each layer a particular name while defining the model. <b> Later, you can access a particular layer with its name </b>. Let's look at the layers with their names. ",155e3672,0.37777777777777777
17705,0b01138ad120fc,6961463c,"**To make the work easy, I'll set numbers instead of strings**",0b4b72e6,0.3780487804878049
17707,9c26c5dcd46a25,ddd2348d,"#### <font color=""#114b98"" id=""section_1_4"">1.4. Analyse des corrélations linéaires</font>
Pour analyser les corrélations linéaires entre nos variables quantitatives, nous allons réaliser un **test de corrélation de Pearson** et afficher ses résultats dans un heatmap :",1bbbb677,0.3780487804878049
17708,c4386b8a01d66e,dc925312,# Chloramines,dc732bf5,0.37815126050420167
17709,4ae6a182abac64,988558fc,* **SibSp Feature** ,418676c5,0.37815126050420167
17715,d76896b30cebd3,475c7548,Distribution of Categories,1b4e8f34,0.3783783783783784
17716,62037c5832129c,79d75fcb,"## Algorithm selection with nested cross-validation
Cover if time permits",61474350,0.3783783783783784
17718,bdf23d2d396916,0dc96495,#### 3 Boyut olduğu için 4 sutun seçemezdim. Bu yüzden yüksek korele olan sutunlarımdan sadece birini seçerek grafiği çizdirdim. 👇,b0e45a49,0.3783783783783784
17719,bbb3f4b76a4559,bcefaef7,"### SMOTE
SMOTE give us easily over/down sampling imblance dataset. In this kernel, I chose oversampling smaller label. If your target is not binaly_classification, you can use this cell without changes.  
But you have to change k_neighbors to fit your data. This sets how many neighbors data SMOTE chose.  
details : https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html

And unfotunately, downsampling and bagging may be better approach...  
https://www.semanticscholar.org/paper/Class-Imbalance%2C-Redux-Wallace-Small/a8ef5a810099178b70d1490a4e6fc4426b642cde",75185823,0.3783783783783784
17720,2dda7facf3c1e0,8d6bbe1c,"## Preprocessing the data into the *piano roll* format

### Reviewing sample piano rolls 

### Why do we use 128 timesteps?
In this tutorial, we use 8-[bar](https://en.wikipedia.org/wiki/Bar_(music)) samples from the dataset. We subdivide those 8 bars into 128 timesteps. That's because each of the 8 bars contains 4 beats. We further divide each beat into 4 timesteps. 

This yields 128 timesteps:

$$ \frac{4\;timesteps}{1\;beat} * \frac{4\;beats}{1\;bar} * \frac{8\;bars}{1} = 128\;timesteps $$

We found that this level of resolution is sufficient to capture the musical details in our dataset.

### Creating samples of uniform size (shape) for model training 

For model training, the *input piano rolls* must be the same size. As you saw when we used the `play_midi` function, each sample isn't the same length. We use two functions to create *target piano rolls* that are the same size: `process_midi` and `process_pianoroll`. These functions are wrapped in a larger function, `generate_samples`, which also takes in constants that are related to subdividing the .mid files.

#### In the code cells below:
- `generate_samples` is a function used to ingest the midi files and break the files down into a uniform shape
- `plot_pianoroll` uses a built in function `plot_track` from the  [`pypianoroll`](https://salu133445.github.io/pypianoroll/visualization.html) library to plot a piano roll track from the dataset.",45552d2b,0.3783783783783784
17724,b7b1057764fa02,7e426c82,"The testing images look rather similar to the training images - this is because they have both been taken from the same dataset. Therefore, we can already expect that a model that does well with the training images will also do well with these testing images.

Do the evaluation images look similar to these? Let us have a look.",5053a192,0.3783783783783784
17725,a6c34cd514e30e,645f2405,Distribution graphs (histogram/bar graph) of sampled columns:,bf603ddd,0.3783783783783784
17726,e1fff2f67cbe32,bccdba05,> Timestamp represents the time from the first user interaction to the current one and Prior question elapsed time represents how long it took a user to answer their previous question bundle.,c6dfde64,0.3783783783783784
17737,917957c6c4065f,7a5182de,# 분석,55b8ed68,0.3790849673202614
17739,20e1ba19eb9b5e,4b579e60,"More or less all the observations of TotalBsmSF are ""in-liners"" (not outliners).",4569bfc1,0.3793103448275862
17741,656185a18260be,48f1d68c,"# Prepare Features and Define QA Postprocessing

These functions are copied directly from HF sample scripts, I didn't modify them. ",0318cab5,0.3793103448275862
17743,00d295edcd117e,84d5be18,展现一些训练图像。,f5810f4b,0.3793103448275862
17749,858da4bb312f67,e32223dd,"## Train Models
Let's start training! 

As I don't want to break the pretrained weights of disciminators(VGG16) at initial epochs, weights are frozen at first 3 epochs. Then train all layers.",9cca4391,0.3793103448275862
17750,5fc2f23dfbeeb1,5c58653d,"### Concatinate Train & Test Data

There were ""not a number: NaN"" state on both keyword and location columns, thus fillna that is used.",f37b4110,0.3793103448275862
17752,ef6d1e959a873e,33b0ad6c,filling null values with randomly same column values,f11a1f43,0.3793103448275862
17757,eb800c50fcfbb2,e6224c3c,"# Model

![Untitled Diagram.png](attachment:1859e0b5-5f35-4b91-b04c-3a9e8a6febc3.png)",e7173f4d,0.3793103448275862
17762,6903d3f38c6a66,97293dca,"The first of the **plt.subplot()** parameters specifies the number of rows and the second the number of columns. 

The graph looks a bit frustrating. In this case, you can use **plt.tight_layout()** to solve the frustration.",6067ce5e,0.3793103448275862
17764,6a1d04e8153df3,7c4b819a,"**Observations -**
- Minimal deaths occurred in every age group.
- Above 66 there are less deaths.
> Overall, I can say that deaths are common in every age group and many are alive but according to this data many alive of age greater then 66.",38572b05,0.3793103448275862
17770,4b7039cb44a54c,e3fc9235,# CFG,24e806af,0.3793103448275862
17773,fb5c6021d127ef,a5e97386,"## Model creation

Now it's time to turn this data into a model. You'll use the `CREATE MODEL` statement that has a structure like: 

```sql
CREATE OR REPLACE MODEL`model_dataset.bike_trips`
OPTIONS(model_type='linear_reg') AS 
-- training data query goes here
SELECT ...
    column_with_labels AS label
    column_with_data_1 
    column_with_data_2
FROM ... 
WHERE ... (Optional)
GROUP BY ... (Optional)
```

The `model_type` and `optimize_strategy` shown here are good parameters to use in general for predicting numeric outcomes with BQML.

**Tip:** Using ```CREATE OR REPLACE MODEL``` rather than just ```CREATE MODEL``` ensures you don't get an error if you want to run this command again without first deleting the model you've created.",dd05cbd3,0.3793103448275862
17775,84127ade6fde87,809b2877,Note that letter_t holds a one-hot-encoded character per row. Now we just have to set a one on each row in the correct position so that each row represents the correct character. The index where the one has to be set corresponds to the index of the character in the encoding:,f55d05b6,0.3793103448275862
17782,c80939c7c626cf,e2021f2f,"# We now need to change the values of Sex because ML algorithms cannot classify with ""male"" or ""female""",b9ac31e2,0.3795620437956204
17785,fdc3afd309b850,6166f33f,"<a id=""gnv""></a>
### 6.2.3 Garage Null Values",966bde38,0.37962962962962965
17789,b10bd75889dad9,4d387778,### Data Visualization,ee00ceee,0.38
17790,83df814455f06c,fc33b6fb,The `class` target variable is ordinal in nature.,c9cff71a,0.38
17791,4cd25e50c7e007,6fcf91c5,"**The bike rental is more during Clear, Few clouds, Partly cloudy**",ceb0c525,0.38
17792,2ada0305b68956,90d6e1b8,### 63. Palette = 'Spectral',133e26f4,0.38
17795,0687cd5c8597db,10f23432,"### **Splitting the Dataset into Training, Testing and Cross Validation Datasets**",4edec76a,0.38
17800,30fdc4a6e3c1db,367ed219,"What we get:
* Most products dont change prices at all and most changes are restricted to 10-15\\$
* Household items have the highest price changes over the years ",6111ddee,0.38011695906432746
17801,2f47abddfd1928,ce9808c9,"We can quickly see how some cabins are more likely to survive, possibly because it was easier to get to the deck or because they were far from the first sinking part of the boat.

Also seems that there is some correlation between Survived-Pclass-Cabin, which makes sense.

Looking to these results, we can leave the missing values as other as it still provides some useful information.

To work better with this feature I will map it to convert it to numerical feature.",ae33cc0b,0.38016528925619836
17802,e9b9663777db82,3fbea1cc,#### 5.Quantitative analysis for Number of Floors feature,648e8507,0.38016528925619836
17808,d8ff894670d506,9ddaac7f,The states that have most number of schools that opened project to gather donations:,eb0fb7de,0.38028169014084506
17809,b01ee6cb674fa3,0cd4120b,"...and the results of weekday and month are standard, as expected

so, nothing to do with it :)

---

Next, lets check the column detail and see what we can extract from it ",a8ffd35e,0.3804347826086957
17811,ac1abfe1dfe815,064a35c4,"**The number of positive, negative, and neutral tweet every day**",6529dbcb,0.3805309734513274
17815,870a7144fa75ad,19851e9a,******Convert deopendant feature ham and spam into dummies**,2a8c3427,0.38095238095238093
17817,15eb884262ba09,91b22c99,**The World as we know it (Random Centroids)**,d703bdab,0.38095238095238093
17821,7a058705183598,ef50bf28,Machine Learning,b0ead917,0.38095238095238093
17822,b4ecd6e4277e3c,e0c6716e,Extra feature part taken from https://github.com/wongchunghang/toxic-comment-challenge-lstm/blob/master/toxic_comment_9872_model.ipynb,94d79d5f,0.38095238095238093
17827,7454fdc444df16,d6b95f95,"## Visualizing the Breast Tissue
Earlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue. 

We can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the  most common areas where the cancer appears. 

If position of the crop has significance then perhaps we can use it as an input feature for our model.",a7818ef5,0.38095238095238093
17829,a76e0e8770b7a0,9c76bca5,İnterestingly 1970 have terrors. Which Terror???,02863d3b,0.38095238095238093
17830,066c5ee1ef39e6,284fd99a,### Load Validation and Test data,0f394e1b,0.38095238095238093
17835,fda19edaf5c621,463b9734,Let's check the participation by Age-Group,5cfff76e,0.38095238095238093
17843,8985a124d4b657,cd2906a5,"Since this is a time seris data, we should be predicting the values after looking at a set of values rather than just a single value like we usually do. This takes into account the correlation between the data points and the timestamps. Because the neighbours should be considered for how the values change over time. Let's define a function to do this.",586d1846,0.38095238095238093
17849,04bac111ffbe9c,7ae80aec,"##### CONCLUSIONS :
1. Most people were travelling alone.
2. The highest number of family members one was travelling with was 10
3. Maximum number of people travelling alone were from 3rd class.
4. No person from 1st or 2nd class travelled with > 5 family members,

##### 3. Remove PassengerId and Ticket",82576b17,0.38095238095238093
17860,e16860fce156b0,f6587def,"#<b><mark style=""background-color: #9B59B6""><font color=""white"">Understand the relationship between two columns with plot(df, x, y)</font></mark></b>


Next, we can explore the relationship between columns x and y using plot(df, x, y). The output depends on the types of the columns.

When x and y are both numerical columns, it generates a scatter plot, hexbin plot and box plot:

https://sfu-db.github.io/dataprep/user_guide/eda/plot.html",2054f1ce,0.38095238095238093
17861,a758983a68c014,d124aecb,![caption](https://miro.medium.com/max/335/1*uYiqfNrUIzkdMrmkBWGMPw.png),ab89f181,0.38095238095238093
17863,c818250dd720eb,74d8d568,"# Data
It took a little time to get used to working with multi-level tiff files. Multi-level means the file contains the same image at three different resolutions, corresponding to a downsampling of 1,4 and 16. We see here an sample from the same image at each of the three resolutions starting with the lowest before zooming in twice 4x in each case on a particular section.",68ee40de,0.38095238095238093
17867,9169c4e9c33c90,98c05696,"# Author

[Back to top](#Top)",725bf880,0.3813559322033898
17869,225b4fe5d3894a,b4c3fdd7,"<a id=""6a""></a>
### a. Data Cleaning
Missing values can be dealt in follwoing ways:
1. Get rid of the corresponding values
2. Get rid of whole features
3. Set missing values to some value (zero, mean, median, etc)",4b4197b3,0.38144329896907214
17876,f0fab078f8533b,460638c1,## e. Converting 'listed_id' and country to a list of show types and list of countries respectively,bdb5ea32,0.38181818181818183
17879,04ff2af52f147b,725b9a75,"We will move them to 'A' deck, the closest physically to the boat deck which conveniently only has other first class passengers.  This will ideally lead to better predictions as we only have a samply size of one for 'T' deck.  We no longer need the *Cabin* feature either now that we have extracted the data we could from it.",d5f37be9,0.38202247191011235
17882,e19e307b3fd188,efe5ba43,### Fire insurance,2173955b,0.3821138211382114
17883,7cfd96218dd933,08078f78,"#### **ATTENTION**
* HERE IS ANTALYA - MANAVGAT
* THE STRONGEST FRP WAS SEEN IN ANTALYA IN THE LAST 11 DAYS.",7c34d96c,0.38235294117647056
17884,c65a65d4041018,d0e20efe,"As expected most of the students have none to little education. And there are a lot of starting DA, DE and DS.

One interesting thing which I see is that there are a lot of experienced DE in Russia. I think they are programmers who switched career to SE.",824fb229,0.38235294117647056
17888,71b75664517244,d0d5f3b5,### Chelsea,fc905af5,0.38235294117647056
17889,1d1598b6fa2aa7,42cb8d64,"On the second chart, we have already used the ```plotly.graph_objs``` instead of the ```plotly.express```, which allows us to fine-tune the charts.

More information - https://plotly.com/python/line-charts/",e066accf,0.38235294117647056
17890,52cfd66e9ec908,2096f3eb,# Metadata Exploration,c74adcdf,0.38235294117647056
17894,21bce4ec54b3fa,34896e7e,Create train test splits,35546e30,0.38235294117647056
17895,842547b2def18c,572ff72e,Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.,b8efde6d,0.38235294117647056
17901,a0a5baa6c7e12a,d896afd9,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Pair Correlations Between Features</div>",551d41de,0.38235294117647056
17902,ab657da5329e3f,d591f756,# Datasets,021526f8,0.38235294117647056
17903,99821bc6a45be6,cd238c34,"> ## Data Generator:

When working with a relatively large dataset, it is often impossible to load all of the datapoints into memory for model training. This is especially a problem with images, which take up a lot of RAM when decompressed from the .jpg or .png format to numpy arrays. The process of loading images also takes time. To combat both of these problems, a Data Generator is used. With a Data Generator, only a small portion of data is loaded to memory at a time while training. This is done by generating batches of images and applying augmentations to them in real time before passing them to the model. Additionally, this can be done at the same time as the model is training on a different thread. This eliminates both the problems discussed earlier.",b9d59346,0.38235294117647056
17906,7f74a04ae75792,3cac9d8b,### Count all NaN in each column,d01e91da,0.38235294117647056
17909,726833f92fb87a,d4ba1772,**We cannot draw any particular conclusion.**,7dc5e1b6,0.3825503355704698
17910,5f32117bcd5255,466fbabd,#### EXPOSURE INFORMATION,85882abf,0.3825503355704698
17914,0932046e1f485d,7e5ff97a,The most reviewed apps on the store,218cc7a3,0.3828125
17917,957e035ba5b9d5,a5f00320,## Fine-tuning the network ,778ab3d3,0.3829787234042553
17925,f6648e47713411,dc86ff6d,"Chúng tôi đã plot một vài hình ảnh trong training data ở trên (các giá trị RGB có thể được nhìn thấy bằng cách di chuột qua hình ảnh). Các phần màu xanh lá cây của hình ảnh có giá trị màu xanh lam rất thấp, nhưng ngược lại, các phần màu nâu có giá trị màu xanh lam cao. Điều này cho thấy rằng các phần màu xanh lá cây (healthy) của hình ảnh có giá trị màu xanh lam thấp, trong khi các phần unhealthy có nhiều khả năng có giá trị màu xanh lam cao. **Điều này có thể cho thấy rằng kênh màu xanh lam có thể là chìa khóa để phát hiện bệnh trên cây trồng**",f4af4d1c,0.3829787234042553
17929,541d0fa0e26b80,737d2ea8,Let's Check  for Correlation between attributes,a29e0f29,0.38333333333333336
17933,37b09262279764,74d672db,"Nominal columns: <b>Sex</b><br>
Ordinal columns: <b>Embarked</b><br>",37c4c417,0.38333333333333336
17934,b10bd75889dad9,4b0f1f44,#### Understanding the Data : Data Visualization for Numeric data,ee00ceee,0.38333333333333336
17936,63d0d9b9a8c7d2,34092042,# Doing the similar things on Test set too,e32e5933,0.38333333333333336
17938,62487bcd70b199,506d617d,## <a id='4.8.'>4.8. Distribution of Categorical Variables across Loan and Non Loan Cusotmers</a>,f6ae50af,0.38333333333333336
17939,07f5853e4db8f8,8a6fbfa0,"# Product dataset 

| Name | Description |
| :--- | :----------- |
| LP ID| The unique identifier of the product |
| URL | Web Link to the specific product |
| Product Name | Name of the specific product |
| Provider/Company Name | Name of the product provider |
| Sector(s) | Sector of education where the product is used |
| Primary Essential Function | The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled |
",d13c2c32,0.38333333333333336
17941,712198370d5521,6feb484d,"The above stats show some discrepancies in mean Income and Age and max Income and age.

Do note that  max-age is 128 years, As I calculated the age that would be today (i.e. 2021) and the data is old.

I must take a look at the broader view of the data. 
I will plot some of the selected features.",5882e04c,0.38333333333333336
17946,738bfced935b69,c79530f7,"The car price with Manual transmission in the Petrol, Diesel, Hybrid, and Other fuel are less than 60000 pound, while the car price with Automatic transmission in the all types are cheap and expensive, and the car price with Other transmission in all types are less than 23000 pound.",2d3c592d,0.3835616438356164
17948,3d08ca7656dec0,d946a46a,# age,bd3f87e3,0.3835616438356164
17949,1eb62c5782f2d7,93a4b030,"### Step: 
- Ketika sebuah random variable $x$ berdistribusi normal, kita dapat mencari probabiliti dimana $x$ berada di suatu interval dengan menghitung area dibawah normal curve untuk intervalnya. 
- Untuk mencari area dibawah normal curve, pertama convert upper dan lower bounds interval ke $z$-scores. 
- Lalu gunakan Standard Normal untuk mencari areanya.",bb69f147,0.3835616438356164
17951,d96642860ab3dd,7fb7431d,"### Error RuntimeWarning: divide by zero encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) This error is because fare feature contain 0 values, but it must not have zero(0) values. which means zero is nan value",98419d48,0.38372093023255816
17953,b01ee6cb674fa3,241d3037,"It can be seen that the 60's and 70's were marked by the space race, then a 'calm' was observed by the 80's and so on until the last 4 years.

Wow, does a new spacerace has began? ",a8ffd35e,0.38405797101449274
17956,fdc3afd309b850,3fb1db62,"The same technique will be used here, but now we will fill with 1 the places without references of garage",966bde38,0.38425925925925924
17957,0a1fcda859252c,4e1341bb,"## Augmentation
Data augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But augmentation can be much more helpful where the dataset is imbalanced. You can generate different samples of undersampled class in order to try to balance the overall distribution. 

I like [imgaug](https://imgaug.readthedocs.io/en/latest/) a lot. It comes with a very clean api and you can do hell of augmentations with it. It's worth exploring!!
In the next code block, I will define a augmentation sequence. You will notice `Oneof` and it does exactly that. At each iteration, it will take one augmentation technique out of the three and will apply that on the samples ",13a38774,0.38461538461538464
17961,a915263bc207da,b7e58bda,### Creation of corelation matrix,b17ebcda,0.38461538461538464
17963,d78988cb5a1b02,0287737b,# **Random Forest**,233f3a92,0.38461538461538464
17964,e8a3483b83a26e,42d35237,no categorical data and no missing values,0c5db5fd,0.38461538461538464
17965,669ce946943d60,5f3552e1,### preprocess,0f63c4ce,0.38461538461538464
17967,aae204e78a48d1,288484b8,"We can clearly see from this graph, that the bulk of the attrition base is from 'Blue' customers.  But does this reflect the portfolio as a whole?",53ab6133,0.38461538461538464
17968,e1a69c71c2c282,ca5ee310,# feature analysis,b2ca7d3a,0.38461538461538464
17970,582cb872d19026,e3bf2f36,"
******************************************************************************************************************************
#### As Game Action category have a greater number of installs with respect to other categories as shown in Figure above. 
******************************************************************************************************************************",8d966d69,0.38461538461538464
17971,9a96e1588410ee,cc10cbfb,"## Settingup & Training the model
With my basic knowledge of fastai and using the sample code provided in the [fastai course](https://course.fast.ai/). Tried resnet18 and resent 50 as well, but not so great results. So finally settled with resnet50.",3fa6cf92,0.38461538461538464
17973,a8c042af6b7245,4b859d25,"### 연관분석
url : https://hezzong.tistory.com/entry/python-%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99%EB%B6%84%EC%84%9DA-Priori-Algorithm",2487ac62,0.38461538461538464
17974,a4f8ad33c823c5,b9902215,"Replace all the missing vital measures will 0. This is done with the assumption not all the blood tests may have performed on the patient. Hence, it could be possible for the patient to have nil readings.",fcd48307,0.38461538461538464
17979,163ceeb80d6923,aa503fb4,## Create a Model,4adfbb90,0.38461538461538464
17984,3536195ad632ee,f377032d,"**Regularization and optimizatio**n

In the model there are hundreds of thousands of parameters. 
To find the best set ... or one of the best sets of parameters in a reasonable time - there are several tricks you may apply.
Without going into details these tricks are dropout, batch normalization, ADAM. These are already there, defined in the model.

There is one more trick in this kernel which is data augmentation. This is a very useful trick and it does a little change in the original image. Using this trick you alter the original image just a little and you do this change in a random way. So while training you take the original image then alter a bit and you do the training. 
Two things which I learned from this.
1. Original image never used as input for the network during training.
2. Practically you never train with same exact image.",3c26aafc,0.38461538461538464
17987,b27bdd02db1bbd,81166cc5,"## Hint
- 그룹 합계 ",79340a85,0.38461538461538464
17988,e7237da7cbec10,a7f154b1,"3.  Min Max Sclaler 
",5fcf5e3d,0.38461538461538464
17990,5c6fcd59adac6e,31560e65,So 244 prescribers prescribed opioids in the state of Alabama more than 10 times in the year of 2014,f365ba13,0.38461538461538464
17991,5af9bf52e5f17c,20e51500,"Plotting confirmed cases and fatalities by country.

In decibel log10 scale

0 dB = 1 case

10 dB = 10 cases

20 dB = 100 cases

30 dB = 1,000 cases

40 dB = 10,000 cases

50 dB = 100,000 cases",f98ab90d,0.38461538461538464
17996,897ca904b74a98,9a6f2309,##### Blood Variables,c5844ad4,0.38461538461538464
17997,020c28a360b0cd,ef63e90b,"<table>
  <tr>
    <th>Requirement</th>
    <th>Description</th>
    <th>Points</th>
    <th> Earned </th>
  </tr>
  <tr>
    <td> Originality </td>
    <td> All code is your own. Any code snippets that are not your own are properly cited. No more than 5 lines from any website. </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
  <tr>
    <td> Planning & Reflection </td>
    <td> The ""make a plan"" section includes a detailed description of how you plan to complete the project. Reflection is thoughtful and answers all required questions. </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
  <tr>
    <td> Compiles </td>
    <td> Code Compiles </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
  <tr>
    <td> Comments </td>
    <td> Code is properly commented </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
    <tr>
    <td> Conventions </td>
    <td> Naming and style conventions are approriate </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
        <tr>
    <td>  </td>
    <td> </td>
    <td></td>
    <td></td>
  </tr>
      <tr>
    <td> Questions </td>
    <td> All relevant questions are asked. In the buzzfeed quiz, there are at least 4 questions. </td>
    <td> 10 points </td>
    <td>  </td>
  </tr>
            <tr>
    <td> Results </td>
    <td> Results are accurate based on the user's responses. In the president project, the user is told why they do / do not qualify. In the buzzfeed quiz, there are at least 4 unique results. </td>
    <td> 10 points </td>
    <td></td>
  </tr>
        <tr>
    <td> Logical Operators </td>
    <td> and, or, or not are used at least once in a meaningful way. </td>
    <td> 15 points </td>
    <td></td>
  </tr>
          <tr>
    <td> Conditionals </td>
    <td> Conditional statements are used to get the appropriate result. </td>
    <td> 15 points </td>
    <td></td>
  </tr>

</table>

---

""Final"" Score: 

---",2ba397f0,0.38461538461538464
17998,dbe40fdf51456d,49379c56,"### Example 1.
The ground truth relationship between the treatment feature and the outcome is given by 
$$  \theta(x) = e^{2x}$$",f3e8a1e4,0.38461538461538464
18001,44f6a002ecd033,c72f4b46,For this part of the process I am going to use simple imputer to quickly and effectively deal with my numeric missing data,70bbe106,0.38461538461538464
18002,09751c520b0616,5bb9b9eb,"- Making plots<br>
After making plots we found that some columns have low variance so we decide to drop them.",a4d0c7e9,0.38461538461538464
18004,6e8f3e8ed1c241,3cd9bfc9,"So since the the xkcd list was in a txt format, and it had a few too many classifications required for the demo version of the app.  
I couldn't find a proper way to find what classified as ""red"" for instance so these are just the rgb values kept in the array.  
Unfortunately we simply didn't ahve enough time left in development because when we finally go to the stage of removing backgrounds our exam weeks rolled over which halted the progress.  
As time goes on, I'll do my best to translate all the color survey findings into the array here.",c2cfb626,0.38461538461538464
18008,4f69b7bb1ca287,a47a273b,## Preprocessing,34abc6a1,0.38461538461538464
18009,d4c5aaa4b36810,1ed5d148,### (Can run notebook from here),65441f28,0.38461538461538464
18013,4d91e84c564cbe,701a5dfe,`sorted` returns a sorted version of a list:,355a43e3,0.38461538461538464
18016,5f4ae633cfd090,40f3c728,More than 38% of these values are null,a30a16e2,0.38461538461538464
18019,09cabbffb7909b,12149b07,"### Notes
1. To use sklearn with cudf... convert `cudf.Series` / `cudf.DataFrame` `.to_pandas()`..
2. Using cudf is the same as using pandas but `cudf` instead of `pd`..",835687fc,0.38461538461538464
18024,80ad12f326ab70,7755ec9e,Above concatenates the engagement data from all remaining districts in one dataframe by adding the key column 'district_id' to each engagement file as shown below.,da404a16,0.38461538461538464
18029,95efc1ad1d3e26,478f733b,"Show results for different hidden dimensions (original + hidden_dim=32, 64, 128, 256, 384, 512)",79de1120,0.38461538461538464
18030,8ac70416723897,d9fea0d1,Starting Exploratory Data Analysis,d32fd8f6,0.38461538461538464
18032,21122355e39af4,4b24ff7f,"QUESTION: How many “misclusterings” do we have?

3",88b95e2b,0.38461538461538464
18033,d0f6276d5b628c,2db26d06,This dataset has been produced on the total votes and effective ratings. This will help us get the best recommendation as per the client response found in the dataset.,c64f5ce5,0.38461538461538464
18034,c115e287523aab,8268d5db,## Train-Test Ditribution,feb1288b,0.38461538461538464
18039,63b44c85e32c1f,cc8b93a9,**count( )** is used to count the number of a particular element that is present in the list. ,fb9b9562,0.38513513513513514
18040,979f1e99f1b309,59fc5817,***THE plot show that most of players didn't walk too much and that cause they were killed after they down***,d1bfebbf,0.38524590163934425
18045,917957c6c4065f,a96bc78f,## 1. 인기동영상의 기준은 무엇인가,55b8ed68,0.38562091503267976
18047,2ada0305b68956,2d696055,### 64. Palette = 'Spectral_r',133e26f4,0.38571428571428573
18051,675b60eaf415a6,c3814c74,"* We now have train and test data ready  
* But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  
* To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  
* Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2",68c0b725,0.38571428571428573
18052,fe6750354fb64f,bdc3f421,# Pie Chart,271741f0,0.38571428571428573
18056,9f3710be6aea65,cc5d432a,We have seen there are two variables with family relations (SibSp & Parch) so why we can not create a variable that resumen the family member for one person? And also count the people who travel alone in the ship. ,ae9bda88,0.38596491228070173
18057,fe7360cddc13e5,0ce61990,"Müşterilerin transaction sayısı ile transactionlar arasındaki geçen süre arasındaki ilişkiye bakarak, ve transactionlar arasındaki sürenin üstel dağılıma, t zamanda x transactionın gerçekleşme olasılığının da poisson dağılımına uyduğu varsayımlarından yola çıkarak bu iki dağılımın fonksiyonlarına veriler uydurulduğunda P(X(t) = x| λ, p), aşağıdaki gibi formülize edilebilir.",8979e423,0.38596491228070173
18059,d8fb26c4197325,7b7d4f33,"# Feature Engineering
fonte: https://www.kaggle.com/skooch/xgboost",b190ac50,0.38596491228070173
18065,5ce12be6e7b90e,41bbe547,"If grade is  lower than 50 than add 10 points to it and print the new value.<br>
At the end print 'finished' in any case.",c0ab62dd,0.38596491228070173
18067,30fdc4a6e3c1db,544ad2d3,## 1.3 Calender,6111ddee,0.38596491228070173
18073,5e1d001f8764e0,ee011851,# ONE HOT REPRESENTATION,62be464c,0.38636363636363635
18074,da199f8fb59439,03eccd5d,> *Netflix has around 4500 Movies & almost 2000 Tv Show*,baaa665d,0.38636363636363635
18076,f269d2fbd5f1be,7703366e,# EDA on Prep Dataset,1264c440,0.38636363636363635
18081,6d66ced0028dea,9c5a46ee,"**Floor, HouseFloor**",f50aae52,0.38636363636363635
18082,5083d7a61f2426,d0d0fa12,"The most difficult part in time series is to separate the data in batches, transforming in proper way.

There is a keras API for help us make this.

The parameter length, is how much of previus data we want to use to make predictions",541a0fec,0.38636363636363635
18084,450fda47b03baa,b663d23e,Veri çerçevemizin Rating ve Install özniteliklerinin benzersiz değerlerini görüntüleyelim.,62c04adb,0.38636363636363635
18085,be2f4d8a6b73ca,6c801311,**Visualising discrette feature with output variable count plot**,5d8ce40a,0.38636363636363635
18091,cb570c7b7f0501,346a3379,"it's easily to notice that we have an outlier at age = 115.

cases above 45 maybe have smaller ratio than cases below 45.",a200a0ec,0.38666666666666666
18092,7e1da639035ac5,4d542831,### <a id='8.2'>8.2 Comparing distribution of races with each other with Economic Need Index</a>,120b6c23,0.38666666666666666
18094,91eaec994e0c6f,55d455cb,"<b>For plotly plots, you can double click on legend to visualize data parts separately. You can also zoom in, zoom out and autoscale plots.</b>",376aef10,0.38666666666666666
18096,37e461081e47c5,5a13e300,"We can see that most categories have fairly stable quantities; either zero or somewhere less than 4000 per month. However, there are 5 categories that used to be sold in quantities in excess of 10k each month but all those quantities are now under 10k as well. Those 5 categories are Movie, Games PC, Games, Gifts and Music. ",b3e6549e,0.38666666666666666
18098,510b8303776bb6,1db66eae,### Converting ordered categorical fields to numbers,18080db8,0.3867924528301887
18099,43e60eb1362f5c,acd1e00c,Since there are three dataset it is required to merge all the three data set so that we can use it during the visualization in a proper way.,87934234,0.3867924528301887
18101,c80939c7c626cf,03396025,"# male= 0, female = 1",b9ac31e2,0.38686131386861317
18103,b05ee1ea1c8269,3ede015b,# The latest ranknig of Omicron perc_sequences,19e4d303,0.3870967741935484
18107,0caaec057f7184,08306207,item 20949 is having extremely high sales compared to other items.,b875533e,0.3870967741935484
18117,c0ddb77bf32e2b,625b42cd,"It's time to have a closer look at the column 'NHMC', 'THC', 'CH4'.
 
| column | NA Ratio |
|--------|----------|
| NMHC   | 0.568776 |
| THC    | 0.568071 |
| CH4    | 0.568071 |


***NMHC***

Accoding to [wikipedia](https://en.wikipedia.org/wiki/Non-methane_volatile_organic_compound), NHMC is 'non-methane hydrocarbons' (非甲烷烴,指除甲烷以外所有的總稱)
> An important subset of NMVOCs are the non-methane hydrocarbons (NMHCs). Methane is excluded in air-pollution contexts because it is not harmful. Its low reactivity and thus long lifetime in the atmosphere, however, makes it an important greenhouse gas.

(是有機化合物的一種，只由碳和氫組成。烴類包括了烷烴、烯烴、炔烴、環烴及芳烴，是許多其他有機化合物的基體。非甲烷烴，主要包括烷烴、烯烴、芳香烴和含氧烴等組分，為光化學煙霧形成的主要物質，對大氣污染有重要的影響。因為甲烷相對較穩定，故討論碳氫化合物的污染時不包括在內。)



***THC*** total hydrocarbons 總烴

***CH4*** Methane 甲烷

Some we can make a assumption :

$THC = NMHC + CH4$

Let's see if there is chance we  can fill NAs by adding 'NMHC', 'CH4' back. ",a0cb45f7,0.3870967741935484
18121,9ceb7278784462,639210e2," ## <a id='15'> 11. Sample Data</a>
 * We get some data to test models.",3768a567,0.3870967741935484
18123,c7c5edb41205e4,bd1a0737,##### ,658ec411,0.3870967741935484
18125,ad26c020235dfc,3496b1a5,Plot the correlation matrix:,bf766e48,0.3870967741935484
18127,b59b5aaeedb1fb,1bece7ad,> ### univariate analysis,1ad63faf,0.3870967741935484
18129,fdbbd573ba31c2,77707417,### blade_length(m),f7c28d74,0.3875
18133,b01ee6cb674fa3,a4365c55,## Detail,a8ffd35e,0.38768115942028986
18134,2343dc02ffb96a,460095cc,"# Split into X_train, y_train, X_test, and y_test.",29aa95a4,0.3877551020408163
18136,e69a496109e7d8,7e182252,"So we take No.of.AxillaryNodes and do pdf,cdf",1c640591,0.3877551020408163
18139,ffd1df95ca5289,f321895d,we can see that response are more if the average balance of a person is more,db00c338,0.3877551020408163
18142,f35bf4df70d310,27056213,Apply the mapping to the training & test set of data,10bb859a,0.3877551020408163
18153,869a39a3d4dea2,b28ffe0a,"we define set of methods to perform 
* translation - shifting the image bottom or up, left or right
* rotation - rotate image by certain degree
* resize - increase or decrease the size of the image
* flipping - flip the image either horizontally, vertically and finally by both
* croppping - crop the image",9020daf8,0.38823529411764707
18163,fc8e0042411c46,8181070f,- Leads spending more time on the weblise are more likely to be converted.,af476c2a,0.3887147335423197
18165,166a62ebb4fc3a,6562c6c8,"Here, we have analyzed the relationship between the 10 key attributes and the diagnosis variable by only choosing the ""mean"" columns.",db48a079,0.3888888888888889
18168,4ba67fa2de9e6e,26f7f2a1,"* Keep this in mind for when you start to look for patterns in detection accuracy or mAP by source as some regions are under represented in teh train set.  
",8e0dd482,0.3888888888888889
18174,dd3721cb49c1fd,bf63789a,"<a id='5'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">5. <b>Plotting the results using the Plotly module  📈</b></h1>
  </div>
</div>",1a53fdd9,0.3888888888888889
18181,cf39cde80e66b7,b8c668bd,"![](https://miro.medium.com/max/1189/0*sA9a9MlNiZ1dI7so.jpg)

> MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It’s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.",aed4bc9b,0.3888888888888889
18182,9289395e9c480f,19316dc9,# Check the keywords of the problems.,55d03d67,0.3888888888888889
18186,fbb1f9d3818830,7ec3509c,"# Image Visualisation

My dear Watson selecting these images was not a coincidence since no human action is ever truly random. ( **sorry for the Sherlock lingo** )

But on a serious note the reason why I chose these images is to see the quality of features learned by a pretrained model on imageNet. As mentioned a model with good grasp of features should mainly be able to group pixels of the same/similar objects into the same cluster ( **We give clusters color & overlay it on the image. Therefore pixels belonging to same cluster will have same color**). 

Each of these images has multiple actors, objects, interesting texts on the image etc. (**sorry for the I am SHER-locked spoiler**). 

*Mainly I wanted to see can the pretrained model differentiate between these features.*
",c7027f86,0.3888888888888889
18188,3597174a998d4d,46f98183,#### 2.2.1.2 country information,276892ed,0.3888888888888889
18191,1fcf9c261518d6,a736152e,# III. Feature Engineering,be646fb0,0.3888888888888889
18194,ab6da5994949a3,e67e69c5,## Visualizing ANN Test Set results,fae6b91d,0.3888888888888889
18195,9ad9a97e628bfa,010beebf,"정보가 부족해 섣불리 판단하기 어렵지만, 일단 가장 공통적으로 나타나는 코드가 많은 방법을 통해 (/와 . 모두 분리) Code를 분리해보겠다. / 또는 . 를 통해 두 가지 이상의 코드가 기재되어있는 경우 첫 번째 코드만을 추출해 하나의 Attribute으로 만들어보겠다. ",0a7e1136,0.3888888888888889
18196,245c89d02f3f5f,91e50f55,## Δημιουργία περιβάλλοντος,61a1eacd,0.3888888888888889
18203,0f5085b162bd9f,635cf9ff,# DBSCAN,a3d989ee,0.3888888888888889
18204,c9dc8d00773da4,290521b0,"# Row info

`137 * 236 = 32332`

0 ~ 255 grayscale data

with `image_id`",d9aa2f85,0.3888888888888889
18206,d6cbd7160961dc,278c4e30,## 4.3.2. Results: Second Digit,36d74664,0.3888888888888889
18210,10b5af05d804ff,2c34c655,Be carefully! We can't mirror x and y features. Because our teams are in different corners. Let's prepare new x and y for d-team.,4a9b1705,0.3888888888888889
18218,df2a7968c08ee4,1cbf3c4c,"### Torch Dataset

A custom Torch Dataset class must implement three functions: __init__, __len__, and __getitem__. 

__init__ is run once when creating the object. We initialize the object with data, labels, and transformations if we choose to do so. By default Transforms is set to None.
",a2ba0a72,0.3888888888888889
18219,c6f8ff61a5fa87,9b59c618,"# <span style=""color:blue;""><strong>6.Model Training</strong></span>",3eea586b,0.3888888888888889
18230,726833f92fb87a,33f778ed,## Default vs campaign success,7dc5e1b6,0.38926174496644295
18237,f91f58d488d4af,f008e61e,"Now, we need to define a notion of *distance* -- that is, a function that calcualtes the distance between two images.",5df1bbf3,0.3894736842105263
18241,c13f73168789c2,1948e62e,"### 2.3 Select data at specified row and column location<a id='14'></a>
Syntax : `df.iloc[row_index, column_index]`",16175052,0.38961038961038963
18243,90691864eb68c7,a2a65bfd,Missing values Imputation,3555ef9b,0.38961038961038963
18245,722cd844dfbe8f,970e8ac8,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_1_3"">Data cleaning</span>

In the animation of the scans projected above, we notice that a certain number of images at the beginning or at the end of the sequence has a lot of black area.

These images will therefore be useless in our models and may even cause over-training.
**When creating the image sequences, we will therefore start from the central image of each folder and we will then take the same number of images upstream and downstream**.

Let's take an example from a single image :",0cedb385,0.38961038961038963
18247,c65a65d4041018,169b1313,### Salary,824fb229,0.3897058823529412
18248,dac3c8204a2d1b,374a3bf2,"we can see that 56.4 percent of the 550 bestseller books are non-fiction books. While this is useful information, it would be even better to group these books not only by genre but also by year. This way, we can see which genre was more popular in which year.",b0d2d0dc,0.3898305084745763
18251,bb0905d33ae417,492d1cfd,`fit_one_cycle`is a method implemented by the library and proposed in [this paper](https://arxiv.org/pdf/1803.09820.pdf) to produce more accurate results and faster convergence. [This post](https://sgugger.github.io/the-1cycle-policy.html) is a great explanation of why `fit_one_cycle`works over the standard `fit`.,25fd1965,0.3898305084745763
18253,a44368590e878a,6063e037,### Region,77743ba8,0.3898305084745763
18254,f2e5e9fb9eaaf7,8b0a2e2f,### 4.2.2 Features f26 - f50,048e0d08,0.3898305084745763
18255,149cb8d3489224,681d8952,### User location,116858e7,0.3898305084745763
18258,a077820f7ab459,f390f9b5,"### Import a pretrained model
https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3",05a43104,0.3898305084745763
18264,83df814455f06c,c9af5b6f,### Missing values in variables,c9cff71a,0.39
18269,8d70dcae7f40a3,135e08a4,### **Scaling data**,472c71ce,0.3902439024390244
18270,74a03887600114,cc065d9d,"In our kaggle kernel,we don't have much memory so we have to subset our dataset,we are going to take 1M rows",c0ffb2f0,0.3902439024390244
18272,514d8de15cb7ef,81f9137e,"### Basic Concepts of NLP 
<ol>
    <li> Bag of Words -- It is an approach used in document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier</li>
 <li> TFIDF Values -- Term frequency and Inverse document frequency helps us to find the importance of each word in the document
    </li>
</ol>",cfe111b2,0.3902439024390244
18274,fd4017c1514157,9640ef24,"Location data might be a good feature since It is poosible that certain bird species are from particular regions only.
For example, here :
The Bananaquit (banana) seems to only occur in Central and South America.
House Sparrow (houspa) has occurrences around the globe.",fd8f0896,0.3902439024390244
18277,47b2c9be5e31cb,73493a6c,Correlation matrix:,7d4afe56,0.3902439024390244
18279,5169abdc647412,75435b57,## model creation,28efc68d,0.3902439024390244
18280,0b01138ad120fc,3a9ac397,**Dataset Creation**,0b4b72e6,0.3902439024390244
18281,0e09587faffa8f,b246922e,The vehicle plate type with the highest number of summons was **PAS**,0d563d61,0.3902439024390244
18287,7a058705183598,208d9571,"As Iris Dataset is classification between three types of flowers. I am going to use logistic regression, Decision Tree Classifier, Random Forest Classifier, SVM & Gridsearch, K Neighbors Classifier & DNN classifier with tensorflow estimators.",b0ead917,0.3904761904761905
18288,7454fdc444df16,7858cb54,To simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.,a7818ef5,0.3904761904761905
18291,ff3a8ce61fab6a,7227ff7f,or Another way,9afe1654,0.390625
18295,14defffcd250f3,9b76f2b5,# Test Dataset,3a683b94,0.39080459770114945
18296,d5f78aa381f58d,28f27db3,"From the above box plots, outliers are present in trtbps, chol, thalachh, oldpeak, caa, thall. Yet, I'm not going to remove them because of the sensitivity and risk of medical data as it's different than the other kind of data. The exclusion of outliers has a dramatic impact on the type I error.",d60f358f,0.39080459770114945
18299,ba4b3bd184acbb,cd2816f8,The Sentiment column only has 3 unique values so it would definitely benefit from being converted into a `category` datatype.,0f5de724,0.39097744360902253
18306,e3fb4c6300cb56,d5da66b4,"<a id=""4""></a> 
## Pie Chart",8ebbdf89,0.391304347826087
18314,17a24d566ffa59,4dae9c63,"""Singlular Value Decomposition (SVD) allows us to reduce the dimensionality of a matrix. Instead of analyzing a full document-term matrix with all documents and all terms, we can reduce the matrix into a lower rank representation. In this, we combine the meaning of terms by compressing the number of columns.

To reduce the size of our matrix without losing much quality, we can perform a low-rank approximation on matrix C. This is done by keeping the top k values of Σ and setting the rest to zero, where k is the new rank. Since Σ contains eigenvalues in descending order, and the effect of small eigenvalues on matrix products is small, the zeroing of the lowest values will leave the reduced matrix C' approximate to C. How to retrieve the most optimal k is not an easy task, since we want k top large enough to include as much variety as possible from our original matrix C, but small enough to exclude sampling errors and redundancy. To do this in a formal way, the Frobenius norm can be applied to measure the discrepancy between C and C_k. A less extensive way is just to try out a couple of different k-values and see what generates the best results.""

SOURCE: https://simonpaarlberg.com/post/latent-semantic-analyses/",89049e56,0.391304347826087
18316,71c3c1eab0377d,987ced42,#### Discretization of the Numeric columns : Age ,52b4e360,0.391304347826087
18320,69d50f5e1373f1,4ae91201,"Tasks have multiple `train` input-output pairs. Most tasks have a single `test` input-output pair, although some have more than one.",ec7545ee,0.391304347826087
18322,0e2a23fbe41ca9,57748357,"Observations:
- A clean linear relationship b/w numerical_1 and numerical_2
- will remove one column when being used in the model
- also handle the outliers (what will be the threshold)",64e4762c,0.391304347826087
18324,73ca9abcc2034e,463985d4,**Count the number of characters and length of a title**,cec3446c,0.391304347826087
18327,548f961125248d,4b94a978,"## Baseline Model <a class=""anchor"" id=""fifth""></a>",d8c5e8b8,0.391304347826087
18332,7e275c8d5ff2a0,6491007e,# HOSPITAL DATA,b3afcc98,0.391304347826087
18337,598b6228760590,30bafbb3,- Add a column to indicate whether the person boarded the Titanic alone.,be30ab66,0.391304347826087
18338,2ada0305b68956,d8bcbf78,### 65. Palette = 'Wistia',133e26f4,0.3914285714285714
18343,2a123b4e8f9433,e542b004,# Build and train a Support Vector Machine,0a082218,0.3917525773195876
18348,fc8e0042411c46,71c50080,**Website should be made more engaging to make leads spend more time.**,af476c2a,0.39184952978056425
18349,62037c5832129c,50ef83ec,"If you want to select among different machine learning algorithms we can use nested cross-validation
* You have an outer k-fold cross-validation loop to split the data into training and test folds
* Inner loop is used to select the model using k-fold cross-validation on the training fold.
* After model selection then the test fold is then used for model evaluation.",61474350,0.3918918918918919
18350,e4525eb0c96f28,4de703df,"### Sales by Genre over time

In order to further contextualize the plot of sales by genre, we plot it over time.

By doing so we can further analyze the impact of genre on sales and come to a few of these following conclusions:
- Sports games are the top grossing genre from the previous graph, but only has a slight increase in sales over time. The same can be said of a few other top categories (Action, Fighting)
- Genres with higher growth over time are MMO, Shooter, Action/Adventure.
- The newer a genre is compared to others has no correlation to sales. Action/Adventure, MMO, Party, Music, and Visual are all newer genres made between 2000-2010, yet aren't similar in terms of sales growth over time.",2093a1f1,0.3918918918918919
18354,917957c6c4065f,66b170a3,![image.png](attachment:image.png),55b8ed68,0.39215686274509803
18358,7cfd96218dd933,fbee3860,### MAXIMUM BRIGHTNESS DATA,7c34d96c,0.39215686274509803
18360,52cfd66e9ec908,258c4369,"Now that we can explore the images, we can also get a little down and dirty when it comes to the ZARR files. It's rather simple to use with the Python library for exploring them, especially the fact that it's NumPy interoperable.",c74adcdf,0.39215686274509803
18364,d0080e3a39bc5c,cebd4b88,![Transformation 2](https://i.imgur.com/UOvgajn.jpg),2fcde4cf,0.39215686274509803
18374,c84925c8171900,5bcff8f6,"<div class=""alert alert-block alert-info"">
    <span style='font-family:Georgia'>
        <b>Insight: </b><br>
        We have successfully imputed or removed null values 
    </span>
</div>",e21ff7ec,0.3925233644859813
18376,99f84fa59cb1da,6cfb19c4,"#### Data is highly skewed. So, we need to perform a good cross validation in order to decide which model and experiments performs better. You can never trust the public leaderboard :P",41e95f63,0.39285714285714285
18379,df51d4c54fbb91,daa61e13,# trainning part,4226dd72,0.39285714285714285
18380,8dd655515e7d18,ef366337,### Extraversion Analysis,895f41cf,0.39285714285714285
18381,87e94f864d74be,9a25f209,"### Fix ""country"" missing values",294bfe9f,0.39285714285714285
18382,0dd3ac2d55efd7,b46fe0b2,"1. There are other type of embedding models too such as Word2Vec and FastText. Word2Vec has again two types skipgram model which predicts context words based on target word and CBoW(continous bag of words) model which predicts target words based on context words. 
2. Word2Vec model ignored morphological structure of each word and considers a word as single entity. FastText model considers each word as a bag of character n grams and then takes average of embedding of these n grams. Rare words get good representation using FastText model.",e9aa2cc2,0.39285714285714285
18388,1a285e4c830f3f,d888e843,"- Wählen Sie nun an Hand des obigen Plots einen passenden Wert für C, und werten Sie in der nächsten Zeile Ihre Supportvektormaschine aus",360b50e9,0.39285714285714285
18389,f4514ec092a771,3b2ac8a2,### Evaluation data,3739ab1e,0.39285714285714285
18390,1c381451c17150,842d9a1e,"# Create the Model
The model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they work roughly twice as fast as regular LSTMs layers if conditions are right.
",e79b530f,0.39285714285714285
18391,585c280865b46e,84667ca3,# UMAP visualization,4d6056f1,0.39285714285714285
18392,9c33d1955302bf,1a1912ea,# **Train Test Split**,0d9cfc89,0.39285714285714285
18393,b8849a04581d32,6d0e5ef1,"#### Let's define the purpose of the model. Let's look at the results of the actions of the special teams. The future model is conceived to predict the outcome of the Punt. There are eight outcomes for him, each of which happened a completely different number of times for all the games presented in the data. It can be assumed that the model will be able to distinguish them, which means that the target's choice of the specialTeamsResult model is justified. ",b8a568cd,0.39285714285714285
18401,6a80f915608fc2,e52c1714,"## <a id=""gVectors"">g-Vectors for each Target MoA</a>
Back to <a href=""#Index"">Index</a>


The 772 g values for a sample are its g-vector.  Here we calculate the average g-vector for all the ids that have a given MoA=1. ",636938eb,0.39285714285714285
18405,565ad413cd802f,be5e3c2d,"I'm using a validation percentage of 10%, but you can use a smaller or larger percentage. One good strategy is to determine a good set of hyperparameters, and then retrain on a smaller validation set for your final submission.",397b074e,0.39285714285714285
18406,b290039151fb39,03ef38a2,"The starter model is based on DenseNet121, which I found to work quite well for such kind of problems. The first conv is replaced to accommodate for 1 channel input, and the corresponding pretrained weights are summed. ReLU activation in the head is replaced by Mish, which works noticeably better for all tasks I checked it so far. Since each portion of the prediction (grapheme_root, vowel_diacritic, and consonant_diacritic) is quite independent concept, I create a separate head for each of them (though I didn't do a comparison with one head solution).",1836a79c,0.39285714285714285
18411,e67925694c07d3,136d6959,apparently no mistake on DAYS_EMPLOYED vs DAYS_BIRTH,83af4c4a,0.39325842696629215
18414,b10bd75889dad9,b2741ee5,#### We will plot scatter plots for all numeric columns in batches for better visulization,ee00ceee,0.3933333333333333
18415,91eaec994e0c6f,acce4e9b,### 2.3.1 State Level Analysis,376aef10,0.3933333333333333
18419,918040fad252ec,85dcb2e9,Membuat Datagen,966fcd8f,0.39344262295081966
18421,0858e1bb3cbaca,b608e833,# Summarize,78548374,0.39344262295081966
18424,4c47839b067546,627eeb4a,## Построим наивную модель,1f517b02,0.39361702127659576
18430,a2444ab5d5f147,bcb8baf0,"### There are words like `a, am, and, any... ` etc, such words don't carry any significant importance as such 
### Such words are called `Stop Words` and generally they are omitted from the analysis

### Let's find and remove such stop words from our dataset

```
If you have not downloaded already, use this

import nltk
nltk.download('stopwords')
```",10617755,0.3939393939393939
18431,b42180a6a5b42f,7a216f9a,"Neste dataset, iremos carregar os ***dados do dia atual*** bem como do ***dia anteior*** a fim de constatar o percentual de casos óbitos atualizados nas últimas 24 horas.
Importante frisar que caso o dia 14 de maio (pico atual) venha permanecer como o ponto mais alto de óbitos acumulados em 24 horas, poderemos confirmar que a pandemia no Brasil já atingiu o percentil 50. O que significa dizer que estamos em declínio. ",987cea5f,0.3939393939393939
18434,dbd96dd275dc60,adf6c94e,"## Fill missing values

### Fill numeric missing values first",1ed493a8,0.3939393939393939
18440,a2573183738753,93f24f5b,# prepare model,f6429599,0.3939393939393939
18442,3c2033cc99c12c,d1a5749f,"**Findings:** *From the above table and barchart, we could find that those with similar distribution usually do not conribute much to the difference between classes.*",dfa22a54,0.39416058394160586
18444,99bf357eaf61f1,94822084,"From above zoomed heatmap it is observed that GarageCars & GarageArea are closely correlated .
Similarly TotalBsmtSF and 1stFlrSF are also closely correlated.
",9d92fafe,0.3942307692307692
18449,06c7ba9203293f,f3ebacbf,# Analysing 'categories' series from combo_df DF,1e1a2b48,0.39436619718309857
18452,9bcfa825c8b2e6,c420b31b,"Şeker hastalarının cildi kalınlaşır.Sayısal değer olduğundan ve belli bir metriğe indirgenemediğinden
bu şekilde bırakılır.",220f36e4,0.39436619718309857
18453,631cd434fc3aa2,654c2d5e,"* _GarageCond_, _GarageQual_, _GarageFinish_, _GarageType_: replace NA with None.",2b74febb,0.39436619718309857
18455,396bc36edb95d3,ae038447,http://webgraphviz.com/,965e4f8f,0.39444444444444443
18461,c950cff74e51ac,df68fc15,**Price Distribution of Room Types**,d59bf323,0.39473684210526316
18469,52ee792e228d54,dd43f41c,"#### Let us categorize the ZIP code into bins and use weight of evidence numerical transformation on them.
(https://www.kdnuggets.com/2016/08/include-high-cardinality-attributes-predictive-model.html)",5096094e,0.39473684210526316
18472,b01ee6cb674fa3,698e6d09,"So, the column detail contains the followin information: Rocket | Mission name, as can be seen by the entry 205(3rd in the list), that launched the BepiColombo mission and used an Ariane 5 rocket for doing so.

Lets split this data into 2 new coluns: 
- detail_rocket for the rocket name
- detail_mission_name for the mission name or the payload, if it is a satelite",a8ffd35e,0.39492753623188404
18474,4ae6a182abac64,2e26b3aa,"- People with SibSp or spouses were less likely to survive, therefore people with no children were more less likely to survived than those with one children or two.",418676c5,0.3949579831932773
18475,fc8e0042411c46,e67051cd,## Page views per visit,af476c2a,0.3949843260188088
18482,743ae010f5e875,a5be6b72,#### Ken Matching,02c54445,0.3953488372093023
18487,8539260444e6b5,c51a2a5d,# Results of Preprocessing data (Removing stopwords & Lemmatization),0369463f,0.3953488372093023
18488,d96642860ab3dd,38d27ad7,# 2. Pre-Process Feature Engineering,98419d48,0.3953488372093023
18489,2e40928927c0d4,077fabd5,**Showing randomly chosen Proliferative DR image one at a time** ,b6385ef2,0.3953488372093023
18490,20b372b6e4e276,89fde6a0,Long 'selected text' are not predicted correctly (get too long),ec8b0860,0.39552238805970147
18494,3b5903412fe741,b4319207,It's also possible to pass a list:,ad231969,0.3958333333333333
18497,95656e8d666b16,d1c72531,#### Anova F-value,65e88599,0.3958333333333333
18502,eb0854a6601407,762cba31,"Have you noticed that some of the data we have is missing, this can be judged by the long smooth connected areas without hesitation.

- We can clearly see that some investments miss parts of their timeseries or end earlier.
- Looking back into the competition description, we find: ""The ID code for an investment. Not all investment have data in all time IDs.""",6d107747,0.3958333333333333
18503,28a1ff0f223da9,3f59483e,"# Task-2
**How many professors we have in Data Sciences, Artificial Intelligence, or Machine Learning?**",c945b27d,0.3958333333333333
18505,5f32117bcd5255,745c2dac,### HST OPTICAL THREE,85882abf,0.3959731543624161
18507,f3c8651cb08234,09abfff7,# Feature Importance,37f86e36,0.39622641509433965
18511,a070fd03ae8ed2,4f12053e,"## 4.4 Метрики, матрицы ошибок и кривые",c0ec4138,0.39622641509433965
18517,e3f3f108cd3869,7223d943,**Feature Engineering**,2b78de2d,0.39655172413793105
18519,a09e20bb9b5259,c8a4dd43,# Size 224,99475ae5,0.39655172413793105
18521,00001756c60be8,ac301a84,"*На обучении на один признак больше, чем на тесте*",945aea18,0.39655172413793105
18523,a1ba5ffd30dbde,83b8a5d7,#### Min-Max Scaler ,48e57546,0.39655172413793105
18526,20e1ba19eb9b5e,ff26b772,## 2.2 Target variable,4569bfc1,0.39655172413793105
18528,d42518f6cb0995,c1192819,We can see that active users do much better than novices. But anyway average user score is lower than the overall % of correct answers. It means heavy users have even better scores. Let's look at them.,26913a9b,0.39655172413793105
18529,1cd8be6e679620,adc0effc,"* In Test data seq_length = 107 have seq_scored = 68   #[629]
* In Test data seq_length = 130 have seq_scored = 91   #[3005]",3ce15a43,0.39655172413793105
18532,2f47abddfd1928,c676fe8d,"Now we can see how all the columns, but Survived, have no missing values.

The next step is to procceed to the exploration analysis.",ae33cc0b,0.39669421487603307
18534,8985a124d4b657,a1544ef8,"The below function called split_sequence splits the sequence into sets of n values. This n is given as n_steps (step_size). For example, if n=3, we split the sequence in groups of 3. We create 2 empty lists and append the split sequences.",586d1846,0.3968253968253968
18536,f3d5d8917ce5df,ea041883,"# Merge! <img src=""https://www.roadtrafficsigns.com/img/md/X/left-lane-merge-sign-x-w4-1l.png"" align = ""right"" width = 200>
Now we'll combine all the reference data to the training and test data into the training and test data frames.

In addition to the standard train/test dfs, we're also going to take the same steps with the submission data.",e45112f8,0.3968253968253968
18537,06ecf7a304c309,905b73be,"20 epochs 만으로 입력 이미지를 다시 잘 구성하는 것을 확인할 수 있었습니다.
이제 오토인코더로 노이즈를 없애는 예시를 살펴봅시다. ",714de627,0.3968253968253968
18539,ee23a565163388,b684d353,"**Inference**
- 38% of the patients who have high BP also suffered heart failure.",88aacbc4,0.3969465648854962
18540,7f74a04ae75792,2763fa30,"### Count all NaN in the dataframe (both columns & Rows)
",d01e91da,0.39705882352941174
18543,02b7e38902069e,d99c2016,"#The model gives out a cosine similarity of 0.67 which means that the sentences are pretty close, and that’s correct.",726a03a0,0.39705882352941174
18549,eb0ecd6bebeb15,a27c068b,"Aynı iki veriyi scatterplot ile tekrardan görselleştirelim fakat bu sefer ""variety"" parametresi ile hedef değişkenine göre kırdıralım. 

3 farklı renk arasında sepal değişkenleriyle bir kümeleme yapılabilir mi? Ne kadar ayırt edilebilir bunun üzerine düşünelim.",d7b93a60,0.39705882352941174
18550,2ada0305b68956,b6b2f10a,### 66. Palette = 'Wistia_r',133e26f4,0.39714285714285713
18551,b61ab8f81dc03d,fc7e1e51,"Let's transform the ""Age"" column from float type into integer.",64d05394,0.3971631205673759
18552,56785caebaa256,3e933613,"### 3.2.4. Weekend quarantine as holidays<a class=""anchor"" id=""3.2.4""></a>

[Back to Table of Contents](#0.1)",a792961a,0.3971631205673759
18558,1eb62c5782f2d7,ed0cf6b1,"### Contoh 1


- Sebuah survei menunjukkan bahwa orang menggunakan HP-nya selama rata-rata $1.5$ tahun sebelum membeli HP baru. 
- Standard Deviasinya adalah $0.25$ per tahun. 
- seorang pengguna hp dipilih secara acak 
- Cari probabiliti dimana pengguna HP akan tetap menggunakan HPnya selama kurang dari $1$ tahun sebelum membeli HP baru. 
- Asumsikan bahwa lamanya waktu orang menggunakan HP mereka berdistribusi normal dan direpresentasikan oleh variable $x$.",bb69f147,0.3972602739726027
18559,738bfced935b69,56a5a8f2,"The car tax with Manual transmission:
* 0 to 580 pound in the Petrol fuel.
* 0 to 325 pound in the Diesel fuel.
* 0 to 150 pound in the Hybrid and Other fuel.

The car tax with Automatic transmission:
* 0 to 580 pound in the Petrol and Diesel fuel.
* 0 to 195 pound in the Hybrid fuel.
* 0 to 300 pound in the Other fuel.

The car tax with Semi-Auto transmission:
* 0 to 580 in the Petrol and Diesel fuel.
* 0 to 150 in the Hybrid fuel.


",2d3c592d,0.3972602739726027
18565,4daf6153275cbf,329e02a8,### Localization Analysis,51db1961,0.39759036144578314
18567,5ce12be6e7b90e,a452e238,"## `else` statements

We can add _else_ statements to perform commands in case the condition is __not__ met, or in other words, if the boolean is False.

![if else flow](https://raw.githubusercontent.com/yoavram/Py4Life/master/lec1_images/if_else_flow.jpg)",c0ab62dd,0.39766081871345027
18570,d1ff7e10ee0102,eb490507,"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:

* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!
* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).
* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').
* 'FullBath'?? Really? 
* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?
* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.

Let's proceed to the scatter plots.",2cc71c3c,0.3977272727272727
18576,98a6794067932a,c4f1f125,"**2.4 Ventes par région**

Maintenant que nous avons identifié le comportement des ventes en fonction des différents segments de clients, des types de livraison ou même des sous-catégories de produits, il était désormais temps d'évaluer le comportement des ventes en fonction des différentes régions desservies par l'entreprise.",08600fe2,0.39805825242718446
18579,fdc3afd309b850,ff6d20a4,"<a id=""cfnv""></a>
### 6.2.4 Condo Fees Null Values",966bde38,0.39814814814814814
18580,2f0f808765fc67,3dc2e642,# Applying Machine Learning Models,fd1f6494,0.39814814814814814
18583,ac1abfe1dfe815,f599f4a8,"**The number of positive, negative, and neutral tweet every day for each airline**",6529dbcb,0.39823008849557523
18584,9169c4e9c33c90,f3490470,Let's look at how many times each author made the Top 50:,725bf880,0.3983050847457627
18585,1294fb4c86f993,35e6460b,Find the extra 5 states in `guns`,4471e513,0.3983050847457627
18586,e19e307b3fd188,25318bab,"The value of **fire insurance** has a positive influence on **rent amount**. Most of the values are between **3,00** and **200,00**.",2173955b,0.3983739837398374
18588,ba4b3bd184acbb,c332b1d1,***,0f5de724,0.39849624060150374
18590,7e89d387feb9f5,fd2a4a97,### Добавленный числовой признак №7. Является ли город столицей,989e3a1b,0.39855072463768115
18591,0e2a23fbe41ca9,4b945a0f,"### 2. Anon Categories
category_1, category_2, category_4",64e4762c,0.39855072463768115
18592,eda49464dd6d1b,d9e1aca1,"# Data Preprocessing for Random Forest
* Male and Female are converted to integers
* Vehicle Age is given dummy variables and converted to integer
* Vehicle damage is converted to integer.
* Age and annual premium are scaled to numbers closer to 0",8421f81f,0.3986013986013986
18593,63b44c85e32c1f,80f9b13e,**append( )** function can also be used to add a entire list at the end. Observe that the resultant list becomes a nested list.,fb9b9562,0.39864864864864863
18594,917957c6c4065f,bba0d41e,## 2. 현재 인기 동영상의 전반적인 상황은 어떠한가  ,55b8ed68,0.39869281045751637
18595,6a80f915608fc2,88af3875,### Assemble a dataframe of the 206 average g-vectors,636938eb,0.39880952380952384
18596,4c47839b067546,029e74e4,Изменим категориальные признаки  и model_name  brand  на числовые,1f517b02,0.39893617021276595
18598,9395559895004f,42d3f182,## CNN_Model,b5a0494b,0.4
18599,ea1ba5e7ba436c,70b90910,"# Scatter Plot

* Parameters

    *     x = x axis
    *     y = y axis
    *     mode = type of plot like marker, line or line + markers
    *     name = name of the plots
    *     marker = marker is used with dictionary.
    *     color = color of lines. It takes RGB (red, green, blue) and opacity (alpha)
    *     text = The hover text (hover is curser)
    *     data = is a list that we add traces into it
    *     layout = it is dictionary.
    *     title = title of layout
    *     x axis = it is dictionary
    *     title = label of x axis
    *     ticklen = length of x axis ticks
    *     zeroline = showing zero line or not
    *     y axis = it is dictionary and same with x axis
    *     fig = it includes data and layout
    *     iplot() = plots the figure(fig) that is created by data and layout",8dd6985c,0.4
18600,7a058705183598,b23017d8,1. Logistic Regression,b0ead917,0.4
18603,5ffe6aa38958a1,42894d20,**Embark Location**,11f5412e,0.4
18605,0885edf1a61429,58c3299d,"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4704212%2F9ca088bb386abf7114543c019c1d8a5f%2Ffig.png?generation=1609892974092435&alt=media)

+ Tp label(species id is ""9"") is given at around 49~52sec.
+ But trained model discovered many pseudo-labels in the whole clip.

Is that true?  
Let's listen.",cf8ce75e,0.4
18608,c91c137284976f,c4b2e742,#### 1.3. Preparing data,c6888c0a,0.4
18609,123382acf2a162,1a89400c,# Submission,a0e20386,0.4
18615,8cd6656a65e6e7,e4892361,## Plot the 2D graph,c8e1697a,0.4
18618,ff9142eb631dd5,c9681598,"## Install the latest Xgboost for GPU acceleration
#### 2 times faster than Lightgbm on CPU (4 cores)",453131ac,0.4
18622,38b79494ac749e,6a858606,### Sweet spot,39162a40,0.4
18624,9f0ccf5b9e8f03,11920316,"#An outbreak of severe Kawasaki-like disease at the Italian epicentre of the SARS-CoV-2 epidemic: an observational cohort study
By Lucio Verdoni, MD; Angelo Mazza, MD; Annalisa Gervasoni, MD; Laura Martelli, MD; Maurizio Ruggeri, MD;Matteo Ciuffreda, MD; et al.

Published:May 13, 2020DOI:https://doi.org/10.1016/S0140-6736(20)31103-X

Children diagnosed after the SARS-CoV-2 epidemic began showed evidence of immune response to the virus, were older, had a higher rate of cardiac involvement, and features of MAS. We therefore showed that SARS-CoV-2 might cause a severe form of Kawasaki-like disease.

#Implications of all the available evidence

Outbreaks of Kawasaki-like disease might occur in countries affected by the SARS-CoV-2 pandemic, and might present outside the classic Kawasaki disease phenotype. This condition might be serious and requires prompt and more aggressive management. Future research on the cause of Kawasaki disease and similar syndromes should focus on immune responses to viral triggers. https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31103-X/fulltext ",66691203,0.4
18628,4fd4b6a80d40e3,52a599be,"## Dot Product of A Matrix

![image.png](attachment:image.png)",f6913cc3,0.4
18629,061d6757dfbce0,65fa54c6,## Exploratory Data Analysis,c0c2915a,0.4
18632,10c5a39a87c47e,dad69c83,## Step 7: Label Encoding<a id='step-7'></a>,09c7337a,0.4
18635,0687cd5c8597db,be874005,![image.png](attachment:2a9e5e72-1dfc-43f7-bb54-14491a20e2f3.png),4edec76a,0.4
18638,2bd6c370695ea7,3a1f7c06,## Numerical Features,cbe6aec8,0.4
18642,37e461081e47c5,e7c8de2e,#### Prepare final dataset for Modeling,b3e6549e,0.4
18644,6998861ff6ff01,f82dc108,"Now when I check the first few rows of the new column, I can see that the dtype is `datetime64`. I can also see that my dates have been slightly rearranged so that they fit the default order datetime objects (year-month-day).",ea9e72cf,0.4
18645,b547f0f38f7744,0be8b6ac,### Show Images,b6ba66b3,0.4
18648,c73e07ad6d25c5,d76c161c,## Fare,3ab391fb,0.4
18652,3dd4294f903768,e145b283,"We can see that most of the data is distributed in a normal distribution, but there are also restaurants who got a rating of 0.",0d89d098,0.4
18658,2a9a149f306b6c,9589523b,## Train model with transfer learning,295c52ce,0.4
18661,80729c2598eb26,c89b10f0,the category with the most projects seems to be Films and Video the category with the most number of relative successes is Music and Theater and the category with the most relative failiures is Technology,153e0b56,0.4
18662,cb570c7b7f0501,40d46e87,"### Research Question 2  ( Is it about the gender ! )
Maybe we a have a gender who is more committed than the other?

joke(""if we have a feminist here .. she will definitely choose Females"")
maybe our data has another opinion.",a200a0ec,0.4
18665,fdbbd573ba31c2,641fc860,### blade_breadth(m),f7c28d74,0.4
18670,a8c042af6b7245,1eb03603,"### Data quality checks

#### checking missing values
Missing are represented as -1",2487ac62,0.4
18671,7dd46c750653eb,7e859222,"**Inference**

* Comparatively Female birth rate was always less than Male Birth rate

* Male Birth Rate was Highest in 2016 and was lowest in 2011.

* Female Birth Rate was Highest in 2016 and was lowest in 2011.

2016 seems to be good year for Moscow",c2644713,0.4
18679,f2f2db16a2f86c,dde10657,"Many features have negligible correlation between them.

Features like **population,total_bedrooms,total_rooms,households** are highly correlated.

Maximum correlation is seen between **households and total_bedrooms**.

**median_house_value** has maximum correlation with **median_income**.",ffc6a115,0.4
18682,b3e48999ed0d00,1226aeda,## Target Feature Distribution,fe9ada0f,0.4
18684,6fad63bfd45ef9,58cabcee,Train data have 40000 labelled examples. The image resolution is 28 x 28. There are no missing values. There are almost equal number of examples for each label.,b3c6f1d6,0.4
18689,36c35f0a9f70f7,27d5c71d,## Decision Tree Classifier,67358bc7,0.4
18691,5d5c9480b5a0a3,58cb3668,342 passengers survived while 549 passengers died,04d82e2d,0.4
18695,6b65d81a5743dd,2bcea94e,Let's check the correlations of other features with a heat map,4080a2d2,0.4
18697,8696921d9adc93,129300d8,"**3. Normalization**

Converting data from [0...255] value to [0...1]",b8908b23,0.4
18702,3597174a998d4d,73c03b43,"The country variable has 177 values, and the sum of the top 20 booking accounts for 94.21% of all booking. ",276892ed,0.4
18703,2bace980aeb34c,373111e9,"The code yields a value around 17862 for the mean absolute error (MAE).  In the next step, you will amend the code to do better.

# Step 1: Improve the performance

### Part A

Now, it's your turn!  In the code cell below, define your own preprocessing steps and random forest model.  Fill in values for the following variables:
- `numerical_transformer`
- `categorical_transformer`
- `model`

To pass this part of the exercise, you need only define valid preprocessing steps and a random forest model.",dc05ef6c,0.4
18704,b7452d87e4abfe,7b71c847,**Next we query a match. I chose the first match**,9c75a86d,0.4
18705,ce7abd85d777b5,ad669e20,2. Features Engineering,0a340dbb,0.4
18712,f4b603905215b7,153555ff, # 1. Logistic Regression,efe1d587,0.4
18717,09bac0c221388e,9ce60dc0,## Frequency-Based Sentence Scoring,bea4aa2e,0.4
18720,49ac6594c8f5cf,0c27a0fa,**Is Central better or Others ???**,6f19f28a,0.4
18721,e0a041e5e2372f,44737e9a,"* The above are the missing values which are left and does not satisfy any condition
* So, according to the location these values will be filled manually
",7c4357b2,0.4
18723,5626e84c4e6bf8,9f79ad33,"# The interpretation

Each time the map is generated different but these are my observations from maps in different versions.

 - Clearly more optimization is required as there is much overlap of weights
 - Most anomalies/outliers have the bright cyan three-pronged star(tri_down) label associated with them which is a Sandal
 - Most of the X, diamonds and three-pronged stars overlap with each other so they have similar features which is expected as these are all kinds of footwear i.e., sneakers, ankle boots and sandals respectively
 - Brown squares i.e., trousers have the most distinct and well-formed clusters with minimal overlapping and a significant number of features covered
 - Plus sign i.e., bag is not mapped on much features which means that many features have been extracted for bags 
 - circles, pentagons and hexagons overlap with each other a lot and this is expected as they are T-shirts, pullovers and shirts respectively and so are expected to have similar features.
 - Star(dress) and triangle(coat) are the labels that overlap a lot with other labels
 - Much more than half of the extracted features have the following labels: Sneakers(X), Shirts(Hexagon), Trousers(Square), T-shirts/tops(Circle)

Many more things can be interpreted but these are just some easily discovered observations.",e2ecb669,0.4
18724,caaa6793391520,0d63f804,"Some difference in the standard deviation can be explained, because we fitted the model on the incomplete data.

Now we need to do the same for the categorical features",1e79f342,0.4
18728,04bac111ffbe9c,86eb03f6,"##### 2. FEATURE ENGINEERING
-  Separate Title from Name and then drop Name
-  Decode Pclass and OHE it.
-  OHE Sex
-  OHE Embarked


1. Separate Title from Name


-  Create a new column named 'Title'
-  Apply regex to extract title from name
    -  Check for anomalies in the extracted titles and clean the untidy data.
-  Drop Name",82576b17,0.4
18730,03048e86a6d806,69824391,"Respondents from US, Australia & some European countries mostly work in the companies where ML methods have been well established. In Asia, the companies where the respondents work are still in early stage at applying ML methods.",1285c231,0.4
18733,21c1e34efd71b8,0f6ec7f1,"I will be trying the Near Miss and the SMOTE Cross Validation functions
Near Miss - in my terms, this function will reduce the number of majority class to the number of minority class, it uses vector distance method to do so. It picks up records that are closest to the minority class data.
SMOTE - This method adds minority class records. ",23b2cdd6,0.4
18734,4945eab98d7d39,fd06e525,Total number of shows/movies in different platforms.,46258ffb,0.4
18735,1c7dacc7f36c8c,588594de,## Hint,871a833c,0.4
18738,71c3c1eab0377d,9c2b8563,"# There are two ways of doing it
1. Binning by quantiles.
2. Fixed interval Binning",52b4e360,0.4
18744,65245c6e88a2ee,d324fdae,## Doing Test Train Split,71d6e90e,0.4
18748,8854f72e7e9be0,306dcd1d,"**Custom Generator**
* Takes in ids,meta_data,target
* Output: Images,meta_data,target

> In Next Update: Augmentation will be added",2a1031b7,0.4
18759,639e8aae4e046e,16b72f40,**Feature Engineering: session level mapping**,77deb4cb,0.4
18760,519e936017c30a,d24ad51d,"A la vista de los resultados obtenidos, podemos ver que la evolución es muy favorable llegando a registrar en 2006 un ingreso de casi 700 millones de dólares. Los años posteriores a este, no son tan favorables por lo cual el nivel de ventas va disminuyendo registrando ingresos inferiores a los obtenidos en el 2002.

Finalizado nuestra indagación, vamos a ver cómo se han desarrollado las ventas de las diferentes zonas geográficas. Este conjuto de datos recoge 4 tipos ventas, las ventas de Norte América, las de Europa, las de Japón y  las del resto de países.",dc34915d,0.4
18761,ff83da40bcdb19,292f4b16,"**Part4:**
*The CNN part.*

1. First as always we are importing libraries.

2. Than we are giving the parameters.

a) Activation: There is lots of activation function we are using activation function for learning part and activation function must be a differentiable function other way our model can not learn(you may want to look at the activation fucntion)

b) Convent: This are our hidden layers here we have 6 layer of CNN with a fully connected layer, and then the output layer.

c) Learning Rate(LR): It's like a little steps if you give this too small it will take lots of time to train a model.

## I may have some mistakes in here just let me know.",36b5ec8c,0.4
18762,169177b6e9edea,c82a222d,<h2><b>Family Size,ca42152f,0.4
18763,9276fa5cc2fef6,fb0ca675,"Oh, looks like someone was in dire need of completing their math homework. But, its surprising to see it marked as insincere. However, its important to keep in mind that there is significant noise in our data

From [data page](http://https://www.kaggle.com/c/quora-insincere-questions-classification/data);

> Note that the distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and sanitization measures that have been applied to the final dataset.

",24aa6a52,0.4
18764,62487bcd70b199,b3e0cf9d,"## Inference:
all the distributions are identical except for CD Account",f6ae50af,0.4
18766,6338f6b0178d13,54a7246d,"# Splitting the data for training and testing
",ae81b18b,0.4
18767,892be0a523578c,156a7881,"**6.1** By comparing the calculated data to the provided, I found that:
* Total minutes in bed from calculation is different from the one in sleepDay_merged, may be is because of the different definition according to the metadata. 
* Records in the calculated data are more than those in sleepDay_merged.csv, which may indicate some missing records from the provided daily data
* The total minutes alseep is not matched for more than 300 records, and the gap between is not a small one",b0e8d7c0,0.4
18771,171494b45650a2,bc56616f,### Price Distrbution,9c8cc578,0.4
18773,840534f2908a9c,b66f8ca6,"*Apart from Manhattan, we can see heavy pickups and dropoffs near JFK and La Guardia Airport.*",8081c3cc,0.4
18779,50d4ddf1953997,b41451b2,"Similarly, we can look at genres by by date added.",90bdddd6,0.4
18782,d905cde3391d2b,9340cebe,"## Mode

**Mode** is the number occurring most often in the dataset.
* It is only meaningful if we have many repeated values in our dataset
* If no value is repeated, there is **no mode**
* A dataset can have ***one mode***, ***multiple modes*** or ***no mode***.

Let's try to retrieve mode for our dataset.",067dba39,0.4
18783,67b7354e96113a,946cd17c,"**Creating a new feature based on Age**

1) Since age is from 0-100 we can form a new column based on age

        Age Group    Class
        0-18      -  Minor 0
        18-40     -  Adult 1
        40-60     -  Middle age 2
        60-100    -  Old 3
        ",dca94250,0.4
18791,5a8c553e21c70f,45d21f7f,Combine Class 0 and 1 samples again and shuffle.,9ebd9d8f,0.4
18797,513ce405d7f6a3,09621699,# wordcloud of negative review,8461e086,0.4
18801,3c2033cc99c12c,f11d66fd,#### Plot about time ,dfa22a54,0.40145985401459855
18803,979f1e99f1b309,3c96cfaf,***The plot showing that many of player finished the game in last places where a few of them finished it in the fisrt one***,d1bfebbf,0.4016393442622951
18805,b86bda7afe3ac3,26c677e8,Prepare a keras model. ,16197934,0.4017094017094017
18807,c84925c8171900,27218117,"<a id=""eda""></a>
<h2>   
      <font color = blue >
            <span style='font-family:Georgia'>
            5. Exploratory Data Analysis
            </span>   
        </font>    
</h2>",e21ff7ec,0.40186915887850466
18810,842547b2def18c,e90e4be3,"### Converting a categorical feature

Now we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.

Let us start by converting Sex feature to a new feature called Gender where female=1 and male=0.",b8efde6d,0.4019607843137255
18814,8ec771f5600a61,5c37e44c,So till now we've imputed missing age of all the datapoints in the train and test dataset.,48364c1f,0.4020618556701031
18817,b01ee6cb674fa3,d1b9feed,"Wow, that is some great ammount of rockets and missions!
Lets check the unique names of the rockets, not the mission's name, because that would be huge",a8ffd35e,0.40217391304347827
18820,d5f78aa381f58d,a2a0d20b,### Normalisation,d60f358f,0.40229885057471265
18822,9c26c5dcd46a25,417b27e0,"Cette matrice des corrélations ne nous apporte pas rééllement d'informations mais confirme mathématiquement des éléments logiques : `salt_100g` est très fortement corrélé avec `sodium_100g`, `fat_100g` avec `satured-fat_100g`... Il faudra cependant tenir compte de ces fortes corrélation dans nos modèles, la **colinéarité** dégradant les performances.


On remarque également que des **corrélations linéaires existent entre le nutriscore et certaines variables** plus que d'autres :",1bbbb677,0.4024390243902439
18824,fd4017c1514157,26e83616,### <font color ='red'>Date</font>,fd8f0896,0.4024390243902439
18825,2cb457b60dd246,df48b5c1,### Visualize  augmented images,339367df,0.4025974025974026
18832,4ae464582bac51,61279439,## BIVARIATED ANALYSIS,ca6a52ce,0.4025974025974026
18833,5f32117bcd5255,13488401,#### TARGET INFORMATIONS,85882abf,0.40268456375838924
18834,726833f92fb87a,3f9083ab,**We do not have enough data for customers with default=1 to draw a conclusion.**,7dc5e1b6,0.40268456375838924
18837,593d1d3d1df05a,c15ad30d,# A function to find Contours again in the cropped license plate,bc682ffe,0.4027777777777778
18838,c01049afb6d307,925a4341,# Visualization,d37d3b5d,0.4027777777777778
18842,2ada0305b68956,bbfe78a3,### 67. Palette = 'YlGn',133e26f4,0.40285714285714286
18844,e58e68e4eeefe5,80abd2fc,"Considering all the features, the accuracy with **Logistic Regression is 78% and Random Forest is 85%**",a87662ce,0.40298507462686567
18847,21413205980558,4d70785f,"#  We can see that the marital status has no correlation with the balance, and the distribution difference is not big. which is basically between 0 and 10000, but the married people will have a higher value in the balance<p>
# 我们可以看出婚姻状态对于盈余而言没有什么相关性，分布差别也不大。基本都是在0-10000，只不过结婚的人在盈余上会有更高的值",84197de0,0.40298507462686567
18849,1a222fee3089d2,2ac29b6c,## **Age category**,59ab8894,0.40298507462686567
18851,57070ad5e0f94f,86d0df99,"# **Turning ""Name"" and ""Sex"" to numeric completely and checking for correlations(ofc linear)**",d97edc41,0.4032258064516129
18857,4ae6a182abac64,c49e4e3f,* **Embarked and fare features**,418676c5,0.40336134453781514
18861,9d27afa9ca3f96,f13507f3,need to copy the images to working since YOLOv5 need to write and /kaggle/input does not allow to do it,2d86a18d,0.40350877192982454
18862,e03eb63c1f725d,afd910f4,"<a id=""4""></a>
<font size=""+2"" color=""blue""><b>Removal of Stopwords</b> </font><br>",e204b7e3,0.40350877192982454
18866,fe7360cddc13e5,ea503e36,<font color='red'> **3- Pasifleşme oranı p'nin ve t Zamanda Gerçekleşecek Transaction Sayısının Beklenen Değerinin Türetilmesi**,8979e423,0.40350877192982454
18867,30fdc4a6e3c1db,a43ee397,"What we see:
* The calender data is given for all the 1913 days in the sales data (actually we have 1969 days)
* We have at max 2 events in a day for which the event names and the event types are given
* We also SNAP days flags for each state separately i.e. all states have different SNAP days.

Knowing a bit about SNAP won't harm :- [SNAP](https://www.feedingamerica.org/take-action/advocate/federal-hunger-relief-programs/snap)

**What is SNAP?**

SNAP stands for the Supplemental Nutrition Assistance Program. SNAP is a federal program that helps millions of low-income Americans put food on the table. Across the United States there are 9.5 million families with children on SNAP. It is the largest program working to fight hunger in America.

**What kinds of groceries can be purchased with SNAP?**

Households can use SNAP to buy nutritious foods such as breads and cereals, fruits and vegetables, meat and fish and dairy products. SNAP benefits cannot be used to buy any kind of alcohol or tobacco products or any nonfood items like household supplies and vitamins and medicines.
N.B. So we can expect SNAP can help sales in food items ",6111ddee,0.40350877192982454
18873,71d3e4aee86e3e,31866a8d,> # Bar Chart Comparison,69706f0b,0.40384615384615385
18875,44f6a002ecd033,46fc2430,#### Cleaning Categorical Variables,70bbe106,0.40384615384615385
18876,99bf357eaf61f1,bcb8e913,"### Pair Plot 

#### Pair Plot between 'SalePrice' and correlated variables

Visualisation of 'OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd' features 
with respect to SalePrice in the form of pair plot & scatter pair plot for better understanding.",9d92fafe,0.40384615384615385
18877,6f1481148352e9,f982ed94,"**The study period is 20 years - from 1998 to 2017. Over the entire period, there were almost 700k fires. The largest number of fires was in 2003, 2015 and 2016.**

**On average, there were ~ 35k fires per year.**",7cfbdb8f,0.40384615384615385
18879,d0f6276d5b628c,d40e499f,#### Best Rating Recommendation,c64f5ce5,0.40384615384615385
18884,fdc9f4863744b1,8f788550,There are null values in the SALE PRICE data. ,b4529365,0.4041095890410959
18888,c7e5f658090347,f3df4cbe,<a id='Initial_ML'></a> ,43c78e7d,0.40425531914893614
18891,5f674175839b32,27d86973,<font color=pink>The action genre has the best selling games.,53a2e343,0.40425531914893614
18899,04ff2af52f147b,f0df9a85,"**Feature Correlations:**

Now that we have concluded dealing with the null values, we can investigate the correlations between our features to look for further insights into the dataset.  These insights will guide us in engineering other features as well as illuminate any possibly redundant features.",d5f37be9,0.4044943820224719
18900,5f27526aa6c113,f6d32710,Distribution is same for train and test,a5c26ab6,0.4044943820224719
18901,312135b445bd23,0e7b0a20,Read the 400 most frequent word vectors. The vectors in the file are in descending order of frequency.,8ced381f,0.4044943820224719
18902,ee23a565163388,a96955a7,## **Does platelets count really matter**,88aacbc4,0.40458015267175573
18904,0c57e3132ae184,3b50f460,"### III. Encode Ordered Categorical (Ordinal) Columns into Integers

Ordinal columns are categorical variables that include categories that are ordered in nature. Instead of using ```OneHotEncoding```, it is more appropriate to convert them into integers to reflect the ordered nature of these variables.",f6bac298,0.40476190476190477
18908,27778055896e17,179632f0,# KNN Classifier,1dbe0165,0.40476190476190477
18915,31268b33de97b5,56e602be,# Visualisation for categorical Features,1e6f7d14,0.40476190476190477
18918,a758983a68c014,6b54dce4,Let's put it in form needed for training.,ab89f181,0.40476190476190477
18923,2d40f383473fa4,a7f30c0f,Looks like for the distribution is better to fill the mode.,1da1eff0,0.40476190476190477
18924,2f47abddfd1928,5004a7df,"# 3. Data Exploration

In this step we will try to get a better understanding of each feature and its relation with Survived mainly but also between each other.

First we can start by plotting a similar heatmap as before to identify strong correlations to study.",ae33cc0b,0.4049586776859504
18931,ac9b48d531bad9,0c8914a3,Creating dummy variables for categorical data,95965e35,0.40540540540540543
18932,8276973853faa1,2963e75c,"> Rajasthan, Haryana ans Punjab are top 3 Veg consumption states",88da542b,0.40540540540540543
18951,510b8303776bb6,8a0ffef8,As we can see all the ordered categorical data fields in the data set have been converted into numbers. ,18080db8,0.4056603773584906
18952,81712ee7510ac5,fd4e431e,"WE can nest nest list inside of one another
",c4685e79,0.4057142857142857
18954,ea4e559a86d613,7ea0299f,"### Target Variable:
1. Very HIGH class imbalance. We need to take this in consideration when Modeling.
2. Age distribution:
    * Benign: follows a normal distribution
    * Malignant: a little skewed to the left, with the peak oriented towards higher age values.",eff47843,0.4057971014492754
18958,548f961125248d,3513cae7,"* It is important to tell CatBoost which columns are categorical and which ones are text. If no information is provided - CatBoost assumes all features are numerical. 


* Default values of CatBoostClassifier() parameters depend of the type of input data - CatBoost automatically applies the best settings. Catboost can distinguish between binary & multiclass problems - it will appropriately assign 'Logloss' as the 'loss_function' for Binary problems, 'MultiClass' for multiclass problems and 'RMSE' for regression problems. 


* Default number of 'iterations' is 1000. I set early stopping rounds to 100 (for boosting algos high patience values give best models) for the first run and I also selected 'use_best_model'= True . (When we fit using the model, we want to use the best model, rather than the potentially substandard model saved in memory at the end of training). 'custom_metric' provides an additional plot to moniter while CatBoost fits (It does not change training performance). 'eval_metric' is the metric used for 'best model' selection. 


* When fitting 'eval_set' is optional. If we provide 'eval_metric' and 'use_best_model' (metric used for overfit detection), we will need to provide 'eval_set'. 


* AUC selected as parameter of choice - as Kaggle competition requires this. Logloss also studied to prove Yandex's claims about Catboost's superior performance. 


* Input data can be in many different tabular forms. 
    - If only a dataframe is provided, first column is assumed to be the target. Rest of the columns are assumed to be features. 
    - We can provide a dataframe of features and a dataframe/array of target values, as we do in Sklearn. 
    - The Pool() class is specific to CatBoost. ",d8c5e8b8,0.4057971014492754
18960,598b6228760590,ce3ed95c,"- According to the background information, the first letter in the Cabin indicates the type of cabin number, so we can temporarily ignore the following numbers, as long as the cabin number is the same, the type is the same, but the spatial location is different.",be30ab66,0.4057971014492754
18966,ba4b3bd184acbb,df58c573,"# Data Cleaning

The next step in the data science lifecycle is to clean our data for analysis.

### Modifying Values

First we will focus on the Price column. 

Remember our test to find values that couldn't be converted to floats?",0f5de724,0.40601503759398494
18968,3cc097a5859dc1,40ebf935,# **Cleaning Data**,14380d73,0.40625
18972,2c3a6969252dc0,9f9c177b,Students with Research experience tend to have better LOR.,d30f10ce,0.40625
18973,69130a37583a06,9bdcc0e8,"## CARD variable distribution  :
* first plot is distribution of card1 continious variable on the basis of target variable .The one thing i have noticed was fraud transaction rate much higher than No fraud transaction.
* For card2 we have nothing special to conclude .
* For card3 there are 2 to 3 three value who have most of the data
",65a4de1c,0.40625
18976,117fc0956643d0,df0ffc29,"## Step 2. Load Pretrained ALBERT Model

[ALBERT](https://arxiv.org/abs/1909.11942) has similar architecture as other BERT models, but it is on the basis of a transformer encoder with Gaussian Error Linear Units (GELU) nonlinearities. ALBERT uses a different embedding method than BERT. In more detail, ALBERT uses two-step word embedding that first projects a word into a lower-dimensional embedding space and then extends it to the hidden space. Furthermore, ALBERT uses a cross-layer parameter sharing to improve parameter efficiency; it only uses feed-forward network (FFN) parameters across layers to share attention parameters. Another difference between ALBER and BERT is that ALBERT uses a sentence-order prediction (SOP) loss to avoid topic prediction and focus on modeling inter-sentence coherence.

We have pre-trained the ALBERT model with **[SQuAD v1.1](https://rajpurkar.github.io/SQuAD-explorer/)** followed by **BioASQ 6b** factoid datasets. The **SQuAD** dataset is a reading comprehension QA dataset posed by crowdworkers on Wikipedia articles, and the answer to each question is a span of text from the corresponding article. The BioASQ 6b dataset is biomedical semantic QA pairs, in which we assume that the contents are closely related to COVID-19. The limitation of pretraining with only **BioASQ** dataset is the lack of data. The primary reason for pretraining our ALBERT model with SQuAD v1.1 is to overcome the data shortage of using only **[BiOASQ](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0564-6)** dataset. 

We have pre-trained ALBERT base model with 3 different approaches (1) **SQuAD v1.1** only (2) **BioASQ 6b** factoid only (3) **SQuAD v1.1** followed by **BioASQ 6b** factoid. Each model is evaluated using **BioASQ** test dataset, and we have confirmed that the model pre-trained on both dataset have achieved the highest performance. Therefore, we will use the model with the most top performance for the COVID-19 task. 

The models are pre-trained using GPU available in google colab. The pretraining process using both **SQuAD v1.1** and **BioASQ 6b** factoid dataset took approximately two hours. 
The pre-rained model is uploaded in the input directory. ",68cef9fd,0.40625
18979,c85c94076e9c3a,d884532f,### - Merging some columns,3ea0c443,0.40625
18982,93f5423667b9d5,e678967a,"### ↑ team_id = 1,22,25,31,43,48,77,84,89あたりが強そう",55bdf071,0.40625
18987,e19e307b3fd188,f9a865f0,### Furniture,2173955b,0.4065040650406504
18988,5f4ae633cfd090,fa74084a,Let's see the countplot,a30a16e2,0.4065934065934066
18996,b9bc7dc9f582e5,dbd93078,# **Imputing Null Values**,15cc4d28,0.4067796610169492
18998,2a56d6b0e153f2,32fa879c,"HERE,THE INDIVIUALS ARE NOT EXPERIENCED THOUGH , THEY ARE LIKELY TO BE PLACED.",8dc315e6,0.4067796610169492
19001,c4bca5d86a38c3,28e6e6f5,Haciendo OneHotEncoding de la columna Pclass,e23d297c,0.4067796610169492
19003,dac3c8204a2d1b,711a92ba,# Grouping By Genre And Year,b0d2d0dc,0.4067796610169492
19007,c09fac3c943d51,974ba9cc,Clearing rare categories and setting 0 to NaNs:,678d076d,0.4069767441860465
19009,a5a419dc7245b0,10b30743,#### Ordinal Encoding on Education,4279726e,0.40707964601769914
19013,4883314a96dc34,02950f22,### Multivariate plots to understand relationship between attributes,50d36836,0.4074074074074074
19015,e0f03003a69819,ec966056,Normal distribution is here in the bmi claims . Quite ideal for our approach,609ad1f4,0.4074074074074074
19018,db5a369894fef6,413d9a01,"Just created a new data frame containing the active cases over time for each country. WOHOOOO!

- Then create a plotting function similar to before",065aaf61,0.4074074074074074
19019,ac04ba639d1c93,16d77934,"Based in others ideais we can:<br>

- train a model to predict `fc`, `sd`,`	pso`,`pso` features and create a feature that is the sum of this 4 features;
- add this feature to train and test and train the same model to compare performance;
- train a better model;",748059d5,0.4074074074074074
19021,e6576e985ccc71,0769f7ef,"#I wanted number 115. It returned many, No 115. ",63d2c3a5,0.4074074074074074
19023,fce6f1b02867e3,d133341f,## iloc retrieve the data points using the index values of rows/columns and doesnot accept boolean arguments,3fb572c2,0.4074074074074074
19028,2b39f4ff896f97,5383265b,Print the classes,3ddfe182,0.4074074074074074
19032,ffc3bb768dcf97,f366c928,Time to preproces. I'm gonna use Scikit-Learns One-Hot Encoding,15ce84be,0.4074074074074074
19034,d128317750d689,7d4cb69e,"Next, we need to define the hyperparameters. Epoch is basically how much the network is going to train (beware of overfitting). Batch is the number of training samples that will be fed to the network in one iteration. Learning rate defines how fast will the network adjust and, well, learn. This hyperparameter is pretty important and tricky to adjust, 0.001 worked pretty well for me.
You can experiment a bit and see what works best for you.",d87f7428,0.4074074074074074
19038,c6f8ff61a5fa87,653061d7,## 1.nuSVR,3eea586b,0.4074074074074074
19040,b9328fe3b0cefc,38df4855,"We see(可以看到)：
- Connecticut won the most Championships, 5 times in total. Followed by Baylor, 2 times.(冠军数最多的是Connecticut，共获得5次锦标赛冠军，其次是Baylor，获得2次冠军)
- Notre Dame was the top runners up, 5 times, followed by Mississippi St.(亚军数最多的是Notre Dame，5次，其次是密西西比的2次)",3a35eb23,0.4074074074074074
19041,7baeb0ffc6659e,595f287b,**Embarked**,8cbebba9,0.4074074074074074
19042,1883198d6d8c3c,b4d5e654,"<h3>Read/Save Other Data Formats</h3>



| Data Formate  | Read           | Save             |
| ------------- |:--------------:| ----------------:|
| csv           | `pd.read_csv()`  |`df.to_csv()`     |
| json          | `pd.read_json()` |`df.to_json()`    |
| excel         | `pd.read_excel()`|`df.to_excel()`   |
| hdf           | `pd.read_hdf()`  |`df.to_hdf()`     |
| sql           | `pd.read_sql()`  |`df.to_sql()`     |
| ...           |   ...          |       ...        |",69a1d458,0.4074074074074074
19043,233cb23d9e01b9,afb870b7,## Instantiate the tuner and perform hypertuning,ffa56c19,0.4074074074074074
19047,b809d07ddd17ed,8d3d9133,"## Prepare training, validation and test data lists",e32bf3b1,0.4074074074074074
19049,f4b9042e693b6c,7ec78d9f,"Here, I define a model class for the timm models.",676cacc9,0.4074074074074074
19050,ab6da5994949a3,b3c081ea,### Creating a func to evaluate model's performance.,fae6b91d,0.4074074074074074
19053,7a8493d72b98e8,2b416b7e,**missing values** handling,7c28e80a,0.4074074074074074
19056,09751c520b0616,12900afc,### Making some new features,a4d0c7e9,0.4076923076923077
19058,d07915a6e6992e,4a2d6b1e,"# Feature engineering
![FE.png](attachment:FE.png)",2b912140,0.4076923076923077
19061,98a6794067932a,c3bbee83,La première cellule de code de cette section sert donc à représenter le montant total des ventes effectuées par état. Le code permet donc d'effectuer la somme des ventes pour chacun des états et affiche ensuite le tableau complet. Cette analyse nous sera utile afin de déterminer les états où se trouvent nos meilleurs clients au niveau des ventes.,08600fe2,0.4077669902912621
19067,0e7ac281a19feb,3b07dd6a,## Custom Trainer,5b84d10f,0.40816326530612246
19068,eb33e05704d647,a500e650,"Generate the ground_truth anchors
",cd80436d,0.40816326530612246
19070,12f4d16fc21645,a230ff1c,"<h3 style='color:Red'>N, P, K values comparision between crops</h3>",c7752038,0.40816326530612246
19075,62487bcd70b199,27846b39,## <a id='4.9.'>4.9. Distribution of Numerical Features across Loan and Non Loan Cusotmers</a>,f6ae50af,0.4083333333333333
19082,2ada0305b68956,aee1d06d,### 68. Palette = 'YlGn_r',133e26f4,0.4085714285714286
19083,0caaec057f7184,6e76527d,"# Train data and Test data overlapped?
",b875533e,0.40860215053763443
19085,3c2033cc99c12c,de13147a,"**Note:** *In this part, I will aggregate the data of amount and transfrom the format of time into hour to see whether there exists any distribution features.*",dfa22a54,0.40875912408759124
19088,a0b321057e7402,8de4b0f4,"From the plot above we can see a few things:

* Bitcoin prices are facing a strong upward trend.
* There are some seasonal elements to the price (+- 500)
* Most of the noise in the data was generated during the ""hype"" phase of 2018-2019.",5f73fb91,0.4090909090909091
19089,e323e594ef918f,1825588b,"For visualization, we will focus on a single image for now. Let's grab one from our validation set and show the image along with the labels. Note that we trained a model with random cropping, and we will apply the same transform here. We will select one with class 12:

### excerpt from hosts' notebook describing cell classes:

![centrosome.png](attachment:centrosome.png)",6e829ab6,0.4090909090909091
19099,b066ab2167199c,d7e58353,- Observed that **there is no categorical features** in this dataset. Only have **numerical features of int64 & float64**.,18a1753d,0.4090909090909091
19104,d83e5b44d1b80d,110858f8,"***From the sample dataset, 78% of survey contribution is from male and 19% is from Female.***",62845930,0.4090909090909091
19108,73d8e56bc709b1,3aff8243,"Spain has most players in top european teams, meanwhile european countries took the top 5, and got 8 seats in the top 10.  
As representatives of South American football, Brazil and Argentina followed.",78ec3cce,0.4090909090909091
19111,d1ff7e10ee0102,997518c8,#### Scatter plots between 'SalePrice' and correlated variables (move like Jagger style),2cc71c3c,0.4090909090909091
19113,8578b9a8d00730,00b15606,## ***ResNet Model Building***,7648721b,0.4090909090909091
19116,c2be02442e8cfd,88393aa4,# Univarite analysis,2d364acc,0.4090909090909091
19127,5ce12be6e7b90e,08b89e13,"## Exercise: if and else

",c0ab62dd,0.4093567251461988
19130,726833f92fb87a,afecc805,## Loan vs campaign success,7dc5e1b6,0.40939597315436244
19133,bbaa07ad21cf4e,5fa3b105,"## 4. WordCloud <a class=""anchor"" id=""6""></a>

",3ab6b254,0.4095238095238095
19136,55a5e31d03df9f,8c887ad8,"We evaluate for our test set and the accuracy is *0.039*, it may seem low and it's but is a little better than random guessing, the probability of random guessing is 100% divided by 36 classes which is around 0.027. 

The `history_model_mlp` contains the losses and accuracy for the training and validation, what we want to achieve is every epoch to reduce the loss in the training and validation. If for example we just improve the loss within the training but not on the validation it's a simptom that we're **overfitting**, if by the other hand the loss is not improving it's a simptom of **underfitting**. 

<img src=""https://miro.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png"" width=""600px"" > 

What can we do when we are **underfitting**? Increase complexity of the model it could be as the following:
- Adding more layers
- Increase the number of hidden units
- Change the activation function
- Change the optimization function 
- Change the learning rate
- Fitting on more data 
- Fitting for longer

What we can do if we are **overfitting**?
- Add more data, which gives the model more chance to learn between samples.
- Data Augmentation, increasing the diversity of training without collecting more data.
- Better data, removing poor samples for example an image where there isn't a clear LEGO figure. 
- Use transfer learning, take a model pretrained patterns from one problem and adapt it to your use case. 

[More in depth about this topic here](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)
",06dce00f,0.4095238095238095
19137,7454fdc444df16,3fecbc97,"### Binary target visualisation per tissue slice 
Before we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:",a7818ef5,0.4095238095238095
19140,4daf6153275cbf,02707d76,#### 1. Local Restaurants Ratio,51db1961,0.40963855421686746
19142,601e18072783b4,95fb88f5,# Merge IMDb and Netflix dataset,36b2b1fa,0.4098360655737705
19143,bcd7e398c4d0ec,21bf4b52,The number of color maybe the significant feature for model training.,77a143f6,0.4098360655737705
19144,0858e1bb3cbaca,83823239,"Now, to get a quick summary of the dataset, we can use function

**.sum()**",78548374,0.4098360655737705
19148,83df814455f06c,e7383af7,We can see that there are no missing values in the dataset. I have checked the frequency distribution of values previously. It also confirms that there are no missing values in the dataset.,c9cff71a,0.41
19149,4cd25e50c7e007,3ef78668,**The temperature and atemp variable is highly coorelated as we can see in above heat map**,ceb0c525,0.41
19156,9eed0fae1c7958,e9d8b1d1,# visualize (plot) some images from training data with their labels,3fb1438e,0.41025641025641024
19158,50b03ce5b1a286,26d519b4,#ValueError: invalid literal for int() with base 10: 'None',d49896a5,0.41025641025641024
19159,d4c5aaa4b36810,95c6269c,Lets keep the columns that have at least 147 entries. ,65441f28,0.41025641025641024
19160,c8c4705cca1ebb,ae70ed17,Normalization for item_cnt_day. The values of item_cnt_day compress from 0 to 1000,6d9d7107,0.41025641025641024
19161,4bbe953f82d29b,7b0a0354,"### День, окно, лаг и разность",772301f2,0.41025641025641024
19162,4d91e84c564cbe,295c65b0,`sum` does what you might expect:,355a43e3,0.41025641025641024
19168,49ee86d074de69,f0699013,"## Education
* 1:High School
* 2:Graduate
* 3:Postgraduate
* 4:Master or Doctor",71ccc6d3,0.41025641025641024
19171,840534f2908a9c,c5ef6b35,* **Distribution of fare amount heatmap**,8081c3cc,0.4105263157894737
19172,f91f58d488d4af,0d5c7e95,"For overall accuracy, we need to calculate the distance to ideal 3 for every image in the validation set.",5df1bbf3,0.4105263157894737
19179,3cd78d8d6d56e4,b62beda4,## Building Model Architecture,9f632e94,0.4107142857142857
19180,f13534449a3750,79ed15fe,"The Challenge of detecting the ships in the images can be thought as a classification problem for pixels, where, for each image, we need to classify 768 × 768 pixels in one of two classes: ship and no-ship. At pixel level the problem of imbalance data is more accentuated than the case of considering the whole image. To find the proportion between ship and no-ship at pixel level we need to consider the total number of pixel and the pixel where the ships are. The total number of pixel is 768×768×n_imgs while the number of 'ship pixels' is the sum of the all the pair positions of the  strings encoded in 'EncodedPixels'. Finally the total amount of 'no ship pixels' is just: total pixels - 'ship pixels'. We tried to do that but, while counting the numbers of ship pixels, we exceeded the ammount of memory provide by Kaggle. Therefore we decided to report the results of other notebooks and discuss the results.
",8b7f3332,0.4107142857142857
19182,738bfced935b69,c03ff2d1,"The distribution between price and mileage is skewed to right, and notice the Semi-Auto transmission Most densely distributed. ",2d3c592d,0.410958904109589
19184,596389bed473be,dd4f9aad,"# loc

Selects based on column and row name


# iloc
Selects on column and row index",5f8af156,0.410958904109589
19185,1eb62c5782f2d7,af3635b5,### Solusi,bb69f147,0.410958904109589
19188,b0c2805cd5c087,8a640bff,Image routledge.com,0446f327,0.4111111111111111
19194,c84925c8171900,3daa0a26,"<a id=""stat""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.1 Overall Statistics
            </span>   
        </font>    
</h3>",e21ff7ec,0.411214953271028
19196,957e035ba5b9d5,46462b05,## Save model,778ab3d3,0.41134751773049644
19202,e93a41c03638fe,d0870148,# 3. Model Building:,7363527b,0.4117647058823529
19205,395ed8e0b4fd17,01b9640b,# <center>CANDELSTICK CHARTS</center> ,7573ea31,0.4117647058823529
19206,52cfd66e9ec908,f683366d,"Also, a gentle note that we can use the ChunkedDataset to generate CSV files of scenes.",c74adcdf,0.4117647058823529
19208,c65a65d4041018,0417ece8,"It is difficult to compare salaries in different countries. 100-200k $ could be a norm for an American DS and a good salary in general, but in Russia 10-20k is a really big salary.",824fb229,0.4117647058823529
19211,523123dad03177,52fa5236,# 5. Test preparation course,48a5e4e6,0.4117647058823529
19212,a871419285588a,cb00e00d,### Analyze the repartition of classes,5e08e15f,0.4117647058823529
19215,4ae6a182abac64,398ab100,Let's now see how the embarkation site affects the survival.,418676c5,0.4117647058823529
19217,6aaee7fdbc7945,323a01b9,# **Question 1**,dae653ae,0.4117647058823529
19221,71b75664517244,181e0143,"Chelsea is 2nd best team here, the performance charts show that chelsea is onfire around 2004 - 2008, and doing just fine up until now",fc905af5,0.4117647058823529
19225,1d1598b6fa2aa7,fc88897a,"### Bar charts

```plotly``` also supports bar charts.",e066accf,0.4117647058823529
19229,64169805aacf17,096d2a59,## Define parameters for neurla painters and optimization,1f12ded0,0.4117647058823529
19234,7f74a04ae75792,5dd21052,"### How should we handle missing values? (Should we drop the rows? or impute them with values?)
",d01e91da,0.4117647058823529
19237,a0a5baa6c7e12a,ca6ac1e3,"<img src=""https://raw.githubusercontent.com/gvyshnya/tab-dec-21/main/AutoViz_Plots/Cover_Type/Heat_Maps.png"">",551d41de,0.4117647058823529
19240,4fa553c2b837d4,221090aa,"# Data Wrangling
Data wranling is needed because a large dataset may contains many null values in it. These null values can make your model poor. So, there are three solutions to overcome those null values",c65a23e9,0.4117647058823529
19244,8f50c9c16db95f,3491605b,"# Kick the ball far, if you want to gain more yards <a id=""far""></a>
Kicking the ball far, closing to the end zone will result in a lower yard line once the receiving team possess the ball. And it will require the latter one a longer distance to return the ball. On the other hand, the kicking team players will have much more time and space to tackle the ball.
From the output below, it implies that

* When comparing two kicks differ by one yard in the kick length, the longer kick length on average have **~0.36** units more yards gained than the shorter one.
* The difference is statistically **significant** (p < 0.05) and it means there is strong evidence that there is a real association between kick length and number of gained yards in this population.
* The R-squared value is 0.238, indicating that **23.8%** of the variation of number of gained yards can be explained by the kick length.

And based on the findings, I'd suggest taking the kick length of the ball as a metric for evaluating how good a kick off is.",26cc763a,0.4117647058823529
19245,fa02c409161192,3aba7caf,## 1.3 Training using the training data,e97077f7,0.4117647058823529
19248,d0080e3a39bc5c,5fba5b48,**3RD TRANSFORMATION**,2fcde4cf,0.4117647058823529
19250,e4c6dd957eb5ce,30b06c84,"We can't see a very clear pattern considering these date features, but something call my attention. <br>

Some findings about the charts:
 - In days: It's apparently normal... All days of month are very similar less the day 31 that make all sense. 
 - In days of Week: We can see that Saturday and Sunday have the smallest % of registration with 10.7 ~ 10.9%;
 - In Months: We can see that December and January are the months with less registrations. 
 
Now, Let's use the crosstab function to see if have any relation between months and week days",2e383665,0.4117647058823529
19253,55c34673c1f760,40227e61,# Data Augmentation,2663c47f,0.4117647058823529
19256,fdc3afd309b850,b8fa6a7e,"<a id=""vnv""></a>
### 6.2.5 Value null values",966bde38,0.41203703703703703
19259,3fb15e6e48aec2,88bae3b3,* Cound do more with the Ticket but lets such as split the prefix on the forward slash as well as cleanup some spelling or punctuation differences \ But to save time lets remove the punctuation and use this instead,9d1f4358,0.41228070175438597
19260,fe7360cddc13e5,aefdb3c5,Müşterinin t zamanında aktif olma ihtimalini aşağıdaki formülizasyon verir.,8979e423,0.41228070175438597
19261,6cade0b6a41ba2,fcacec9d,## 3.7. Work Type,e6110293,0.41228070175438597
19266,3dd4294f903768,a62fcff7,***,0d89d098,0.4125
19267,fdbbd573ba31c2,11d70ae8,### windmill_height(m),f7c28d74,0.4125
19268,254cccd5145725,5f9425e1,Let's have a look at the dependancy rate column.,a49b4037,0.4125
19275,06ecf7a304c309,02edebc2,"### 2.2 UseCase 2 - Image Denoising

오토인코딩은 매우 유용합니다. 이제 다른 예시를 살펴보겠습니다. 

많은 경우에 입력 이미지는 노이즈를 가지고 있습니다. 오토인코더는 이를 제거할 수 있습니다.
우선 train_x와 val_x 데이터를 이미지 픽셀과 함께 준비해봅시다.

![noiese MNIST](https://www.learnopencv.com/wp-content/uploads/2017/11/denoising-autoencoder-600x299.jpg)",714de627,0.4126984126984127
19278,72d528df923403,8070ca2b,"- It seems that HOUSEHOLD_1 has a growing demand tendency.
- FOODS_3 seems to have its peaks in the mid-year.
- Demand on FOODS_2 is growing in the last periods.",d51c8e8e,0.41304347826086957
19279,9b5de3823ad5ab,2f1b9033,## Classification task,33e48774,0.41304347826086957
19281,a6b9837940ee38,f45eb488,"* Next, we split the training dataset, validition dataset and test dataset with the ratio of 8:1:1 using tf.split.",52d2acc7,0.41304347826086957
19282,b01ee6cb674fa3,bd0b45d3,"So far,so good, but there is also room for improving

Next, lets explore the column company name and see what can be extracted",a8ffd35e,0.41304347826086957
19283,7e89d387feb9f5,7c2afb47,"#### Добавим в датафрейм информацию о населении города и стране
#### Городов не так много, так что гуглим и вносим данные в словарь вручную...",989e3a1b,0.41304347826086957
19285,0e2a23fbe41ca9,f2af1ff6,"Observations:
- category_1 is a binary column (N/Y)
- category_4 is a binary column (N/Y)
- category_1 and category_4 have very high number of N
- category_2 has 5 categories and also nulls (11,887)
- category_2 seems like a ratings column with 1 being the most frequent rating followed by 5 and 3.",64e4762c,0.41304347826086957
19292,b10bd75889dad9,01abd713,#### Looking at the scatter plots above we see that the numerical data is not that much skewed,ee00ceee,0.41333333333333333
19294,7e1da639035ac5,86583f22,### <a id='8.3'>8.3 Racial distribution in community and private schools</a>,120b6c23,0.41333333333333333
19299,44f6a002ecd033,b2e6332d,For cleaning the categorical variables I am going to use get_dummies to effectively clean. This will also prove effective for my model.,70bbe106,0.41346153846153844
19303,18a96bb5711ed9,f7b35c75,"Mediterranean region has the highest total dead and missing number. We can especially see the noticiable color difference in the northwest side of Libya **(Tripoli).**
I searched on google about this city, and I could find an abundant amount of news articles about attacks and refugee missing incidents. <br><br>

**Here are some sources to take a look:** <br>

* https://www.aljazeera.com/news/2019/07/1000-killed-battle-libya-tripoli-190708191029535.html
* https://www.bbc.com/news/world-africa-48849595

",e79768db,0.41379310344827586
19305,14defffcd250f3,1ceca758,**Missing Values**,3a683b94,0.41379310344827586
19306,cd10f3afd970b3,be10492e,Let's take a quick look at what the data looks like:,2db3c8e4,0.41379310344827586
19308,d5f78aa381f58d,f5a696f1,"Since most of the machine learning algorithms use Euclidean distance between two data points in their computations, this is a problem. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by a method called feature scaling.",d60f358f,0.41379310344827586
19313,1750367e54f407,386e941c,# Build the model,a8e655b2,0.41379310344827586
19315,a4f0a3e1316ff9,a9e17c9b,"# Convert Table to Wide Format

Lets make the columns different countries with values",53bf0160,0.41379310344827586
19317,fb5c6021d127ef,8cc7c53c,"## 3) Exercise: Create and train the model

Below, write your query to create and train a linear regression model on the training data.",dd05cbd3,0.41379310344827586
19319,6b54e39f86bdb5,1acf9c01,"## Learning rate annealer

I use the Keras method ReduceLROnPlateau to reduce the learning rate when the accuracy on the validation set doesn't increase anymore.",198084bc,0.41379310344827586
19326,20e1ba19eb9b5e,137d0f37,**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.,4569bfc1,0.41379310344827586
19328,5ea840754577e3,84db56fc,"Even though 1st class contains the least amount of people; **24.24%** of total, The highest fraction of people that survived were from 1st class **(62.96%)**. And unfortunately, Most number of people had their tickets booked in the 3rd class **(55.1%)** and the most number of people that didn't survive were from 3rd class **(75.76%)**.

Similarily, 1st class tallied lowest in deaths **(37.04%)** and 3rd class tallied lowest **(24.23)** in survival rates. Consequently, as a persons socio economic status is higher, that chances of survival is also higher.",9cf9b73f,0.41379310344827586
19330,401338428b2d1c,92ed2c65,## Feature Scaling,e4b768be,0.41379310344827586
19332,9535bb04ae042c,18f854be,## v) Ordinally Encoding the Categorical Features,165b6fae,0.41379310344827586
19335,84127ade6fde87,aa089410,## One-hot encoding whole words,f55d05b6,0.41379310344827586
19337,1cd8be6e679620,5fe9ade6,## Detailed Analysis,3ce15a43,0.41379310344827586
19339,45921c50ac56fa,ba455686,List of words after lemmatizaion and reducing it to lower case.,465973eb,0.41379310344827586
19347,38b79494ac749e,6ee60d70,What is the optimal `degree` to go with?,39162a40,0.4142857142857143
19348,2ada0305b68956,bd891809,### 69. Palette = 'YlGnBu',133e26f4,0.4142857142857143
19349,fe6750354fb64f,e412aec5,# Time Chart,271741f0,0.4142857142857143
19350,9c044fa3072552,181f77ff,We can also see which states have the most accidents by using the `State` column from our dataframe,1362842e,0.4142857142857143
19360,8cefb86a675e5d,8f9e6c1d,**Inspecting a typical row from our features:**,79f9e69b,0.4146341463414634
19363,514d8de15cb7ef,82aa7d18,### Using Bag of Words,cfe111b2,0.4146341463414634
19367,0b01138ad120fc,66d9ae71,**Train/Test Split**,0b4b72e6,0.4146341463414634
19368,fd4017c1514157,96f366fb,"while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.",fd8f0896,0.4146341463414634
19369,8d0aebab1e5914,7b260236,"# Data-preprocessing
### Standardizing data",084e671f,0.4146341463414634
19374,f6648e47713411,d8d29d43,"# RGB Analysis
Histogram là một biểu diễn đồ họa cho biết tần suất xuất hiện của các giá trị màu khác nhau trong hình ảnh. Trong không gian màu RGB, các giá trị pixel nằm trong khoảng từ 0 đến 255 trong đó 0 là màu đen và 255 là màu trắng. Phân tích biểu đồ có thể giúp chúng ta hiểu được phân bố độ sáng, độ tương phản và cường độ của hình ảnh. Bây giờ chúng ta hãy xem biểu đồ của một mẫu được chọn ngẫu nhiên từ mỗi danh mục.",f4af4d1c,0.4148936170212766
19375,510b8303776bb6,37f4cffc,## Continuous data fields,18080db8,0.41509433962264153
19379,f015d0147e8fbf,dd255c50,### 6. Installments Payments Data Table `installments_payments.csv`,518954fb,0.41509433962264153
19381,0ad8d416b89b78,b62a2641,# Feature Engineering and Model Development,0b0562f0,0.41509433962264153
19385,30fdc4a6e3c1db,3cc3daef,"We have 10 National and Religious events, 6 Cultural Events and 3 Sporting events (30 events in total) in a year",6111ddee,0.4152046783625731
19386,5ce12be6e7b90e,dffe8a00,"If kmh is bigger than 120 print 'SLOW DOWN!' if it is not print 'safe driver',",c0ab62dd,0.4152046783625731
19387,9169c4e9c33c90,88e69a35,"Clearly, most of the authors appear only once in the top 50 charts.",725bf880,0.4152542372881356
19388,1294fb4c86f993,2906b456,Normalizing the `states` in both Data Files,4471e513,0.4152542372881356
19390,a4f8ad33c823c5,17b70651,The below correlation plot was created to identify any multi-collinearity between the apache measures.,fcd48307,0.4153846153846154
19394,09751c520b0616,a4b0c8f0,#### (i) Age - age of house,a4d0c7e9,0.4153846153846154
19395,c115e287523aab,40af5077,"# Light EDA
If you're too lazy to do **EDA** by yourself. Then definitely this library is for you. You can use **Pandas-Profiling** to do bunch of **EDA** with vey few lines of code. 😉",feb1288b,0.4153846153846154
19396,03048e86a6d806,d8425178,### Budget on Machine Learning and/or Cloud Computing Services,1285c231,0.4153846153846154
19400,d07915a6e6992e,aa1f6074,"

What we need to do to process following variables  - 

**PassengerID** - No action required

**PClass** - Have only 3 numerical values. We will use it as it is.

**Name** - Can be used to create new variable Title by extracting the salutation from name.

**Sex** - Create dummy variables

**Age** - Missing value treatment, followed by creating dummy variables

**SibSP** - Drop the variable

**Parch** - Drop the variable as most of the values are 0

**Ticket** - Create dummy variables post feature engineering

**Fare** - Missing value treatment followed by log normalization

**Cabin** - Create dummy variables post feature engineering

**Embarked** - Create dummy variables",2b912140,0.4153846153846154
19402,c13f73168789c2,aac5e461,"### 2.4 Select multiple rows and columns<a id='15'></a>
Syntax : `df.iloc[[row_index1, row_index2, ...], [column_index1, column_index2, ...]]`",16175052,0.4155844155844156
19403,90691864eb68c7,d7b0064b,We have missing values in n_hos_beds variable : 498 of 506,3555ef9b,0.4155844155844156
19405,75adb7945ef9bd,8adf70dd,## 6. Create statistics from texts,785c5095,0.4155844155844156
19406,722cd844dfbe8f,4a9dbdc2,"In this example, only 7% of the image has colored pixels.      
We will now look at the distribution of the **colorization rates of the images in the full FLAIR folder** :",0cedb385,0.4155844155844156
19407,241cf32abb22d8,2f2b97fa,The AUC score for the top 10 features selected by Random Forest Importance is 0.753.,47157066,0.4155844155844156
19418,c80939c7c626cf,07f86d60,"# Now we know that some Age columns have missing values
Statistically we can use Title Median age",b9ac31e2,0.41605839416058393
19419,5f32117bcd5255,37760f05,#### WORLD COORDINATE SYSTEM,85882abf,0.4161073825503356
19426,cf46cd6f7c55c0,02f046f5,# Step 1 and 2 - Build first QDA model and predict test,191b86b8,0.4166666666666667
19427,1c5aaf7bea6414,eca9d908,# Exclude Cancelled Transactions,34d8f42d,0.4166666666666667
19430,a69d41047fdd3e,de475a18,"### 2) Explore the table schema

How many columns in the `crime` table have `TIMESTAMP` data?",b1f28647,0.4166666666666667
19443,e2a907e1c7d7f9,ed86d555,"## Deal With Unbalance Data
Menggunakan Metode Under-Sampling
Yaitu data yang lebih banyak, datanya akan disamakan dengan data yang lebih dikit",f09fb692,0.4166666666666667
19447,87e94f864d74be,8a446586,"There are some entry where ""country"" has multiple values. so i think i will add a new column with just the first one so we can check witch regions have more productions",294bfe9f,0.4166666666666667
19449,7c7a7db391c517,e2ef8ed9,## 2.2 Loaded the data and print it.,f53450dc,0.4166666666666667
19454,268a610bbc64b4,64a5918a,Percentage of new users,8a16f301,0.4166666666666667
19455,c349ee5a821411,4b642a13,I want to pack the data in continents.,572b269d,0.4166666666666667
19456,02773bdc5d3c7a,f06f9e99,*3. Remove the time column as time reveals the time after the first transaction and logically should not impact the target column*,86245f35,0.4166666666666667
19467,98ea617d18c9cc,fee7ddc0,# Creating Data generators,e6316d11,0.4166666666666667
19472,b10bd75889dad9,d78518ca,#### Lets plot some boxplots to check outliers,ee00ceee,0.4166666666666667
19473,64a336ac34d95c,911e9969,"## Co-relation matrix
",be73a990,0.4166666666666667
19478,999258a81ba32a,e8fb3300,# Ball 4 Frequency Chart,48cd3d21,0.4166666666666667
19488,30c8dc87ce52ca,f03ece63,EFFECT OF PCA ON MODEL PERFORMANCE,805e9d67,0.4166666666666667
19492,c18267b203f28a,b875257d,"# Brief exploratory data analysis (EDA)
First we'll print out the shapes and labels for a sample of each of our three datasets:",09ca8efb,0.4166666666666667
19495,f6488772605bb5,0f7b33bc,## Defining the Model (Convolutional Neural Network),068d4697,0.4166666666666667
19497,b3681fd423741d,6f40734c,# 3. Bagaimana distribusi tahun edisi mobil-mobil bekas tersebut?,1aec06ce,0.4166666666666667
19499,2a377ced98d67a,5e9d78f3,"* GRE Score ranged from 290 to 340 with 50th percentile lies in 308-325 region and median is around 318.
* TOEFL Score ranged from 92 to 120 with 50th percentile lies in 103-112 region and median is around 108.
* University rating ranged from 1 to 5 and most 50th percentile lies in 2-4 and median is around 3.
* SOP was evaluated in range 1-5 with most SOP in 2.5-4 range score and median lies in 3.5.
* Letter of Recommendation (LOR) was scored in range of 1-5 with most LOR in range 3-4 and most median is around 3.5.",262231a8,0.4166666666666667
19501,2c8119a4061997,2ee21a68,Cross entropy loss is applied independently to each part of the prediction and the result is summed with the corresponding weight.,1836a79c,0.4166666666666667
19502,712198370d5521,cadc731d,"Clearly, there are a few outliers in the Income and Age features. 
I will be deleting the outliers in the data. ",5882e04c,0.4166666666666667
19505,b3e0b7e9ff6849,4cd894f8,"<p>Check out descriptive statistics of numerical variables again! Look at 99% quantile of Quantity and Price columns and compare with the maximum values. We can say that there are some outliers.</p>
<p>Let's find out these outliers and replace them with the highest limit.</p>",f6e4bb0d,0.4166666666666667
19508,63d0d9b9a8c7d2,348f119a,As we need to find for all the test values so for the missing value with the most occuring column value,e32e5933,0.4166666666666667
19511,7650e0ac081e94,3f5e49d7,### Load the LIVECell data,0081cee5,0.4166666666666667
19512,166a62ebb4fc3a,9cfebd14,"Here, we have analyzed the relationship between the 10 key attributes and the diagnosis variable by only choosing the ""se"" columns.",db48a079,0.4166666666666667
19518,6b2776f151ed9c,9f269b15,# MinMaxScaler - StandardScaler - Normalizer,40406d5c,0.4166666666666667
19521,fc8e0042411c46,27538331,- Median for converted and unconverted leads is the same.,af476c2a,0.4169278996865204
19525,ce9ed5e2d601d7,d64967d1,"# Scaler transformer
By using RobustScaler(), we can remove the outliers
![](https://github.com/furyhawk/kaggle_practice/blob/main/images/Scalers.png?raw=true)",f58a2f43,0.41732283464566927
19536,e58e68e4eeefe5,8cb5cc88,#### Trying with different sets of features based on the observations.,a87662ce,0.417910447761194
19538,21413205980558,f2218996,# 3.4  Education,84197de0,0.417910447761194
19541,979f1e99f1b309,c0788781,***THE plot show that the player who finished the game in advanced places kill more people than other which is normal***,d1bfebbf,0.4180327868852459
19544,f0fab078f8533b,56578495,## f. Converting duration into a bin with different lengths,bdb5ea32,0.41818181818181815
19546,917957c6c4065f,40ff0f3e,"likes, dislikes, comment_count, tag_count, description_length 다섯가지 항목은 최소값 항목을 살펴보니 0인 사례들이 있습니다.  
각 항목 별로 확인해보았으나 데이터에 오류는 없었습니다.  
해당 항목들은 인기동영상이 되는 것에 영향을 주지 않거나, 영향이 작은 것으로 판단할 수 있습니다.  
위의 다섯가지 항목에서 파생한 likes/views, dislikes/views, comment_count/views, dislikes/likes 항목들도 마찬가지로 영향이 크지 않을 것입니다.

views를 중점적으로 탐색하고, 나머지 항목들은 간단하게 살펴보겠습니다.


*title_length를 제외한 나머지 항목은 평균값과 최대값의 차이가 크기 때문에, 극단적으로 높은 값들을 제외하면 실제 평균은 다소 낮아질 가능성이 있습니다.",55b8ed68,0.41830065359477125
19557,806ce45c8fa303,92efeed3,"## Visualize the data with t-SNE

TNSE(t-distributed Stochastic Neighbor Embedding) is one of the dimensionality reduction method other than PCA and SVD. This will supress some noise and speed up the computation of pairwise distance between samples. ",3e5c34dc,0.4186046511627907
19563,b86bda7afe3ac3,052d8006,Validate results,16197934,0.4188034188034188
19566,27d5291d6365ba,5ecfa2a2,# Total Transaction by Age,96b30229,0.4189189189189189
19567,e4525eb0c96f28,c8764cec,"### Sales per Country

In order to further analyze the year vs sales correlation, we decided to also analyze sales by the country column to see if a games country of development had any impact upon the games sales.

Japan and USA are known to be major game producers. As veteran game production countries, a hypothesis about them could be that in comparison to other countries they would have higher sales, or a growth of sales over time as game development technology would also improve over time.

Once again removed Wii Sports as it's an outlier.

New Column:
- Country_Sales: Total sales per country

New Dataframe:
- **country_df**: Holds Country_Sales which assigns total sales to each country. ",2093a1f1,0.4189189189189189
19568,e1fff2f67cbe32,d2387e16,> Average user score is lower than the overall % of correct answers(bottom left graph). It means heavy users have even better scores(bottom right graph).,c6dfde64,0.4189189189189189
19570,63b44c85e32c1f,5281e68a,But if nested list is not what is desired then **extend( )** function can be used.,fb9b9562,0.4189189189189189
19572,55a5e31d03df9f,a9977ab3,"# <a name=""cnn"">CNN - Convolutional Neural Network</a>

The convolutional neural network excels working with images. Why? Because it recognizes patterns in the data taking into consideration not the Flatten pixels in which finding patterns isn't that easy but rather take into consideration the pixels around using the convolution layers. 

What is a convolution?
*""The convolutional neuron performs an elementwise dot product with a unique kernel and the output of the previous layer’s corresponding neuron.""* This may sound complicated but imagine in other words it's to find the **filters** (which are the equivalent of the weights we discussed before) that minimize the losses. 

To understand the process we need to define the following:
1. Kernel size: (or filter size) refers to the dimensions of the sliding window over the input.
2. Stride: indicates how many pixels the kernel should be shifted.
3. Padding: Padding conserves data at the borders, for example adding zeros symmetrically around the edges of an input.

Now how the process work is to start at the top left corner and place the extract the pixels from the size of our kernel size, we multiply this kernel by the filter (element-wise multiplication) and add all the outputs of this multiplication, after this is done with the stride that we define we move our kernel for example 1 step to the right and do the same process. You can visualize it as the following:

<img src=""https://www.researchgate.net/profile/Baptiste-Wicht/publication/322505397/figure/fig5/AS:583063998308353@1516024698839/A-valid-convolution-of-a-5x5-image-with-a-3x3-kernel-The-kernel-will-be-applied-to.png"" width=500px>

You can see the Full MIT Deep learning [here](https://youtu.be/AjtX1N_VT9E), it covers all these topics precisely and in an unique way and also can read the [CNN Explainer](https://poloclub.github.io/cnn-explainer/) which is a great source.

We're actually gonna replicate the network architecture from the CNN explainer a Tiny VGG.

**As we have done before lets implement it and discuss the code afterwards.**",06dce00f,0.41904761904761906
19576,7f74a04ae75792,7a8941cc,#### Handling Missing Value for `Cust_Last_Purchase` column,d01e91da,0.41911764705882354
19577,c65a65d4041018,26f65354,### Years of experience,824fb229,0.41911764705882354
19578,56e58d53ac9c57,06c35528,now let's fill them,90e2ab8e,0.41935483870967744
19579,e8c6480a3122b3,78553921,We can deduce that passengers having more than 4 siblings or spouses didn't survive. ,40dc4cca,0.41935483870967744
19582,c0ddb77bf32e2b,74a4c3d0,"Unfortunately there is no NA we can fill back. THC mainly distributes from 1.9(25%) to 2.2(75%).

Although this leptokurtic distribution  might be promising we impute the NA row with median and build a fine data, but  in my preference, I will just exclude these variables, one reason is that the redudancy will increase computation cost.",a0cb45f7,0.41935483870967744
19585,0cb456a5456cf9,177c069d,# **Q3 Answer** <br> The average occupation days of resort hotel from 2015 - 2017 are larger then 4 days and that for city hotel is about 2-3 days.<br> 假日酒店的平均预订时间在2015-2017之间在4天左右，而城市酒店平均预订时间相对较少，在2-3天之间。,5701729c,0.41935483870967744
19589,7dec6bdea6d779,1eec5551,f2_score taken from [this Kernel from Alexander Teplyuk](https://www.kaggle.com/ateplyuk/keras-starter),18be5949,0.41935483870967744
19590,098fedfcd07456,0c9b7162,"# The Image are reshaped to 28,28 as gray scale image expect 2*2 Matrix and we reshaped this earlier to 
1. To print we are just reshaping this back to 28 * 28 
1. cmap=gray is required for printing the gray scale image the steps is required for only gray scale image.
1. By Default imshow expects a colour image cmap is not required for colour image
1. The print is show there is no change in data happend after scaling down . ",052ece26,0.41935483870967744
19594,0925f172b5eb74,d4b6fd6f,"# Kaggle's TPU configuration

As mentioned before, the sole purpose of this kernel is to perform the computation-intensive tasks that require Tensor Process Units (TPUs), this is why we won't dig into data analysis or data preprocessing and we'll get right to the training part. 

The following piece of code allows the connection of the Kaggle Kernel to the available TPUs or GPUs.",ec34cd72,0.41935483870967744
19596,0caaec057f7184,4fd13bfd,### items in shops,b875533e,0.41935483870967744
19597,0d9a2067267ba1,e948c45c,We can't extract a valuable informatiom from these box plot all what we can say is that the Energy intensity can show off extreme high values at each modality,abc194fb,0.41935483870967744
19600,a3e8d6ef4c5188,ad104705,"Let us understand the graphs one by one.

cap-shape: Categories 'x' and 'f' have approximately equal number of poisonous(p) and edible(e) mushrooms. They don't differentiate between 'p' and 'e'. Category 'b' has mostly edible mushrooms. Category 's' and 'c' have very small presence. (This might not be a good feature)

cap-surface: Category 's' and 'y' have equal 'p' and 'e' mushrooms. Category 'f' have 50% more 'e' mushrooms. Very little presence of category 'g'. (This might not be a good feature)

cap-color: Each category has both the types of mushrooms. (This might not be a good feature)

bruises: Category 't' are majorly edible musrooms and category 'f' are poisonous.

odor: Each category is either a poisonous or edible mushroom.

gill-attachment: (This might not be a good feature)

gill-spacing: It does not distinguish very clearly but can be a helpful feature.

gill-size: Each category is either a poisonous or edible mushroom.

gill-color: Here we can see, few colors distinguish very clearly between 'p' and 'e'/

stalk-shape: (This might not be a good feature)

stalk-root: Can be used as a feature.

stalk-surface-above-ring: Can be used as a feature.

stalk-surface-below-ring: Can be used as a feature.

stalk-color-above-ring: Can be used as a feature.

stalk-color-below-ring: Can be used as a feature.

veil-color: (This might not be a good feature)

ring-number: (This might not be a good feature)

ring-type: Can be used as a feature.

spore-print-color: Can be used as a feature.

population: Can be used as a feature.

habitat: Can be used as a feature.
",7c8212dd,0.41935483870967744
19601,78998e078eaaa1,f9ae9a8c,# XGboost Model,2b29364c,0.41935483870967744
19607,ee23a565163388,19c10f4c,"**Inference**
- Even though the difference in platelet counts is subtle between two groups, the affected patients have comparatively less platelets.",88aacbc4,0.4198473282442748
19608,2ada0305b68956,e0884f84,### 70. Palette = 'YlGnBu_r',133e26f4,0.42
19609,83df814455f06c,ed05b01b,"# **10. Declare feature vector and target variable** <a class=""anchor"" id=""10""></a>

[Table of Contents](#0.1)",c9cff71a,0.42
19611,7dd46c750653eb,f8dc2b12,Comparing Birth Rate of Male and Female Monthwise,c2644713,0.42
19619,fc8e0042411c46,44c50e3a,**Nothing can be said specifically for lead conversion from Page Views Per Visit**,af476c2a,0.4200626959247649
19621,c4386b8a01d66e,19fcb0c8,# Conductivity,dc732bf5,0.42016806722689076
19624,b01ee6cb674fa3,2ef48edc,"56 agencies/companies launching rockets into space!

Now, how this info can be treated? 

At first glance, no info can be splitted as was done before, but a research can be conducted and info can be extracted. The ideia is to include 2 new columns:
- company_nature (is it public or private?)
- company_hq (country where its HQ is located)


The company nature refers to a situation such as NASA and SpaceX

Company HQ is then set to solve the situation for the 'country' that was accounted for the launch. 

NASA is an US agency, so its HQ is located in USA, but what about Sea Launch, 
> a multinational spacecraft launch service that used a mobile maritime launch platform for equatorial launches of commercial payloads on specialized Zenit-3SL rockets' [wiki page] 
 

Checking wiki, the company started with a share between Russia, US, Ukraine and Norway and now it is mostly Russian, as it is confirmed by s7space.ru, its official website. 

This part of the dataset has to be done manually, with web research and considerations to be assigned to each company 
",a8ffd35e,0.42028985507246375
19627,17a24d566ffa59,f9e27c91,![](https://s3.amazonaws.com/nlp.practicum/reduced_svd.png),89049e56,0.42028985507246375
19636,d1ff7e10ee0102,97db412d,"Get ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!",2cc71c3c,0.42045454545454547
19640,26b93b6f4dc148,ca9468b9,### Droping Irrelevent Features ,6f667d22,0.42105263157894735
19647,0d8df2c2983694,2c9350e2,"We want to know whether the bank marketing strategy was successful, so we need to transform the outcome variable into 0s and 1s in order to perform a logistic regression.",9bf7fa4e,0.42105263157894735
19650,9e27af2600925c,6f0eb6d4,"**Expected Output**: 


<table style=""width:15%"">
    <tr>
        <td>  ** w **  </td>
        <td> [[ 0.]
 [ 0.]] </td>
    </tr>
    <tr>
        <td>  ** b **  </td>
        <td> 0 </td>
    </tr>
</table>

For image inputs, w will be of shape (num_px $\times$ num_px $\times$ 3, 1).",9b556435,0.42105263157894735
19652,d93a87fdbdb3d2,6514fea5,"> **Stemming**

Stemming means elimination of affix.
Instead of saving all shape of words, a stemmed word can reduce the size of index and increase the accuracy.

<code>PorterStemmer</code><br>
It eliminates the suffix.<br>

<code>LancasterStemmer</code><br>
It is similar to <code>ProterStemmer</code>, but the performance is better.
* presumably -> presum
* cement -> cem
* owed -> ow
* ear -> ear

<code>SnowballStemmer</code><br>
It supports for the 13 languages except for English.
* Autobahnen -> autobahn</n>

<code>WordNetLemmatizer</code>
* dogs -> dog
* churches -> church
* abaci -> abacus",30d079c3,0.42105263157894735
19655,d369f200a84c2a,f6ab7c4f,Creation of the convolutional layer,8fef4d48,0.42105263157894735
19657,a1dcd92986bc84,b9f2dd96,"## Implement the text encoder

We use [BERT](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1)
from [TensorFlow Hub](https://tfhub.dev) as the text encoder",730acaaa,0.42105263157894735
19661,ba4b3bd184acbb,462220da,"It seems that the Price has a $ character at the beginning in some cases.

We can use the `replace` method combined with regular expressions (regex) to remove the $ character.",0f5de724,0.42105263157894735
19664,826ccb616bd2a8,2b714de5,"## Decision Tree
1.  Split the dataset into thirds: train & testing
2.  Set the depth at 4 (At 4, we find 100% accuracy)
    -  Usually 100% accuracy is frowned upon as there may be `overfitting`. However, due to the population being 200 and the quantity of categorical variables used in the final recommendation, I feel comfortable with the precision/recall score spitting out perfection.",4d7df2ec,0.42105263157894735
19666,29437539745aa5,29d103a7,"<a style=""font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"" id=""setup"">2&nbsp;&nbsp;NOTEBOOK SETUP</a>",c17b490a,0.42105263157894735
19669,038abade89e59f,03afc0a7,# Model,cb32a3fe,0.42105263157894735
19671,99afe9f3af6dbc,9cc1c561,"**K-means clustering**

Dengan menggunakan matrix tf-idf, dapat dilakukan algoritma pengelompokkan untuk lebih memahami struktur tersembunyi di dalam description.",cdec9b3a,0.42105263157894735
19677,31b564f11ef638,3948b9ba,### Drop useless features,424f9692,0.42105263157894735
19678,1f3295ed0d4e4a,dd9f6ae8,# Dask Lazy dataGenerator,b0aeb172,0.42105263157894735
19680,f35ee6e9fab592,7c421764,"The issue with a simple value count on the columns for this dataset was that video games have multiple ```developers```, available on multiple ```platforms``` and are of varied ```genres```. value_counts() would count the groupings and not the individual counts",b15f7073,0.42105263157894735
19684,caa0ce2715bf34,fa69bd07,## Treat Missing Values,78a5dc51,0.42105263157894735
19688,2f47abddfd1928,dd41d392,"## 3.1. Pclass

Seems that Pclass has a good correlation with survived, age, fare and Cabin_Letter.

Lets inspect these relations.",ae33cc0b,0.4214876033057851
19689,e9b9663777db82,f22d5726,#### 6.Quantitative analysis for Number of Bathrooms feature,648e8507,0.4214876033057851
19690,842547b2def18c,23279eb3,"### Completing a numerical continuous feature

Now we should start estimating and completing features with missing or null values. We will first do this for the Age feature.

We can consider three methods to complete a numerical continuous feature.

1. A simple way is to generate random numbers between mean and [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation).

2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...

3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.

Method 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.",b8efde6d,0.4215686274509804
19694,71b75664517244,8ecf646a,### Manchester City,fc905af5,0.4215686274509804
19696,835a7b4e660d23,85018f10,### Melt & Pivot,53bc7a6e,0.42168674698795183
19700,ff3a8ce61fab6a,ab20f02e,"<hr>

## Exampel 2 ",9afe1654,0.421875
19701,bd380b97b5c894,76a9337b,"Even at the level of groups of men with benign formations and malignant formations and the same groups, women most often have a form on the torso, that is, benign formations (on the left of both histograms) are observed on the torso with about the same frequency as malignant ones.",66f2562a,0.42201834862385323
19702,188731d7fa0604,9ddca772,## EDA,7cc543d3,0.4222222222222222
19703,d96e03a9e7c030,43f5ac1b,"Before diving into the results, let's provide some definitions to the columns in this data frame:
* `model (string)`: variable corresponding to the column name in `interim_modeling_df`
* `categorical (bool)`: whether or not the variable was categorical
* `model_r2 (float)`: coefficient of determination of the model
* `model_adj_r2 (float)`: adjusted r-squared of the model (similar to the coefficient of determination, but takes into account the number of variables in the model. In this case, the number of variables included in the model would vary only if it was based on a categorical column
* `median_absolute_error`: median absolute error associated with the model's test set
* `pred_col (string)`: only applicable to categorical variables. If not null, the column `pred_col` corresponds to this dummy variable included in `interim_modeling_df`
* `pred_coef (float)`: coefficient of the variable (`model` if a numeric column, `pred_col` if a categorical variable)",d2b72ced,0.4222222222222222
19704,d58491f2896fc1,7c370ca0,**Bütün sınıflandırıcılar için uygunluk değerini döndermeye yarayan fonksiyondur.**,514bfdff,0.4222222222222222
19705,e25c0f830df3f4,b12c78a9,# Displaying Negative comments,fdcf7189,0.4222222222222222
19709,d6cbd7160961dc,de471587,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 800.

* The center graph shows the result for city G, which had the worst chi-squared value. 

* The rightmost graph shows the result for city A, which had the best chi-squared value. 

Note that the chi-squared values for the second digits are much lower compared to the one of the first digit. It happens since the theory predicts a more random distribution for the numbers in second digit. ",36d74664,0.4222222222222222
19710,7341f069d9b2ee,c25db81f,Looking for Correlations,e0a49e62,0.4222222222222222
19711,49ac6594c8f5cf,953f2dcb,Central students gets placed more.,6f19f28a,0.4222222222222222
19712,d77e6d61ad2e8b,a8eb26ad,# Plot a Model,03fd0e96,0.4222222222222222
19713,6fad63bfd45ef9,158fe7df,## Test Data,b3c6f1d6,0.4222222222222222
19715,5be39e4e35cec7,cde39122,"<a id = ""6""></a><br>
# Basic Data Analysis",14d617c9,0.4222222222222222
19720,3597174a998d4d,558d6df8,"As the ratio of cancelation varies from different countries, I consturct a new variable named country_new.",276892ed,0.4222222222222222
19724,9bcfa825c8b2e6,785355e5,DiabetesPedigreeFunction: aile öyküsüne dayalı olarak diyabet olasılığını puanlayan bir işlev,220f36e4,0.4225352112676056
19726,631cd434fc3aa2,7849a436,"* _GarageYrBlt_, _GarageArea_ and _GarageCars_: since a missing value means no garage, we can raplace them with zeros.
**NOTE**: Issues with setting _GarageYrBlt_ to 0? Another solution could be imputing by mode groupinb by neighboor (read [here](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard314950) for more info). ",2b74febb,0.4225352112676056
19729,6a80f915608fc2,5a1745fb,### Which MoAs (targets) have the most detectable g-vectors?,636938eb,0.4226190476190476
19733,8ec771f5600a61,c0221209,***********************************************************************************************************************,48364c1f,0.422680412371134
19734,e19e307b3fd188,a645943f,There are about 3x more unfurnished houses than furnished,2173955b,0.42276422764227645
19736,726833f92fb87a,bcb82409,**It looks like that customers with a personal loan tends to refuse the deposit**,7dc5e1b6,0.4228187919463087
19739,3f451680b1857b,73e37b0a,# Split,56c45a1b,0.4230769230769231
19740,897ca904b74a98,225da5d8,# Preprocessing,c5844ad4,0.4230769230769231
19751,95efc1ad1d3e26,51425ae6,## Model (Conv),79de1120,0.4230769230769231
19760,6f1481148352e9,33cb1c4e,"**Now let's check the number of fires by state in 2003, 2015 and 2016.**",7cfbdb8f,0.4230769230769231
19761,cf08b03b002c13,44699490,"Despite the fact that NSFW content pops up only once in two hundered posts, the average score of NSFW content is alsmost 4 time higer 738 to 197.",104d416f,0.4230769230769231
19763,582cb872d19026,15f5d53a,"
******************************************************************************************************************************

#### Now, the ""installs"" column in our dataset has been brought to the desired format and is ready to be analyzed. 
******************************************************************************************************************************",8d966d69,0.4230769230769231
19765,2facf256353117,ff1fb5b7,## load image data,18f579be,0.4230769230769231
19768,a915263bc207da,989c22ce,To set the min_periods it is needed to understand the statistics of number of ratings given per movie,b17ebcda,0.4230769230769231
19769,af6556ced704f6,a4a2f6a0,"***Tidy Data***
        
   * We get tidy data using **melt()** function. 
   * In other words, to shape it according to the features I want
   * Why we use melt()?
    
        Because it is necessary to use the predecessor melt () function to visualize it with seaborn.  
   * If we want to reverse of melting, we get pivoting data using **pivot()** function
        
        ",881577c0,0.4230769230769231
19775,fc8e0042411c46,a44c983f,## Last Activity,af476c2a,0.4231974921630094
19777,c80939c7c626cf,9719dc86,# 5 Age,b9ac31e2,0.4233576642335766
19781,513ce405d7f6a3,6573da8f,# wordcloud of Very natural review,8461e086,0.4235294117647059
19783,b9bc7dc9f582e5,7f029281,Our data has missing values at random and hence can be predicted using multiple imputaion. Univariate imputation might introduce bias as missing data has no pattern.,15cc4d28,0.423728813559322
19784,f2e5e9fb9eaaf7,f872c9eb,### 4.2.3 Features f51 - f75,048e0d08,0.423728813559322
19785,a44368590e878a,c79c8aac,### State,77743ba8,0.423728813559322
19788,149cb8d3489224,77db7299,### Tweet source,116858e7,0.423728813559322
19789,9169c4e9c33c90,30a33fb5,### Which authors appeared a total of 6 or more times?,725bf880,0.423728813559322
19790,a077820f7ab459,4441070c,### Set the weights of the imported model,05a43104,0.423728813559322
19792,a81661cc35d8d2,2b30096f,"<font size=""3"">Observations:</font>

1. In both the above set of visualizations, we can see that serum_creatinine and creatinine_phosphokinase variables have a lot of outliers. We will see if we can deal with these by scaling. A min max scaler will not work since it is affected by extreme values, and will not do much as far as reducing variance is concerned.


2. Another data transformation to explore would be convert serum_creatinine and creatinine_phosphokinase into a binary categorical variables, basis certain cutoff point. A category value of 1 would indicate elevated creatinine levels or Hyponatremia respectively.",3331f113,0.423728813559322
19796,ed8009f482b380,5b367ab9,Drop 'id' column and also the original categorical columns,e99941fa,0.423728813559322
19798,4b64dc653fb7eb,379c2ea3,"<a id=""3""> Step 3 - Split combined words </a>

For instance, converting **whyAreYou** to **why Are You **",57675cc2,0.42424242424242425
19801,efd44ce2c08541,65c2ba6a,# Similarity: Image+BERT,ebc2d00c,0.42424242424242425
19805,3793c438a71b52,e1011315,"# Concatenating all data
final = pd.concat([old_data, new_data], axis=0)",13eb76df,0.42424242424242425
19806,f166950fa915f8,191a905b,### Tokenize Text,a7f6ca5e,0.42424242424242425
19807,2f964d08c25d93,9a3e5245,Distribution graphs (histogram/bar graph) of sampled columns:,1f2e4468,0.42424242424242425
19812,510b8303776bb6,45352408,### Box plot of all continuous data fields,18080db8,0.42452830188679247
19814,738bfced935b69,96ded34b,## Data Analysis by tax,2d3c592d,0.4246575342465753
19817,fdc9f4863744b1,3027883a,"There are 10,000 values showing up as null for SALE DATA. These needs to be removed from our observations. ",b4529365,0.4246575342465753
19822,ac1abfe1dfe815,fc58934d,----,6529dbcb,0.4247787610619469
19824,9a040a4f21091e,20f42691,"The mis-classified toxic as non-toxic speech is about what I expected. For example, the only thing that makes the fourth comment toxic is the ""with your mom"" comment at the end, which can easily be non-toxic depending on the context. 
However, the mis-classified non-toxic speech as toxic was surprising. For starters, I definitely would consider at least two of the comments to be toxic, even though in the data they are labeled as non-toxic. This poses serious issues for the analysis going forward if the dataset itself is mislabeled.

Let's take a look at which words contribute to the classifier labeling toxic vs. non-toxic comments:",f591b57d,0.425
19828,5ffe6aa38958a1,1bd39750,"It appears that embarking location has more correlation with Male passengers than with female passengers

**Pclass**",11f5412e,0.425
19830,5626e84c4e6bf8,85ee9674,"# Analyzing the results
**Minisom objects provide us with enough data to perform good analysis of our results and gain more insights**",e2ecb669,0.425
19831,edc19e349fe80a,373b7678,## Building a model using linear regression,7882221a,0.425
19836,fdbbd573ba31c2,03e02f98,***,f7c28d74,0.425
19837,1011899b959f44,8ab15316,5. How did battles in the year 299 affect the number of major_deaths? (Hint: Use conditional selecting),0b112382,0.425
19839,3dd4294f903768,fbddc3af,"Now, we will do some **feature engineering** and try to get more from our dataset.",0d89d098,0.425
19840,62487bcd70b199,d6211884,"## Inference:
Data distribution of both the classes are similar across all numerical variables",f6ae50af,0.425
19843,ee9ddc756b2d4a,9c129271,### Metodo 2: implemento non-negative matrix factorization,e367eab3,0.42528735632183906
19849,f6648e47713411,30fa8bda,## Phân phối Kênh Đỏ,f4af4d1c,0.425531914893617
19852,04e6b0d3c70f46,8c503530,### Define Model,56344f77,0.425531914893617
19853,c7e5f658090347,7ad9d37a,"## Initial Machine Learning Tests Prior to Proper Model Building.

#### Above we saw that:
1. There is a large class imbalance to be dealt with.
2. There are potentially outliers present. Further, the violin plots showed us some of the distributions do not have a gaussian distribution. This means that the outlier removal method chosen should be okay working with non-normal distributions.
3. Some features could be removed from the model as they do not appear to contribute much. 


#### Additional Considerations: 
1. Given what I have described above, there is a clear danger of having way too many things to try in combination with one another. I will therefore first test some of the ideas above out on a single, rapid to train model (logistic regression), and then make some decisions based on that on what to take forward for a second round with some other models. Edit: I added random forest after based on some observations. 

#### Therefore:
1. Building pipelines will be valuable here due to the desire to evaluate the impact of multiple things (e.g. outlier removal, feature removal, up and down sampling). 
2. As I want to evaluate multiple protocols I will use stratified k-fold cross-validation. This means my training and testing data will be combined and I will use this approach to select my best model before final validation of this model with the ""evaluation set"".

3. I will use the Interquartile Range Method to try to detect and remove outliers from the model. We have so few samples of fraud that we will keep all of them, but for not fraud samples we will apply outlier removal. 

4. I will evaluate my models with the roc_auc_score (as recommended by the challenge author). This provides a balanced view of model quality for both imbalanced and balanced datasets (test_train runs will always have a balanced dataset so technically not required at this stage).",43c78e7d,0.425531914893617
19855,957e035ba5b9d5,caccc852,## Visualize training history,778ab3d3,0.425531914893617
19859,2ada0305b68956,4d54c250,### 71. Palette = 'YlOrBr',133e26f4,0.4257142857142857
19861,ac04ba639d1c93,9a2d4176,"<a id=""id4""></a> <br> 
# **4. Data Pre-processing** ",748059d5,0.42592592592592593
19862,e0f03003a69819,1c98da83,# Region-Charges Relatioship,609ad1f4,0.42592592592592593
19867,2f0f808765fc67,aa522a2b,# **C. Ridge Regression**,fd1f6494,0.42592592592592593
19868,9ad9a97e628bfa,d88daa69,"**Name**
다음으로 이름에 포함된 정보를 추출해보겠다. 
생존과 관련해서 이름으로부터 추출할 수 있을 법 한 정보는 우선 호칭 (성별 등의 정보 포함 가능)이 있을 것으로 보인다.
*이 부분은 Baseine kernal : https://www.kaggle.com/youhanlee/youhan-s-baseline 을 참고하였습니다. ",0a7e1136,0.42592592592592593
19871,918040fad252ec,aebee404,Memberikan keterangan loss akurasi dan metric akurasi,966fcd8f,0.4262295081967213
19877,eb0ecd6bebeb15,512f1080,value_counts() fonksiyonu ile veri çerçevemizin ne kadar dengeli dağıldığını sorgulayalım. ,d7b93a60,0.4264705882352941
19878,9cec5ddf8b6f49,ddc9dde7,### After Outlier Treatment,d39fc8e7,0.4264705882352941
19880,156bbcff05dcea,0836174a,# Modelling,66ad1fe9,0.4264705882352941
19881,99821bc6a45be6,18e5e36c,"# Define Models:

We will test many different models to see which is better fit for this problem. They all incorporate transfer learning, which utilize a pre-trained model to use as a base and layers are added afterwards to adapt to the problem we are working with. This reduces the time needed to train the network. All of the models utilizes a different base model, however each is a Convolutional Neural Network (CNN). CNNs are the base structure for image classification tasks as they are able to extract features from images - edges, lines, and other shapes - without explicitly defining them. ",b9d59346,0.4264705882352941
19884,dc0b0e1cb46c6f,d299a081,"<a id='2.2'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;"">2.2 Top vaccines Laboratories</p>",47b17a7b,0.4264705882352941
19885,02b7e38902069e,c1f5522f,#Above the similarity is 0.09326018,726a03a0,0.4264705882352941
19886,e4c6dd957eb5ce,b6af753a,## Crosstab Analysis - Day of the Week by Months,2e383665,0.4264705882352941
19890,91eaec994e0c6f,45359a18,"- California has the highest number of sales.
- We observe again the sales decrease to their lowest level (previously observed with a single item time series) at the end of every year, let's try to get more details about this pattern !",376aef10,0.4266666666666667
19894,67b7354e96113a,7ab3ddec,"**Creating a feature based on Sibsp and parch**

sibsp	- of siblings / spouses aboard the Titanic	
parch	- of parents / children aboard the Titanic	

Since these two features represent Family we create new Column Family",dca94250,0.4266666666666667
19898,9c26c5dcd46a25,120384c4,"Logiquement, les variables utilisées dans le calcul des nutriscore sont bien corrélées linéairement à ce dernier. **<font color=""green"">Notre concept d'application pourrait donc être d'estimer le grade Nutriscore et le score correspondant grâce à une régression linéaire multiple sur les variables les plus corrélées, tout en utilisant la catégorie du produit<font>**. 
    
Nous allons vérifier si cette théorie fonctionne grâce à la **régression linéaire multivariée**.",1bbbb677,0.4268292682926829
19899,0b01138ad120fc,a83a0e7e,"**Time-step will be 2, so, given the previous 2 days, the RNN will discover what's the next food  
Given Monday(1) and Tuesday(2) RNN have to return Pancake(3)**",0b4b72e6,0.4268292682926829
19900,30fdc4a6e3c1db,3112aa9e,So every month we have 10 SNAP days for all the 3 states and it has been consistent througout the years which might fall on different days in different states,6111ddee,0.4269005847953216
19901,5ce12be6e7b90e,e6810729,## `while` loop,c0ab62dd,0.4269005847953216
19902,312135b445bd23,2648f717,Train t-SNE in order to reduce embeddings to 2-dimension for visualization purpose:,8ced381f,0.42696629213483145
19904,04ff2af52f147b,bcd187f9,"**Creating DeckClassSurvProp feature:**

When investigating our initial assumptions, we saw that the survival rates for the various classes were quite disparate.  Later, when filling the null values for the *Cabin* feature, we saw that the various decks seemed to be organized in a way as to segregate the passenger classes.  These factors suggest that we may extract some predictive value from taking a closer look at the deck and class combinations, and their survival rates.",d5f37be9,0.42696629213483145
19905,e67925694c07d3,36b81ca6,target = 1 skews toward younger people,83af4c4a,0.42696629213483145
19907,98a6794067932a,f3a1f566,"La cellule ci-dessous ne nous permet pas de tirer d'analyses supplémentaires, mais elle sera utile lors des analyses suivantes. En fait, ce code permet d'associer la colonne ""State"" à l'index de ce tableau, ce qui nous sera utile par la suite afin de modifier le nom des états.",08600fe2,0.42718446601941745
19913,ee23a565163388,bfc3c070,## **What difference does serum creatinine make**,88aacbc4,0.42748091603053434
19915,b01ee6cb674fa3,5e9ed1f3,"with so many companies, it would be good to create a function that assigns the nature and hq to a given company

And also 2 more counters, one with the unmarked companies and the other with the marked companies",a8ffd35e,0.427536231884058
19916,7e89d387feb9f5,ec01b425,### Добавленный числовой признак №8. Население,989e3a1b,0.427536231884058
19920,912bb73358069c,5036b5c3,Evaluations of keras outputs and pytorch outputs,0cf9db82,0.42857142857142855
19922,764c279bb8005c,d1d1a9b3,## Image Augmentation,78f93ad0,0.42857142857142855
19923,c54ea4523bd49c,57f7034d,Make the training/validation split to measure training accuracy,097ccba2,0.42857142857142855
19926,2473d004f92592,b4b256d2,**Splitting the SMS data into Test and Train data**,18d3b6ee,0.42857142857142855
19930,e78e7edae89049,8319d4e4,# Autoregression model,9cef1d94,0.42857142857142855
19934,80f86fa2d88ff1,6f407bfe,Scale our data ,f5cead1f,0.42857142857142855
19935,241cf32abb22d8,0e58258e,### F-Score,47157066,0.42857142857142855
19942,659f5f3ef8aa0e,7835f9d3,Now you're ready to read in the data and use the plotting functions to visualize the data.,3654c2d0,0.42857142857142855
19943,663bbc9eaf267b,b042a7dd,"* There are only 3 obervations for Electric cars. For simplicity, we merge ""Electric"" to the ""Other"" category.",32445529,0.42857142857142855
19948,8dd655515e7d18,96c64fdd,"**Analysis:**

The 'Extraversion' is uniformly distributed between min-19 & max-60",895f41cf,0.42857142857142855
19951,df51d4c54fbb91,f9c00be8,## Simple,4226dd72,0.42857142857142855
19954,04bac111ffbe9c,382c3071,"##### NOTE : Found unmatching titles like -> 'Don','Rev','Mme','Ms','Major','Lady','Sir','Mlle','Col','Capt','Countess','Jonkheer'",82576b17,0.42857142857142855
19955,c98a0dbd5eb6d7,ca88361f,"We will now create separate series for the following countries of interest:
1. United States
2. Italy
3. China
4. Rest of the World",33c15d04,0.42857142857142855
19959,e04e5204572e7e,9912f972,### *Assign the labels to the images. Don't you think it's important to let the farmer know not to butcher a high paying sheep?!*,6c888be9,0.42857142857142855
19961,3cd78d8d6d56e4,44697058,"### Architecture for Hyperparameter Optimization
- Amount of Layers
- Amount of Neurons
- Learningrate
- Checkpoints
- Early Stopping ",9f632e94,0.42857142857142855
19964,5f4ae633cfd090,892d3fb0,"A majority of these values belong to 0, then 1,2,3 with a rare appearance in 4. All belong to float64 type. Also, these are bout related.",a30a16e2,0.42857142857142855
19965,72e098fe5b2a04,e7531367,# roberta large no pt 4 layer 461,5399eebd,0.42857142857142855
19967,4ae6a182abac64,794af8e8,* **Passenger Class**,418676c5,0.42857142857142855
19971,53f302571cd4ac,088ae3a4,"## Polymorphism:
Polymorphism means having many forms i.e, one thing can take different forms.<br>
In OOP, it refers to the functions having the same names but carrying different functionalities.
#### Ways of Implementing Polymorphism:
- Duck Typing
- Operator Overloading
- Method Overloading
- Method Overriding",62c28443,0.42857142857142855
19975,3879ef16f5eb28,f3f375c8,# Bar plot,784b8d8e,0.42857142857142855
19976,ddbe4806ed061e,053b84a1,"<span style=""color: #2130b8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 28px"">EDA + Data Preprocessing</span>",a0d5fd77,0.42857142857142855
19977,60d500d196eb42,00cb9987,Now you're ready to read in the data and use the plotting functions to visualize the data.,2ad55f3f,0.42857142857142855
19979,99f84fa59cb1da,a156699d,## Stratified KFold Cross Validation,41e95f63,0.42857142857142855
19985,8985a124d4b657,7c417a30,"Here the number of features = 1 as we will be predicting a single value. Let's reshape the split sequences into the format of number of rows, number of columns. (shape[0], shape[1]). In the output, we can see that groups of 3 since n_steps = 3 have been obtained.",586d1846,0.42857142857142855
19986,55a5e31d03df9f,bfe7d939,"## <a name=""cnncreate"">Creating and compiling a CNN</a>",06dce00f,0.42857142857142855
19995,2730840089c8eb,87166476,"### Going between strings and lists: `.split()` and `.join()`

`str.split()` turns a string into a list of smaller strings, breaking on whitespace by default. This is super useful for taking you from one big string to a list of words.",34d27dac,0.42857142857142855
20000,be357c1e2c975d,51c457d5,### Task 3: Generate Training and Validation Batches,2486a061,0.42857142857142855
20003,e16860fce156b0,3a67edf7,"#When x and y are both categorical columns, it plots a nested bar chart, stacked bar chart and heat map:",2054f1ce,0.42857142857142855
20005,565ad413cd802f,0371c29d,### Data Loaders,397b074e,0.42857142857142855
20007,e69a496109e7d8,d30bc607,The above plot shows nearly 82% percent of survived people had **No.of.AxillaryNode** count <= 5 and gradually it decreases,1c640591,0.42857142857142855
20008,f1e162ddd14f11,d99a2ed3,let's separate the dependent and independent variable now,cdb2e771,0.42857142857142855
20011,b74076b2f8ba1d,82bc0216,Now you're ready to read in the data and use the plotting functions to visualize the data.,9ace22d4,0.42857142857142855
20016,b7298d6aaff625,aede341c,### Scaling Data ,bdf24bf7,0.42857142857142855
20019,b6e698d389d0d3,f7f6db78,# split data (train set and validation set),f02f68b5,0.42857142857142855
20020,adb8441ad28019,0e9a0b12,"<div class=""alert alert-success"">
    <h1 align='center'>Splitting up the data into training and test sets</h1>
</div>",d89de993,0.42857142857142855
20023,98fd05fcc5c3e3,ca6199cf,## Removing Unnecessary Columns ,55fe7ece,0.42857142857142855
20026,7454fdc444df16,0ed1056f,"### Insights
* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.
* Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.
",a7818ef5,0.42857142857142855
20028,6f4795cfdc96c7,dd3ee9cc,Now you're ready to read in the data and use the plotting functions to visualize the data.,1f3ab82f,0.42857142857142855
20035,867a9f977fa945,efc4627e,"# wordnet is divided into four total subnets such as

                 1 Noun
                 2 Verb
                 3 Adjective
                 4  Adverb
                
# woi= word of interest",2740fcca,0.42857142857142855
20036,c65f7b375af4ef,9a50c6cd,# Bir de Kullanıcı başına ortalmayı bulalım ,871d53ca,0.42857142857142855
20044,916ccf243827f1,668ad7e8,## 6. Initialise Adam parameters ,5147f4d2,0.42857142857142855
20045,6471597c5d2f66,a6b14ff7,Now you're ready to read in the data and use the plotting functions to visualize the data.,a41b4abe,0.42857142857142855
20050,324c699253abc2,7950f52e,Identifying the correlation between those variables,7e81e44e,0.42857142857142855
20052,47012add33109f,a9080052,"## Transfer Learning

Transfer learning is popular approach in deep learning where a model trained on a large dataset is reused for another classification task. We typically load a pre-trained model as a starting point instead of creating a model from scratch and retrain the model to adjust the weights for the specific dataset that we have.

The fastai library includes several pretrained models from torchvision, like resnet18, resnet34, resnet50, resnet101, resnet152, alexnet, etc. More details [here](https://fastai1.fast.ai/vision.models.html)

## ResNets

Residual networks are deep convolutional neural networks introduced by Microsoft paper [""Deep Residual Learning for Image Recognition""](https://arxiv.org/pdf/1512.03385.pdf). These CNNs use shortcut connections to skip one or more layers. ResNet addresses vanishing gradient problem which is a problem with gradient-based learning methods and backpropaga- tion where the gradient becomes very small after certain epochs, effectively preventing the weights from adjusting its value. ResNet50 has 50 layers and each resnet block is three layers deep.ResNet models are trained on ImageNet dataset

Here are the steps

- Load pre-trained resnet50
- Use 1 cycle policy and find learning rates. Detailed explanation is [here](https://iconof.com/1cycle-learning-rate-policy/) 
- Retrain with best learning rate
- Validate and Interpret results",b4c2e4e2,0.42857142857142855
20055,f35bf4df70d310,ac22c64c,## 3. Train the Multilayer Neural Network model,10bb859a,0.42857142857142855
20056,84d1ef55b89e17,4c6e2fb2,**Class Definitions** ,2d0b9d51,0.42857142857142855
20057,ffd1df95ca5289,5e125859,if a person talk more to a sales person at an average there is a high chance of him being converted,db00c338,0.42857142857142855
20061,8017d8ece39e95,f90dd829,# Train linear models,868ff74e,0.42857142857142855
20068,1fac5edd4063ba,10ffd399,![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSi-YpZqBs1SVbVKSNGUkyClQNWzHlpM4xd5Q&usqp=CAU)pinterest.com,04bc01e0,0.42857142857142855
20072,5f32117bcd5255,f7a3dea1,#### EXPOSURE INFORMATION,85882abf,0.42953020134228187
20073,726833f92fb87a,02ca4a28,## Contact vs campaign success,7dc5e1b6,0.42953020134228187
20078,fe7360cddc13e5,ab4acf75,O halde bunun tüm olasılıkları içeren olasılık yoğunluk fonksiyonu aşağıdaki şekilde yazılabilir.,8979e423,0.4298245614035088
20084,0caaec057f7184,8aa1c9b2,old items already lauched in shops,b875533e,0.43010752688172044
20089,593d1d3d1df05a,826efd05,# A function to seperate the characters,bc682ffe,0.4305555555555556
20090,c01049afb6d307,191b3c1b,"## Weight -- Bodymassindex
* The correlation of weight and Bmi is very high and close to direct proportion.
    * Because; BMİ = Weight/Height**2
    * We can reduce these three variables to one. Only BMI remains.",d37d3b5d,0.4305555555555556
20097,3c2033cc99c12c,f19a0e17,"**Findings:** *From the above distribution plot we could find that the fraud class time are more likely to lie in the scope of [2,9]*",dfa22a54,0.4306569343065693
20098,d07915a6e6992e,a923c126,**PassengerID**,2b912140,0.4307692307692308
20100,03048e86a6d806,47a0c247,It can be insightful to know how much an enterprise has spent their budget in Machine Learning/Cloud Computing Service depending on their current state of Machine Learning application.,1285c231,0.4307692307692308
20102,09751c520b0616,d5d7da56,- Age can't be negative,a4d0c7e9,0.4307692307692308
20107,a4f8ad33c823c5,4af734c2,"From the above heatmap,
* The variables gcs_eyes_apache,gcs_motor_apache and gcs_verbal_apache are strongly correlated with ventilated_apache.
* The variable intubated_apache is strongly correlated with the ventilated_apache.",fcd48307,0.4307692307692308
20108,a8c042af6b7245,1e5226bc,"* ps_car_03_cat and ps_car_05_cat have a large proportion of records with missing values. Remove these variables.

* For the other categorical variables with missing values, we can leave the missing value -1 as such.

* ps_reg_03 (continuous) has missing values for 18% of all records. Replace by the mean.

* ps_car_11 (ordinal) has only 5 records with misisng values. Replace by the mode.

* ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.

* ps_car_14 (continuous) has missing values for 7% of all records. Replace by the mean.",2487ac62,0.4307692307692308
20112,434f930cb58aee,38a0b46c,"We will define a class **Generators** to create train, test and validation generators. In this generator we will use the keras ImageDataGenerator class to preprocess and generating batches of data. As we are accessing the filenames of the images from a csv file, we will *flow_from_dataframe* method. Keras provide various such [Image data processing methods](https://keras.io/api/preprocessing/image/). ",0e1d3554,0.43103448275862066
20118,1dd9c6aa74d289,32cdc851,"For example, as an average climber who starts to climb outdoors, it takes about 1 year 3 months for men and 1 year 5 months for women to send their first outdoor 6a. From here to the first 7a, it will take an extra 1 year 1 month for men and 1 year 3 months for women of bouldering outdoor.",5ef9a1be,0.43103448275862066
20120,1750367e54f407,963b9712,I am using an EfficientNetB3 on top of which I add some outputs layers to predict our 5 disease classes. I decided to load the imagenet pretrained weights locally to keep the internet off (part of the requirements to submit a kernel to this competition).,a8e655b2,0.43103448275862066
20123,84127ade6fde87,0f61dfdf,"We have one-hot encoded our sentence into a representation that a neural network could digest. Word-level encoding can be done the same way by establishing a vocabu- lary and one-hot encoding sentences—sequences of words—along the rows of our tensor. Since a vocabulary has many words, this will produce very wide encoded vectors, which may not be practical. We will see in the next section that there is a more efficient way to represent text at the word level, using embeddings. For now, let’s stick with one-hot encodings and see what happens.",f55d05b6,0.43103448275862066
20128,fdbbd573ba31c2,824112a4,### turbine_status,f7c28d74,0.43125
20129,917957c6c4065f,84b3f29d,"수치형 변수의 분포는 전체적으로 오른쪽으로 꼬리가 긴 형태입니다.  
극단적인 케이스의 영향을 많이 받았다고 할 수 있습니다.",55b8ed68,0.43137254901960786
20130,d0080e3a39bc5c,93d50dd3,![Transformation 3](https://i.imgur.com/FQ9Zuej.jpg),2fcde4cf,0.43137254901960786
20133,fa02c409161192,aecffa46,"As seen above, the input has size $28\cdot28\cdot1 = 784$ and the output is has size $10$. Therefore we know what the external structure of our NN should be. Let's define out NN.",e97077f7,0.43137254901960786
20136,7cfd96218dd933,8a17af0f,"#### **ATTENTION**
* THE STRONGEST BRIGHTNESS VALUES WERE SEEN IN TURKEY (MEDITERRANEAN FIELD) IN THE LAST 11 DAYS.
* THIS MAY BE AN INDICATOR THAT THERE WAS NO ANOMALY IN THIS REGION.

#### **ATTENTION**

* ANTALYA HAS MORE HOTSPOT RECORDED THAN OTHER POINTS.
* ADANA AND MERSİN HAVE ALMOST SAME NUMBER WITH THE ISLAND OF ITALY.

#### **ATTENTION**
* USE THE INTERACTIVE MAP. CLOSE TO THE REGIONS.
* THE HIGHEST NUMBER CAN SHOW WHERE THE FIRE STARTED FIRST IN THAT AREA.",7c34d96c,0.43137254901960786
20141,52cfd66e9ec908,85e6d575,But are we sure this is enough? Enough knowledge to satisfy us? No it's not. I will be using Kkiller's dataset for further tabular data exploration from henceforth.,c74adcdf,0.43137254901960786
20143,2ada0305b68956,7024a53f,### 72. Palette = 'YlOrBr_r',133e26f4,0.43142857142857144
20145,738bfced935b69,21c61413,"We filtered data nine columns in 4 groups by tax column:
* 1. by tax = 0
* 2. by tax between (1, 125)
* 3. by tax between (126, 145)
* 4. by tax between (146, 580)
",2d3c592d,0.4315068493150685
20146,fdc9f4863744b1,8200ebdd,"Standardization or normalization is not required in this case. I corrected the data types, removed duplicate data and there are no missing data values. I can further start analyszing the data set. Woot!",b4529365,0.4315068493150685
20149,f91f58d488d4af,98da03e9,"Now, let's write a function `is_3` to tell whether a number is 3 or not!",5df1bbf3,0.43157894736842106
20152,be2f4d8a6b73ca,b0ac2aac,"<div class=""alert alert-block alert-info"" style=""text-align:center""> 📌<b>Insights: </b>Sex: Male is having more heart disease compared to female<br>Chest pain type ASY: Asymptomatic having more heart disease compared to others<br>exercise-induced angina: yes and ST_Slope: Flat is having more compared to others<br>And peoples with ST_Slope: up, ExerciseAngina: No and chest pain type: ATA (ATypical angina) mostly don't have heart disease</div>",5d8ce40a,0.4318181818181818
20161,a0b321057e7402,c46b73c7,"# **Stationarity**

**What is stationarity?** Stationarity in (plain English) means that the statistical properties of a ceratin variable do not change over time.

**Why you do it?** It simplifies the whole analytics process and allows for a structured approach to the problem.

**Do I need it for SERIMAX?** Not really. Statsmodels SARIMAX has a (by default enabled option) that enforces stationarity. However, it is a great tool to analyse the data and should be used.

Its common practice to use it and most models nowadays assume that the data is stationary. To determine the stationarity of data I am going to use the Dickey-Fuller test. The DF tests a null hypothesis that a unit root is present in an autoregressive model. If the value is less then <0.05 then the data is stationary.",5f73fb91,0.4318181818181818
20167,da199f8fb59439,7ae19abb,> *Most of the ratings are given by TV-MA and TV-14 for movies and tv-series.*,baaa665d,0.4318181818181818
20175,b7b1057764fa02,be731f07,"We see that the evaluation images do not, in fact, look at all similar to the training images. They have different hues and vastly different backgrounds. Indeed they are a much better embodiment of 'real-world data'. Therefore these will make a good test for our model's performance on 'real-world' ASL images.

**Note**: I did not use any sort on the evalutation arrays before printing. This is because the evaluation arrays did not go through a `train-test-split`, and therefore their sorting was never disturbed.

# 4. Preprocessing: One-hot enconding the data

I briefly covered label encoding earlier, where each letter of the labels is associated with a number:

`A` is encoded as `0`
`B` is encoded as `1`
`C` is encoded as `2`
`D` is encoded as `3`
 ...
 `nothing` is encoded as `28`
 
So, currently, our labels for each of the letters are encoded as categorical integers, where `'A', 'B' and 'C'` are encoded as `0, 1, and 2`, respectively. However, `keras` models do not accept labels in this format, and we must first one-hot encode the labels before supplying them to a `keras` model.

This conversion will turn the one-dimensional array of labels into a two-dimensional array. Each row in the two-dimensional array of one-hot encoded labels corresponds to a different label. The row has a `1` in the column that corresponds to the correct label, and 0 elsewhere.

For instance,

*    `0` is encoded as `[1, 0, 0]`,
*    `1` is encoded as `[0, 1, 0]`, and
*    `2` is encoded as `[0, 0, 1]`.

The image below summarizes these concepts:

<img src=""https://i.imgur.com/CtSIsMP.jpg"" alt='Label encoding and One-hot encoding' style=""width: 800px;""/>

One-hot encoding is easy to do using `keras`:",5053a192,0.43243243243243246
20176,297cbe4a23c4bf,c8bbd2f6,# Dropping Less Important Features,a843e619,0.43243243243243246
20179,d76896b30cebd3,ebcb4625,of just 20 categories,1b4e8f34,0.43243243243243246
20181,63b44c85e32c1f,687e637d,**index( )** is used to find the index value of a particular element. Note that if there are multiple elements of the same value then the first index value of that element is returned.,fb9b9562,0.43243243243243246
20182,a6c34cd514e30e,26645251,### Let's check 2nd file: ../input/listings_summary.csv,bf603ddd,0.43243243243243246
20183,2dda7facf3c1e0,8a07698c,"### Augment the data for better results and training

We are going to do the following things in this section:

1) Adding or removing notes during training

2) Removing random notes from a target piano roll to create input piano rolls

3) Adding random notes to the target piano roll to create input piano rolls 
",45552d2b,0.43243243243243246
20186,bdf23d2d396916,5ab96894,# MODEL SELECTİON,b0e45a49,0.43243243243243246
20191,56785caebaa256,9c214855,"## 4. EDA<a class=""anchor"" id=""4""></a>

[Back to Table of Contents](#0.1)",a792961a,0.4326241134751773
20193,b61ab8f81dc03d,9a4393ce,"<a id=""mapping""></a>
## Mapping",64d05394,0.4326241134751773
20195,99bf357eaf61f1,445822a9,#### Box plot - OverallQual,9d92fafe,0.4326923076923077
20197,5ce12be6e7b90e,07faae0e,"We use `while` loops to do something again and again, as long as a condition is met.  

![while](http://www.tutorialspoint.com/images/python_while_loop.jpg)

The syntax is very similar to that of `if` statement.

Let's count how many times it takes to get a random number greater than 90. ",c0ab62dd,0.4327485380116959
20198,30fdc4a6e3c1db,bf0fc958,Let's look at the unique days at which the 10 SNAP days of a month exists over the years,6111ddee,0.4327485380116959
20208,8ec771f5600a61,4811ff95,# Analysing the missing value,48364c1f,0.4329896907216495
20211,892be0a523578c,7fe7d88a,"## weightLogInfo_merged.csv
* Only 8 participants have data, and only 2 of them have a record more then 20 days",b0e8d7c0,0.43333333333333335
20212,864302b10e7730,4496dc9d,"# To see the distributions, we plot the *histogram*, *kde* (kernel distribution evaluation) and *Jointplot*",e9dd1d2d,0.43333333333333335
20214,be616f0785c32d,3f5d265a,"[Go Top](#top)


###### Code for extracting chemical features 2",b78e18aa,0.43333333333333335
20215,e78f177ca86768,d4562719,## Tuning learning rate,120e25c1,0.43333333333333335
20216,bc058fe14d3d1b,25d6104c,season,d0273670,0.43333333333333335
20220,62487bcd70b199,01aecfb5,# <a id='5'>5. Data Preprocessing</a>,f6ae50af,0.43333333333333335
20223,07f5853e4db8f8,9368f0cf,"# Engagment dataset information

The engagement data are aggregated at school district level, and each file in the folder `engagement_data` represents data from one school district. The 4-digit file name represents `district_id` which can be used to link to district information in `district_info.csv`. The `lp_id` can be used to link to product information in `product_info.csv`.

| Name | Description |
| :--- | :----------- |
| time | date in ""YYYY-MM-DD"" |
| lp_id | The unique identifier of the product |
| pct_access | Percentage of students in the district have at least one page-load event of a given product and on a given day |
| engagement_index | Total page-load events per one thousand students of a given product and on a given day |
",d13c2c32,0.43333333333333335
20226,b10bd75889dad9,2d182326,#### Outlier Treatment,ee00ceee,0.43333333333333335
20229,37b09262279764,c502af49,"<b>Label Encoder replaced 'C' with 0, 'Q' with 1 and 'S' with 2</b><br>",37c4c417,0.43333333333333335
20232,9276fa5cc2fef6,759d18b4,"Now, let us create bulk of these common features; 
",24aa6a52,0.43333333333333335
20235,b547f0f38f7744,a171e3a7,Show images without `bboxes`:,b6ba66b3,0.43333333333333335
20240,061d6757dfbce0,9b30eee6,"### Examine the Distribution of the Target Column

The target is `meter_reading` - Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.

Meter readings are for differnt meters, read as {0: electricity, 1: chilledwater, 2: steam, hotwater: 3}. Not every building has all meter types.",c0c2915a,0.43333333333333335
20244,ac1abfe1dfe815,02400102,"Cheking if the class of a tweet and a day it was written in it are dependent, using `Chi-square test`  
p value was much less than 0.05, which mean the day and sentiment are related, which makes sense.",6529dbcb,0.4336283185840708
20247,4daf6153275cbf,202ca2d1,"With %58, Italy has the most local restaurants.",51db1961,0.43373493975903615
20249,c65a65d4041018,bc353cdc,"Most countries have similar patterns, but America has more experiences DS. This is understandable as DS popularity began there.",824fb229,0.4338235294117647
20255,614ba9f0c62677,12ef2c88,"<a id=""4""></a>
## Convolutional Neural Network 
* CNN is used for image classification, object detection 
* <a href=""https://ibb.co/kV1j9p""><img src=""https://preview.ibb.co/nRkBpp/gec2.jpg"" alt=""gec2"" border=""0""></a>",b8551335,0.4339622641509434
20256,0ad8d416b89b78,9054bf93,The first step in this process is to convert the Income attribute the target attribute into 0 and 1 for implementation in model training.,0b0562f0,0.4339622641509434
20265,979f1e99f1b309,ef2aa43b,***THE plot show that the player who finished the game in advanced places walk more than other which is normal***,d1bfebbf,0.4344262295081967
20267,4913b61a68d355,100e1fd8,# CNN Model,6e269c6a,0.43478260869565216
20268,0e2a23fbe41ca9,aeba7566,"Observations:
- Ratio of merchants with N to Y is 3.4 for category_2 as 1
- Ratio of merchants with N to Y is 4.2 for category_2 as 5
- All others revolve around 1.5",64e4762c,0.43478260869565216
20274,b01ee6cb674fa3,a9adad6c,"# SpaceX
A private company from USA",a8ffd35e,0.43478260869565216
20275,fe118026267a88,4475b8f8,"## 3.

Create a variable `ingredients` with a `pd.Series` that looks like:

```
Flour     4 cups
Milk       1 cup
Eggs     2 large
Spam       1 can
Name: Dinner, dtype: object
```",612efa48,0.43478260869565216
20276,ea17e798ab2d9d,9830a7b4,![https://www.google.com/search?q=sigmoid+function+images&rlz=1CAIGZW_enGB893&sxsrf=ALeKk03MAXsU-SaQYnjt-zRrvikVaVbYgg:1597652629260&tbm=isch&source=iu&ictx=1&fir=VjglH3kRCVECZM%252CavewwmVOe63F1M%252C_&vet=1&usg=AI4_-kT93sbNoWWf-AveFyir-RNKD7H7LA&sa=X&ved=2ahUKEwik0MPr56HrAhWGOcAKHcjGA6UQ9QEwAHoECAoQJg&biw=1300&bih=572#imgrc=VjglH3kRCVECZM](http://),27dd8e53,0.43478260869565216
20284,f6c1eb62cceb70,083cce58,Now let's choose the features,90a1b790,0.43478260869565216
20287,3319c5c562f607,fdb37316,"# Text cleanings
* Lower case
* Remove Puncuation
* Remove Frequents Words
* Remove Rare Words
* Stemmer
* Lemmatization
* Removing stop words
* Remove URLS
* Expanding the deontracted words ",f298250a,0.43478260869565216
20289,ea4e559a86d613,bc81dd2c,**Relationship between target variable and gender column**,eff47843,0.43478260869565216
20290,9d9da6c439b96b,13e2f6cb,"From the heatmap, NA_Sales and EU_Sales has strong correlation with Global_Sales, it may show that these two columns strongly influance global sales",361cc7d9,0.43478260869565216
20293,90ead00a8ee283,d4feb045,"**Exercise 3**: Create a `Series` that looks like this:

```
Flour     4 cups
Milk       1 cup
Eggs     2 large
Spam       1 can
Name: Dinner, dtype: object
```",612efa48,0.43478260869565216
20305,ab6da5994949a3,cf1d31f2,"# Logistic Regression Model
## Fitting Logistic Regression model to the Training set",fae6b91d,0.4351851851851852
20306,fdc3afd309b850,940e5c2f,"<a id=""NF""></a>
# 7 New Features",966bde38,0.4351851851851852
20309,869a39a3d4dea2,11b5057a,"## Image Arithmetic <a id=""image_arithmetic""></a>",9020daf8,0.43529411764705883
20313,9ceb7278784462,39cdb5b5, ## <a id='16'> 12.Linear Regression</a>,3768a567,0.43548387096774194
20315,ad26c020235dfc,24988c8e,Fill missing values with column mean value:,bf766e48,0.43548387096774194
20317,4d91e84c564cbe,012ee917,We've previously used the `min` and `max` to get the minimum or maximum of several arguments. But we can also pass in a single list argument.,355a43e3,0.4358974358974359
20320,163ceeb80d6923,9fb00ddf,## Train,4adfbb90,0.4358974358974359
20323,34fff8ce731b03,fa3af88c,"O nosso *dataset* contém os dados de treinamento e de teste do nosso modelo. Aqui os dados de teste são aqueles que **não** possuem rótulo e serão utilizados na solução final. Dentro dos dados de treinamento (""defaulting is not null"") vamos dividir nosso *dataset* entre dados de treinamento do modelo e dados de validação, sendo 80% para o primeiro conjunto e 20% para o segundo. Queremos predizer o valor da coluna *defaulting*, mas ela é do tipo boolean e deve ser transformada para o tipo inteiro para o nosso algoritmo de aprendizado de máquina (Árvore de Decisão) conseguir fazer a classificação.",6f9e5b2e,0.4358974358974359
20324,897ca904b74a98,82a539a8,## Sub variable categories creation ,c5844ad4,0.4358974358974359
20328,0a1fcda859252c,f36876d9,"### Training data generator 
Here I will define a very simple data generator. You can do more than this if you want but I think at this point, this is more than enough I need.",13a38774,0.4358974358974359
20333,d4c5aaa4b36810,7bda2565,We don't lose too many countries if we just drop the ones with missing data so lets do that. ,65441f28,0.4358974358974359
20336,ba4b3bd184acbb,85bca632,"Now that we solved that issue, let's retry the `floatTest` to see if there are other non-float values in the Price column.",0f5de724,0.43609022556390975
20338,4c47839b067546,4b024cb0,"Наивная модель дала результат 127 . Это очень слабый результат, с ним будем сравнивать результаты других моделей. ",1f517b02,0.43617021276595747
20343,5a8c553e21c70f,5f24aa32,## Minority Class Upsample with SMOTE,9ebd9d8f,0.43636363636363634
20362,2ada0305b68956,ccee743f,### 73. Palette = 'YlOrRd',133e26f4,0.43714285714285717
20366,bd0e173abb7b52,c9812cf2,"**6. Is it true that people who earn more than 50K have at least high school education? (*education – Bachelors, Prof-school, Assoc-acdm, Assoc-voc, Masters* or *Doctorate* feature)**",9bce3b0d,0.4375
20367,c5fef7cc592736,e9e4915e,We add the test dataset for inference. All the transforms applied to the valid dataset will be applied to the test dataset,d21dc2c1,0.4375
20368,386c42a7fb27a4,c94eb763,## Sandwiches,9e9f6974,0.4375
20370,51a46d0a7597f5,ee06da89,"<br/>
<A name=""section1.3"">3. Can we predict the Value of a player based on its attributes (like accuracy, shot power, reactions, dribbling etc)?</A>",e9e25b17,0.4375
20372,7a75cba9317186,94c2e5f2,"* %time: Time the execution of a single statement
* %timeit: Time repeated execution of a single statement for more accuracy
* %prun: Run code with the profiler
* %lprun: Run code with the line-by-line profiler
* %memit: Measure the memory use of a single statement
* %mprun: Run code with the line-by-line memory profiler",3d7e3235,0.4375
20376,96c4c0e36b8ec0,3cde0d31,"What the graph tells us:
* On average, women on the Titanic were younger.
* The oldest man on the titanic was 80",4dd6de8c,0.4375
20377,2a377ced98d67a,3d3c1bfe,### 3.3. Lineplot for showing relation between all features and target,262231a8,0.4375
20378,3dd4294f903768,f0edec9b,"First, we have to change the cost column. Let's look at how many currencies we have.",0d89d098,0.4375
20379,eb0854a6601407,64ce4be3,"time_id: The ID code for the time the data was gathered. The time IDs are in order, but the real time between the time IDs is not constant and will likely be shorter for the final private test set than in the training set.

Yes the IDs are in order from 0-1219 with 8 missing (?) time_ids.

One time id may belong to 1st Jan 2:00 IST, the next one can be 4th Jan 12:00 IST, the other one 5th Jan 16:00 IST and so on.

Clearly the number of data points (rows) in each time_id is not constant.

The following time_ids are not present. I don't think it should be an issue since we anyway don't have a constant gap between consecutive time_ids.",6d107747,0.4375
20380,49f2274c1dd516,dadedff4,"# Summary of Datasets Available
This section will provide a description of each of the datasets in the UNCOVER data set as well as their features.",06b0ffee,0.4375
20382,254cccd5145725,c26ada3d,1. From the above information we can see that the dependancy column has yes and no values.  For this we map the 1's to yes and 0's to no. ,a49b4037,0.4375
20387,c85c94076e9c3a,cce1f104,### - Reducing columns,3ea0c443,0.4375
20393,28a1ff0f223da9,3dc7fd68,"The number of persons who mentioned data science, machine learning and artificial intelligence specifically as thier area of interest are as follows:
* Data Science: 1
* Machine Learning: 53
* Artificial Intelligence: 34",c945b27d,0.4375
20394,3b5903412fe741,bfada3a1,"Finally, it's worth knowing that negative numbers can be used in selection. This will start counting forwards from the _end_ of the values. So for example here are the last five elements of the dataset.",ad231969,0.4375
20402,f5ca8fb6a465f3,c03e34f8,# Inference,56c45a1b,0.4375
20403,e82462cdc998a7,ebd92901,"### 4.5 CV folds<a class=""anchor"" id=""4.5""></a>

[Back to Table of Contents](#0.1)",b39bf244,0.4375
20405,9ec2fb131cf677,6ec93143,# TED TALKS BY COMMENTs,211ea6bd,0.4375
20406,9daf8b4a46725e,74eb8ff2,### Data Visualization,7d9cc411,0.4375
20409,08e3444f9eddcf,ee8f4220,# Training model,1d9d4f73,0.4375
20414,436ceac778d184,17cfb885,"# <sub>2.</sub> <span style='color:#F7765E'><sub>MODEL GENERATION</sub></span>

<b>Base Model</b>

- The same model is used from [notebook](https://www.kaggle.com/stefankahl/birdclef2021-model-training), with the exception of a three layer <b>input shape (X,X,3)</b>

<b>Pretrained Models</b>

- Pretrained Models all require 3 layer inputs, in the input shape.
- Pretrained Models are also provided in the function, <code>pretrained_model</code>, which requires one to specify which <b>head model</b> is chosen. 
- The <b>tail end</b> Dense Layer is also fixed, by no means optimal and adjusted to be used for classification in this problem.
- <b>head weight coefficients</b> are often fixed to prevent overfitting, the same is done here.

<b>The Rest</b>

- Compilation settings, <b>optimiser</b>, <b>loss function</b> & <b>evaluation metric</b> are all identical to the previous notebook.
- <b>Callbacks</b> are all quite standard, <b>TqdmCallback</b> is used to reduce keras training output.
- <b>Train & Validation Generators</b> are used for training and evaluation during training, defined earlier.
- <b>Results</b> of the <b>evaluation metric (accuracy)</b> & <b>model loss</b> are plotted for each iteration of image dataset passes (epoch).",db256ccd,0.4375
20417,6f05f4ea9addbf,a5c956e3,we have ***successfully*** cleared all missing data from our dataset,dfb04c84,0.4375
20418,fae5023faa435f,d26718a0,Historical data for all the banks under Bank Nifty (NSEBANK),b37c893b,0.4375
20423,3c2033cc99c12c,76f526e7,## Dimension Reduction ,dfa22a54,0.43795620437956206
20425,2f47abddfd1928,6b88d9f5,"We can clearly see that socio-economic position matters in order to survive.

1st class shows higher percentage of survived passenger, 2nd class is balanced but 3rd class shows a big difference being quite likely to die if you where in 3rd class.",ae33cc0b,0.4380165289256198
20428,7454fdc444df16,3435accf,"### Repatching the Actual Breast Tissue Image
Now it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.",a7818ef5,0.4380952380952381
20433,e67925694c07d3,e4e8327b,### Trainset correlation,83af4c4a,0.43820224719101125
20437,596389bed473be,d260b042,# Conditional Selectors,5f8af156,0.4383561643835616
20439,fdc9f4863744b1,a3e6a4db,## Data Exploration,b4529365,0.4383561643835616
20443,a4f8ad33c823c5,19489844,"## Handling the missing weight, height values

The mean height of the females and males was calculated respectively. Whereever the height of the females was missing, its value was replaced with the mean height of the females. The same logic applied to the height and weight of the males and females. In cases in which gender details were not available, the values were replaced with the mean height and weight of all the patients.",fcd48307,0.43846153846153846
20449,c3498779cda661,5f8b088f,# Parte 2: Clasificación de Carros con Agrupación Jerárquica (Hierarchical Clustering),0f531b65,0.43859649122807015
20451,6cade0b6a41ba2,61a9a945,##### This data represent a bit like categorical nominal variable. Hence we will keep them as it is.,e6110293,0.43859649122807015
20454,3fb15e6e48aec2,52252c09,"# Option: Can analyse the Ticket column futher
* cleanup potential spelling issues
* Seperate Ticket prefix futher on forward slash",9d1f4358,0.43859649122807015
20455,9e27af2600925c,2dbcedcb,"### 4.3 - Forward and Backward propagation

Now that your parameters are initialized, you can do the ""forward"" and ""backward"" propagation steps for learning the parameters.

**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.

**Hints**:

Forward Propagation:
- You get X
- You compute $A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$
- You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$

Here are the two formulas you will be using: 

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$",9b556435,0.43859649122807015
20469,e19e307b3fd188,0f874efa,The fact that the house **is furnished** increases **the rent amount**,2173955b,0.43902439024390244
20470,62582b8036fbfe,287027ad,##### Remove Stopwords,6c2160db,0.43902439024390244
20471,8d70dcae7f40a3,07c645ea,### **Training with defaults parameter**,472c71ce,0.43902439024390244
20476,47b2c9be5e31cb,48a58176,Scatter and density plots:,7d4afe56,0.43902439024390244
20478,9c26c5dcd46a25,a39eadb5,"### <font color=""#ea1c60"" id=""section_2"">2. Analyses multivariées : Régression linéaire multivariée.</font>

Quand une variable cible est le fruit de la corrélation de plusieurs variables prédictives, on parle de **Multivariate Regression** pour faire des prédictions. Toutes ces variables prédictives seront utilisées dans notre modèle de régression linéaire multivariée pour trouver une **fonction prédictive** du type :

$$\large F(X) = \epsilon + \alpha x_1 + \beta x_2 + \gamma x_3 + ... + \omega x_n$$

où :
- $\large \epsilon$ est une constante,
- $\large \alpha , \beta , \gamma$ représentent les coefficients de notre fonction prédictive $\large F(X)$,
- $\large X$ est un vecteur de variables prédictives.

Dans un premier temps, nous allons considérer comme **variables prédictives uniquement l'ensemble des variables numériques**. Nous étenderons par la suite le modèle aux variables catégorielles. ",1bbbb677,0.43902439024390244
20479,0e09587faffa8f,5c35caa3,"Amongst the different registered vehicle plate IDs, the maximum number of tickets were issued to **no-plate-id (blank plates)**",0d563d61,0.43902439024390244
20481,c84925c8171900,20deb978,"<a id=""yearwise""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.2 Year Wise Analysis
            </span>   
        </font>    
</h3>",e21ff7ec,0.4392523364485981
20482,a2444ab5d5f147,88651997,"### nltk provides us with 179 unique stopwords, we will use this set to filter our `CleanPhrase`",10617755,0.4393939393939394
20485,5f4ae633cfd090,1da3fbf4,"According to the official UFC site, 'kd' refers to knockdown, so these could be knockdowns dealt to/by the blue player",a30a16e2,0.43956043956043955
20486,f3c6048d1058e3,193d645e,"- This model gives us accuracy of 69%. This model is not able to perform the best way possible as negative reviews contain any negative words. We know, after assigning individual scores to all the words, final sentiment is calculated by some pooling operation like taking an average of all the sentiments",1d9056b0,0.4396551724137931
20487,b61ab8f81dc03d,9d5a7ac4,To solve the Cabin problem of missing data and transformation you have to understand better the data,64d05394,0.4397163120567376
20488,56785caebaa256,b505b39a,"## 4.1. Plots - Confirmed cases over time<a class=""anchor"" id=""4.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.4397163120567376
20489,957e035ba5b9d5,08b25d4d,## Evaluate test data,778ab3d3,0.4397163120567376
20490,fdc3afd309b850,c6066dc4,"Enriching our features to get a better result we will add a few new features. 
 * Geo Location, using the GeoPy library to plot the apartments on the map and calculate the Distance to the Brasília Downtown
 * Also, we will Web Scraping on Wikipedia to find Per Capita Income (PCI) and Population of each AR.
",966bde38,0.4398148148148148
20495,83df814455f06c,7ad9ded3,"# **11. Split data into separate training and test set** <a class=""anchor"" id=""11""></a>

[Table of Contents](#0.1)",c9cff71a,0.44
20499,7e1da639035ac5,c2f75feb,# <a id='9'>9. Rigorous instruction analysis</a>,120b6c23,0.44
20501,cee088a6840708,fbfa11b1,"# Step 6 -> Extract the graph from the data and put it into a DGL graph 

DGL graph is just a simple graph, but it works well with tensorflow.",55463e1c,0.44
20503,4cd25e50c7e007,37910825,"# Dummy Variables
##### Creating the dummy variables",ceb0c525,0.44
20505,91eaec994e0c6f,73345816,<b> It was Christmas effect !</b>,376aef10,0.44
20506,0687cd5c8597db,39c21176,## **Building a Convolution Neural Network**,4edec76a,0.44
20507,10c5a39a87c47e,06a3c2ad,## Step 8: Model Building: CNN<a id='step-8'></a>,09c7337a,0.44
20508,274b32da3b19a8,045abff7,## Helper Function,408f7268,0.44
20515,5cb7f999fd1ecb,ab4cb4e9,"# Data Cleaning
### Removed unnecessary",88b54f70,0.44
20516,bbad077c274022,f7d70239,"*It is clearly visible that I walk more in  the mornings and the evenings. To be specific, I walk more in the mornings for about 2 hours (7 AM to 9 AM) and about 3 hours in the evenings (7 PM to 10 PM).*",3c2e3dea,0.44
20519,cb570c7b7f0501,2b2496d9,"we hardly can say that MEN are more committed than Women.
it's almost the same ratio.",a200a0ec,0.44
20526,caaa6793391520,b0afe532,"In the multinomial distribution there is no single parameter responsible for standard deviation. However we can observe, that scaling the standard deviation of the normal distribution is equivalent to scaling `x`. If we do a similar transformation in the multinomial distribution, this would be equivalent to raising the parameters to the power of $\frac{1}{s}$, where $s$ is the scaling factor",1e79f342,0.44
20530,87e94f864d74be,f8f13c6b,### Let's check the clean data,294bfe9f,0.44047619047619047
20533,1294fb4c86f993,1d9d3e26,"<a id='eda'></a>
## Exploratory Data Analysis


<img src=""Images/EDA.png"" align=""right"" width=""500"" height=""500"" /><br>
### Research Question 1 : Which states have had the highest growth in gun registrations?",4471e513,0.4406779661016949
20536,a81661cc35d8d2,0bd73127,***,3331f113,0.4406779661016949
20542,2a56d6b0e153f2,44f28872,"HERE, MARKETING & HR ARE LIKELY TO PLACED WITH SALARY 3L PER ANNUM , IT IS FOLLOWED BY MARKETING & FINANCE",8dc315e6,0.4406779661016949
20543,dac3c8204a2d1b,0230fc9a,"As it can be seen from the above line plot, almost every year, non-fiction books dominated the Amazon Bestsellers over fiction books",b0d2d0dc,0.4406779661016949
20544,b660910fcc2954,cf9fa35e,"## Scatter plot of continous features (train/test)


We will use scatter plot of train and test features, feature by feature.",80b74f88,0.4406779661016949
20545,9169c4e9c33c90,f7c2108e,Dr. Seuss appears a total of 9 times. Which books of his made the Top 50 over the years?,725bf880,0.4406779661016949
20549,21bce4ec54b3fa,b250e623,"# The benchmark

Typical framework for such experiments is building the simplest solution first and then trying different techniques to improve it.
Let's use the usual Random Forest classifier, which is the go-to choice for prototyping, idea testing and experimentation.
We'll fit it on all features to get the benchmark and then try to reduce the number of features while maintaining as high accuracy as possible.
Besides accuracy score, let's keep track of prediction time - a pretend scenario for when real time speed is important, this will add more motivation for dimensionality reduction.
Remember, simple = good :)",35546e30,0.4411764705882353
20552,ab657da5329e3f,d362fb92,# Dataset visualizations,021526f8,0.4411764705882353
20558,a0a5baa6c7e12a,0f5fe933,"Although many numeric features in this dataset are often categories with numeric discrete values, looking at the Pearson correlation between such variables can still bring some more insights on highly associated/'correlated' features. From that standpoint, we observe that

- *'Wilderness_Area1'* and *'Wilderness_Area3'* are highgly correlated, and one of such features can be removed from the training set withough compromising the model accuracy",551d41de,0.4411764705882353
20559,842547b2def18c,c0a60f47,Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations.,b8efde6d,0.4411764705882353
20561,02b7e38902069e,5b318eff,"<h1><span class=""label label-default"" style=""background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px"">Indic NLP Library</span></h1><br>",726a03a0,0.4411764705882353
20562,99821bc6a45be6,168d4c31,# Linear Model:,b9d59346,0.4411764705882353
20569,7f74a04ae75792,1d56fcde,### Before handling missing values,d01e91da,0.4411764705882353
20572,c65a65d4041018,4c686385,### Years of learning ML vs self-confidence,824fb229,0.4411764705882353
20574,4c47839b067546,7fc7080f,"### Рассмотрим каждый признак и, по возможности, заменим категориальный признак на числовой или бинарный",1f517b02,0.44148936170212766
20575,c13f73168789c2,46745133,"## 3. Selecting top n largest values of given column<a id='16'></a>
Syntax : `df.nlargest(n, 'column_name')`",16175052,0.44155844155844154
20577,722cd844dfbe8f,71967f6e,"It can be seen that the majority of the images are completely black.

# <span style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"" id=""section_2"">Preprocessing</span>
We will be using several preprocessing techniques on our images.
- Crop images to reduce black areas.
- Resize images.
- Application of a denoising filter.

**We will not apply image equalization** as the different types of scans already have voluntary contrast variations.

## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_2_1"">Crop and resize the images</span>",0cedb385,0.44155844155844154
20578,2cb457b60dd246,d391c72b,### Set Up the Generators,339367df,0.44155844155844154
20580,4ae464582bac51,938646a4,"**1)** Most people who have had a stroke work in the private sector, because it is where they most employ them.

**2)** Next comes self-employed workers",ca6a52ce,0.44155844155844154
20584,62487bcd70b199,f851efc7,## <a id='5.1.'>5.1. Data Split into Test and Train</a>,f6ae50af,0.44166666666666665
20585,743ae010f5e875,b136000b,# Masked Dataset Modeling,02c54445,0.4418604651162791
20589,22bd95f4807a23,c738b5ae,A majority of individuals are between 40 and 50 years old.,c05d356f,0.4418604651162791
20592,d96642860ab3dd,d00530c0,### 2.1 Handaling Missing Value ,98419d48,0.4418604651162791
20594,1660daf8867980,dc162e54,Best action value for each state:,42d7cffc,0.4418604651162791
20596,fc8e0042411c46,f06c3fda,"- Most of the lead have their Email opened as their last activity.
- Conversion rate for leads with last activity as SMS Sent is almost 62%.",af476c2a,0.44200626959247646
20598,b01ee6cb674fa3,4c570ec1,"# CASC - China Aerospace Science and Technology Corporation

public owned chinese agency
",a8ffd35e,0.4420289855072464
20601,840534f2908a9c,afee0f3b,"*In the scatter plot, we saw the high density of pickups and dropoffs from and to JFK and La Guardia Airport.
Let us look at over time how fares are from La Guardia and JFK*",8081c3cc,0.4421052631578947
20608,0fc0cbf884acd6,21ab203a,# XG Boost on Store by Month,064949f1,0.4423076923076923
20612,a5a419dc7245b0,a98aec68,#### Checking for Correlated Columns,4279726e,0.4424778761061947
20619,0858e1bb3cbaca,c15cdc79,This can also be tailored to specific products by combining **.loc[]** and **.sum()** ,78548374,0.4426229508196721
20621,ee23a565163388,9a0e4006,"**Inference**
- The serum creatinine level in the patients who suffered heart failure is more than the others.",88aacbc4,0.44274809160305345
20623,9c044fa3072552,e5483518,### Start Time,1362842e,0.44285714285714284
20628,38b79494ac749e,4497758b,### Cross-validation,39162a40,0.44285714285714284
20629,2ada0305b68956,18bf399d,### 74. Palette = 'YlOrRd_r',133e26f4,0.44285714285714284
20630,fe6750354fb64f,43df515b,--------------------------------,271741f0,0.44285714285714284
20631,726833f92fb87a,7c0a270a,**We cannot draw any particular conclusion.**,7dc5e1b6,0.4429530201342282
20632,5f32117bcd5255,a68de382,#### TARGET ACQUISITION PARAMETERS,85882abf,0.4429530201342282
20635,d1ff7e10ee0102,28fae202,"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.

One of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).

The plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).

Ok, enough of Rorschach test for now. Let's move forward to what's missing: missing data!",2cc71c3c,0.4431818181818182
20636,73d8e56bc709b1,1c6faa3e,"Then let's see age distribution of some countries.
# # 1. Spain",78ec3cce,0.4431818181818182
20644,510b8303776bb6,0f1bd59a,### Removing outliers from continuous data fields,18080db8,0.44339622641509435
20648,fdbbd573ba31c2,f8c2a408,### cloud_level,f7c28d74,0.44375
20652,4fd4b6a80d40e3,52ad21da,![image.png](attachment:image.png),f6913cc3,0.4444444444444444
20657,c73e07ad6d25c5,8f222071,### Embarked,3ab391fb,0.4444444444444444
20658,dd3721cb49c1fd,8629010d,"<a id='6'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">6. <b>Performing Grid Search for hyper-parameter optimization</b></h1>
  </div>
</div>",1a53fdd9,0.4444444444444444
20659,d77e6d61ad2e8b,e4e23f31,## Decision Boundary Plot,03fd0e96,0.4444444444444444
20663,4b4117cf42ef8d,733b4f9a,# Regularization,457cd6f4,0.4444444444444444
20664,9ca9a30fc69d9b,9b5b715e,## Countries that have hosted the most games,f715c2e5,0.4444444444444444
20665,233cb23d9e01b9,8c5a91af,###callback to clear the training outputs at the end of every training step,ffa56c19,0.4444444444444444
20666,3ac432b2cac29c,fb3fa282,"## Step 3: Make Predictions with Validation data
",a358669e,0.4444444444444444
20667,c8bf959b9608cf,c48b31c4,"### Accessing layer by their name
We already know that we can access any layer by their name. We are using layer 'block5_conv2', which is fairly deep, for capturing the content of the image. 

### Capturing content 
We already know that deeper layers are able to capture most of the features of an image. And layer 'block5_conv2' is deep enough to capture the features. Also, <b>to capture more abstract features, we are using deeper VGG-19 instead of VGG-16</b>. Note that the content loss is calculated at only one layer which is generally one of the final layers. We'll calculate the content loss in the 'block5_conv2' layer. To calculate the style loss, we'll use multiple layers since style loss is calculated at multiple layers as you're already aware. 

#### Note that along the dimension of batch size, the first is content image, second is style image and third the generated image get the content image feature. We will seperate those to calculate the loss. ",155e3672,0.4444444444444444
20671,d905cde3391d2b,522d7933,"As we can observe, `[4, 14]` occurs **11 times** in the dataset. 

Hence, **Mode = [4, 14]**,

We can verify using `pandas.DataFrame.mode` method",067dba39,0.4444444444444444
20672,f998cece696659,aff52c10,"<a id=""2""></a><br>
# Linear Regression",7964297e,0.4444444444444444
20673,b9328fe3b0cefc,9c950556,"### Top 10 in 32,16,8,4,2(进入32,16,8,4,2强次数前10的球队)",3a35eb23,0.4444444444444444
20678,c6f8ff61a5fa87,8b9a7ea7,## 2.SVR,3eea586b,0.4444444444444444
20679,24e550b8226932,964726fc,##### train:,0caee953,0.4444444444444444
20685,d96e03a9e7c030,0f1d86fd,"Let's look at how the variables did by sorting by each model's r-squared value. Looking at this data frame, you'll notice a few things:
* The `pred_coef` column can be interpreted as such: If it is positive, the variable has a direct relationship with the percentage of testtakers at a school. If it is negative, the variable has an indirect relationship (i.e., the predictor variable tends to *decrease* as the percentage with the percentage of testtakers *increases*
* As such, the findings are similar to the Research Alliance for New York City Schools, mainly that variables related to statewide test performance are positively correlated to participation in the SHSAT. Additionally, variables concerning underrepresented groups are negatively correlated with participation in the SHSAT. As an example, in row 9 of the data frame below, `pct_students_w_disabilities_2017_val` has a negative coefficient in the `pred_coef` column. This can be interpreted as the higher percentage of students with disabilities at a school, the lower percentage of SHSAT takers in that school
* Many of the shared variables included in both the School Explorer CSV and the CSV with data scraped from the Dept of Ed are **very** similar to one another. The variable `Economic Need Index` (line 16) shows the same data as the `econ_need_index_2017_val` variable scraped from the DoE; these two models perform nearly identical to one another",d2b72ced,0.4444444444444444
20689,b0c2805cd5c087,1b185fdf,Image techtudo.com.br,0446f327,0.4444444444444444
20702,0f5085b162bd9f,6780066a,# Affinity Propagation,a3d989ee,0.4444444444444444
20703,c968dbd8d49ae6,9459d19c,"# **Lets check the Heatmap of correlations. Categorical will be included, although some of them won't be of much use in the study.**",dfb2684d,0.4444444444444444
20705,95d896e75f9a50,2b1b803d,"For good measure, let's see what the AUC is of a model where we use all features with a score < 0.6 when used by themselves.",2721b6f5,0.4444444444444444
20708,fdc3afd309b850,c2e8cc92,"<a id=""gl""></a>
## 7.1 Geo Location",966bde38,0.4444444444444444
20711,7ff97196d5db8c,4b68412a,# Scatterplot between variables along with histograms,3d82be43,0.4444444444444444
20717,9289395e9c480f,a46b442c,"Specifically, we want to know what the literature reports about:

- What is the best method to **combat the hypercoagulable state** seen in COVID-19?
- What is the **efficacy** of **novel therapeutics** being tested currently?",55d03d67,0.4444444444444444
20722,cb4ad8ed4cb300,f2ebc2e8,# 4. Define Measure of Similarity,7c0f3236,0.4444444444444444
20723,06ecf7a304c309,e05a9e4b,이번 오토인코더 네트워크에는 convolutional layer을 추가합니다. 왜냐하면 convolutional networks는 이미지 입력에 대해 매우 잘 작동하기 때문입니다. 입력 데이터를 convolutional network에 넣기 위해서는 28 * 28 matrix로 reshape해야합니다.,714de627,0.4444444444444444
20727,aa7db7b023d0a2,22f6ec94,#Code by Rafael Batista  https://www.kaggle.com/faelk8/prevendo-sobreviventes-do-titanic-autokeras-e-h2o/notebook,ec912af3,0.4444444444444444
20732,5ce12be6e7b90e,f9f792cf,## Exercise: Find another exercise,c0ab62dd,0.4444444444444444
20733,c1984e64b35234,166dff8a,There is a Y chromosome with SNP data present so the patient is a male.,1811225b,0.4444444444444444
20735,55ce731a138ca7,f37ee005,# Train & Save Model,4996250b,0.4444444444444444
20738,0c452d3a0b9339,5b2bf9c0,"# BasePair,Structure & LoopType Distributions.",5d857385,0.4444444444444444
20742,49ac6594c8f5cf,ed842f68,But Others earn more on average,6f19f28a,0.4444444444444444
20743,b6c0ad74f95b8c,337f249c,Convert the dataframe to a dictionary,5de5b241,0.4444444444444444
20749,9085cba2265204,a572fac3,# Preprocessing ,de766eb3,0.4444444444444444
20750,917957c6c4065f,b1cf9758,### 2.1. views,55b8ed68,0.4444444444444444
20755,07544ba83da480,eda9b541,# Question 2: What City had the highest number of sales?,dc2f52b1,0.4444444444444444
20757,56cc8fb47bef6a,3f6db103,"<p>&nbsp; <span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Create vectorizer that takes the top 5000 most frequent words</span></span></p>",652d6670,0.4444444444444444
20759,10b5af05d804ff,4e31d188,Let's make team features/,4a9b1705,0.4444444444444444
20760,b39684e6670dd7,21c49991,## get scale weight by column,83de9873,0.4444444444444444
20763,df2a7968c08ee4,f79af775,"Here we are instantiating both the training and validation datasets. 

We pass these datasets into Pytorch Dataloaders, which retrieve features and labels one batch at a time. The data is reshuffled every epoch so each batch is different through every epoch.",a2ba0a72,0.4444444444444444
20767,613bf7bfdcb9e3,4adf4955,"# axis = 1 (left to right common value)

default direction is axis = 0",32beb65d,0.4444444444444444
20769,ac04ba639d1c93,75f6626b,## Feature generation,748059d5,0.4444444444444444
20770,cf39cde80e66b7,9dd146a9,"# <div class=""h3"">MSE: Mean squared error</div>
<a id=""m2""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.4444444444444444
20774,d6cbd7160961dc,395afe87,## 4.3.3. Results: Third Digit,36d74664,0.4444444444444444
20777,6d29650083cbde,b8babe4d,"**4.  FORECASTING THE 2017-2018 TALENTS AND PERFORMANCE ANALYSIS**

Using the decision tree fitted on players from 2009-2010 to 2016-2017, I forecast which strikers are expected to shine in 2017-2018. My goal is to find players which are expected to be in the top 10% of their league in 2017-2018 but who were in 2016-2017 rated below 715 (i.e. *relatively* under the radar) and under 30. 

I then match this list of players with the players' actual performance and compare my sample to a randomly selected sample of the same size.",e65fd993,0.4444444444444444
20778,5be39e4e35cec7,98933c6e,"* Pclass - Survived
* Sex - Survived
* SibSp - Survived
* Parch - Survived",14d617c9,0.4444444444444444
20779,6dcfe6a610d86b,399ec625,"For convenience, we may define a function for the code above:",d05c59da,0.4444444444444444
20781,e2a94f078e1161,839f97f0,Now to do the same for Science,5fc53059,0.4444444444444444
20782,fc8e0042411c46,7224f48f,## Country,af476c2a,0.445141065830721
20786,3c2033cc99c12c,3074c912,*In this part I will select a few unsupervised learning method to have a brief view of the dimension reduction algorithm*,dfa22a54,0.44525547445255476
20787,0932046e1f485d,deea3061,Checking for correlations between the numerical data.,218cc7a3,0.4453125
20788,4ae6a182abac64,e5cfe8db,## 2. Feature Engineering,418676c5,0.44537815126050423
20795,e1fff2f67cbe32,a8447de3,"> There is relationship between the average score for the active user, and the number of questions answered; there is relation average timestamp and average correct answer can be useful for baseline.",c6dfde64,0.44594594594594594
20796,62037c5832129c,fd90c1ab,From the results above you can see the **SVM model is better than the DecisionTreeClassifier**.,61474350,0.44594594594594594
20798,63b44c85e32c1f,fe2324fd,"**insert(x,y)** is used to insert a element y at a specified index value x. **append( )** function made it only possible to insert at the end. ",fb9b9562,0.44594594594594594
20799,e4525eb0c96f28,9e797e37,"### Plotting Sales by Country

Using a violin plot to show sales by country allows us to directly see how each country compares to each other in terms of sale density.

From the plot below we can conclude that Japan and US lead in terms of highest sales, but it's still hard to tell because of datapoint density. We know from the data that Japan has multiple gaming titles in the 30+ million category, yet it isn't shown because of how skewed the violin plots are.

We can conclude one of our current hypotheses, which is that though Japan and the US are the highest producers in game development, their average sales per game is in the 1-2 million range. What is interesting is that South Korea's average sales per game is higher than both Japan and United States.

We can also observe South Korea and Finland having fairly dense sales groupings compared to their non-Japan/US countries. We will further explore these.",2093a1f1,0.44594594594594594
20800,a566b5b7c374e7,a79b3aaa,"### Initial Impressions:
- My weekends are associated with generally worse sleep habits and metrics. Start and End timing are both later than during the week. I tend to get less Deep Sleep during the weekend, and my Restfulness Score tends to be lower. My Average Resting Heart Rate is higher on weekends, while my Average HRV is lower. ",b3dc5545,0.4460431654676259
20803,c115e287523aab,cef696c1,## Train,feb1288b,0.4461538461538462
20804,d07915a6e6992e,faf1c2b3,**Pclass**,2b912140,0.4461538461538462
20805,09751c520b0616,70119ea9,- fixing negative value in Age,a4d0c7e9,0.4461538461538462
20807,2d75fd881827b8,ad751dd1,**XGBOOST**,107b5299,0.4461538461538462
20809,f2f2db16a2f86c,8d30ca9e,"The correlation between all the factors is depicted by this.

It is clear from this that the price of the houses don't vary linearly with any of the parameters.",ffc6a115,0.4461538461538462
20821,8dd655515e7d18,60b1b68d,### Agreeableness Analysis,895f41cf,0.44642857142857145
20824,98a6794067932a,2a535b82,"Encore une fois, la cellule ci-dessous ne sert pas à effectuer des analyses, mais plutôt à ajuster nos données. Ce code permet de créer un dictionnaire assignant le nom complet des états à leur abréviation qui sera utilisée éventuellement dans nos analyses.",08600fe2,0.44660194174757284
20828,f6648e47713411,312b2348,"### Quan sát :
Các giá trị kênh màu đỏ có vẻ gần như phân phối chuẩn, nhưng hơi lệch về bên trái (Độ lệch âm). Điều này cho thấy rằng kênh màu đỏ có xu hướng tập trung nhiều hơn ở các giá trị cao hơn, vào khoảng 100. Có sự thay đổi lớn về giá trị màu đỏ trung bình trên các hình ảnh.",f4af4d1c,0.44680851063829785
20830,5f674175839b32,2d82bc52,"<font color=pink>This table shows the best selling games in each genre
for e.g= in action genre ""inFAMOUS: Second Son"" is the most selling video game and it is on the XOne platform.",53a2e343,0.44680851063829785
20831,04e6b0d3c70f46,ace1d673,### Model 1: This model gave result of 0.005. ,56344f77,0.44680851063829785
20836,b61ab8f81dc03d,3f83961d,"### Decks
* **A Deck**, also called the Promenade Deck, extended along the entire 546 feet (166 m) length of the superstructure. It was reserved exclusively for First Class passengers and contained First Class cabins, the First Class lounge, smoke room, reading and writing rooms and Palm Court.

* **B Deck**, the Bridge Deck, was the top weight-bearing deck and the uppermost level of the hull. More First Class passenger accommodations were located here with six palatial staterooms (cabins) featuring their own private promenades. On Titanic, the À La Carte Restaurant and the Café Parisien provided luxury dining facilities to First Class passengers. Both were run by subcontracted chefs and their staff; all were lost in the disaster. The Second Class smoking room and entrance hall were both located on this deck. The raised forecastle of the ship was forward of the Bridge Deck, accommodating Number 1 hatch (the main hatch through to the cargo holds), numerous pieces of machinery and the anchor housings.[b] Aft of the Bridge Deck was the raised Poop Deck, 106 feet (32 m) long, used as a promenade by Third Class passengers. It was where many of Titanic's passengers and crew made their last stand as the ship sank. The forecastle and Poop Deck were separated from the Bridge Deck by well decks.
 
* **C Deck**, the Shelter Deck, was the highest deck to run uninterrupted from stem to stern. It included both well decks; the aft one served as part of the Third Class promenade. Crew cabins were housed below the forecastle and Third Class public rooms were housed below the Poop Deck. In between were the majority of First Class cabins and the Second Class library.
 
* **D Deck**, the Saloon Deck, was dominated by three large public rooms—the First Class Reception Room, the First Class Dining Saloon and the Second Class Dining Saloon. An open space was provided for Third Class passengers. First, Second and Third Class passengers had cabins on this deck, with berths for firemen located in the bow. It was the highest level reached by the ship's watertight bulkheads (though only by eight of the fifteen bulkheads).
 
* **E Deck**, the Upper Deck, was predominantly used for passenger accommodation for all three classes plus berths for cooks, seamen, stewards and trimmers. Along its length ran a long passageway nicknamed Scotland Road, in reference to a famous street in Liverpool. Scotland Road was used by Third Class passengers and crew members.
 
* **F Deck**, the Middle Deck, was the last complete deck and mainly accommodated Second and Third Class passengers and several departments of the crew. The Third Class dining saloon was located here, as were the swimming pool, Turkish bath and kennels.
 
* **G Deck**, the Lower Deck, was the lowest complete deck that carried passengers, and had the lowest portholes, just above the waterline. The squash court was located here along with the traveling post office where letters and parcels were sorted ready for delivery when the ship docked. Food was also stored here. The deck was interrupted at several points by orlop (partial) decks over the boiler, engine and turbine rooms.

https://en.wikipedia.org/wiki/Titanic
",64d05394,0.44680851063829785
20837,73893f0467d5e3,068631d8,## For mean_smoothness ,279787c6,0.44680851063829785
20839,869a39a3d4dea2,4959b216,"Generally arithmetic means we think of addition, subtraction etc. however we are going to see the same in here with respect to the image data. Lets imagine that you want add 100 to image matrix or tensor and if the image is represented with uint8(0-255), then we think what values a image data would take and whether it is bounded with in 0-255 or exceed the limits(it wont exceed because it is uint8) if it is not going to exceed whether it will limit the values which is exceeding the 255 to 255 or what happens.  ",9020daf8,0.4470588235294118
20840,513ce405d7f6a3,189720bc,# Wordcloud of Very postive review ,8461e086,0.4470588235294118
20841,e19e307b3fd188,284c4d3a,## Analysis of *not so important* features,2173955b,0.44715447154471544
20846,fe7360cddc13e5,7ca33a1b,Bu durumda beklenen değer hesaplaması yapıldığında transaction sayısının beklenen değeri şu denklemin çözümlenmesiyle elde edilecektir:,8979e423,0.4473684210526316
20853,52ee792e228d54,428c1dea,"### Hmm.. Somethings not right here. Earlier, we saw that number of customers who did not use the credit card was higher (>3500). But the CCAvg says only 106 customers did not have spending on credit card. 
#### As the saying goes, ""The numbers speak for themselves"", let's assume CCAvg is right and reset the values of CreditCard.",5096094e,0.4473684210526316
20860,7454fdc444df16,11d801d8,"uint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers.",a7818ef5,0.44761904761904764
20861,04bac111ffbe9c,244bd403,2. Decode Pclass,82576b17,0.44761904761904764
20868,21413205980558,c8ac429e,"# There seems to be little difference in the surplus status among different education levels, because the median value of concentrated surplus is almost the same<p>
# 不同教育程度的盈余状况似乎相差不大，因为集中盈余状态的中值都差不多.",84197de0,0.44776119402985076
20869,1a222fee3089d2,136ef868,## **Fare category**,59ab8894,0.44776119402985076
20874,f3c6048d1058e3,bac3d931,"## N-gram Analysis
- The order that words are used in text is not random. In English, for example, you can say ""the red apple"" but not ""apple red the"". The general idea is that you can look at each pair (or double, triple etc.) of words that occur next to each other. In a sufficently-large corpus, you're likely to see ""the red"" and ""red apple"" several times, but less likely to see ""apple red"" and ""red the"". This is useful to know if, for example, you're trying to figure out what someone is more likely to say to help decide between possible output for an automatic speech recognition system. These co-occuring words are known as ""n-grams"", where ""n"" is a number saying how long a string of words you considered.",1d9056b0,0.4482758620689655
20877,a1ba5ffd30dbde,12136408,### Train-Test Split,48e57546,0.4482758620689655
20878,20e1ba19eb9b5e,7c09a1e5,"SalePrice variable is not normal but right skewed -> does not follow the diagonal line and has positive skewness.
If we want to use linear models efficiently we have to transform the data so it will be normally distributed.",4569bfc1,0.4482758620689655
20882,5fc2f23dfbeeb1,73ca6252,"### Re Library
Remove everything except the letters (a-zA-z) from keyword, location and text columns",f37b4110,0.4482758620689655
20883,00001756c60be8,45082c89,**Приведение типов**,945aea18,0.4482758620689655
20887,5ea840754577e3,e9a28609,### Feature: Age,9cf9b73f,0.4482758620689655
20890,1750367e54f407,10fe46fd,"At this point, you may have noticed that I have not used any kind of normalization or rescaling. I recently discovered that there is a Normalization layer included in Keras'pretrained EfficientNet, as mentioned [here](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/#keras-implementation-of-efficientnet).",a8e655b2,0.4482758620689655
20891,fb5c6021d127ef,7c1bebb6,Write your query below:,dd05cbd3,0.4482758620689655
20892,00d295edcd117e,02d06754,## 定义一个卷积神经网络,f5810f4b,0.4482758620689655
20893,fb9296ecd0cb2a,f9714444,"# Build a model

upd1. add mape_scorer

upd2. add fnc to features

upd3. add degrees features

upd4. add fcn/500 from https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging , switch to Ridge regression

todo: fit on all available targets (currently observation is dropped if any target is missing)",aa66d98c,0.4482758620689655
20897,84127ade6fde87,df32b934,"We’ll define clean_words , which takes text and returns it in lowercase and stripped of punctuation. When we call it on our “Impossible, Mr. Bennet” line , we get the following:",f55d05b6,0.4482758620689655
20900,d42518f6cb0995,854bdb0e,"Timestamp, the average score for the active user, and the number of questions answered can be useful for baseline.",26913a9b,0.4482758620689655
20904,6a1d04e8153df3,521cc525,"**Observation(s):**
- If the axilliary nodes are greater then 9, then the chances of leaving is more or we can say patient is almost safe.(3row,3column)

>if axil_nodes>= 9 :
   patient is safe
 else :
   patient is unsafe
   
- Women's who are of age greater then 67 had lost thier life in Operation year.(2row,2column)
- As we can see women's of age 30-80 whoever had axillary nodes less then 10 or to be more precise less then 5 has more chances of death. To predict the result we can make small model on this too ; 
 if age >= 30 & age <= 80 :
      if axil_nodes <=10 :
         patient is not safe or more chances of death
(This is a small example )

>Some of the plot are same.
",38572b05,0.4482758620689655
20907,25ed87d1f0cb06,471a8e72,"# DenseNet121<a class=""anchor"" id=""4""></a>",7ca68782,0.4482758620689655
20910,858da4bb312f67,fd527d69,## Check Generated Image,9cca4391,0.4482758620689655
20911,6903d3f38c6a66,03fda075,"But should it be the same size depending on the subplot? For example, bar graphs and pie charts are often very different in ratio.

In that case, the layout should be different.

In that case, you can easily use the grid system using **plt.subplot2grid**. ",6067ce5e,0.4482758620689655
20915,2ada0305b68956,cbefd3be,### 75. Palette = 'afmhot',133e26f4,0.44857142857142857
20916,c84925c8171900,715cde35,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.2.1  Year Wise Video Game Release Count 
            </span>   
        </font>    
</h4>",e21ff7ec,0.4485981308411215
20917,80ad12f326ab70,59f0f4cb,"#### Exploratory Data Analysis:
In this analytics, trends in digital learning are uncovered; how engagement with digital learning relates to factors like district demographics, broadband access, and state/national level policies and events.",da404a16,0.44871794871794873
20922,ce9ed5e2d601d7,d1d4fbe6,"# Train Model and Create Submissions #

Once you're satisfied with everything, it's time to create your final predictions! This cell will:
- use the best trained model to make predictions from the test set
- save the predictions to a CSV file

$Softmax: \sigma(z_i) = \frac{e^{z_{i}}}{\sum_{j=1}^K e^{z_{j}}} \ \ \ for\ i=1,2,\dots,K$

K - number of classes

$z_i$ - is a vector containing the scores of each class for the instance z.

$\sigma(z_i)$ - is the estimated probability that the instance z belongs to class K, given the scores of each class for that instance.

$Relu(z) = max(0, z)$

Binary Cross Entropy: $-{(y\log(p) + (1 - y)\log(1 - p))}$

For multiclass classification, we calculate a separate loss for each class label per observation and sum the result.

$-\sum_{c=1}^My_{o,c}\log(p_{o,c})$


    M - number of classes

    log - the natural log

    y - binary indicator (0 or 1) if class label c is the correct classification for observation o

    p - predicted probability observation o is of class c

",f58a2f43,0.44881889763779526
20929,f35bf4df70d310,1b2184ea,"For this task, I'm going to use the **Multi Layered Perceptron** model",10bb859a,0.4489795918367347
20930,12f4d16fc21645,ae2a8b7e,"<h3 style='color:Red'>NPK ratio for rice, cotton, jute, maize, lentil </h3>",c7752038,0.4489795918367347
20931,eb33e05704d647,90491a76,"Define loss functions for all four outputs
",cd80436d,0.4489795918367347
20939,9d9da6c439b96b,ba9fa660,## Data Insight,361cc7d9,0.4492753623188406
20943,17a24d566ffa59,6c19ffbe,1. ![](https://s3.amazonaws.com/nlp.practicum/svd2.png),89049e56,0.4492753623188406
20946,b01ee6cb674fa3,27589896,"# Roscosmos
Государственная корпорация по космической деятельности
Roscosmos State Space Corporation

The Russian space program, successor to the soviet space program, since 1992",a8ffd35e,0.4492753623188406
20947,e3fb4c6300cb56,7886e70b,"<a id=""5""></a> 
## Lm Plot",8ebbdf89,0.4492753623188406
20949,312135b445bd23,f9d5c274,Plot the most frequent 400 word vectors in a 2-dimensions plot:,8ced381f,0.449438202247191
20950,04ff2af52f147b,50e1fe1c,"Comparing the training set and the combined set, we find that there are no *Deck*, *Pclass* combinations in the combined set that aren't already represented in the training set.  With that in mind, we can now safely apply these values as a new feature for both the train and test sets.  This feature is called *DeckClassSurvProp* and represents the proportion of passengers, with the same *Deck* and *Pclass* combination as the given passenger, who survived (only using data from training set to avoid leakage).",d5f37be9,0.449438202247191
20952,bd380b97b5c894,abeba2cf,## **Individual patient information**,66f2562a,0.44954128440366975
20953,726833f92fb87a,5a105bdd,## Housing vs campaign success,7dc5e1b6,0.44966442953020136
20964,5ffe6aa38958a1,1819b792,"# 2.3 Data Cleanup & Feature Definition

Lets find if there is any missing data and if it is an error. 
Also, convert non-numeric data into numeric. 

Note that this will be done for both training and test data because we need to perform similar operations on both data sets. We did not use test data for analysis. 
",11f5412e,0.45
20965,f8bcb6d96fd560,855930d0,Lets make a normalization for our x data,390bb0f0,0.45
20966,1005ca950e8a81,afb6f70d,"**Question 5: Shall the centroids belong to the original set of points?**
Pas forcément non!",52570331,0.45
20970,8bb432d338a70b,544b1992,TensorFlow,7aab1dfd,0.45
20974,712198370d5521,e84c68b5,"Next, let us look at the correlation amongst the features. 
(Excluding the categorical attributes at this point)",5882e04c,0.45
20975,541d0fa0e26b80,7844b7c4,This gives us the distribution among two attributes.,a29e0f29,0.45
20977,6e28c4f557f736,71168ee7,## Model Building,021fdf75,0.45
20986,edc19e349fe80a,db00d2f7,### 1. Check training data to find a suitable approach,7882221a,0.45
20989,b10bd75889dad9,5141c47b,#### Lets plot the Heatmap to check correlations,ee00ceee,0.45
20993,09bac0c221388e,dbb03cd1,> This approach is the most facile and most straightforward one. Here we utilize information theory to assign each sentence of the input with a score that is predicated on relative frequencies. A high value for a sentence betokens that its content is liable to be informative.,bea4aa2e,0.45
20995,c18267b203f28a,b409151f,The following code chunk sets up a series of functions that will print out a grid of images. The grid of images will contain images and their corresponding labels.,09ca8efb,0.45
20999,bfe6c7096b1ad0,f164f7e2,## Никакой предобработки - просто заполняем пропуски,fffd95e0,0.45
21001,396bc36edb95d3,fcacdacf,#### Predicting on Training and Test data,965e4f8f,0.45
21004,5ce12be6e7b90e,83c4aaac,"Add the correct expression to the while parentheses so that the while loop will only end <br>
when both your cards are aces.",c0ab62dd,0.4502923976608187
21005,30fdc4a6e3c1db,9cc94bcc,For CA:,6111ddee,0.4502923976608187
21006,ee23a565163388,f4ed6df1,## **Impact of Sodium level in Heart Failure**,88aacbc4,0.45038167938931295
21010,3d77c1560bd16e,2b6355e3,> I believe the counties showing more than 100% of population vaccinated indicates that people from neighboring counties traveling into nearby counties to get the vaccine.,87c141ca,0.4507042253521127
21011,d8ff894670d506,fc9c6ed3,**Analysis of Data Using mean median and percentile functionalities**,eb0fb7de,0.4507042253521127
21013,631cd434fc3aa2,4747d716,"* _BsmtCond_, _BsmtExposure_, _BsmtQual_, _BsmtFinType2_, _BsmtFinType1_: Na means no basement. ",2b74febb,0.4507042253521127
21014,979f1e99f1b309,244a70c9,***THE plot show that the player who finished the game in advanced places use boosts more than other which is normal***,d1bfebbf,0.45081967213114754
21018,523123dad03177,7a9c8b09,# 6. Math Scores,48a5e4e6,0.45098039215686275
21022,4fa553c2b837d4,3ba429f8,##  Compare the Mean Absolute Error of each solution.,c65a23e9,0.45098039215686275
21023,64169805aacf17,8923588d,## Create the Neural Painter,1f12ded0,0.45098039215686275
21024,d0080e3a39bc5c,d90697f7,**ARCHITECTURES USED**,2fcde4cf,0.45098039215686275
21025,1a0bd2f72bbe36,7577da6f,"### Those are the outliers in the numerical values:
#### We will ignore them because we aren't building any model:",2fa311dc,0.45098039215686275
21027,52cfd66e9ec908,a8e05c43,"We have a literal wealth of information that we can use here to our benefits, including familiar features like:
1. x, y,  and z coords
2. yaw
3. probabilites of other extraneous factors.",c74adcdf,0.45098039215686275
21028,71b75664517244,7af9dc30,Manchester City made a breakthrough on 2012 and doing great right now,fc905af5,0.45098039215686275
21029,ba4b3bd184acbb,93312304,"It looks like one of the values in the Price column is `Everyone` which definitly does not seem right.

We can use boolean indexing to find the culprit row.",0f5de724,0.45112781954887216
21040,f15eac23fbcc9d,d38f6100,"Pretty good score on training set, but unfortunately it must be overfitting. Let's do it the right way.",ea46d8af,0.45161290322580644
21044,0cb456a5456cf9,25b8f473,# **PART 2** <br>Machine Learning: Prediction<br>机器学习：预测客户的退订行为,5701729c,0.45161290322580644
21046,1c2948c70624cf,8763fdbf,# **Data Visualiztion**,eeb97474,0.45161290322580644
21048,57070ad5e0f94f,5e07a851,# **Correlation Matrix**,d97edc41,0.45161290322580644
21053,16862cb02d73d5,e0fcf0db,"Now here we have 12 metrics on which we have classified anomalies based on isolation forest.We will try to **visualize** the results and check if the classification makes sense.

Normalize and fit the metrics to a **PCA** to reduce the number of dimensions and then plot them in 3D highlighting the anomalies.",d7ffa1a6,0.45161290322580644
21054,0d9a2067267ba1,eb026f2a,### Correlation analysis,abc194fb,0.45161290322580644
21056,0caaec057f7184,c5b38330,old items already launched,b875533e,0.45161290322580644
21058,78998e078eaaa1,40377399,"## Trained on only 1 training image pixels 
",2b29364c,0.45161290322580644
21060,921fff7d3040db,c54856ee,# 4. Encode and split data,5f36ced9,0.45161290322580644
21065,99bf357eaf61f1,0de1aba6,#### Box plot - Neighborhood,9d92fafe,0.4519230769230769
21066,44f6a002ecd033,40c8e8db,#### Merging Numerical Dataset and Categorical Dataset Together,70bbe106,0.4519230769230769
21069,1eb62c5782f2d7,a250417b,"### Interpretation  

- $2.28\%$ orang akan tetap menggunakan HPnya selama kurang dari $1$ tahun sebelum membeli HP baru. 
- Karena $2.28\%$ kurang dari $5\%$, ini termasuk kedalam **unusual event**.",bb69f147,0.4520547945205479
21075,71c3c1eab0377d,f629c427,#### Discretization of the Numeric columns : Fare,52b4e360,0.45217391304347826
21079,a76e0e8770b7a0,e96b8aa6,Don't any information about 70s teror event. ,02863d3b,0.4523809523809524
21081,1084376bc4897c,13bb2a45,"## 3.3 Next, lets analyze discrete features using countplot",1b598487,0.4523809523809524
21085,066c5ee1ef39e6,e7ea45db,"## Create Pipline

**Use the following methods**

TFIDF
RIDGE",0f394e1b,0.4523809523809524
21087,adb8441ad28019,551571d7,"<center>
    <strong>Here, we are trying to predict the fetal health of the patient using the given data. Hence, the `fetal_health` will be the y label and rest of the data will be the X or the input data.</strong>
</center>",d89de993,0.4523809523809524
21090,1fac5edd4063ba,3f655f87,"#Valerie Thomas

She is an African American scientist and inventor best known for her patented illusion transmitter and contributions to NASA research. 

Valerie Thomas' interest in math and science were not encouraged until her college years. After graduating with a degree in chemistry, Thomas accepted a position at NASA. She remained there until her retirement in 1995. During that time, Thomas received a patent for an illusion transmitter and contributed broadly to the organization's research efforts.

Valerie Thomas was born in February 1943 in Maryland. Fascinated by technology from a young age, Thomas was not encouraged to explore science. At the age of 8, she checked a book called The Boy's First Book On Electronics out of the local library. Her father would not work on any of the projects with his daughter, despite his own interest in electronics.

Thomas attended a high school for girls that downplayed math and science. After graduating from high school, Thomas finally got a chance to explore her interests as a student at Morgan State University. She was one of only two women at Morgan to major in physics. Thomas excelled in her studies. She graduated from Morgan and accepted a position as a data analyst at NASA.
https://www.biography.com/scientist/valerie-thomas",04bc01e0,0.4523809523809524
21092,b4ecd6e4277e3c,a118f958,### SAVE DATASET TO DISK,94d79d5f,0.4523809523809524
21094,a758983a68c014,81a244d3,"Now, replace words with their indexes.",ab89f181,0.4523809523809524
21095,2d40f383473fa4,2b7d4131,"Time to work with `Age`, since the distribution is near from normal, it's good to fill all `NaN` values with the median to not harm the distribution.",1da1eff0,0.4523809523809524
21097,3c2033cc99c12c,75aeb716,"### PCA(Principle Component Analysis)  
*Principal component analysis (PCA) is an unsupervised algorithm that creates a linear combination of original features. The new features are spatially orthogonal, which means they are not related. In addition, they are arranged according to the magnitude of the ""explainable variance"" value. The first principal component (PC1) explains the largest variance in the data set, PC2 explains the second largest variance, and so on.*
+ The mathematical principle of the dimension reduction 
+ The advantage of the PCA method 
+ The disadvantages of this kind of method ",dfa22a54,0.45255474452554745
21100,169177b6e9edea,dd13c0e1,"Labels=['Criança(0-10)','PréAdolescente(10-15)','Adolescente(15-20)','JovemAdulto(20-30)','Adulto(30-40)','Adulto(40-50)','Adulto(50-60)','Idoso(60-90)']
Age_cat=pd.cut(Dados_eng.Age,[0,10,15,20,30,40,50,60,90],labels=Labels)
print(Age_cat.value_counts())
Dados_eng['Age_cat']=Age_cat.astype('object')
#encoder=OneHotEncoder()
#Age_hot=encoder.fit_transform([Age_cat])
#Dados_eng",ca42152f,0.45263157894736844
21106,23df07a474aaae,4580662b,# Training and Testing Models,0ea40276,0.4528301886792453
21109,f3c8651cb08234,639dcf96,# Split data into training set and test set,37f86e36,0.4528301886792453
21110,f015d0147e8fbf,d1c2c9f2,### 7. Credit Card Balance Data Table `credit_card_balance.csv`,518954fb,0.4528301886792453
21111,614ba9f0c62677,cd800930,"<a id=""5""></a>
### What is Convolution Operation?
* We have some image and feature detector(3*3)
* Feature detector does not need to be 3 by 3 matrix. It can be 5 by 5 or 7 by 7.
* Feature detector = kernel = filter
* Feauture detector detects features like edges or convex shapes. Example, if out input is dog, feature detector can detect features like ear or tail of the dog.
* feature map = conv(input image, feature detector). Element wise multiplication of matrices.
* feature map = convolved feature
* Stride = navigating in input image.
* We reduce the size of image. This is important bc code runs faster. However, we lost information. 
* We create multiple feature maps bc we use multiple feature detectors(filters).
* Lets look at gimp. Edge detect: [0,10,0],[10,-4,10],[0,10,0]
* <a href=""https://imgbb.com/""><img src=""https://image.ibb.co/m4FQC9/gec.jpg"" alt=""gec"" border=""0""></a>
* After having convolution layer we use ReLU to break up linearity. Increase nonlinearity. Because images are non linear.
* <a href=""https://ibb.co/mVZih9""><img src=""https://preview.ibb.co/gbcQvU/RELU.jpg"" alt=""RELU"" border=""0""></a>",b8551335,0.4528301886792453
21120,ff3a8ce61fab6a,22a99fe7,"<hr>
## Exampel 3 

In this exampel we will build **linear regression** equation using tensorflow. <code>equation = w.X + b</code>",9afe1654,0.453125
21121,a566b5b7c374e7,88b02581,## ANOVA and Correlation for Critical Metrics and Categorical Variables,b3dc5545,0.45323741007194246
21128,7e1da639035ac5,4f66dda9,### <a id='9.1'>9.1 Rigorous instruction % distribution</a>,120b6c23,0.4533333333333333
21129,cb570c7b7f0501,16f281d3,"### Research Question 2  ( Is it about one of the diseases ! )
",a200a0ec,0.4533333333333333
21134,225b4fe5d3894a,c2c062fb,"<a id=""6b""></a>
### b. Handling Text and Categorical Features
We will handle the text feature ""ocean_proximity"" that we dropped earlier as it cannot be fed directly into any ML model",4b4197b3,0.4536082474226804
21135,2a123b4e8f9433,01470a99,Distribution of validation scores,0a082218,0.4536082474226804
21137,ab6da5994949a3,01071f82,## Logistic Regression Training Results,fae6b91d,0.4537037037037037
21141,4ae6a182abac64,c98537a9,* **Inspect the missing values** ,418676c5,0.453781512605042
21144,a4f8ad33c823c5,d6ceb904,"### Handling missing BMI values

Missing bmi values were calculated with the filled height and weight values whose missing values were replaced. The Body Mass Index (BMI) is a person's weight in kilograms divided by the square of the height in metres. We will validate the BMI values provided using the height and weight of the patients. 

The height in the data is provided in centimetres. 

 https://www.cdc.gov/obesity/adult/defining.html

- Obese is defined as 30 or more.

- Overweight is defined as more than 25

- Normal BMI is defined to 18.5 to <25

- Underweight is defined to be less than 18.5

",fcd48307,0.45384615384615384
21146,b61ab8f81dc03d,15b57a4a,"![image.png](attachment:image.png)

Titanic cutaway diagram

https://en.wikipedia.org/wiki/First-class_facilities_of_the_Titanic#/media/File:Titanic_cutaway_diagram.png",64d05394,0.45390070921985815
21148,957e035ba5b9d5,36d5e214,# Pre-Trained Network Part 2 (Experimental),778ab3d3,0.45390070921985815
21149,2ada0305b68956,144769ce,### 76. Palette = 'afmhot_r',133e26f4,0.4542857142857143
21153,0cb9adc158b705,7c318d14,"Looks good to me, what do you think?

We are done with data, time for some training.",3abf056e,0.45454545454545453
21154,663bbc9eaf267b,ee7db47e,## transmission,32445529,0.45454545454545453
21156,e9b9663777db82,30e1f893,#### 7.Quantitative analysis for Balcony feature,648e8507,0.45454545454545453
21161,2f47abddfd1928,2978a495,"We can see how the median of the first class is the highest with most of the people (25%-75%) between 30 and 50 years old. Not many outlayers, although the extreme quartiles covers almost the range because there are some children and elder people.

Second class is similar but with main body of the box slightly younger and the extremes not so spread.

The 3rd class as expected is the youngest one, but without children and babies and just elder people as outliers. Also the age range that covers is quite smaller.

All 3 follow a kind of normal distribution, being by far the 3rd class the most common one.

As summary, we can clearly see the relation between Age and Pclass.",ae33cc0b,0.45454545454545453
21162,9c19668d6b7295,ad97f3aa,## Model,35be7001,0.45454545454545453
21166,dd02a9b545f742,669f5f27,# Phase 1 | Object Building - Object 1: Linear Model Project,7116cd2d,0.45454545454545453
21170,1466e61d45b718,62dc68bb,"The next hidden code cells define functions for plotting data. Click on the ""Code"" button in the published kernel to reveal the hidden code.",b062d92b,0.45454545454545453
21173,da199f8fb59439,ba66f325,"## Rating Distribution

- [How does Netflix decide the maturity rating on TV shows and movies? (USE ver.)](https://help.netflix.com/en/node/2064/us)

> Each TV show and movie on Netflix is assigned a maturity rating to help members make informed choices for themselves and their children. Maturity ratings are either determined by Netflix or by a local standards organization. Netflix determines maturity ratings by the frequency and impact of mature content in a TV show or movie. TV show ratings reflect the overall maturity level of the whole series.


|Little Kids | Older Kids | Teens | Mature|
|-|-|-|-|
|G, TV-Y, TV-G | PG, TV-Y7, TV-Y7-FV, TV-PG | PG-13, TV-14 | R, NC-17, TV-MA|

---

### Rating System

>  [Motion Picture Association of America film rating system](https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system)

|Rating|Meaning|
|-|-|
|G|General Audiences|
|PG|Parental Guidance Suggested|
|PG-13|Parents Stongly Cautioned|
|R|Restricted|
|NC-17|Adults Only|

",baaa665d,0.45454545454545453
21176,132fa9714f2046,c669e5fa,"## Exercise 3

** Create the plot below by adding two axes to a figure object at [0,0,1,1] and [0.2,0.5,.4,.4]**",3bb1775f,0.45454545454545453
21183,5a8c553e21c70f,c2b4d3d6,Class 1 is upsampled such that the ratio of samples of Class 1 to Class 0 becomes 0.0025,9ebd9d8f,0.45454545454545453
21184,52479e3c546fa2,e1f7b40e,# Missing data,f2ef2abe,0.45454545454545453
21189,450fda47b03baa,e5afebad,Veri çerçevemizin Rating değişkeninin benzersiz kaç adet değer içerdiğini görüntüleyelim.,62c04adb,0.45454545454545453
21190,b82610a9364f75,54216e85,# Apply tip 1,22c70e69,0.45454545454545453
21192,32e04b08ff52eb,4a620358,Creating Dictionary,8d5b86e0,0.45454545454545453
21194,5e1d001f8764e0,7f4ebde0,# EMBEDDING REPRESENTATION,62be464c,0.45454545454545453
21197,d83e5b44d1b80d,14b89e50,# Duration,62845930,0.45454545454545453
21199,bb3d1b4b9f1248,01533ec5,"# Alcohol consumption view for the first 7 months in 2020 based on PerCapita.
*PerCapita ""Gallons of ethanol per capita age 14 and older in 2020""*",bf7de324,0.45454545454545453
21205,be2f4d8a6b73ca,7f05ea13,**visualising continuous features with output variable using histogram and kde**,5d8ce40a,0.45454545454545453
21206,9519b558f7baec,87213ee7,"Use the block below - kernels are slow so I am giving you the ""best"" params",e32ab82f,0.45454545454545453
21207,5083d7a61f2426,12fbfbfa,Model structure,541a0fec,0.45454545454545453
21209,eda49464dd6d1b,4b2bf552,"### ""id"" will be kept in its own dataframe for the submission later, and not used by the model",8421f81f,0.45454545454545453
21210,90691864eb68c7,09ac6eb0,Variable Transformation and deletion,3555ef9b,0.45454545454545453
21212,75adb7945ef9bd,14b1dafa,All of the statistics have very low correlation with the target variable,785c5095,0.45454545454545453
21216,016abae0483764,4024dafe,"The results show that its not that prominent that the result is positive all the time, rather insterestingly its negetive maximum time.",bc9f289b,0.45454545454545453
21218,3a15bac33f2a74,4c692757,**Decision Tree Classifer**,25204b71,0.45454545454545453
21220,d1ff7e10ee0102,36e906ef,"# 4. Missing data

Important questions when thinking about missing data:

* How prevalent is the missing data?
* Is missing data random or does it have a pattern?

The answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.",2cc71c3c,0.45454545454545453
21223,b42180a6a5b42f,06a3d0f3,"**Verificando os dados, percebemos que a Series 'date' é um objeto. Portanto, devemos transforma-la em Datetime:**",987cea5f,0.45454545454545453
21231,7c89a32e3562ca,1371421e,# Data Preprocessing,32dd8913,0.45454545454545453
21232,20b372b6e4e276,dccbb008,The main problem is in the predicting of the longest and shortest selected_text which are most or least different from the given text,ec8b0860,0.4552238805970149
21233,e19e307b3fd188,f787ead8,### Animal,2173955b,0.45528455284552843
21236,49ac6594c8f5cf,2f8c490c,**Does work Experience Matter ???**,6f19f28a,0.45555555555555555
21238,3597174a998d4d,dd042f51,#### 2.2.1.3 customers' type,276892ed,0.45555555555555555
21243,eb0ecd6bebeb15,6cab9fb8,"Keman grafiği çizdirerek sepal.width değişkeninin dağılımını inceleyin. 

Söz konusu dağılım bizim için ne ifade ediyor, normal bir dağılım olduğunu söyleyebilir miyiz?",d7b93a60,0.45588235294117646
21244,7f74a04ae75792,febfb776,### After replacing missing values with mean,d01e91da,0.45588235294117646
21245,c65a65d4041018,de43643e,"Quite interesting. Usually people with 2 or more years of ML-experience are confident that they are DS, but in Russia people with 1-2 or even less that 1 year of experience consider themselves to be DS.",824fb229,0.45588235294117646
21247,e4c6dd957eb5ce,c82e918c,"Interesting. <br>
It gives us a table showing the ratio of each day of the week for each month.
- We can note that the weekends are consistently worst than other days but in May it is even worst.",2e383665,0.45588235294117646
21248,99821bc6a45be6,71ed8dda,We decided that as a baseline we would make a very simple model that has only one hidden layer that is trainable. A pretrained ResNet is used as a base but all of the parameters are set to be untraineable. The architecture is shown below:,b9d59346,0.45588235294117646
21254,54004b32784b68,2efb413f,**Encoding with One Hot**,27213ca9,0.45614035087719296
21255,3fb15e6e48aec2,55985138,"# Fix null values
* comments
* ignore Survived null values as they are null in test
* Age - previously used Regression. Now imputed with mean grouped by Plcass & Sex
* Fare - only 1 missing - use mean
* Embarked - only 2 missing , use mode
* Cabin - is a tricky, we can do a classification model to identify but for simplicity lets replace null values with ""None""

### Cabin",9d1f4358,0.45614035087719296
21256,9f3710be6aea65,956afd53,Now lets deal with cabin values. Since there are too many NaN values we can binarize if the passengers have Cabin or not.,ae9bda88,0.45614035087719296
21260,c3498779cda661,70b4bb98,# Importar Datos,0f531b65,0.45614035087719296
21262,e03eb63c1f725d,ac3f6400,"<a id=""5""></a>
<font size=""+2"" color=""blue""><b>Count Vectorizer</b> </font><br>",e204b7e3,0.45614035087719296
21263,6cade0b6a41ba2,1ba0c91d,## 3.8. Residence Type,e6110293,0.45614035087719296
21267,fdbbd573ba31c2,df80fde3,## About Target - windmill_generated_power(kW/h),f7c28d74,0.45625
21272,5f32117bcd5255,b7eabfd4,### HST OPTICAL FOUR,85882abf,0.4563758389261745
21273,0e2a23fbe41ca9,b9a0ed80,"Observations:
- All the merchants under category_1 as Y have all nulls in category_2
- Remove category_1 column when using in training as it's not adding much information
",64e4762c,0.45652173913043476
21277,72d528df923403,3a552a04,"- In all the departments the quantity of items sold is higher on the weekend, except in HOBBIES_2, which keeps the slope flat.",d51c8e8e,0.45652173913043476
21278,b01ee6cb674fa3,b0b507e5,"# United Launch Alliance - ULA

A private company from USA

according to wikipedia,
> The company, which is a joint venture between Lockheed Martin Space and Boeing Defense, Space & Security, was formed in December 2006. Launch customers of the United States government include the Department of Defense (DoD), NASA, and other organizations.
",a8ffd35e,0.45652173913043476
21281,9b5de3823ad5ab,8717b7c8,"### Stratified split between train, validation and test
We're going to use transfer learning, so, for training the top layers, I used the more common split of 70-20-10 (training, validation and test, respectively, in percentage). But, after consideration, since there are more than 400k images, I choose a split of 90-5-5 to train the whole model.

Since `random_state` is defined, we can repeat the experiment any number of times and get the same results. I also stratified the split according to the labels' distribution.",33e48774,0.45652173913043476
21284,ce9ed5e2d601d7,072d5287,## Create Models,f58a2f43,0.4566929133858268
21288,bbaa07ad21cf4e,56de41ad,"## 5. Splitting dataset into Train, Test and Validation set<a class=""anchor"" id=""6""></a>

",3ab6b254,0.45714285714285713
21290,fe6750354fb64f,90db3046,# Machine Learning Models,271741f0,0.45714285714285713
21292,60da9bbfe39c4b,af6783a7,# Monthly EDA,b0dd8ad6,0.45714285714285713
21293,b4e238fbc6464c,bed86f77,## Missing value,fd1a6cba,0.45714285714285713
21295,2730840089c8eb,1c22d1c1,Occasionally you'll want to split on something other than whitespace:,34d27dac,0.45714285714285713
21296,6b65d81a5743dd,5700f8aa,Let's Explore the GamesPlayed feature to see if it correlates ,4080a2d2,0.45714285714285713
21298,171494b45650a2,67fe6ae0,### Size vs Price,9c8cc578,0.45714285714285713
21302,9c044fa3072552,e66d8a74,`Start_Time` is the time at which the accident occured. Let's dive into it!,1362842e,0.45714285714285713
21304,38b79494ac749e,749d3b32,"Ideally, we would choose the the model parameters such that we have the best model performance. However, we want to make sure that we really have the best validation performance. When we do `train_test_split` we randomly split the data into two parts. What could happen is that we got lucky and split the data such that it favours the validation error. This is especially dangerous if we are dealing with small datasets. One way to check if that's the case is to run the experiment several times for different, random splits. However, there is an even more systematic way of doing this: [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html).",39162a40,0.45714285714285713
21306,81712ee7510ac5,16c4347f,**Dictionaries : Uses the curly brackets  {'key1': 'value'}**,c4685e79,0.45714285714285713
21310,0fa9979b5690e9,d200e718,"Os dois blocos de cima demonstram duas maneiras de aplicar a validação cruzada a um modelo. Enquanto o primeiro afirma que a acurácia do modelo é em torno de 72%, o segundo alega 63% com desvio de 23% (que é muito alto). O primeiro tem o objetivo de avaliar o desempenho baseado em uma métrica, enquanto o segundo oferece quais os índices (linhas) que participarão de cada rodada ou execução, dando maior flexibilidade para o programador.

**Mas por que uma diferença tão grande?** A resposta é um conceito do campo de estatística: estratificação. Ter um conjunto estratificado significa que ele é representativo. Ou seja, se sua amostra tem 30 amostras da classe positiva e 30 da classe negativa, ao separar em um conjunto menor, é esperado que mantenha-se essa proporção (50/50). Enquanto que o cross_val_score faz a divisão considerando a estratificação, o KFold não o faz. Para isso, existe uma variante dessa função chamada StratifiedKFold. ",c26eea94,0.45714285714285713
21313,b3e48999ed0d00,99d3c27a,### Transformation of Data,fe9ada0f,0.45714285714285713
21321,f6648e47713411,53ac3794,## Phân phối Kênh Xanh Lá,f4af4d1c,0.4574468085106383
21324,917957c6c4065f,366de101,"views는 수치형 변수 중 최대값의 단위가 가장 큰 항목입니다.  
인기 동영상들은 평균 조회수 243,674건이네요.  
최대 조회수가 억 단위인걸로 봐서 극단값들의 영향이 매우 클 것 같습니다.",55b8ed68,0.45751633986928103
21325,f2e5e9fb9eaaf7,f0991dae,### 4.2.4 Features f76 - f100,048e0d08,0.4576271186440678
21327,a44368590e878a,e7bdbf47,### State / Age,77743ba8,0.4576271186440678
21329,a81661cc35d8d2,82f18a42,# Data Pre-Processing,3331f113,0.4576271186440678
21331,a077820f7ab459,7e345405,### Create model,05a43104,0.4576271186440678
21332,9169c4e9c33c90,ce9dfe6a,"Dr. Seuss only had two books across the top 50 bestsellers,
but ""Oh, the Places You'll Go!"" reached top 50 a total of 8 times.",725bf880,0.4576271186440678
21334,dac3c8204a2d1b,f14288f7,# * Bestselling Author 2009-2019,b0d2d0dc,0.4576271186440678
21335,c4bca5d86a38c3,d783477b,Haciendo OneHotEncoding de la columna Sex,e23d297c,0.4576271186440678
21338,bb0905d33ae417,f9b61a25,"A [simple rule of thumb suggested](https://github.com/hiromis/notes/blob/master/Lesson1.md) by Jeremy when selecting differential learning rates is to:
1. Set the first part of the slice (corresponding to the earlier layers of the model) to a learning rate much smaller than where the loss starts increasing.
2. Set the final slice to a learning rate 0.1x of that used when training the frozen model.",25fd1965,0.4576271186440678
21339,fc8e0042411c46,11bd12ad,"- Most values are 'India' , we can tell core business is coming from India market ",af476c2a,0.45768025078369906
21340,4daf6153275cbf,f5f5d636,"Here map shows the localization rates, darker the color, higher the ratio. **There seems to be high localization in Mediterranean countries.** Central and northern Europe along with UK has low rates, whereas eastern has slightly higher rates.",51db1961,0.4578313253012048
21347,d8d227c158d883,604f50ff,**Upset is more likely to happen in men's tournament than [women's](https://www.kaggle.com/takaishikawa/no-ml-modeling-ncaaw2020)**,3391b4a7,0.4583333333333333
21349,1452c9aef2c2a7,783b58cd,"## for i in range(10):
    signal,label=usd[i]
    print(signal.shape)
    
#durations need to be constant",10b8c047,0.4583333333333333
21351,c01049afb6d307,2f1de113,## Reasonforabsence -- Disciplinaryfailure,d37d3b5d,0.4583333333333333
21353,7ba63a2d9abb58,228ca8d6,"<h2>Top and bottom 20 countries by mortality rate</h2>
<ol>
    <li>Mortality rate: deaths/cases</li>
    <li>Recovery rate: recoveries/cases</li>
</ol>

Mortality rate is an important metric to measure the severity of the pandemic. In the below chart we can see the top 3 countries by total cases are not top in mortality rate.",821a261f,0.4583333333333333
21354,1d5daeca89f48d,21305a83,"## Challgenes/Limitation in Count Vector

1. Increase in size
2. Many 0's
3. no meaningful information",48d478bc,0.4583333333333333
21359,1014e6be391084,a6acc709,The time does not show any particular pattern on the distribution of the transactions,46f9168f,0.4583333333333333
21362,37b09262279764,df45095b,"How does One-Hot Encoding work?<br>
- It extracts the all the <b>categories</b> and makes them columns. In our case Male and Female<br>
- Whenever there occurs a <b>Female</b> in the Sex column, it places <b>1</b> in the <b>Sex_female</b> column and <b>0</b> in the <b>Sex_male</b> column<br>
Link to learn more about One-Hot Encoding: https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science",37c4c417,0.4583333333333333
21364,fdc3afd309b850,e8f60d5c,"Here we will try a few combinations of addresses to try to find the correct geo location. We will try sometimes the Address, sometimes the AR plus city or plus federative unit. Aways prioritizing sending more information for the geocode. ",966bde38,0.4583333333333333
21378,923e97b05be00b,17902363,"Great -- we got it all moved. Now, we will start to use **fast.ai**. This next bit of code loads the images in fast.ai, resizes them to be 256 x 256 pixels, and shows them to us. If we have a more powerful computer we can make the images a bit larger. We'll use 80% of our data for training and 20% for validation.",3a4a22dd,0.4583333333333333
21386,a3ae04b78e45b5,3ff4ab63,**JOINT PLOT**,4195da8b,0.4583333333333333
21390,869a39a3d4dea2,b19e7ee3,"There are versions to approach this and we would consider based on the scenario or problem we are working with
* Open cv - addition and substraction - which limits to 255
* numpy - addition and substraction - which uses modulo to rotate it back from the begining

we are going to see the explore these two below",9020daf8,0.4588235294117647
21391,e93a41c03638fe,de6bd3fe,# 3.1 Text classification with LSTM:,7363527b,0.4588235294117647
21392,738bfced935b69,a7ac2d99,Most data by tax = 0 are engine size between 1 to 2.2 & date of manufactured between 2009 to 2018.,2d3c592d,0.4589041095890411
21395,bcd7e398c4d0ec,aae7fa26,## Size,77a143f6,0.45901639344262296
21396,2105f2c5132866,f92708f5,"# Data Preprocessing for Model
1) Drop null values from Embarked (only 2)

2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with)

Variables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'

3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. 
We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).

4) Impute data with mean for fare and age (Should also experiment with median)

5) Normalized fare using logarithm to give more semblance of a normal distribution

6) Scaled data 0-1 with standard scaler",bfe8023d,0.45901639344262296
21398,918040fad252ec,40d822e9,Memuat model ke model.fit_generator,966fcd8f,0.45901639344262296
21400,087e21401d7dfc,d0b06dd4,# Classification - Predict Gender By Height & Weight,42000489,0.45918367346938777
21401,bdf23d2d396916,e66ada78,### TRAIN | TEST ,b0e45a49,0.4594594594594595
21402,993966a1cb5eb1,5875f2da,## Create Model,5cd181e2,0.4594594594594595
21404,ccabe7a86825ce,ca2b5e97,**For ORD_5 use the Label encoding**,d766cbf9,0.4594594594594595
21407,0ea937ba28c08d,c4d2ab34,## Categorical features,03057d2c,0.4594594594594595
21412,e1fff2f67cbe32,533252bf,### Correct Answers by Content,c6dfde64,0.4594594594594595
21413,62037c5832129c,5d34da9b,## Dealing with class imbalance,61474350,0.4594594594594595
21415,63b44c85e32c1f,695f986c,"**insert(x,y)** inserts but does not replace element. If you want to replace the element with another element you simply assign the value to that particular index.",fb9b9562,0.4594594594594595
21417,ac9b48d531bad9,7f971723,# COMPARING MODEL PERFORMANCES,95965e35,0.4594594594594595
21418,bbb3f4b76a4559,130a63b5,"By the way, fearture engineering needs a lot of variables (and RAMs). The next cell show variables with memory usage. If you find needless huge variable(s), you can drop it to exec  
del var",75185823,0.4594594594594595
21420,d5f78aa381f58d,8400fe90,# Modelling,d60f358f,0.45977011494252873
21426,91eaec994e0c6f,e47cb05e,"- Sales average is increasing over years.
- Weekends correspond to the highest sales average.
- Sales average is almost the same over different months.",376aef10,0.46
21427,7dd46c750653eb,9babe5f1,"**Inference**

* Comparatively Female birth rate was less than Male Birth rate across the years

* Male Birth rate was highest in month of July and was lowest in month of February

* Female Birth rate was highest in month of July  and was lowest in month of May

",c2644713,0.46
21429,0687cd5c8597db,c868af6e,"<img src=""https://www.mathworks.com/help/deeplearning/ug/deep_learning_architecture600pixels.png""/>",4edec76a,0.46
21434,2ada0305b68956,5f981d46,### 77. Palette = 'autumn',133e26f4,0.46
21438,ac1abfe1dfe815,a26635c5,---,6529dbcb,0.46017699115044247
21439,a5a419dc7245b0,1bfe229e,#### Correlation Among Columns > 0.5,4279726e,0.46017699115044247
21442,4b4117cf42ef8d,ddfbfbbb,### 1- Ridge Regression,457cd6f4,0.4603174603174603
21445,8985a124d4b657,031b2ee6,"You can see below that, we predict the value for the first 3 values, then consider that output as one of the 3 values in the next set.
For example, we preedict 0.1 first, then we take that 0.1 as input in the second set and so on.",586d1846,0.4603174603174603
21449,5f27526aa6c113,1cd4ff47,most claims are from age group 20-30,a5c26ab6,0.4606741573033708
21453,71b75664517244,8bf80986,### Arsenal,fc905af5,0.46078431372549017
21454,7cfd96218dd933,97ec0920,"#### ATTENTION
* THE POINT WITH THE BRIGHT T31 (STRONGEST FIRE SIGNAL) AT THE HIGHEST VALUES LATITUDE: 36.82831
* THE POINT WITH THE BRIGHT T31 (STRONGEST FIRE SIGNAL) AT THE HIGHEST VALUES LONGITUDE: 28.23372
* THE POINT WITH THE BRIGHT T31 (STRONGEST FIRE SIGNAL) AT THE HIGHEST VALUES DATE: 2021-07-29
* THE POINT WITH THE BRIGHT T31 (STRONGEST FIRE SIGNAL) AT THE HIGHEST VALUES TIME: 07.43 FOR TURKEY",7c34d96c,0.46078431372549017
21456,842547b2def18c,155914df,"Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.",b8efde6d,0.46078431372549017
21457,fc8e0042411c46,97de9dc1,"**They have potential to make business from US, Middle East & Europe**",af476c2a,0.4608150470219436
21461,b61ab8f81dc03d,9560fe1c,"First we can fill the missing values for ""N"", that can means ""null"" or ""None"".",64d05394,0.46099290780141844
21462,957e035ba5b9d5,6ca89c94,"As we did above we will use the pre-trained VGG19 network. Only this time we will run the model on our training and validation data once and record the output in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.

The reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you're working on CPU, and we want to only do it once. 

Note that this prevents us from using data augmentation.",778ab3d3,0.46099290780141844
21463,396bc36edb95d3,2fac95e7,#### Feature Importance,965e4f8f,0.46111111111111114
21465,020c28a360b0cd,5c357ce8,# Make a Plan,2ba397f0,0.46153846153846156
21469,03048e86a6d806,aca25288,"Based on the plot above, there are still a lot of employers that have not applied ML methods to their business yet (which might also reflected on their budget for ML/Cloud computing services). For the employers that have well established ML methods, 410 of them have spent around USD 100,000 or more for ML/Cloud computing services.",1285c231,0.46153846153846156
21475,d07915a6e6992e,c01035e6,**Name**,2b912140,0.46153846153846156
21476,d78988cb5a1b02,c1c96239,# **Logistic Regression**,233f3a92,0.46153846153846156
21484,9b42412e75d640,a6761817,"I will eliminate 6 variables : fixed acidity, citric acid,residual sugar, chlorides,free sulfur, pH .These were values that had highest correlation with other features or the lowest correlation with the wine quality.",b616570a,0.46153846153846156
21485,d4c5aaa4b36810,136bbe95,"Now we need to adjust some values to per capita values. After that we can begin exploratory data analysis.

Factors that need adjusting to something per person are:
* Refugee population by country or territory of origin

Factors needed for modification that then need to be removed are:
* Population, total",65441f28,0.46153846153846156
21489,49ee86d074de69,77c7b9a3,"<a id = ""12""></a><br>
## Logistic Regression",71ccc6d3,0.46153846153846156
21493,c970849d1f6da2,55dd6383,"# Training fasttext model
* This uses skipgram model, but you can choose between skipgram, CBOW etc. Since the skip-gram model is designed to predict the context, instead of predicting word by context, I went with this because we can then match a single/few words to a document more effectively, because the embedding of the single word will be taking context into account. ",056e3955,0.46153846153846156
21495,09751c520b0616,d3966d2d,"It is strange to find that the house was sold in 2007 before the YearRemodAdd 2009<br>
- So we decide to change the year of sold to 2009",a4d0c7e9,0.46153846153846156
21496,4d91e84c564cbe,0c59f976,"## Interlude: objects

I've used the term 'object' a lot so far - you may have even read that *everything* in Python is an object. What does that mean?

In short, objects carry some things around with them. You access that stuff using Python's dot syntax.

For example, numbers in Python carry around an associated variable called `imag` representing their imaginary part. (You'll probably never need to use this unless you're doing some very weird math.)",355a43e3,0.46153846153846156
21510,866b157128a09c,5d3fd42b,## Crease Siamese Loader,0909f459,0.46153846153846156
21512,3e325daf577158,44720361,"## Test Transformations
Here let us see difference between original image and transformed image",c873dfec,0.46153846153846156
21513,5f4ae633cfd090,cac1f97c,"As B_odds increase, there are more ""Red"" winners. Same for blue, so it seems that, with the exception of a few outliers, all of the winner are the ones that are bet against. These seem like odds to lose rather than odds to win",a30a16e2,0.46153846153846156
21519,a8c042af6b7245,7720f63f,"#### Numpy 다차원 배열을 1차원으로 바꾸는 것을 지원하는 3개의 함수가 있습니다.

바로 ravel(), reshape(), flatten() 입니다.

참고로 ravel은 ""풀다""로 다차원을 1차원으로 푸는 것을 의미합니다.",2487ac62,0.46153846153846156
21525,6f1481148352e9,658c744d,"**The largest number of fires was in Mato Grosso - 7к, Rio - 3.4к и Sao Paulo - 3.3к.**",7cfbdb8f,0.46153846153846156
21526,5c6fcd59adac6e,a5ff0823,"911,474 prescriptions were written in the year of 2014 in the state of Alabama, how does this compare with the amount of death? How drastically does it vary by state?  ",f365ba13,0.46153846153846156
21527,f2f2db16a2f86c,ed6f15e1,### **Feature Engineering**,ffc6a115,0.46153846153846156
21530,3cb96bd8eb364b,7f6139ef,### Data Grouping and Enhacing,3157af7e,0.46153846153846156
21531,d0f6276d5b628c,9b45f3a5,#### Best popularity recommendation :,c64f5ce5,0.46153846153846156
21536,aae204e78a48d1,b777e5fc,"The attrition base seems to very closely reflect the portfolio, so it doesn't seem like the card type makes any difference to attrition

**Hypothesis 2: not proven**",53ab6133,0.46153846153846156
21544,669ce946943d60,a73f517c,## Model,0f63c4ce,0.46153846153846156
21546,5ce12be6e7b90e,68192c20,# Sequences,c0ab62dd,0.4619883040935672
21547,30fdc4a6e3c1db,803559ff,"We can see that we 10 unique values. So, all the years SNAP days fall on the sam 10 days i.e. 1st of every month to 10th of every month in CA",6111ddee,0.4619883040935672
21548,c4386b8a01d66e,bc5cdca2,# Organic_carbon,dc732bf5,0.46218487394957986
21551,510b8303776bb6,a7b7e769,## Line Plots for all continuous data fields ,18080db8,0.46226415094339623
21553,3dd4294f903768,9eda6df5,"So we have 12 different currencies. We have to treat each currency different. The currency rate was taken from www.XE.com.
We will convert each cost to dollars.",0d89d098,0.4625
21554,254cccd5145725,1b4ddd3d,# Outliers,a49b4037,0.4625
21559,21413205980558,b1c04ca2,# 3.5  Housing and Loan,84197de0,0.4626865671641791
21575,ac04ba639d1c93,31b6b0c1,"I use this great kernel to get x,y,z position. https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark",748059d5,0.46296296296296297
21577,5f32117bcd5255,756bb03f,#### TARGET INFORMATIONS,85882abf,0.46308724832214765
21578,726833f92fb87a,ff47200a,**Customers without a Housing Loan tends to accept the deposit.**,7dc5e1b6,0.46308724832214765
21580,169177b6e9edea,d9653f62,https://www.encyclopedia-titanica.org/titanic-victim/andrew-emslie-johnston.html,ca42152f,0.4631578947368421
21581,f91f58d488d4af,370fb374,"Now, we can calculate the accuracy for each of 3's and 7's, by taking average of all 3's and it's inverse for 7's.",5df1bbf3,0.4631578947368421
21582,c65a65d4041018,83ccb03a,## Multiple choice queations,824fb229,0.4632352941176471
21584,b10bd75889dad9,04044659,"#### Some useful insights from above heatmap:

- The numeric columns having strong correlation among each other

- Columns highly correlated with each other :

    - Postive correlation
       + 'onnet_mou_6','onnet_mou_7','onnet_mou_8','std_og_t2t_mou_7','std_og_t2t_mou_7','std_og_t2t_mou_8'
       + 'std_og_t2m_mou_6','std_og_t2m_mou_7','offnet_mou_6','offnet_mou_7','offnet_mou_8'
       + 'loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','loc_og_t2t_mou_7','loc_og_t2t_mou_8',
            'loc_og_t2m_mou_6','loc_og_t2m_mou_7','loc_og_t2m_mou_8
       + 'roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8',
",ee00ceee,0.4633333333333333
21585,514d8de15cb7ef,11238682,### Using TF-IDF Values,cfe111b2,0.4634146341463415
21587,8d0aebab1e5914,743b2f76,"# Find the co-variance matrix which is : 
                                                            A^T * A`
* Find covariance matrix of dataset by multiplying the matrix of features by its transpose 
* It is a measure of how much each of the dimensions vary from the mean with respect to each other


Covariance is measured between 2 dimensions to see if there is a relationship between the 2 dimensions, e.g., relationship between height and weight of students

* Positive value of covariance indicates that both dimensions are directly proportional to each other, where if one dimension increases the other dimension increases accordingly

* Negative value of covariance indicates that both dimensions are indirectly proportional to each other, where if one dimension increases then other dimension decreases accordingly

* If in case covariance is zero, then the two dimensions are independent of each other",084e671f,0.4634146341463415
21588,9c26c5dcd46a25,b9ba3371,"Nous devons ici **supprimer les variables fortement corrélées** à savoir : `sodium_100g`, `carbohydrates_100g` et `fat_100g`",1bbbb677,0.4634146341463415
21590,5169abdc647412,1cb37506,## Accuracy ,28efc68d,0.4634146341463415
21595,0b01138ad120fc,4c06bac4,**Fixing dimensions**,0b4b72e6,0.4634146341463415
21597,8cefb86a675e5d,0ae56b58,**Let us also display our target values:**,79f9e69b,0.4634146341463415
21606,ea4e559a86d613,2eef7727,"1. There are more males than females in the dataset
2. However, the percentages are ~ the same",eff47843,0.463768115942029
21608,7e89d387feb9f5,420ec0f4,### Добавленный числовой признак №9. Код страны,989e3a1b,0.463768115942029
21610,b01ee6cb674fa3,00cc1f46,"#  Japan Aerospace Exploration Agency -JAXA

The japanese space agency

JAXA is a fusion of the following preceding agencies: NASDA, ISAS, NAL 

Born by october 2003
",a8ffd35e,0.463768115942029
21611,a1a31459abf078,1fe39913,"Next we look at three attributes associated to a question - Task Container, Bundle_id and part - 
* **Majority of task containers have a very small size**
* Distribution of total question count across bundles has an uneven distribution with multiple peaks, biggest peaks are seen in the bins of 0-99,200-299 and 900-999
* Distribution of correct answer across bundles looks very similar to distribution of correct answers across individual questions
* We see that in our training dataset, **40% of questions belong to Part 5**. **Part 7 is the least common of all. The correct_answer rate across all parts ranges from 0.6-0.7**

## Prior Question Elapsed Time and Prior Question had Explanation",66fc0f54,0.463768115942029
21613,9d9da6c439b96b,d85c21e1,## Annual Number of Games Published,361cc7d9,0.463768115942029
21616,8ec771f5600a61,f8f9ed0e,# Handling the missing values Embarked,48364c1f,0.4639175257731959
21618,fc8e0042411c46,e663121e,## Specialization,af476c2a,0.46394984326018807
21623,99f84fa59cb1da,70b5ddf4,"Since the data is highly skewed we will perform a Stratified KFold cross validation, which preserves the target distribution in each fold (which helps to avoid overfitting)",41e95f63,0.4642857142857143
21627,f13534449a3750,7732c900,"**Results** (considering the whole dataset):

Ship:    0.001 (127777104) 

No ship: 0.999 (113446373040)",8b7f3332,0.4642857142857143
21630,6a80f915608fc2,92be41d5,### Which gs (features) have the most variation across the Target MoAs?,636938eb,0.4642857142857143
21631,f4514ec092a771,836962b9,"Similarly, evaluation data contain utt2spk and wav.scp file. ",3739ab1e,0.4642857142857143
21636,b290039151fb39,caa20a3c,## Loss,1836a79c,0.4642857142857143
21640,9c33d1955302bf,081724e5,# **KNN**,0d9cfc89,0.4642857142857143
21641,92e9fc3a0ff5c0,f3f189f2,## **Neutral comments with 'zero' Polarity and subjectivity**,d53da425,0.4642857142857143
21651,1c381451c17150,495488c4,"# Checkpoints and Custom Callback
We will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results.",e79b530f,0.4642857142857143
21657,bddd799cdbbae8,bf6a6a46, # <a id='5'> 5. Spliting the data</a>,b44e3c08,0.4647887323943662
21658,9bcfa825c8b2e6,850f4716,"Vücut Kitle İndeksi (BMI)
BMI değeri 30'dan küçükse şeker hastası olma ihtimali düşük ama 30'u geçerse yüksek.",220f36e4,0.4647887323943662
21661,3fb15e6e48aec2,9cf70f23,## Fare Median,9d1f4358,0.4649122807017544
21662,fe7360cddc13e5,248140ac,-----------------------------------------------------------------------------------------------,8979e423,0.4649122807017544
21663,b2e2c792b886ac,47ce8d1b,Запускаем обучение нашей сетки,2a184b39,0.46511627906976744
21665,d96642860ab3dd,6e2429b8,#### 2.1.1 Age column Missing values,98419d48,0.46511627906976744
21666,22bd95f4807a23,8a0b0df4,## On average how did each age group vote?,c05d356f,0.46511627906976744
21667,743ae010f5e875,03cef355,### Paths and Hyperparameters,02c54445,0.46511627906976744
21670,c09fac3c943d51,ee973d58,# Features,678d076d,0.46511627906976744
21673,2e40928927c0d4,0d82ae9f,**Split the train data into train and test(validation) set**,b6385ef2,0.46511627906976744
21675,1dd9c6aa74d289,040efaff,"## Summary Q1:
1. Like many skills, it will improve rapidly at first, and then the time required to improve will increase. Or instead, I like to see the level up of each grade as a non-linear growth.
2. The two climbing principles may not be completely equivalent in concept, but their curves are highly similar on a growth trend.
3. As you can imagine, the higher the level, the fewer people can climb, so does the error increases. Similarly, we have a smaller group of female climbers, and it makes the overall female prediction errors larger.
4. The difference on progress rate between men and women may exist or may be negligible, however, given the statistical error it has, it would be hard to say. In fact, it actually shows that the individual differences are greater than the gender differences.

- Caveat: Climbers who actively log their ascent activities on 8a.nu are probably going to be pretty good. This may be a high standard.",5ef9a1be,0.46551724137931033
21677,d42518f6cb0995,423b86c0,**Answers by content**,26913a9b,0.46551724137931033
21687,434f930cb58aee,5456f462,"After generaing data in required size and format, it is always nice to take a look at the data. Therefore we will use a simple function to display some images along with labels.",0e1d3554,0.46551724137931033
21688,ee23a565163388,5aa8f545,"**Inference**
- There is no significant difference observed in the two groups.",88aacbc4,0.46564885496183206
21689,2ada0305b68956,a6672e6c,### 78. Palette = 'autumn_r',133e26f4,0.4657142857142857
21692,1eb62c5782f2d7,0d314323,### Contoh 2,bb69f147,0.4657534246575342
21694,fdc9f4863744b1,236dfb72,There is not much correlation with commercial units in a neighborhood and sale price,b4529365,0.4657534246575342
21696,0a918602a04693,04296b19,"# **Information generated from EDA**


**High Churn rate**
1. Telecom users who don't have dependents
2. Telecom users who have fiber-optics internet
3. Telecom users who don't have online security, online backup, device protection, tech support
4. Telecom users who have month-to-month contract
5. Telecom users who have paper billing system
6. Telecom users who have electronic check payment system
7. Telecom users who have low tenure
8. Telecom users who have high monthly charges and high total charges",c1ef0e95,0.4659090909090909
21701,98a6794067932a,a1abd399,"La cellule ci-dessous permet également d'effectuer un changement au sein de notre dataframe afin de pouvoir utiliser ces données dans nos analyses par la suite. Ce code permet de prendre le dictionnaire créé dans la cellule précédente et d'associer l'abréviation à la colonne ""State"" selon l'état représenté dans l'index.",08600fe2,0.46601941747572817
21702,9169c4e9c33c90,15f9c823,"### Authors with Repeat Top 50 Appearances

Of the authors that made repeat appearances, how many total times did they appear?",725bf880,0.4661016949152542
21703,1294fb4c86f993,936a545b,"Figure above suggests that <b>Connecticut, California and West Virginia</b> stand as the hightest 3 states in guns regesteration growth from 1998 to 2020",4471e513,0.4661016949152542
21704,ba4b3bd184acbb,1ce6bb93,"We can see that the values are shifted over by one column probably due to this app not having a Category.

It also happens that for this row the Reviews column contains the value `3.0M` which is the same issue that the `floatTest` found.

Let's quickly fix it by replacing shifting the values over one column and replacing the missing Category with a null value.",0f5de724,0.46616541353383456
21707,6e472c6c591c7d,22f446b0,"For a hint or the solution, uncomment the appropriate line below.",65532a3d,0.4666666666666667
21708,d6ddbe57f59cf7,2bc667e4,# Scatter plot,504a3cda,0.4666666666666667
21709,4bada947d597ac,70cb639c,# Define Image generators to feed images into our CNN model,eab5094a,0.4666666666666667
21713,7a058705183598,3b15823e,2. Decision Tree Classifier,b0ead917,0.4666666666666667
21723,864302b10e7730,44d6a65a,# 1. Histogram (DistPlot),e9dd1d2d,0.4666666666666667
21724,9276fa5cc2fef6,54c22d67,## The Meta Features Based On Word/Character,24aa6a52,0.4666666666666667
21731,07d6ca51d43510,3ed4f586,> Here we get the maximum Silhouette Score at __k = 2__ but also has an appreciable at __k = 4__ also. So let's try with both of them.,38e74a14,0.4666666666666667
21733,6a1ae8234c7653,eb7cb8c0,I will start considering only some quantitative features with no missing values in the train data set:,2d643c72,0.4666666666666667
21734,f7436bc492474c,3d678f26,few empty comments are to be removed or else  sklearn will complain.,328fd235,0.4666666666666667
21741,051b118f751e77,c995a8eb,"# Multi-Model Training and Comparison
Let's initialize, train and test multiple models",9fad25fd,0.4666666666666667
21744,37b09262279764,e1cb45d8,### Splitting the data,37c4c417,0.4666666666666667
21746,2bd6c370695ea7,2b461896,## Sequences,cbe6aec8,0.4666666666666667
21749,3597174a998d4d,6d1993f8,"In this dataset, 4 variables can be used to describe customers' type. They are customer_type, market_segment, agent and company.",276892ed,0.4666666666666667
21751,91eaec994e0c6f,00509a32,### 2.3.2 Store Level Analysis,376aef10,0.4666666666666667
21754,80ecc4c67a9f54,6b98863e,## Last name frequency,4bbf546c,0.4666666666666667
21755,5b92c712910a11,35e16d9b,# Removal of Frequent words,e1d17100,0.4666666666666667
21756,b547f0f38f7744,324377b9,Show images with `bboxes`:,b6ba66b3,0.4666666666666667
21757,d58491f2896fc1,049431a0,**initilization_of_population Fonksiyonu random olarak bir popluasyon oluşturmaya yaramaktadır.**,514bfdff,0.4666666666666667
21759,62487bcd70b199,f341165e,## <a id='5.2.'>5.2. Normalise the data</a>,f6ae50af,0.4666666666666667
21762,f89f8540df580e,f14dd6ca,# Pipeline for prediction,83579ee7,0.4666666666666667
21773,7454fdc444df16,5d4568a3,"Let's now visualize what some of the reconstructed tissue cells look like by using the function above. 

Note: The function can be improved by having edges on the mask, rather than overlaying a color on top. This will help us preserve the true image of the tissue cells.",a7818ef5,0.4666666666666667
21775,42e0005bed28aa,8d817ef4,><h3>Dividing dataset into x(features) & y(target)</h3>,5616d451,0.4666666666666667
21780,892be0a523578c,4f3dcc93,"<div>
    <h1 id=""Process"" style=""color:#4e79a7"">
        Process
    </h1>
 </div>",b0e8d7c0,0.4666666666666667
21782,55a5e31d03df9f,20bb6930,"As you can see the syntax for constructing this model was a little different than we did before, this was due to the use of the **Functional API** instead of the **Sequential API**. Functional API may seem a little more complicated but allows more flexibility while creating a model, i.e. non-linear topology, shared layers, and even multiple inputs or outputs.

Now we used new layers while creating this model:
- Conv2D: This layer allows the convolution we discussed before. We here specify the number of filters that we will use, kernel size, and more. 
- MaxPool: The objective of the MaxPool layers is to reduce the dimensionality, for a kernel that we specify we compare all the pixels inside that kernel and leave only the max value. 
- Flatten: We already do the convolution and the max pooling, now this allows us to at the end create the dense layer that will classify our lego images. 

> Please notice that the trainable parameters for this model are around 1M, this is ~15X less than our initial MLP. 🤔 What do you think happens? Should it be performing worst or better than our previous model? Let's train and check it out.",06dce00f,0.4666666666666667
21783,6998861ff6ff01,52bd973d,"Now that our dates are parsed correctly, we can interact with them in useful ways.

___
* **What if I run into an error with multiple date formats?** While we're specifying the date format here, sometimes you'll run into an error when there are multiple date formats in a single column. If that happens, you have have pandas try to infer what the right date format should be. You can do that like so:

`landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True)`

* **Why don't you always use `infer_datetime_format = True?`** There are two big reasons not to always have pandas guess the time format. The first is that pandas won't always been able to figure out the correct date format, especially if someone has gotten creative with data entry. The second is that it's much slower than specifying the exact format of the dates.
____",ea9e72cf,0.4666666666666667
21784,e25c0f830df3f4,0487fd0a,# Displaying highly positive reviews,fdcf7189,0.4666666666666667
21788,979f1e99f1b309,5b4139ef,***THE plot show that the player who finished the game in advanced places destroyed more vechicle than other players***,d1bfebbf,0.4672131147540984
21795,c13f73168789c2,75b9f522,"## 4. Selecting top n smallest values of given column<a id='17'></a>
Syntax : `df.nsmallest(n, 'column_name')`",16175052,0.4675324675324675
21796,241cf32abb22d8,a12d4d7f,The AUC score for the top 10 features selected by F-Score is 0.812.,47157066,0.4675324675324675
21797,75adb7945ef9bd,92c291ed,"## 7. Most frequent words and bigrams

What are the most common unigrams (single word) and bigrams (two-word sequence)?",785c5095,0.4675324675324675
21804,ad26c020235dfc,6dcb8ac1,Scale data:,bf766e48,0.46774193548387094
21806,5ce12be6e7b90e,f05642b0,"## Strings

Strings are ordered collections of _characters_. 

_Ordered collections_ means that elements are numbered with _indexes_: 0, 1, 2, 3, 4...  
Note that the first index is 0, __not__ 1!

We can create new string usings single- or double-quotes: `'` or `""`.",c0ab62dd,0.4678362573099415
21807,30fdc4a6e3c1db,881a73f3,For TX:,6111ddee,0.4678362573099415
21809,3f25b363afec54,b5f611a2,"So 15,714 patients have 1 entry, 5878 have 2 entries and so on.

- Now let us look at the number of health camps in train data and their distribution",bbdaae25,0.46808510638297873
21810,5f674175839b32,019b2046,# Visualizations,53a2e343,0.46808510638297873
21821,eda49464dd6d1b,54fa0790,"<font size=""+3"" color='#540b11'><b> Data Modelling and Evaluation </b> </font>",8421f81f,0.46853146853146854
21824,c85c94076e9c3a,5a13d8e8,# Exploratory Data Analysis,3ea0c443,0.46875
21827,69130a37583a06,afc67484,"### Feature card4-card6 distribution :

* As we can see Visa cards are dominating the transaction record.
* Mode of the trasanction is debit from 75% of the data.
",65a4de1c,0.46875
21828,93f5423667b9d5,0f289ddc,# 日本語名に変換,55bdf071,0.46875
21831,2c3a6969252dc0,bd3d65bd,Students with better LOR tend to have Higher SOP.,d30f10ce,0.46875
21833,96c4c0e36b8ec0,d6cde323,**Split up the data between the people who survived and pershied**,4dd6de8c,0.46875
21838,2a724fb7835cdc,9c9e06bc,**Numerical Feature Extraction**,c38ac61d,0.46875
21839,49f2274c1dd516,d4d1098f,"## Coders Against Covid
Crowdsourced map of testing locations across the US.",06b0ffee,0.46875
21841,5e02999ca74e7e,55b5e485,"# **Encoding, Splitting, Modeling**",b69da28e,0.46875
21843,6f05f4ea9addbf,36f342fc,"# Exploratory Data Analysis

Let's analyse, plot and work on different aspects of the project",dfb04c84,0.46875
21845,ac1abfe1dfe815,e9cd1b64,"### What is the reason behind the negative experiences?  
it's customer service, in all of the airlines except for Delta, Late flight.
As before, Virgin America has the least negative reasons, while US Airways,United American is the highest.",6529dbcb,0.4690265486725664
21847,4883314a96dc34,ed00df0b,---,50d36836,0.4691358024691358
21856,ffd1df95ca5289,1452db8d,we can see that there is high chance of a person to not response if a person is not frequently contacted,db00c338,0.46938775510204084
21859,2343dc02ffb96a,385bcbca,# Explore the data with a Scatterplot + Density Plots,29aa95a4,0.46938775510204084
21862,e69a496109e7d8,013487e1,"The above plot shows that 57% people who had not survived also had **No.of.AxillaryNodes** count <=5 but it is low compared to survived one. And you can also see there is slight increase in probability between range 20-30, where there is no increased probability in survived plot",1c640591,0.46938775510204084
21865,f166950fa915f8,0974f51a,### Label Encoder ,a7f6ca5e,0.4696969696969697
21867,726833f92fb87a,e9a6b1f2,## Day vs campaign success,7dc5e1b6,0.4697986577181208
21868,4daf6153275cbf,0871549b,#### 2. Prices,51db1961,0.46987951807228917
21869,835a7b4e660d23,2afe9c66, ### Concatenating Data,53bc7a6e,0.46987951807228917
21870,83df814455f06c,19b7d46b,"# **12. Feature Engineering** <a class=""anchor"" id=""12""></a>

[Table of Contents](#0.1)


**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.


First, I will check the data types of variables again.",c9cff71a,0.47
21872,b10bd75889dad9,66b91ea6,"#### Some useful insights from above heatmap:

- The numeric columns having strong correlation among each other

- Columns highly correlated with each other :

    - Postive correlation
       + 'total_og_mou_6','total_og_mou_7','total_og_mou_8','std_og_mou_6','std_og_mou_7','std_og_mou_8'",ee00ceee,0.47
21874,49ee86d074de69,cd720b84,### Create Targets,71ccc6d3,0.4700854700854701
21882,c65a65d4041018,c946056b,### Where do DS get useful information?,824fb229,0.47058823529411764
21883,410285582f4f7e,f01ca72b,"**Age and Gender**

Now we will explore to know, genderwise split among the users through number of males/females and average age of users in each category. 

* Average age of the users hovers around 35. 
* There are more females users than male. ",d026266b,0.47058823529411764
21885,1a0bd2f72bbe36,b4afff28,# Let's do some Analysis:,2fa311dc,0.47058823529411764
21886,c3dfa835621ac4,1556abb0,"# Implementation of CA

OK, now let's create the transition(or step) function of CA based on the filters above. A CA is a list of rules (first rules first matched; once get one matched, skip the rest)",0126bdad,0.47058823529411764
21888,b779c3ce7b671a,9307bbfc,"## Generate planet position history

This codes updates the planets orbit over a number of iterations, storing position and velocity values for animations, orbit plots and velocity graphs.",ca778770,0.47058823529411764
21893,a0a5baa6c7e12a,7901861d,"# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">Call for Action: Let's get the hands dirty</div>

The sections below demonstrate the source code of the express EDA experiment that lead to the insights collected above.

Executing the source code in the sections below will lead to generating the charts used as images in the previous chapter.",551d41de,0.47058823529411764
21894,d0080e3a39bc5c,96f390fc,"Now coming to the different model architectures used, I used the following architectures - 
* Resnet34
* Resnet52
* Resnet101
* Resnet152
* Densenet161
* Densenet201
* SENet154
* ResNext101_64x4d

Among the models, Resnet 152 gave the best performance individually, with the validation F-Score reaching 0.9807 after some fine-tuning.",2fcde4cf,0.47058823529411764
21896,871901bb4aae21,f3ca223e,# One Hot Encoding,79ab34a9,0.47058823529411764
21897,9cec5ddf8b6f49,fe0a28e0,"# Observations - 
* There  are 300000 training rows  and 200000 test rows in the data
* There are 10 categorical columns 
* There are 14 continuous numerical columns
* No NULL values in the data
* No duplicate values present
* The unique category values are **same** in the train and test dataset
* Columns Cont0, Cont6, and Cont8 have outliers - we cap the ouliers at the upper / lower bounds
* For the categorical variables we map the values to numbers ",d39fc8e7,0.47058823529411764
21899,4fa553c2b837d4,d22ccc30,## Soln 1 : Drop columns with missing values,c65a23e9,0.47058823529411764
21901,52cfd66e9ec908,e5f06ffd,"Here we can extrapolate that the variables **centroid_x** and **centroid_y** have strongly negative correlations, and the strongest correlations are between **extent_z** and **extent_x** more than any other, coming in at 0.4. We can also try using an XGBoost/LightGBM model as kkiller has demonstrated in his brilliant kernel as an alternative approach to the problem.",c74adcdf,0.47058823529411764
21902,917957c6c4065f,a7c9ff9f,조회수 상위 3개를 살펴보겠습니다.,55b8ed68,0.47058823529411764
21905,0e662a463309e7,56474a48,Tfidf Vectorizer,84c57e51,0.47058823529411764
21909,523123dad03177,5e86f3b8,The average math scores are 66 out of 100.,48a5e4e6,0.47058823529411764
21913,513ce405d7f6a3,5cec3ef1,# wordcloud of Very very postive review,8461e086,0.47058823529411764
21916,4ae6a182abac64,0d3840b6,* **Missing values in train data**,418676c5,0.47058823529411764
21921,e4c6dd957eb5ce,1ddf69f8,"## Performance Tier Distribution at Kaggle

We have total of 3.4M unique registers in Kaggle. <br>
Before we go further at the timeseries to see the registers by the time, Let's see the Performance Tier Distribution;
",2e383665,0.47058823529411764
21926,1d1598b6fa2aa7,626b0c54,"```plotly.graph_objs``` figures support an ```update_traces()``` method that may be used to update multiple nested properties of one or more of a figure's traces.

Also ```plotly.graph_objs``` figures support an ```update_layout()``` method that may be used to update multiple nested properties of a figure's layout.

More information about Bar charts - https://plotly.com/python/bar-charts/


",e066accf,0.47058823529411764
21932,16ca1123840e9f,2f7d3895,## Company wise Status of Rockets,e8b8f086,0.47058823529411764
21933,8f50c9c16db95f,7c20fe2e,"# Initial analysis - How to kick the ball far <a id=""initial""></a>
The next question is how to kick a ball long distance, which should be a skill for kickers to handle. Before diving into what the data reveals to us, let's re-cap some physics at first. The whole process of kicking a ball is a transition of energy from the kicker's body to the ball. The energy exists due to the motion of an object is known as **Kinetic Energy**. Here is its formula ([4](https://en.wikipedia.org/wiki/Kinetic_energy)) -
> **KE = 1/2 * m$v^2$**

inidicating that, given the mass of the kicker's body is unchanged, **the higher the velocity of the kicking leg, the more energy it will be generated and transmitted to the ball**, which contributes to a longer kick length. 

Based on the knowledge above, here we come up with two assumptions -
* The part of the body contacting the ball is the kicker's foot, so ultimately, **is the speed of the kicking foot a useful predictor for kick length?** 
* **Does knowing the body orientation makes a difference in terms of the kicking length?** - Assuming since much more angle pivoted of the body will absorb more energy from transmitting to the ball


By using multiple linear regression, we can see from the output that 

* The **kicker_s_50**, i.e. the median of the kicker's speed, which we use as a proxy of the foot speed in this case, is positively correlated with the kick length. It indicates that comparing two kicks that the ball was being kicked to the same direction, the one kicker has a faster speed by 1 yard per 0.1 second, its estimated kick length is **~7.1** units longer than the slower one.

* The **direction of the ball being kicked to** is also postively associated with the kick length. The result implies that comparing three kicks with the same kicker's foot speed, if the ball was being kicked to the right, the kick length is estimated to be shorter by **~6.8** units, **~5.2** units than to the center or to the left, respectively.

* The model fit above has an R-squared value as 0.257, which indicates that **25.7%** of the variation of the kick length can be explained by the two independent variables, kicking foot speed and direction, together.",26cc763a,0.47058823529411764
21934,b01ee6cb674fa3,20f817aa,"# Northrop

The Northrop Corp. was founded by 1939 and merged with Grumman by 1994, forming Northrop Grumman, an
>> American global aerospace and defense technology company. With 90,000 employees[4] and an annual revenue in excess of $30 billion, it is one of the world's largest weapons manufacturers and military technology providers.[5][6][7][2] The firm ranks No. 108 on the 2020 Fortune 500 list of America's largest corporations

In the dataset situation, the company name is shorted for Northrop.

As a curiosity, Northrop is responsible for some iconic airplanes, sucha as the P-61 Black Widow, F-5 Tiger and the B-2 Spirit, the stealth strategic bomber",a8ffd35e,0.47101449275362317
21936,0e2a23fbe41ca9,404cd551,"### 3. Location ID cols

city_id, state_id",64e4762c,0.47101449275362317
21937,2f47abddfd1928,0869b2d2,"In this case 1st class shows the biggest range by far, 1st class tickets are the most expensive ones as expected. And also the most variable as they are luxury tickets I suppose they could be more customizable.

The second class shows to be cheaper than 1st and also more concentrated in lower cost with just some tickets falling in 1st class usual values.

The 3rd class is the closest to zero in terms of cost as expected.

All three classess shows quite skewed distribution, being the 3rd one with high skewness very close to zero.

Again we can confirm the correlation between Pclass and fare looking to the distribution.",ae33cc0b,0.47107438016528924
21939,99bf357eaf61f1,d2f5ad3b,"#### Housing Price vs Sales

- Sale Type & Condition
- Sales Seasonality",9d92fafe,0.47115384615384615
21944,d5f78aa381f58d,8eadc36c,"In this notebook 5 different machine learning algorithms will be evaluated on the dataset for prediction analysis: 

1. Logistic Regression (Logistic)
1. Naive Bayes (NaiveBayes)
1. Classification and Regression Trees or CART (REPTree)
1. k-Nearest Neighbors or KNN (IBk)
1. Support Vector Machines or SVM (SMO)
1. Random Forest and Desion Trees
1. XGBoost

Each algorithm will be evaluated using classification accuracy, to measure the performance of each model. First step in the data modelling is to label the dataset with X (matrix of independent variables) and y (vector of the dependent variable). Then create an instance of the model to train and fit the model, then calculate predictions of test set in order to get the classification report.",d60f358f,0.47126436781609193
21947,38b79494ac749e,672b1973,"<img src=""https://scikit-learn.org/stable/_images/grid_search_cross_validation.png"" width=50% />",39162a40,0.4714285714285714
21948,2ada0305b68956,7d3b4771,### 79. Palette = 'binary',133e26f4,0.4714285714285714
21954,fe6750354fb64f,8f41342f,## Predictions using Machine Learning,271741f0,0.4714285714285714
21955,e19e307b3fd188,533f2303,Whether or not to accept animals in the price has a small influence on the increase the **rent amount**.,2173955b,0.4715447154471545
21957,0ad8d416b89b78,06aabd65,"A number of attributes were identified as redundant. This was either due to the attribute providing no meaningful insight into the classification task, not being understandable from the analyst’s perspective, or already describing a feature that was also described by another attribute. Overall this should reduce the dimensionality of the dataset.",0b0562f0,0.4716981132075472
21958,23df07a474aaae,57183121,**Linear Regression**,0ea40276,0.4716981132075472
21964,614ba9f0c62677,6a98dc06,"<a id=""6""></a>
### Same Padding
* As we keep applying conv layers, the size of the volume will decrease faster than we would like. In the early layers of our network, we want to preserve as much information about the original input volume so that we can extract those low level features.
* input size and output size are same.
* <a href=""https://ibb.co/jUPkUp""><img src=""https://preview.ibb.co/noH5Up/padding.jpg"" alt=""padding"" border=""0""></a>",b8551335,0.4716981132075472
21967,312135b445bd23,9bac7bef,"The visualization of the word vectors of the 400 most frequent words makes sense.
We can use those vectors to find synonyms, or related terms, for each input word (or phrase) by comparing its word vectors to the whole corpus vectors using cosine similarity, or any other vectors similarity functions.
Let's see some examples:",8ced381f,0.47191011235955055
21969,04ff2af52f147b,98428b58,"**Create FamilySize Feature:**

The features *SibSp* and *Parch* are highly correlated and do not seem to independently provide valuable information.  As a result, we can improve our model by combining these two features into a new *FamilySize* feature which denotes the number of family members the passenger is travelling with, as well as themself.",d5f37be9,0.47191011235955055
21971,cf39cde80e66b7,5db4a78e,"![](https://miro.medium.com/max/978/0*7RxO773DPeY8IYeD.png)
source: https://www.geeksforgeeks.org/ml-mathematical-explanation-of-rmse-and-r-squared-error/

> MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. The MSE is a measure of the quality of an estimator — it is always non-negative, and values closer to zero are better",aed4bc9b,0.4722222222222222
21974,ab6da5994949a3,0997d8af,## Logistic Regression Test Results,fae6b91d,0.4722222222222222
21986,f18e737fcc4b06,93250c18,## Numeric Variable,087b8637,0.4722222222222222
21989,396bc36edb95d3,b7e1f869,### Model Evaluation - CART,965e4f8f,0.4722222222222222
21992,18a864b56ac3b8,5d08c531,"# Second Verse same as the first!

For the first phase, we reached an accuracy of about 85%, but not to the goal yet, the images for this training set are less than a quarter of their original size. Now the model will be trained again, this time on the images that are about half size of the original images.",f3ca0a7c,0.4722222222222222
21995,1014e6be391084,a1615f57,Removing outliers,46f9168f,0.4722222222222222
21996,166a62ebb4fc3a,08e91296,"Looking at the above plaot, we can verify the presence of **multicollinearity** between some of our variables. For instance, the **radius_mean column** has a correlation of **1** and **0.99** with **perimeter_mean** and **area_mean** columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation. Therefore we should only pick one of the three columns when we go into further analysis.

Another place where **multicollienartiy** is apparent is between the **""mean""** columns and the **""worst""** column. For instance, the **radius_mean** column has a correlation of **0.97** with the **radius_worst** column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its **""mean""** and **""worst""** columns. 

This is somewhat inevitable, because the **""worst""** columns are essentially just a subset of the **""mean""** columns; the **""worst""** columns are also the **""mean""** of some values. Therefore, I think we should discard the **""worst""** columns from our analysis and only focus on the **""mean""** columns.

In short, we will drop all **""worst""** columns from our dataset, then pick only one of the three attributes that describe the size of cells.

Similarly, it seems like there is **multicollinearity** between the attributes **compactness**, **concavity**, and **concave points**. Just like what we did with the size attributes, we should pick only one of these three attributes that contain information on the shape of the cell. I think **compactness** is an attribute name that is straightforward, so we will remove the other two attributes.

We will now go head and drop all unnecessary columns.",db48a079,0.4722222222222222
21999,ce9ed5e2d601d7,0292011b,## Basic neural network blocks,f58a2f43,0.47244094488188976
22005,f0fab078f8533b,9b3be770,### Below we can see the data prepared for analysis,bdb5ea32,0.4727272727272727
22008,63b44c85e32c1f,a73d516a,**pop( )** function return the last element in the list. This is similar to the operation of a stack. Hence it wouldn't be wrong to tell that lists can be used as a stack.,fb9b9562,0.47297297297297297
22009,e4525eb0c96f28,89ca513e,"### Sales by Country over time

After visualizing the plot, we can now come to conclusions on some of our previous hypotheses regarding country. We also explore some of our observations.

For our hypotheses on Japan and the US, we can observe from the graphs that the significance of being one of the two top game producing countries is that the threshold for sales is FAR higher than 3rd place, Europe. In terms of sales growth over time, both the Japan and US are fairly stagnant.

We can also look further into our observation on South Korea and Finland. Both of them have very few values, and therefore the data isn't well represented above. Therefore there is no real conclusion to be gained about how South Korea and Finland are at producing selling games.",2093a1f1,0.47297297297297297
22010,27d5291d6365ba,fe1f42ea,"# Mean Transaction by Age
",96b30229,0.47297297297297297
22011,62037c5832129c,f476a619,"**What is Class Imbalance**
* Class imbalance is a quite common problem when working with real-world data—samples from one class or multiple classes are over-represented in a dataset.
* The breast cancer dataset has 90% healthy patients. If we guessed all the patients to be healthy our accuracy will be 90% but we are not learning anything about the features.
* How do you deal with Class Imbalance. There is no one way to solve this problem:
    1. UpSampling the minority class.
    2. DownSampling the majority class.
    3. Generation of synthetic training samples. - **Synthetic Minority Over-sampling Technique (SMOTE)**",61474350,0.47297297297297297
22012,0caaec057f7184,9601d1e7,test items not in train data (False) = new lauched items,b875533e,0.4731182795698925
22013,ee23a565163388,cb96a23f,## **Is a particular gender more prone to heart failure**,88aacbc4,0.4732824427480916
22018,4c47839b067546,f61692e0,### color,1f517b02,0.4734042553191489
22023,757fa8de4edc4c,a9ba7975,*Calculate Distance between the atoms in a molecule*,87211008,0.47368421052631576
22025,d81d3830152f88,92bcb768,"Control:
- there are 2045 instance in the control group
- Proportion of instance where a team that has a lower or same 3pt-fg than its opponent won is 59.8%",9551eac9,0.47368421052631576
22030,fe7360cddc13e5,a7db6af5,"Buraya kadar modelin dağılımlara bağlı parametreleri olan r,α,a,b değerleri kullanılmamıştı. Başta verdiğimiz likelihood fonksiyonlarının maksimize edilmesi ile bu en iyi parametreler tahmin edilebilir. Ve elde edilen parametrelerle yukarıda elde edilen ham denklemler geliştirilir. Ve geçmiş davranışlardan yola çıkarak örneğin bir müşterinin gelecekteki transaction sayısının beklenen değeri aşağıdaki gibi ifade edilir:",8979e423,0.47368421052631576
22031,31b564f11ef638,9511cee1,### Check final features and types,424f9692,0.47368421052631576
22032,d8fb26c4197325,7ca2ad7e,# xgboost,b190ac50,0.47368421052631576
22034,caa0ce2715bf34,28464479,"### Observations
1. The missing values are present in the fields : CREDIT_LIMIT and MINIMUM_PAYMENTS.
2. I will use median() to fill the null fields.",78a5dc51,0.47368421052631576
22035,52ee792e228d54,cf94d12f,"### Lets take care of the negative values in Experience now. Since we have a strong linear relationship between Experience and Age, let us build a linear model between them and then use it to predict the values of negative experience using Age.",5096094e,0.47368421052631576
22043,f35ee6e9fab592,3ecc0d58,Here is the dataframe of publishers and the number of video game titles under their belt,b15f7073,0.47368421052631576
22058,f14f6708035916,385cbae4,"## Pie Chart Sectors Spent
How much money did I spend in each sector.",ca22d04b,0.47368421052631576
22060,a1dcd92986bc84,ca9e89d9,"## Implement the dual encoder

To calculate the loss, we compute the pairwise dot-product similarity between
each `caption_i` and `images_j` in the batch as the predictions.
The target similarity between `caption_i`  and `image_j` is computed as
the average of the (dot-product similarity between `caption_i` and `caption_j`)
and (the dot-product similarity between `image_i` and `image_j`).
Then, we use crossentropy to compute the loss between the targets and the predictions.",730acaaa,0.47368421052631576
22065,bef2347846e476,bc0dcbfc,To see the content of the files we use head() function with parameter 10.It gives us the first 10 rows of the file from the starting of the file.,cb93bf51,0.47368421052631576
22071,2a123b4e8f9433,7e70ffa3,# Build and train Decision Tree Model,0a082218,0.4742268041237113
22075,80ad12f326ab70,5b9a15c6,* ### Insights on Districts data,da404a16,0.47435897435897434
22079,3c2033cc99c12c,2074309b,"**Note:** *As the pca could explain the main variance of the features, I choose to use a dynamic table to represent the ratio of different compnent contributes*",dfa22a54,0.4744525547445255
22080,c80939c7c626cf,cdb3bc83,"# Until 16 yrs of age there is high chance to survive
we can use limits in above code to see the exact age range of survival",b9ac31e2,0.4744525547445255
22081,2a56d6b0e153f2,95b14af7,"IN THIS GRAPH , MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 62%- 65% IN THEIR DEGREE RESPECTIVELY.",8dc315e6,0.4745762711864407
22087,ed8009f482b380,8a1915dd,## Deal with the imbalanced data,e99941fa,0.4745762711864407
22088,a81661cc35d8d2,15f48925,"<b><font size=""4"">Methodology</font></b>",3331f113,0.4745762711864407
22094,1294fb4c86f993,de3dea60,### Research Question2: Which states have the hightest regestered guns from 1998 to 2020?,4471e513,0.4745762711864407
22098,9f0ccf5b9e8f03,f0958bbb,"#Statistical analysis

The Student's t test, the χ2 method, and Fisher's exact test were done when appropriate for statistical analysis to compare continuous and categorical variables. A p value of <0·05 was chosen as cutoff for significance. Data were analysed with SPSS (version 20.0) and GraphPad Prism (version 5.00 for Mac). The study was approved by the Bergamo Ethics Committee (registration number 37/20, 25/03/2020).",66691203,0.475
22101,5ffe6aa38958a1,06677815,"### 2.3.1 Sex - Numeric value

New series with numerical value of ""sex""",11f5412e,0.475
22103,0d58c434c7db1e,1a0aab30,"Rapid rating - For games 10 minutes and longer.

For bigginer 800 is a good rapid rating. 

for a intermediate, 1300 is a good rating. 

For a expert, 2000 is a good rating",517e01d3,0.475
22107,5626e84c4e6bf8,96f24a3f,"## Heatmap for performance of neurons
We will use *act_res* to generate a heatmap which indicates the neurons which perform better than others.
**Colour given to a neuron represents the number of times it has been winner. Lighter colour shade is directly proportional to this frequency of winning.**",e2ecb669,0.475
22109,1011899b959f44,48761461,"# Summarize
Summary methods in Pandas include:
* .sum() - row sum
* .mean() - row average
* .median() - row median
* .std() - row standard deviation 
",0b112382,0.475
22111,254cccd5145725,29b53106,"Outliers are the values which are really from the distribution of the data. We have to remove these outliers as they affect our Model. There is only one outlier in this data i.e on the rez_esc column and acorrding to the answer from competition host(https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403), we can safely change the value to 5.",a49b4037,0.475
22113,9a040a4f21091e,f84b51cf,So we can see that some non-toxic words snuck into the toxic word list and vice versa.,f591b57d,0.475
22117,56785caebaa256,4f11ad0e,"## 4.2. Statistics<a class=""anchor"" id=""4.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.475177304964539
22118,957e035ba5b9d5,ca39a3c6,"We will set the `class_mode` to `None`. The generator will only yield batches of image data, which is useful to use with `model.predict_generator()`. This means that the generator will only have batches of data and no labels.",778ab3d3,0.475177304964539
22123,0858e1bb3cbaca,a5f8f280,"Similarily, we can get the average data by using

**.mean()**",78548374,0.47540983606557374
22125,eda49464dd6d1b,4b27df42,# Random Forest Classifier,8421f81f,0.4755244755244755
22138,60d500d196eb42,ae17cb7e,### Let's check 1st file: /kaggle/input/Movie-Ratings.csv,2ad55f3f,0.47619047619047616
22140,898d18d501f68d,44b3313d,"in the above box plot for all 7 types of cover type we can observed the mean, median, quartile w.r.t 1st 5 columns",d8bdea2d,0.47619047619047616
22144,87e94f864d74be,9733c8c7,"### Let's Generate new columns based on variables
#### Year and month ",294bfe9f,0.47619047619047616
22146,04bac111ffbe9c,5bb10cd2,"3. Perform OHE on Pclass_new, Sex, Embarked and Title",82576b17,0.47619047619047616
22147,b74076b2f8ba1d,dff6be12,### Let's check 1st file: ../input/IMDB-Movie-Data.csv,9ace22d4,0.47619047619047616
22154,55339ceb40d5e9,728d5fe5,"# Feature Engineering & Feature Selection
* date -> extract year and month
* title -> extract length
* subtitle -> whether contains subtitle or not",d0f687dd,0.47619047619047616
22159,6f4795cfdc96c7,a8b12b64,### Let's check 1st file: ../input/online-job-postings.csv,1f3ab82f,0.47619047619047616
22160,e16860fce156b0,3410c0e5,"#When x and y are one each of type numerical and categorical, it generates a box plot per category and a multi-line chart:",2054f1ce,0.47619047619047616
22164,6471597c5d2f66,3a60eea4,### Let's check 1st file: ../input/kiva_loans.csv,a41b4abe,0.47619047619047616
22165,659f5f3ef8aa0e,f6642b81,### Let's check 1st file: /kaggle/input/supermarket_sales - Sheet1.csv,3654c2d0,0.47619047619047616
22177,06ecf7a304c309,aeb419db,"#### Noisy Images

우리는 의도적으로 이미지에 노이즈를 추가할 수 있습니다.
이미지를 보완하는 imaug 패키지를 이용하여, 반대로 이미지에 노이즈를 만들 수 있습니다.
노이즈에는 다음과 같은 것이 있습니다.

- Salt and Pepper Noise
- Gaussian Noise
- Periodic Noise
- Speckle Noise

여기서는 impulse noise라고 불리는 Salt and Pepper Noise를 사용했습니다.
이 노이즈는 선명하고 갑작스러운 노이즈를 만듭니다. 희소하게 검정/흰 픽셀을 만듭니다.

원본 커널에서 노이즈에 대해 수정해주신 @ColinMorris에게 다시 감사함을 전합니다.

Thanks to @ColinMorris for suggesting the correction in salt and pepper noise.",714de627,0.47619047619047616
22180,916ccf243827f1,731171a8,## 7. Activation Function,5147f4d2,0.47619047619047616
22184,5f32117bcd5255,14cbe46b,#### WORLD COORDINATE SYSTEM,85882abf,0.47651006711409394
22187,c84925c8171900,fd7009a7,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.2.2  Year Wise Video Game Sales
            </span>   
        </font>    
</h4>",e21ff7ec,0.4766355140186916
22188,b10bd75889dad9,46f1db2e,"#### Some useful insights from above heatmap:

- The numeric columns having strong correlation among each other

- Columns highly correlated with each other :

    - Postive correlation
       + 'std_ic_mou_6','std_ic_mou_7','std_ic_mou_8','std_ic_t2m_mou_6','std_ic_t2m_mou_7','std_ic_t2m_mou_8'",ee00ceee,0.4766666666666667
22189,c09fac3c943d51,e2f1d7e0,Based on strange things in dataset:,678d076d,0.47674418604651164
22192,c115e287523aab,185fd888,## Test,feb1288b,0.47692307692307695
22195,03048e86a6d806,922b0a29,### Number of People Responsible for Data Science Workloads in The Company,1285c231,0.47692307692307695
22196,d07915a6e6992e,3e0f73c2,**Title**,2b912140,0.47692307692307695
22199,f2f2db16a2f86c,9c65ea4f,**Modifying the Features to get a Better Model**,ffc6a115,0.47692307692307695
22201,a8c042af6b7245,d086ec23,"#### Checking the cardinality of the categorical variables

Cardinality refers to the number of different values in a variable. As we will create dummy variables from the categorical variables later on, we need to check whether there are variables with many distinct values. We should handle these variables differently as they would result in many dummy variables.",2487ac62,0.47692307692307695
22202,09751c520b0616,8a072960,#### (ii) TotalBsmtBath : Sum of : BsmtFullBath and 1/2 BsmtHalfBath,a4d0c7e9,0.47692307692307695
22203,a4f8ad33c823c5,63b685b2,"### Glasgow Coma Scale
https://www.glasgowcomascale.org/

The Glasgow Coma Scale provides a practical method for assessment of impairment of conscious level in response to defined stimuli.

https://www.mdcalc.com/glasgow-coma-scale-score-gcs

The coma severity scoring is based on eye(4), verbal(5) and motor(6) criteria.

I created a total gcs score to be used as an additional feature.",fcd48307,0.47692307692307695
22205,917957c6c4065f,9ba93029,"1위는 방탄소년단의 뮤직비디오로 게시된지 2일 후 조회수 62,796,390에 도달했네요.  
2위는 연말에 유튜브에서 게시하는 영상 ""The Shape of 2017""고 게시 2일 후 조회수 52,611,730입니다.  
3위는 트와이스의 뮤직비디오로 게시 5일 후  조회수 38,840,787입니다.",55b8ed68,0.477124183006536
22206,2ada0305b68956,88f2b0f0,### 80. Palette = 'binary_r',133e26f4,0.47714285714285715
22207,0a918602a04693,d933c8c1,"
# Pre-Processing of Data",c1ef0e95,0.4772727272727273
22213,a0b321057e7402,a2c38980,"**Box-Cox**

Box-Cox comes from the family of power transformations and is often used as a mean to stabilize variance in a dataset. It is indexed by lambda and in certain times can be used as a differencing technique.",5f73fb91,0.4772727272727273
22215,d1ff7e10ee0102,e8c4d315,"Let's analyse this to understand how to handle the missing data.

We'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.

In what concerns the remaining cases, we can see that 'Garage*X*' variables have the same number of missing data. I bet missing data refers to the same set of observations (although I will not check it; it's just 5% and we should not spend 20$ in 5$ problems). Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'Garage*X*' variables. The same logic applies to 'Bsmt*X*' variables.

Regarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.

Finally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.

In summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.",2cc71c3c,0.4772727272727273
22218,6d66ced0028dea,8be724c4,**HouseYear**,f50aae52,0.4772727272727273
22221,18ce858f90966d,f597cc29,# **Standadization**,09e9caed,0.4772727272727273
22222,73d8e56bc709b1,b23d7666,"So we could say, most of Spanish players in top european teams are really young, and also there are a lot of players over 31 still play a key role in their teams, like Sergio Ramos. (^-^)",78ec3cce,0.4772727272727273
22224,a4aa36df07fd53,2371ed3c,"## Objective

Kita hendak membangun model dimana dengan memasukkan latar belakang ilmu data science seseorang kita bisa menebak berapa persen kemungkinannya menerima gaji sebesar 100.000 keatas.

## Data Cleaning

### Missing value

Sudah kita amati bersama banyak kolom / variabel yang mengandung data NaN, langkah paling simple adalah mendelete data yang mengandung NaN. Tapi tidak selalu menghapus data dengan NaN itu merupakan langkah yang bagus. Di sini coba kita hapus data yang mengandung NaN tapi hanya untuk kolom Gaji. Karena kolom ini yang akan kita gunakan sebagai target latihan.
",d2f42b6d,0.47761194029850745
22230,b0c2805cd5c087,d78be29b,Image dawn.cs.stanford.edu,0446f327,0.4777777777777778
22231,892be0a523578c,a0a6a2a0,"According to the problems stated above, for **dailyActivity_merged.csv**, I selected participants with at least 20 records, and excluded the invalid records.",b0e8d7c0,0.4777777777777778
22232,d6cbd7160961dc,5b861ebf,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 800.

* The center graph shows the result for city E, which had the worst chi-squared value. 

* The rightmost graph shows the result for city G, which had the best chi-squared value. 

Note that the chi-squared values for the third digits are much lower compared to the one of the first digit. It happens since the theory predicts a more random distribution for the numbers in third digit. ",36d74664,0.4777777777777778
22233,3597174a998d4d,de56c337,"For customer_type and market_segment, it can be concluded that:
* The Transient's and the Transient-Party's number of booking is much higher than the Group's and the Contract's. The top 3 markets in terms of number of booking are TA/TO(Online TA and Offline TA/TO included), Direct and Groups.
* When it refers to the ratio of cancelation, there are some conclusions: (1) the ratio of cancelation of the Transient and Contract from Groups market has exceeded 95%. (2)the ratio of cancelation of customers from Complementary, Corporate and Direct market is low.
",276892ed,0.4777777777777778
22234,396bc36edb95d3,7bb35139,#### AUC and ROC for Training data,965e4f8f,0.4777777777777778
22235,49ac6594c8f5cf,fc3974b4,People with work experience get placed less and vice versa .,6f19f28a,0.4777777777777778
22238,a5a419dc7245b0,031ea5b2,### VIF,4279726e,0.4778761061946903
22240,7f74a04ae75792,b6ebc0ff,since the missing values for Cust_Last_Purchase is 0.52 which is more than 50%. so we drop it,d01e91da,0.47794117647058826
22241,33d736abb432d0,2be9055d,## 下面进行分词,d64052a2,0.4782608695652174
22244,4913b61a68d355,079da50d,## Architecture,6e269c6a,0.4782608695652174
22246,7e89d387feb9f5,d5dd09cb,### Добавленный числовой признак №10. Сколько человек населения в среднем приходится на один ресторан,989e3a1b,0.4782608695652174
22247,b49bb7b41806a7,2afc9afc, From the 4th hour the number of page visits increase again increasing dramatically from the 7th hour. The website visits increase again in the 12th hour and peaking in the 17th hour.,d034d34d,0.4782608695652174
22251,ea4e559a86d613,7c0ebc8a,**Relationship between target and anatomy**,eff47843,0.4782608695652174
22252,548f961125248d,7ed52b9c,"* The best model has a **loss of 0.137** and an **AUC score of 89.7%**. 


* RESOURCE & ROLE_DEPTNAME are the most important features. 


* Let us look at model performance of the test set now by making a submission to Kaggle. 
",d8c5e8b8,0.4782608695652174
22256,17a24d566ffa59,f4ebec90,1. ![](https://s3.amazonaws.com/nlp.practicum/truncated_svd.png),89049e56,0.4782608695652174
22265,2b434130adf886,4dc18c95,combine these CNNs and add a classifier,0c4afeca,0.4782608695652174
22266,69d50f5e1373f1,7d721a1c,"Drilling into the first `train` input-output pair, we can see the grids are expressed as 2d lists with integers 0-9.",ec7545ee,0.4782608695652174
22268,598b6228760590,a3af1927,"- **For more than 5 category features, I use leave-one encoding, otherwise, I use WOE encoding or one_hot_encoders.**
- **Here, I gave up the more commonly used one-hot encoding. And to introduce you to a good third-party library-category_encoders**",be30ab66,0.4782608695652174
22274,7e275c8d5ff2a0,88f16933,"<div style=""color:white;
           padding:8px 10px 0 10px;
           display:inline-block;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana"">
    <h1 style='color:white;'>5. World Data</h1>
</div>",b3afcc98,0.4782608695652174
22277,57bad3860b0fa4,bc929abf,# Building The Model,05138a5e,0.4782608695652174
22282,f6648e47713411,f89dcedf,"### Quan sát:
Giá trị kênh màu xanh lá cây có phân phối đồng đều hơn giá trị kênh màu đỏ nhưng lệch phải, với đỉnh nhỏ hơn. Sự phân bố cũng có độ lệch bên phải (trái ngược với màu đỏ) và chế độ lớn hơn khoảng 160. Điều này cho thấy rằng màu xanh lá cây rõ nét hơn trong những hình ảnh này so với màu đỏ, điều này có ý nghĩa, bởi vì đây là hình ảnh của những chiếc lá!",f4af4d1c,0.4787234042553192
22284,3d77c1560bd16e,ea7064d9,"<a id='4'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Vaccine distribution vs population</div>

The following plots show that the vaccine distribution is proportional to the population across counties",87c141ca,0.4788732394366197
22286,631cd434fc3aa2,133eb0d0,"* _BsmtHalfBath_, _BsmtFullBath_, _BsmtFinSF2_, _BsmtFinSF1_, _BsmtUnfSF_, _TotalBsmtSF_: likely a missing values is due to no basement.",2b74febb,0.4788732394366197
22287,06c7ba9203293f,7ef5da2f,# Analysing 'review_counts' from combo_df DF¶,1e1a2b48,0.4788732394366197
22288,d8ff894670d506,a08cf74b,"*It is quite clear from above statistics that our Donations Amount column have lots of outliers since mean is 60 whereas median is 25 which shows that there are plenty of outliers causing mean to rise, second indicator is that we have 25th and 75th percentiles both below than mean. In other words although %75 percent of our data smaller than 50 we have a mean values which is 60.66 which is also a good indicator of outliers. Lastly we can easily say that maximum value is a huge outlier too.*",eb0fb7de,0.4788732394366197
22292,eb0854a6601407,2445b520,"Assets are distributed in a different way, there are assets that are actually more frequently observed and others that are not. A good cv and modelling strategy should keep this into account (stratify if you are working with subsamples).",6d107747,0.4791666666666667
22293,2a377ced98d67a,02540b0f,"### Note: In above Lineplot, it shows linear relation between all the features and target and indicate positive correlation between them.
***With increase of all the feature  like GRE, TOEFL, SOP AND LOR, chance of admission increases***",262231a8,0.4791666666666667
22294,3b5903412fe741,cc24e306,"## Label-based selection

The second paradigm for attribute selection is the one followed by the `loc` operator: **label-based selection**. In this paradigm it's the data index value, not its position, which matters.

For example, to get the first entry in `reviews`, we would now do the following:",ad231969,0.4791666666666667
22295,28a1ff0f223da9,b94ec081,"The number of persons in whose area of interest comes the words data science, machine learning and artificial intelligence are as follows:
* Data Science: 4
* Machine Learning: 91
* Artificial Intelligence: 77",c945b27d,0.4791666666666667
22296,8c7e00ca3dc5a7,f77ba53a,"The description below inffered based on the data defintions and the correlation matrix.
* GarageYrBlt and YearBlt has the high correlation. Because garages are mostly built when the houses are built.
* GarageArea and GarageCars has very high correlation. Because if more cars can be parked, then there will be more garages space and vice versa.
* The '1stFlrSF', 'TotalBsmtSF'  are correlated. It makes sense because usually basements are usually right below the first floor and mostly similar in size.  
* TotRmsAbvGrd and the GrLivArea are correlated. It also makes sense because in both of the columns the basement isn't considered.

So the above features will be dropped for future calculation. ",c83346e4,0.4791666666666667
22299,e82462cdc998a7,d0376542,"### 4.6 Dataset Classes<a class=""anchor"" id=""4.6""></a>

[Back to Table of Contents](#0.1)",b39bf244,0.4791666666666667
22301,95656e8d666b16,a79a6693,All columns are significant,65e88599,0.4791666666666667
22303,64a336ac34d95c,5bbf5db0,## Avg total charges vs tenure,be73a990,0.4791666666666667
22309,91473a39b85068,5dec5bd3,"### Observations:

* 144 tags are used more than 25 times.
* 59 tags are used more than 50 times.
* C# is most frequently used tag 778 times.
* Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm.",6e3d91c2,0.4794520547945205
22311,1eb62c5782f2d7,7fcb55f5,"- Sebuah survei mengindikasikan bahwa setiap perjalan ke supermarket, pembeli menghabiskan rata-rata $45$ menit, dengan Standard Deviation $12$ menit didalam toko
- Rentang waktu yang dihabiskan di dalam toko berdistribusi normal, dan direpresentasikan oleh variable $x$.
    1. Cari probabiliti dimana pembeli akan berada di toko untuk setiap interval waktu dibawah ini:
        - Q1: Diantara 24 dan 54 menit    
        - Q2: Lebih dari 39 menit
    2. Interpretasikan jawabanmu ketika 200 pengunjung memasuki toko. berapa banyak pengunjung yang berada di toko untuk setiap interval waktu ",bb69f147,0.4794520547945205
22312,5ce12be6e7b90e,78b9c174,Strings are objects of type `str`:,c0ab62dd,0.47953216374269003
22313,30fdc4a6e3c1db,ce464cc2,We also see 10 unique values dates that means in TX also SNAP falls on the same dates over the months. These dates are different from CA,6111ddee,0.47953216374269003
22314,087e21401d7dfc,c47dedb0,"> IT IS A MAJORITY WIN CLASSIFIER.

> KNN classification is very simple. It will plot your data point on the graph and select the n number of neighbors(which u define) and related you datapoint to majority.
",42000489,0.47959183673469385
22315,fc8e0042411c46,9175361e,- Focus should be more on the Specialization with high conversion rate.,af476c2a,0.47962382445141066
22316,e19e307b3fd188,382d8873,### hoa,2173955b,0.4796747967479675
22318,9ceb7278784462,4979d3f2, ## <a id='17'> 13.SVR</a>,3768a567,0.4798387096774194
22320,6338f6b0178d13,6e5bfdea,# Applying Logistic Regression,ae81b18b,0.48
22322,50d4ddf1953997,027fd1cd,"While the overall number of TV shows is lower, we can observe that they are being added in increasing amounts.
We can summarise the above findings in two graphs with the sums of all TV shows and movies by release date and date added.",90bdddd6,0.48
22326,2a9a149f306b6c,2053e9f0,"For pretrained==True -> check out the trainable layers, currently only  the batchnorm parameters and the last layer weights can be trained!",295c52ce,0.48
22330,7e1da639035ac5,d4366655,### <a id='9.2'>9.2 Rigorous instruction ratings statistical analysis</a>,120b6c23,0.48
22332,7dd46c750653eb,ca0decb2,Comparing Birth Rate and Death Rate Yearwise,c2644713,0.48
22335,b10bd75889dad9,7e4b6607,#### Understanding the Data : Data Visualization for Categorical data,ee00ceee,0.48
22339,bbad077c274022,38296893,**Now I am curious to know how has been my outdoor walking pattern for the entire month. Let me decipher it.**,3c2e3dea,0.48
22340,519e936017c30a,a5efd9bc,"Como se puede apreciar en las diferentes gráficas, la mayoría de las ventas se registran en la zona de Norte América, seguido de Europa, el resto de países del mundo y Japón, siendo el 2006 el año donde se registran las mayores niveles de ventas. A partir de ese año, las ventas empiezan a disminuir, siendo la zona de Norte América la mayor afectada entre el 2012-2013, donde se puede ver una caída aproximada de 200 millones de dólares.",dc34915d,0.48
22344,9395559895004f,e9cad492,## Callback Functions,b5a0494b,0.48
22348,21c1e34efd71b8,c0794510,"Yes, the accuracy has gone down a bit, but look at the Recall score go UP!! This model is better that the previous one for sure!
Lets check if the SMOTE provides any better results.",23b2cdd6,0.48
22349,10c5a39a87c47e,b062db85,## Step 9: Compiling the model<a id='step-9'></a>,09c7337a,0.48
22351,67b7354e96113a,3c7935aa,"**Fare Category**

Since Fare is From 0 - 512 we'll form a new variable Fare Category based on the quartile's it is distributed
 
       FareCategory  - Quartile
            0        -  0-25%
            1        -  25-50%
            2        -  50-75%
            3        -  75-100%",dca94250,0.48
22354,639e8aae4e046e,f88e1dd3,![](http://)**Feature Engineering: visitor level aggregation**,77deb4cb,0.48
22356,842547b2def18c,f272eb6f,Let us create Age bands and determine correlations with Survived.,b8efde6d,0.4803921568627451
22358,7cfd96218dd933,49dab960,"#### ATTENTION
* HERE IS MUGLA",7c34d96c,0.4803921568627451
22359,52cfd66e9ec908,e9ad8e89,### centroid_x and centroid_y,c74adcdf,0.4803921568627451
22362,241cf32abb22d8,228a4517,### Performance Comparison Using Paired T-Tests,47157066,0.4805194805194805
22363,2cb457b60dd246,470a9d51,# ***Attention module***,339367df,0.4805194805194805
22364,722cd844dfbe8f,cbce5d93,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_2_4"">Global preprocessing function</span>

We can now create a global preprocessing function that will be applied to our MRI images before entering the neural network models. This function will take over the various treatments seen previously.",0cedb385,0.4805194805194805
22367,90691864eb68c7,ff4c50a9,"We are going to create an average variable for dist1, dist2, dist3 and dist4",3555ef9b,0.4805194805194805
22368,4ae464582bac51,fcf3da45,Workers residing in urban and rural areas are distributed even though they are equally,ca6a52ce,0.4805194805194805
22371,44f6a002ecd033,76383654,As can be seen above we no longer have any missing values. We are going to go ahead and split our data back up into our train and test datasets and then do the same process and make a separate dataset that contains our logarithmic columns.,70bbe106,0.4807692307692308
22374,582cb872d19026,ad33e791,"
******************************************************************************************************************************
#### Looking at the outliers in the Total Ratings boxplot, the trend in the Count-Total Rate graph is more clearly understood.
******************************************************************************************************************************",8d966d69,0.4807692307692308
22377,21122355e39af4,0a3f058f,"QUESTION: Shall the centroids belong to the original set of points?

Not necessary, cause the centroids are just the mean of the clusters.",88b95e2b,0.4807692307692308
22381,510b8303776bb6,94ff77ad,## Plotting a correlation matrix for all ordered categorical and continuous data fields vs Sale Price.,18080db8,0.4811320754716981
22383,ba4b3bd184acbb,fab175ee,Let's retry the `floatTest` again for both columns...,0f5de724,0.48120300751879697
22387,4883314a96dc34,dacbe2a9,# Modeling,50d36836,0.48148148148148145
22389,1667a100fc8b42,cab57885,"## Data Manipulation ##

- Apply hot encoding, convert categorical variable into dummy/indicator variables.
- Fill NaN with median for that column.
- Log transformation.
- Change -inf to 0.",6c8cd6b6,0.48148148148148145
22394,d128317750d689,ca9b1907,"Now we define the architecture of the network. As mentioned before, I will be using a basic CNN. I've stopped at 3 conv. layers with maxpool at first two. I've also added a [dropout](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html) after the third conv. layer, which helps with overfitting and is generally nice to have. If you want more details about some specific type of layer or about CNN architecture in general, make sure to take a look at [this](http://cs231n.github.io/convolutional-networks/).

I've also added two custom functions: test and evaluate. Both of them just give feedback about performance of the network. *Test* function compares the number of correct predictions and prints the accuracy, while *evaluate* calculates the accuracy and returns it, which is used for logging while training. ",d87f7428,0.48148148148148145
22397,c6f8ff61a5fa87,3d58011f,## 3.Kernel Ridge,3eea586b,0.48148148148148145
22399,db5a369894fef6,47b3dcf3,"Using `matplotlib` vs. `plotly` for multiple line plots: 
- `matplotlib`: data frame with one country column and dates as new columns
- `plotly`: data frame with one date column and each country as new columns
- `plotly`: In this way you can merge data with `melt`",065aaf61,0.48148148148148145
22402,46778b77b8d195,9aee57ae,# Cleaning & Feature Engineering,ec849695,0.48148148148148145
22404,c1984e64b35234,bd691836,"**Eye color**
",1811225b,0.48148148148148145
22405,e0f03003a69819,83370d17,"# Conclusion
* The difference is not highly impacting is not impacting serously across the different types.
* positively skewed .......",609ad1f4,0.48148148148148145
22409,9ad9a97e628bfa,318a4eae,"상당수의 데이터가 Miss / Mrs, MR/Master에 치중되어있음을 볼 수 있으며 나머지의 경우엔 극소수로, 결과에 큰 영향을 미치지 않을 것으로 보임. ",0a7e1136,0.48148148148148145
22411,f4b9042e693b6c,c7b394ec,"Here, I define a class for the PyTorch Dataset (taken from my [earlier notebook](https://www.kaggle.com/tanlikesmath/seti-et-signal-detection-a-simple-cnn-starter/notebook)).",676cacc9,0.48148148148148145
22417,1883198d6d8c3c,2f7d6f00,<h1>Basic insight of car dataset</h1>,69a1d458,0.48148148148148145
22418,0dfa3e758551c8,e202d9f4,This means sex of the passangers plays an important rule in their survival,7adbb44c,0.48148148148148145
22422,e6576e985ccc71,36ff3c26,#Color channel histograms,63d2c3a5,0.48148148148148145
22424,7baeb0ffc6659e,49e96840,**Family**,8cbebba9,0.48148148148148145
22430,b01ee6cb674fa3,74bdc235,"# ExPace

A chinese public company 

> ExPace (ExPace Technology Corporation)[2] also called CASIC Rocket Technology Company,[3] is a Chinese space rocket company based in Wuhan, Hubei, China. Its corporate compound is located at the Wuhan National Space Industry Base space industrial park. 

> ExPace is a wholly owned subsidiary of missileer China Aerospace Science and Industry Corporation (CASIC), a Chinese state-owned company, and serves as its commercial rocket division.

> ExPace is focused on small satellite launchers to low Earth orbit.[4][2][5][6] ExPace was established in February 2016.[7] ExPace was founded as a Chinese commercial rocket company.[8]

Source: wiki",a8ffd35e,0.48188405797101447
22433,a566b5b7c374e7,0180d29d,### Deep Sleep Time,b3dc5545,0.48201438848920863
22434,f13534449a3750,f18dcd1b,"As we can see from the results,  only 1‰ of the pixels are *ships*, while 99.9% of the pixels are *no-ships*. This imply that the dataset is very unbalanced at pixel level. Below we have reported the results considering only the images with the ships, to understand if the problem persists.",8b7f3332,0.48214285714285715
22436,8dd655515e7d18,482ee2ea,"**Analysis:**

The 'Agreeableness' is uniformly distributed between min-9 & max-41",895f41cf,0.48214285714285715
22437,585c280865b46e,ff274a15,# Visualization colored by S-phase active histones ,4d6056f1,0.48214285714285715
22440,3cd78d8d6d56e4,42d8fcdc,### Hyperparameter Space,9f632e94,0.48214285714285715
22442,b61ab8f81dc03d,9f2eb5f1,"We can note that the cabin code contain in the first character the Deck location. i.e (Cabin B96 = Deck ""B"")
So, we can create a category to be mapped and apply an Encoder to transform it in numbers to be used on the Machine Learning algorithm.",64d05394,0.48226950354609927
22443,56785caebaa256,5598db23,## Describe statistics,a792961a,0.48226950354609927
22446,869a39a3d4dea2,1f40e628,Let see how it works with image data,9020daf8,0.4823529411764706
22448,3fb15e6e48aec2,187669c1,# Embarked-Mode,9d1f4358,0.4824561403508772
22454,fc8e0042411c46,f7904f5e,## Occupation,af476c2a,0.4827586206896552
22456,cd10f3afd970b3,73ab4957,Distribution graphs (histogram/bar graph) of sampled columns:,2db3c8e4,0.4827586206896552
22457,84127ade6fde87,3f5c46be,"Next, let’s build a mapping of words to indexes in our encoding:",f55d05b6,0.4827586206896552
22458,bb8f5d7807718b,4d376977,"# 5.Event Plots

Event plots come in handy when one wants to plot identical parallel lines at pre-defined positions.Such plots are commonly used in neuroscience to display the spiking activity of the neurons over time,",181ec286,0.4827586206896552
22459,4b7039cb44a54c,81a920e3,# Augmentations,24e806af,0.4827586206896552
22460,6a1d04e8153df3,cef89c66,"**You need to know**
PDF (Probability Density Function):- It shows the density of that data or number of data present on that point. Peak shows data is more if the peak is high vice versa.

CDF (Cumulative Distribution Function):- It is representation of cumulative data of PDF ie. it will plot a graph by considering PDF for every data point cumulatively.",38572b05,0.4827586206896552
22462,00001756c60be8,4bb2e30a,**Поиск признаков с выбросами**,945aea18,0.4827586206896552
22468,1750367e54f407,a61b5b6e,"The 3rd layer of the Efficientnet is the Normalization layer, which can be tuned to our new dataset instead of `imagenet`. Be patient on this one, it does take a bit of time as we're going through the entire training set.",a8e655b2,0.4827586206896552
22470,a1ba5ffd30dbde,ea66de21,<h2 align='center'> Logistic Regression </h2>,48e57546,0.4827586206896552
22479,f3c6048d1058e3,661c87e1,"<a id = 11></a>
<h2><font color = MidnightBlue>Unigram Analysis</font></h2>",1d9056b0,0.4827586206896552
22481,1dd9c6aa74d289,4591ef8a,# [Q2. How is my height compared with most climbers?],5ef9a1be,0.4827586206896552
22483,45921c50ac56fa,23b0af95,"# 1.3 POS Tagging
POS or Parts of Speech Tagging is identifying what is the part of speech of the given string.   
Since we are performing Topic Modelling, we will consider only Nouns for our task.   
A POS tag of ['NN','NNS'] corresponds to a Noun.",465973eb,0.4827586206896552
22484,a4f0a3e1316ff9,052a8a8c,Create table,53bf0160,0.4827586206896552
22486,858da4bb312f67,77fd9957,### CBSD to HEALTHY,9cca4391,0.4827586206896552
22487,6b54e39f86bdb5,fe14d811,"## Training the model using Keras data augmentation API

In this second version, I also added a data augmentation to increase the training of my model. I have done so with the Keras preprocessing API. I train the model with real-time data augmentation.",198084bc,0.4827586206896552
22492,9535bb04ae042c,c8542752,## vi) Scaling,165b6fae,0.4827586206896552
22494,2ada0305b68956,048b6514,### 81. Palette = 'bone',133e26f4,0.4828571428571429
22500,5f27526aa6c113,211cc3aa,More claims are from males than females or others.,a5c26ab6,0.48314606741573035
22501,726833f92fb87a,6b9373fc,**We cannot draw any particular conclusion.**,7dc5e1b6,0.48322147651006714
22505,541d0fa0e26b80,59d61182,"Let's Plot Average Total score against different attributes and find some insights , how student marks is varied with respect to other attributes.",a29e0f29,0.48333333333333334
22510,b10bd75889dad9,5d618115,#### Lets plot countplots for the categorical data,ee00ceee,0.48333333333333334
22512,712198370d5521,be8c8f71,"The data is quite clean and the new features have been included. I will proceed to the next step. That is, preprocessing the data. 

<a id=""4""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">DATA PREPROCESSING</p>

In this section, I will be preprocessing the data to perform clustering operations.

**The following steps are applied to preprocess the data:**

* Label encoding the categorical features
* Scaling the features using the standard scaler 
* Creating a subset dataframe for dimensionality reduction",5882e04c,0.48333333333333334
22516,62487bcd70b199,60f7a5b2,# <a id='6.'>6. Model Building</a>,f6ae50af,0.48333333333333334
22517,5f4ae633cfd090,fc482640,"These are the winner distributions where the odds of Blue player winning are greater than 0, but the winners are clearly Red players.
Same would be applicable for B_odds",a30a16e2,0.4835164835164835
22519,917957c6c4065f,1b10cdad,"상위에 해당하는 데이터는 이 정도로 살펴보고, 극단치들을 제외하여 데이터를 살펴보겠습니다.",55b8ed68,0.48366013071895425
22521,e8c6480a3122b3,b18e7014,"Parch and SibSp does not give much information of survival or non-survival, but we can use it to find other features like if the passenger was alone or not. ",40dc4cca,0.4838709677419355
22528,0925f172b5eb74,b96f3e2d,"# Tensorflow set up

In this section we must prepare the data that we have already gathered in the terms that Tensorflow is going to need it, such as *numpy* dataclases, *tensorflow* images or *tensorflow* dataset, as well as adjusting the path where our images are saves in Google Cloud Storage.",ec34cd72,0.4838709677419355
22530,0cb456a5456cf9,65cf59eb,# **Q1**<br> **Which features are important?<br>**哪一些特征比较重要？,5701729c,0.4838709677419355
22534,b59b5aaeedb1fb,684f6aa9,"#### There are 2 outliers in the Height, so removing them will give more accurate data.",1ad63faf,0.4838709677419355
22535,f15eac23fbcc9d,4500fddf,Please ignore the following code if you are not interested. This is from the fast.ai 0.7 structured.py. I put it here because new Kaggle kernel will install fast.ai 1.0 which deprecate this part and this kernel still use some of the functions in structured.py. ,ea46d8af,0.4838709677419355
22536,b90ef792fd07c2,1370d545,# Data Preparation,5e2762eb,0.4838709677419355
22537,57070ad5e0f94f,d652fbc7,# **Dropping Unuseful Features(Probably they are not unuseful but we need to convert them to numbers in a reasonable way and to do this we need to understand the problem more)**,d97edc41,0.4838709677419355
22548,f91f58d488d4af,e8056f09,"* we're getting over 90% accuracy on both 3's and 7's with our simplest model.

But our Pixel similarity approach doesn't learn from it's experience. In other words, we can't reall improve our pixel similarity approach by modifying a set of parameters.

To do better, the system should start some real learning, One that can automatically modify itself to improve its performance. In other words, let's talk about the training process and the SGD.

",5df1bbf3,0.4842105263157895
22549,840534f2908a9c,0e3675ea,"*Distribution of fare amount for both pickup and dropoff to JFK is similar;
the fare amount is much higher when pickup or dropoff are from and to JFK.*",8081c3cc,0.4842105263157895
22550,ff3a8ce61fab6a,6648fab5,"<hr>
# 6. Operations

<p>
    In this section we will talk about <b>Tensorflow Operations</b> <br>
    <ul>
        <li>add</li>
        <li>multiply</li>
        <li>subtract</li>
        <li>divide</li>
        <li>pow</li>
        <li>matmul</li>
        <li>matrix_transpose</li>
    </ul>
</p>

<p>
    Let's talk about <b>add</b> function We can use it to  <br>
    <ul>
        <li>Number summition</li>
        <li>String concatination</li>
     </ul>
     
</p>

## Number summition<hr>
### Exampel 1 
    
",9afe1654,0.484375
22552,0932046e1f485d,53cd0671,"Some strange prices on the ""Price"" axis.",218cc7a3,0.484375
22554,225b4fe5d3894a,cf6fcbb5,"Since classes are not ordinal, we will one-hot encode them",4b4197b3,0.4845360824742268
22562,4b64dc653fb7eb,0ed6a2cd,"<a id=""4""> Step 4 - Convert to lowercase </a>
",57675cc2,0.48484848484848486
22563,efd44ce2c08541,1cce7a2a,# Create Features,ebc2d00c,0.48484848484848486
22565,2f964d08c25d93,38d359ea,### Let's check 2nd file: /kaggle/input/data/review/review_dataset.csv,1f2e4468,0.48484848484848486
22578,156bbcff05dcea,890a9be1,# Prediction,66ad1fe9,0.4852941176470588
22580,dc0b0e1cb46c6f,822982a0,"<a id='2.3'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;"">2.3 Distribution of vaccinatios by country</p>",47b17a7b,0.4852941176470588
22582,3d905ce4828057,59b14111,# 2. Establishment of BG-NBD Model,5b006cc3,0.4852941176470588
22584,9cec5ddf8b6f49,1de46e30,# 3. Preparing Data,d39fc8e7,0.4852941176470588
22585,eb0ecd6bebeb15,94881258,Daha iyi anlayabilmek için sepal.width üzerine bir distplot çizdirelim.,d7b93a60,0.4852941176470588
22586,30fdc4a6e3c1db,c020ffc2,For WI:,6111ddee,0.4853801169590643
22590,98a6794067932a,8b66c8d8,"La cellule de code ci-dessous permet de représenter sur une carte des États-Unis, l'intensité des ventes pour chacun des états avec l'aide d'une charte de couleurs. Plus la couleur est foncée vers le rouge, plus le volume des ventes est élevé dans cet état et plus la couleur est pâle vers le blanc, moins le volume des ventes est élevé dans cet état. Dans un premier temps, ce code permet d'importer les modules permettant de réaliser cette carte et d'utiliser pandas. Ensuite, dans un deuxième temps, le code permet d'établir les différents paramètres devant être pris en compte afin de produire la carte. Cette carte prend en compte le dataframe créé avec l'aide des cellules précédentes. Bien évidemment, cette carte permet aux gestionnaires de constater rapidement les états où l'entreprise effectue ses plus grands volumes de ventes, ce qui pourrait grandement influencer les futurs emplacements de ses centres de distribution. De plus, la carte est interactive, donc elle permet d'indiquer la somme totale des ventes pour chacun des états.",08600fe2,0.4854368932038835
22593,0e2a23fbe41ca9,707af917,"Observations:
- 31.4% data in city_id is null
- There are 216 distinct cities merchants belong to",64e4762c,0.4855072463768116
22594,7454fdc444df16,8cf9c531,"## Data Augmentation
There are couple of ways we can use to avoid overfitting; more data, augmentation, regularization and less complex model architectures. Note that if we apply augmentation here, augmentations will also be applied when we are predicting (inference). This is called test time augmentation (TTA) and it can improve our results if we run inference multiple times for each image and average out the predictions. 

**The augmentations we can use for this type of data:**
- random rotation
- random crop
- random flip (horizontal and vertical both)
- random lighting
- random zoom 
- Gaussian blur 


Also possible to make a heat map that tells you the most likely region that Invasive Ductal Carcinoma is likely to occur, and to see whether the coordinates can be used as a feature or not.

## Note to self: 
#### Please disregard this

Usually before applying the steps above we might be interested in engineering new features based on the our observations and domain knowledge,one of the features that we might consider adding in future versions of this kernal is including the count of surrounding cancerous cells. This would be treated as a time series analysis, we would have to iterate on it until an equilibirium is reached. So we would have micro and macro classification. Also look into using t-svd instead of pca.",a7818ef5,0.4857142857142857
22605,3a6274ed72cc00,9673ca6f,## <a id='5.'> 5. Preprocessing</a>,51369a2a,0.4857142857142857
22606,f50dc95483c98f,5420970d,"## **Feature Scaling**

Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one.
The most common techniques of feature scaling are Normalization and Standardization

For more on Feature Scaling, you can go on with the blog of [`Towards Data Science`](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35)",cd9e9621,0.4857142857142857
22611,ca73f3d2e25b47,b548b0e9,Normalizing numeric based data using log(1+p).,4cd11efe,0.4857142857142857
22612,55a5e31d03df9f,67305727,"## <a name=""cnneval"">Evaluating CNN model performance</a>

We train sucesfully for the same epochs than the MLP now let's check the results we got.",06dce00f,0.4857142857142857
22613,fe6750354fb64f,c6598d83,# 1. Linear Regression,271741f0,0.4857142857142857
22615,9c044fa3072552,f9e847ff,"Since the time here is of `dtype: object`, we need to convert it to Timestamp",1362842e,0.4857142857142857
22616,f4b603905215b7,a26826bc,"
- __Upper Left Square__: The amount of correctly classified by model of no fraud transactions.
- __Upper Right Square__: The amount of incorrectly classified transactions as fraud cases, but the actual label is no fraud .
- __Lower Left Square__: The amount of incorrectly classified transactions as no fraud cases, but the actual label is fraud .
- __Lower Right Square__: The amount of correctly classified by our model of fraud transactions.


- *Recall*: Out of all the positive classes, how much we predicted correctly.  (TP/TP+FN)
- *Precision*: Out of all the positive classes we have predicted, how many are actually positive. (TP/TP+FP)
- *Accuracy*: Out of all the classes, how much we predicted correctly 
- *F-measure*: 2(Recall) (Precision)/(Recall + Precision)",efe1d587,0.4857142857142857
22624,2730840089c8eb,0eeb305b,"`str.join()` takes us in the other direction, sewing a list of strings up into one long string, using the string it was called on as a separator.",34d27dac,0.4857142857142857
22629,fdc3afd309b850,5da8bffb, Fill NaN ,966bde38,0.4861111111111111
22630,593d1d3d1df05a,8880753b,# Separating the Characters in the Plate,bc682ffe,0.4861111111111111
22631,c01049afb6d307,e82c19b5,"
* If there is a not reason for absence, it is a Disciplinaryfailure",d37d3b5d,0.4861111111111111
22633,166a62ebb4fc3a,f5a54cce,Storing main data set in to another vairiable for our record.,db48a079,0.4861111111111111
22637,fdc9f4863744b1,a2490f9d,Interestingly there correlation between Land Square Feet and Sale Price is weak as well,b4529365,0.4863013698630137
22640,c3336e5345f6c5,48663542,##3. Análise exploratória: estatística e dataviz!,a5ec5530,0.4864864864864865
22647,2dda7facf3c1e0,1a0ed2b7,"#Create, Setup the Model. Setup all (Hyper)Parameters",45552d2b,0.4864864864864865
22648,d76896b30cebd3,40232f5c,"Distribution of Countries

",1b4e8f34,0.4864864864864865
22650,b7b1057764fa02,39b4946d,Now we can have a look at one of the labels to see if it is indeed one-hot encoded:,5053a192,0.4864864864864865
22651,a6c34cd514e30e,99caef84,Let's take a quick look at what the data looks like:,bf603ddd,0.4864864864864865
22652,63b44c85e32c1f,77698889,Index value can be specified to pop a ceratin element corresponding to that index value.,fb9b9562,0.4864864864864865
22669,9eed0fae1c7958,e8e7443c,# model,3fb1438e,0.48717948717948717
22673,897ca904b74a98,8679a6d5,## Train / Test Split,c5844ad4,0.48717948717948717
22675,0a1fcda859252c,a6a95bf2,"## Model 

This is the best part. If you look at other kernels on this dataset, everyone is busy doing transfer learning and fine-tuning. **You should transfer learn but wisely**.  We will be doing partial transfer learning and rest of the model will be trained from scratch. I will explain this in detail but before that, I would love to share one of the best practices when it comes to building deep learning models from scratch on limited data.

1. Choose a simple architecture.
2. Initialize the first few layers from a network that is pretrained on imagenet. This is because first few layers capture general details like color blobs, patches, edges, etc. Instead of randomly initialized weights for these layers, it would be much better if you fine tune them.
3. Choose layers that introduce a lesser number of parameters. For example, `Depthwise SeparableConv` is a good replacement for `Conv` layer. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel, it captures more information.  `Xception` a powerful network, is built on top of such layers only. You can read about `Xception` and `Depthwise Separable Convolutions` in [this](https://arxiv.org/abs/1610.02357) paper.
4. Use batch norm with convolutions.  As the network becomes deeper, batch norm start to play an important role.
5. Add dense layers with reasonable amount of neurons. Train with a higher learning rate and experiment with the number of neurons in the dense layers. Do it for the depth of your network too. 
6. Once you know a good depth, start training your network with a lower learning rate along with decay.  

This is all that I have done in the next code block.
",13a38774,0.48717948717948717
22680,4d91e84c564cbe,abcac8ad,"The things an object carries around can also include functions. A function attached to an object is called a **method**. (Non-function things attached to an object, such as `imag`, are called *attributes*).

For example, numbers have a method called `bit_length`. Again, we access it using dot syntax:",355a43e3,0.48717948717948717
22684,4ae6a182abac64,5e89e4a2,* **Missing values in test data**,418676c5,0.48739495798319327
22689,fdbbd573ba31c2,342124a0,"## After EDA

1. 'tracking_id' can be deleted as it's useless.
2. 'windmill_body_temperature(°C)' seems to be have different distribution in training and testing set and it not highly correlated with target, so maybe we'll drop it.
3.  Some features have negative values. Negative value of wind_speed may represent the wind in the opposite flow. But most of negative values are meaningless, such as -99 and -999, which can be considered as missing values. These values should be removed as extreme outliers.
4. 'datetime' should be converted from object to timestamp or datetime format. We can also extract new features like 'Hour' & 'Month'.
5. Features with null values need to be imputed.  
6. Some pairs of features have high correlation or missing value correlation, we should pay attention to these features.",f7c28d74,0.4875
22690,e9b9663777db82,d6d62b7f,#### 8.Quantitative analysis for Furnished feature,648e8507,0.48760330578512395
22691,2f47abddfd1928,3e7fbc81,"The correlation factor calculated before is a bit misleading because the cabin named as ""Other"" has a big weight and it is mostly 3rd class.

But we still can see, as commented before, some relation of Pclass with the allocated cabin.

1st class had allocated mainly cabins from 0 to 2, being 3 and 4 mixed with 2nd class. The 3rd class was concentrated in cabin 5 and 6 and probably in the unknown ones.",ae33cc0b,0.48760330578512395
22692,8d70dcae7f40a3,9a536754,### **Simple evaluation**,472c71ce,0.4878048780487805
22695,9c26c5dcd46a25,62e346ae,Nous allons devoir **standardiser nos données** afin de les placer sur la même echèle *(moyenne = 0 et écart-type = 1)*,1bbbb677,0.4878048780487805
22697,47b2c9be5e31cb,f31bc2b6,### Let's check 2nd file: /kaggle/input/DEvideos.csv,7d4afe56,0.4878048780487805
22703,74a03887600114,fe3b7b89,## Data Visualization,c0ffb2f0,0.4878048780487805
22704,0e09587faffa8f,7c1a1e76,"From the above time series plot, we can observe that the peak of issuing tickets occured between the **September to November 2016** peiod (Time duration - July, 2016 to July, 2017) ",0d563d61,0.4878048780487805
22710,565ad413cd802f,17831f13,## Model,397b074e,0.4880952380952381
22712,ce9ed5e2d601d7,16b71482,"### DCNv2 Parrallel Deep & Cross Network
CrossNet from https://www.kaggle.com/mlanhenke/tps-12-deep-cross-nn-keras",f58a2f43,0.4881889763779528
22713,806ce45c8fa303,889419b1,## Normalize and Scale the features,3e5c34dc,0.4883720930232558
22716,d96642860ab3dd,0d7993ca,#### 2.1.2 Fare column Missing values,98419d48,0.4883720930232558
22719,1660daf8867980,49ba3f6e,# 2.2 Temporal Difference Learning ,42d7cffc,0.4883720930232558
22725,2ada0305b68956,e530c8fa,### 82. Palette = 'bone_r',133e26f4,0.48857142857142855
22727,73d8e56bc709b1,e660316b,# # 2. England,78ec3cce,0.48863636363636365
22728,0a918602a04693,65edaa0a,"Now, lets convert the categorical datasets to 0 and 1. Here, pd.factorize() is used to those categorical columns whose values are in two categories. E.g: YES or NO, Male or Female",c1ef0e95,0.48863636363636365
22734,d77e6d61ad2e8b,85ec2e58,# Precision-Recall Curve,03fd0e96,0.4888888888888889
22741,c8bf959b9608cf,dd5a74c9,"Let's understand the dimension of (25, 47, 512). It denotes <b>512 feature vector, each of height 25 and width 47</b>. Let's see the width/height ratio: 49/25=1.96. This is almost equal to 1.90, which is the width/height ratio of the original image. This shows that both width and height are reduced by same factor during convolution and pooling operation",155e3672,0.4888888888888889
22743,c73e07ad6d25c5,23088665,### Cabin,3ab391fb,0.4888888888888889
22745,d905cde3391d2b,e6c2bad3,"# Measuring spread (variability)

By just measuring the center of the data, one wouldn't get much idea about the dataset. There are various ways of measuring how the data is spread.

## Range

**Range** is the simplest form of measuring variability. It is the difference between largest number and smallest number.",067dba39,0.4888888888888889
22747,d96e03a9e7c030,d306ec39,"#### Other variables of interest identified in the Research Alliance for NYC Schools paper
While much of the above trends in a similar direction as the Research Alliance's paper, not everything is the same. The Research Alliance's paper used `borough` and the minimum distance to one of the Big 3 specialized schools (`min_dist_to_big_three` in the data frame). When looking at including *only* those variables in a model, though, their performance was sub-par. In the case of `min_dist_to_big_three`, it found that there was a *direct* relationship to participation rate of the SHSAT (i.e., the further away a school is to one of the Big 3, the more likely that school has a higher SHSAT participation rate. ",d2b72ced,0.4888888888888889
22749,4fd4b6a80d40e3,141ee116,![image.png](attachment:image.png),f6913cc3,0.4888888888888889
22752,3c2033cc99c12c,9d9a629c,"*In general, the PC1 together with PC2, PC1 with PC4 have relatively better performance.*",dfa22a54,0.48905109489051096
22753,b01ee6cb674fa3,52853314,"# Israeli AeroSpace Industries - IAI

An Israeli public company, founded by 1953",a8ffd35e,0.4891304347826087
22758,04e6b0d3c70f46,17665971,### Execute the pipeline,56344f77,0.48936170212765956
22763,f6648e47713411,e755c6ef,## Phân phối Kênh Xanh Lam,f4af4d1c,0.48936170212765956
22770,12f4d16fc21645,da3e97b8,<h3 style='color:Red'>NPK ratio for fruits</h3>,c7752038,0.4897959183673469
22776,5f32117bcd5255,4efc2f62,#### EXPOSURE INFORMATION,85882abf,0.4899328859060403
22777,726833f92fb87a,5c3e898c,## Month vs campaign success,7dc5e1b6,0.4899328859060403
22778,83df814455f06c,e768b96a,"### Encode categorical variables


Now, I will encode the categorical variables.",c9cff71a,0.49
22779,4cd25e50c7e007,078fd5be,**Splitting the Data into Training and Testing Sets**,ceb0c525,0.49
22784,71b75664517244,45df611d,"Arsenal has their best achievement around 1996 - 2004, after win premier league on 2004 without losing any match, they never finish first anymore.",fc905af5,0.49019607843137253
22785,1a0bd2f72bbe36,dfaa2c85,## 01. Univariate Analysis:,2fa311dc,0.49019607843137253
22788,fa02c409161192,cf2d585a,## 1.4 Testing the Model,e97077f7,0.49019607843137253
22792,629f2918807a9b,a329f25d,"#### Observations:

- Karachi with 5562 orders, tops the list.
- Now Lets start to see the correlation between city and order status.
- Karachi has 386 orders, which are either not complete or not deliverd",be56dc84,0.49019607843137253
22794,4fa553c2b837d4,01a69cbc,"In many cases, you'll have both a training dataset and a test dataset. You will want to drop the same columns in both DataFrames. In that case, you would write",c65a23e9,0.49019607843137253
22799,f015d0147e8fbf,a71a429d,### Engineer Features by Combining Features From Various Tables,518954fb,0.49056603773584906
22803,f3c8651cb08234,77e046c5,# Random Forest Regressor,37f86e36,0.49056603773584906
22804,a070fd03ae8ed2,dcfbfd15,"# 5. Анализ второй модели (2)
---
## 5.1 Расчет прогноза дефолта ",c0ec4138,0.49056603773584906
22805,614ba9f0c62677,08fe6d6d,"<a id=""7""></a>
### Max Pooling
* It makes down-sampling or sub-sampling (Reduces the number of parameters)
* It makes the detection of features invariant to scale or orientation changes.
* It reduce the amount of parameters and computation in the network, and hence to also control overfitting. 
* <a href=""https://ibb.co/ckTjN9""><img src=""https://preview.ibb.co/gsNYFU/maxpool.jpg"" alt=""maxpool"" border=""0""></a>",b8551335,0.49056603773584906
22809,ab6da5994949a3,b5dea38d,## Visualising the Logistic Regression Training set results,fae6b91d,0.49074074074074076
22812,016abae0483764,08776b28,Here we can see that fever contributes to fair amount of possibility of postive result of COVID 19 giving the insight that it is one of the major factor.,bc9f289b,0.4909090909090909
22813,5a8c553e21c70f,e20da903,"## Majority Class Downsample with RandomUnderSampler

Class 0 is downsampled such that the ratio of samples of Class 1 to Class 0 becomes 0.005",9ebd9d8f,0.4909090909090909
22816,e03eb63c1f725d,c7fb1295,"<a id=""7""></a>
<font size=""+2"" color=""blue""><b>Hyper Parameterization (MultinomialNB) for CountVectorizer </b> </font><br>",e204b7e3,0.49122807017543857
22817,6cade0b6a41ba2,13e9c52c,## 3.9. Average Glucose Level,e6110293,0.49122807017543857
22822,fe7360cddc13e5,29d61afa,-----------------------------------------------------------------------------------------------,8979e423,0.49122807017543857
22823,9d27afa9ca3f96,4be49700,make this progress faster by using joblib which uses parralel computing,2d86a18d,0.49122807017543857
22824,9e27af2600925c,e71c36bf,"**Expected Output**:

<table style=""width:50%"">
    <tr>
        <td>  ** dw **  </td>
      <td> [[ 0.99845601]
     [ 2.39507239]]</td>
    </tr>
    <tr>
        <td>  ** db **  </td>
        <td> 0.00145557813678 </td>
    </tr>
    <tr>
        <td>  ** cost **  </td>
        <td> 5.801545319394553 </td>
    </tr>

</table>",9b556435,0.49122807017543857
22827,5ce12be6e7b90e,ab28b5a9,"### Multiline strings

Multiline strings can be defined using `""""""`:",c0ab62dd,0.49122807017543857
22834,bb0905d33ae417,9fb3319a,# Transfer learning,25fd1965,0.4915254237288136
22837,f2e5e9fb9eaaf7,f051c26e,### 4.2.5 Features f101 - f118,048e0d08,0.4915254237288136
22838,ed8009f482b380,6641d903,"What we're gonna do is that we're gonna oversample the minority data (stroke=1) and undersample the majority data (stroke=0). <br>
<br>
We are going to use SMOTE for the oversampling process and RandomUnderSampler for the undersampling process",e99941fa,0.4915254237288136
22840,b660910fcc2954,f7ad60e7,"## Scatter plot of continous features vs. target

Scatter plot of each feature in train vs. target values.",80b74f88,0.4915254237288136
22841,1294fb4c86f993,7c0eea1e,"Figure above suggests that <b>Kentucky, Illinois and Texas</b> stand as the hightest 3 states in guns regesteration totals from 1998 to 2020",4471e513,0.4915254237288136
22842,a81661cc35d8d2,01315ab6,"We will create 3 separate datasets with different levels of feature engineering


- Data set 1: No transformations
- Data set 2: Converting serum_sodium and serum_creatinine to binary variables
- Data set 3: Applying RobustScaler",3331f113,0.4915254237288136
22844,dac3c8204a2d1b,39e46768,# Top 10 authors with highest rated books,b0d2d0dc,0.4915254237288136
22845,149cb8d3489224,2999419c,### Text wordcloauds,116858e7,0.4915254237288136
22846,62487bcd70b199,2e8d9fd9,## <a id='6.0.'>6.0. Functions to be used across all models</a>,f6ae50af,0.49166666666666664
22849,918040fad252ec,9af2dd31,Menyimpan hasil model ke format .h5,966fcd8f,0.4918032786885246
22853,bcd7e398c4d0ec,80200030,## State_label,77a143f6,0.4918032786885246
22859,8985a124d4b657,67d7b304,"Let's define our neural network (LSTM: Long Short Term Memory). Let's add 50 nodes in our first layer with a ReLU (Rectified linear unit) activation. Their shape will be step size, number of features. Then we will add, a dense layer with one node for the output.",586d1846,0.49206349206349204
22864,be9597c72542a2,31e3081b,"# DAY 7
",6f29c6d8,0.49230769230769234
22870,a4f8ad33c823c5,a6060a0c,### Gender Variable,fcd48307,0.49230769230769234
22872,1645979263c148,20a20e87,"## Data Normalization

1. percentile(interquartile range)
2. boxplot     ",fa11663e,0.49230769230769234
22874,e58e68e4eeefe5,0ca2d4f4,"### Conclusion:
If I eliminate a few features and considering **creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, time,**


The accuracies are:
* Logistic Regression --> 87%
* Random Forest -->91%
* Gradient Boosting --> 91%",a87662ce,0.4925373134328358
22875,21413205980558,2fe66ca9,"# With or without housing loans and personal loans will greatly affect the balance, and those without housing loans and personal loans will have more banlance.
# 有无住房贷款和个人贷款会很大程度影响盈余，没有住房贷款和个人贷款的将具有更多的盈余。",84197de0,0.4925373134328358
22876,1a222fee3089d2,a08ad3dd,## **Gender**,59ab8894,0.4925373134328358
22880,7f74a04ae75792,698e33b5,#### Handling Missing Value for `Pur_3_years_Avg_Indirect` column ,d01e91da,0.49264705882352944
22882,e3fb4c6300cb56,74658d98,"<a id=""6""></a> 
## Kde Plot",8ebbdf89,0.4927536231884058
22889,548f961125248d,82a40a08,"## Test set performance <a class=""anchor"" id=""eighth""></a>",d8c5e8b8,0.4927536231884058
22898,9bcfa825c8b2e6,cdacfd1e,Hamilelik sayısının artması da şeker hastası olma ihtimalini artırıyor,220f36e4,0.49295774647887325
22900,738bfced935b69,fc36a620,Most data by tax between 1 to 125 are engine size between 1 to 2.1 & date of manufactured between 2008 to 2017.,2d3c592d,0.4931506849315068
22901,3d08ca7656dec0,2c983d5f,"# conslusion of analysis:
* those heve less oldpeak they have high chance of heart-attack
* those have high thalachh they have more chance of hert-attack",bd3f87e3,0.4931506849315068
22904,1eb62c5782f2d7,174a2264,### Solusi Q1,bb69f147,0.4931506849315068
22905,91473a39b85068,b662b9e2,### Word map for most frequent Tags,6e3d91c2,0.4931506849315068
22913,e5dd725b8fa422,3558e1c7,# [Make a Time Series Stationary](http://),14675d8b,0.49333333333333335
22922,c13f73168789c2,efb50c2c,"## 5. Selecting random sample from the dataset<a id='18'></a>
Syntax 1 : `df.sample(n)` 

Syntax 2 : `df.sample(frac = n)` ",16175052,0.4935064935064935
22926,fdbbd573ba31c2,8168d0bd,# Data Processing,f7c28d74,0.49375
22928,4883314a96dc34,72553d40,"**5 Levels of ML Model Iteration:**
1. Fitting Parameters
2. Tuning Hyperparameters
3. Feature Engineering

---",50d36836,0.49382716049382713
22930,4daf6153275cbf,a1df0a29,"The straight dashed line shows the all average prices along Europe, added for comparison. The 6 countries below average seems to have more weight compared to the rest.",51db1961,0.4939759036144578
22934,513ce405d7f6a3,a560cf09,# EDA,8461e086,0.49411764705882355
22936,ee9ddc756b2d4a,5b7d73e7,"Ho trovato due set di features con due metodi, ora provo due modelli di ML, ma per vedere se ottengo informazioni rilevanti nella classificazione, mischio il dataset (mescolo nello stesso modo i due dataset e il target)",e367eab3,0.4942528735632184
22939,2ada0305b68956,e23138fa,### 83. Palette = 'brg',133e26f4,0.4942857142857143
22942,04ff2af52f147b,aa31db3b,Now we plot counts of *FamilySize* as well as checking the *Survived* distribution for each *FamilySize*.,d5f37be9,0.4943820224719101
22944,396bc36edb95d3,5dcb455d,#### AUC and ROC for Testing data,965e4f8f,0.49444444444444446
22946,0caaec057f7184,fc9a38b6,plot 3 types of data,b875533e,0.4946236559139785
22947,4c47839b067546,5836c3d6,### engineDisplacement,1f517b02,0.4946808510638298
22950,f91f58d488d4af,414e45a5,"#### Stochastic Gradient Descent (SGD):

Instead of trying to find the similarity between an image and an ""ideal image,"" we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category.

For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8.

This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:",5df1bbf3,0.49473684210526314
22951,8ec771f5600a61,66ee6504,# Correct allAdjustments to Data (Train & Test,48364c1f,0.4948453608247423
22959,04bac111ffbe9c,3e65d87f,##### FINAL CHECK OF CURRENT CORRELATION,82576b17,0.49523809523809526
22961,7454fdc444df16,5b7c13f5,"# Pre-processing
Now that we have a function that allows us to vizualize the where the cancer occurs, we can finally begin pre-processing our data to get it ready to be fed into our classifier.  

The next steps that we are going to take are shown below:

* Split Data to Training & Testing Data
* Apply Principle Component Analysis (PCA)

Notice how we split our data into testing and training sets before applying PCA to it. This is because we want to fit the data to our training data and not our testing data as that would bias our results. One more thing to consider is, that when we receive our data to be classified we are always transforming our data onto our already fitted model. Therefore applying the fit to our testing data, would not reflect what happens when classifying in real life.",a7818ef5,0.49523809523809526
22963,bbaa07ad21cf4e,5b62b315,"## 6. Word Embedding <a class=""anchor"" id=""8""></a>
",3ab6b254,0.49523809523809526
22967,bd380b97b5c894,4c96e4d4,## RGB Table,66f2562a,0.4954128440366973
22969,ac1abfe1dfe815,01624929,----,6529dbcb,0.49557522123893805
22972,49ee86d074de69,ae1c1cb6,"### Moderately Absent <= 3
### Excessively Absent > 4",71ccc6d3,0.49572649572649574
22978,2f47abddfd1928,f9d551ca,"## 3.2. Sex

Seems that Sex feature has a good correlation with survived, Parch and fare.

Lets inspect these relations.",ae33cc0b,0.49586776859504134
22979,e19e307b3fd188,494324fb,There doesn't seem to be much correlation between the **hoa** and the **rent price**.,2173955b,0.4959349593495935
22983,ba4b3bd184acbb,bbd7db86,Sweet! It looks like our fixes worked!,0f5de724,0.49624060150375937
22985,3c2033cc99c12c,623ed1bd,"### SVD(Singular Value Decomposition) 
+ The mathematical principle of the SVD 
+ The design of loss function ",dfa22a54,0.49635036496350365
22986,b01ee6cb674fa3,c44aa018,"# Rocket Lab

A private US company with a NZ subsidiary

According to wiki
> Private American aerospace manufacturer and small satellite launch service provider with a wholly owned New Zealand subsidiary.[4]

> Operates a lightweight orbital rocket known as Electron, which provides dedicated launches for smallsats and CubeSats.

It was founded in New Zealand in June 2006, in Auckland, and established its HQ in California, USA by 2013",a8ffd35e,0.4963768115942029
22990,56785caebaa256,2e0314db,## Earliest Cases,a792961a,0.49645390070921985
22994,b10bd75889dad9,5422fc64,"#### Categorical data looks fine, lets move ahead with Data Preparation",ee00ceee,0.49666666666666665
22995,917957c6c4065f,2802d11e,조회수 100만 이상의 회사 3.59%를 제외하고 그래프를 확인해보겠습니다.,55b8ed68,0.49673202614379086
22997,30fdc4a6e3c1db,eb12a661,We also see 10 unique values dates that means in WI also SNAP falls on the same dates over the months. These dates are different from CA and TX,6111ddee,0.49707602339181284
23000,fc8e0042411c46,6b2b475f,"- Working Professionals going for the course have high chances of joining it.
- Unemployed leads are the most in numbers but has around 30-35% conversion rate.",af476c2a,0.49843260188087773
23008,99bf357eaf61f1,65b3c522," # Missing Value Analysis 
 
We will first check the percentage of missing values present in each feature",9d92fafe,0.5
23009,7cfd96218dd933,29f5b8c0,"#### **ATTENTION**
* THE POINT WITH FRP LATITUDE: 36.95595
* THE POINT WITH FRP LONGITUDE: 31.4881
* THE POINT WITH FRP DATE: 2021-07-29
* THE POINT WITH FRP TIME: 20.03 FOR TURKEY",7c34d96c,0.5
23014,5f544a32fb2ce9,466f1ea5,# Image Augmentation,616f33af,0.5
23015,55c34673c1f760,dad56dea,# Model,2663c47f,0.5
23017,a827e04a19562c,807dd981,"### Random Walk

We start with an initial state where the surfer has equal probability to start at any of the pages. We have modelled the transition matrix (pagerank matrix) which helps in finding the transition probabilites for the next state given current state.

The algorithm will converge when there is approximately no difference between two consecutive states. At this point, the vector of probabilities can be seen as probabilities a user is likely to end up in general, and these can be treated as PageRank scores. A higher probability will mean that the webpage/link is more important and vice versa.",ba301c25,0.5
23022,b8849a04581d32,d2589816,"## Data visualization
",b8a568cd,0.5
23026,c09fac3c943d51,9bf7718a,Basic time features:,678d076d,0.5
23029,a6eb631926a4d7,98b1124e,Naive Bayes,0d502aab,0.5
23036,be2f4d8a6b73ca,27153d12,"<div class=""alert alert-block alert-info"" style=""text-align:center""> 📌<b>Insights :</b> people who are aged more than 50 and having MaxHR (maximum Heart Rate) less than 140 and also have more Oldpeak is more likely to have heart disease.</div>",5d8ce40a,0.5
23037,37b09262279764,9eb707e3,"We will train our model on <b>712</b> rows<br>
We will test our model on <b>179</b> rows</b>",37c4c417,0.5
23038,e323e594ef918f,50861bce,fastai models are conveniently splitted after the last convolutional layer. We will need to store the outputs of the first layer group `learn.model[0]` with the hook. Let's confirm that the shape is a 14x14 grid with 512 channels as expected (plus batch size 1 dimension). ,6e829ab6,0.5
23044,2d40f383473fa4,8ec04f0f,"Ok, all `NaN` values were filled, we could proceed to build the model, but AutoML, as any Machine Learning Algorithm, is **NOT** a magic wand! You can feed an AutoML Model with no treated Data, but it will not operate a miracle, you as Data Scientist or any other job role is the miracle! You are the wizard that chant spells (coding) learned from your grimoire (your books, courses, etc.) and add your creativity (the miracle factor).<br>
Hence, let's work one more step, remember name? If you create a new feature called `Family_Name` to get the info if the Passengers are in the same family? I got this insight because it could solve the problem about Cabin missing values, let me explain my idea.<br>

As [the wikipedia article about Titanic Sink](https://en.wikipedia.org/wiki/Sinking_of_the_RMS_Titanic) say that the time of disaster was 02:20 AM, time when much people could be sleep, and family could be sleeping together in the same Cabin or in near Cabins, it could explain why some passengers owned more than one Cabin. Creating this feature will reduce the cardinality in `Name`. I will extract the honorific name too in new feature.",1da1eff0,0.5
23048,38b79494ac749e,77c3eb3f,### Model coefficients,39162a40,0.5
23060,63b44c85e32c1f,ceb4f2af,**pop( )** is used to remove element based on it's index value which can be assigned to a variable. One can also remove element by specifying the element itself using the **remove( )** function.,fb9b9562,0.5
23067,67efe818cb2372,75c1f8e7,"OK:  about 99.47% accuracy in the validation set.
Let's plot the trends on the  training and validation sets and see what our trends look like.",f28a2a34,0.5
23068,9cec5ddf8b6f49,666c01d0,## 3.a) Data Transformation,d39fc8e7,0.5
23070,2ada0305b68956,a6d0c9f0,### 84. Palette = 'brg_r',133e26f4,0.5
23071,e78f177ca86768,c1005490,# Binning Preprocessing,120e25c1,0.5
23072,5ffe6aa38958a1,805c7a64,"### 2.3.2 Embarked - numeric value dataframe
New dataframe with classess assigned to embarked .. Can this instead be a single feature with 3 possible values?",11f5412e,0.5
23073,395ed8e0b4fd17,66f5ed98,### Bitcoin(BTC) Candelstick Chart for last 100 rows,7573ea31,0.5
23090,20e1ba19eb9b5e,b2b733e2,## 2.3 İmpute null and missing values,4569bfc1,0.5
23094,f4514ec092a771,c6d0ade2,"### Prepare language data
Language data required for running Kaldi contain 4 files:  
* *lexicon.txt*: Contain every word in the dataset and its phonemes. Pattern: <word> <phone1> <phone2>...
* *nonsilence_phones*: Every phonemes you have. Pattern: <phone>
* *optional_phones*: List of optional silence phone. I use only <sil>
* *silence_phones*: List of silence phone. I also use only <sil>
",3739ab1e,0.5
23095,68cceffe5bb8ec,569a7f3c,# Build the model,dcbfcd6e,0.5
23096,b7298d6aaff625,59d4db4c,### Reshaping our Data ,bdf24bf7,0.5
23099,204a60bace6fdb,53b3ba1a,## LightGBM (Apply Outlier Elimination),5cb3de8f,0.5
23100,d46508f983e086,46e6e9a5,**Below we can see the pie chart of Classes that are about to be trained and tested for accuracy**,454138b8,0.5
23101,b290039151fb39,f94582f5,Cross entropy loss is applied independently to each part of the prediction and the result is summed with the corresponding weight.,1836a79c,0.5
23106,1cd8be6e679620,210d9f39,"### 🤷‍♂️: '**E**', '**S**' , '**)**' , '**(**'  are highly correlated",3ce15a43,0.5
23110,999258a81ba32a,a44972e9,# Ball 5 Frequency Chart,48cd3d21,0.5
23113,be616f0785c32d,dac6e650,"<div id=""PartCchem""></div>

Visualize the proposed drug similarities:",b78e18aa,0.5
23114,9b5de3823ad5ab,6e488a90,"### Dependencies
#### Metrics

Since Keras/TensorFlow don't have the necessary metrics to evaluate a multiclass task (such as precision, recall and specificity), we're going to use both **micro** and **macro** averaged F1-score and categorical accuracy (available in TF/Keras) as our metrics. For micro/macro F1, we're using [TensorFlow Addons](https://www.tensorflow.org/addons) library.",33e48774,0.5
23116,a758983a68c014,3cdaf45d,"Ok, now we have some input and output, what does it mean? Time to build a model. The architecture of Skip-Gram model is pretty simple. We have One-Hot encoded vector as an input and One-Hot encoded vector as an output. Which means input and output layers are vectors of length which equals the length of vocabulary, where almost all elements are zeros except one which let us identify which word from vocabulary is encoded. In between of Input and Output there is a hidden layer of length we choose. The length of hidden layer predefines the dimension of embedding vectors. The most interesting elements of this NN are weights between hidden layer and two other. Multiplicatin of One-Hot encoded vector with matrix of weights will activate just one corresponding row of this weights matrix. In other words one step of Neural Net will optimize only one row which corresponds to one particular word from vocabulary. This allow to use vectors from weights matrix as vector representations of words afterwards. ",ab89f181,0.5
23117,400bbcc496138f,80a79ea2,Categories for wich using a lower threshold have boosted the accuracy,191b86b8,0.5
23123,e3c0b55ed519e2,e37e5ae8,# RATIO OF PROJECTED INFECTED INDIVUALS TO THE AVAILABLE BEDS,9f51352e,0.5
23124,8696921d9adc93,d402e9e0,"**4. Reshape**
Reshaping images from 784px to [28,28,1] 3D Matrices

Here, if RGB images would have been present then we would have reshaped into [28,28,3] 3D matrices. 

Extra dimenion is channel which is used in Keras.",b8908b23,0.5
23127,a6b9837940ee38,d9115077,"* Convert training dataset,validition dataset and test dataset to tf.data.Dataset. Then change the order of data, set batch_size to 128 ",52d2acc7,0.5
23131,4ba67fa2de9e6e,7e6cf1db,"* Median number of bounding boxes in each image seems to be around 42. Let's keep this is mind in case your detector uses anchors and there might be a need to recalculate anchors that are smaller than perhaps ImageNet (if you plan on using pre trained weights)

* There are **49 images in the train set without bounding boxes.**

* Keep in mind some detectors are better/worse for small objects and some detectors might have performance issues around grid if they use a grid at the detection stage. ",8e0dd482,0.5
23136,30c8dc87ce52ca,72de1250,LOGISTIC REGRESSION USING PCA COMPONENTS,805e9d67,0.5
23139,9ceb7278784462,a03040a6,## Model Tuning,3768a567,0.5
23140,5083d7a61f2426,baea9254,"Trainning the model

If it is taking too long, set less steps or change the look back value",541a0fec,0.5
23142,28a1ff0f223da9,e00f6891,"# Task-3
**Which country hosted majority of our teachers?**",c945b27d,0.5
23144,09751c520b0616,111af6a1,#### (iii) TotalBath : Sum of : FullBath and 1/2 HalfBath,a4d0c7e9,0.5
23151,edc19e349fe80a,742c4f75,"Our data has 400 samples, thus using normal equation is quick and easy to implement.
### 2. Thực hiện Code để Train model và tìm ra weight tại đây !!!",7882221a,0.5
23152,20c9a2456e494a,2da869d0,# Defining the CNN,3e487f55,0.5
23155,c9dc8d00773da4,6aa64b87,"# Image num
50210",d9aa2f85,0.5
23156,72d528df923403,f4c702ed,"- Over time, the average number of units sold in each department has gradually decreased.",d51c8e8e,0.5
23157,ff83da40bcdb19,243319ce,"**PART 5:**
*Here diving our training data 500 image for train and 100 image for test and than more diving as x and y.*",36b5ec8c,0.5
23158,593d1d3d1df05a,e8ded8bb,> Separating the Characters in the Plate to identify them seperatly and using the model definied later for it,bc682ffe,0.5
23166,b10bd75889dad9,6e36451d,## Data Preparation for Modelling,ee00ceee,0.5
23168,bc058fe14d3d1b,f460d733,area,d0273670,0.5
23172,dd3721cb49c1fd,282c085d,"<div style=""margin: 0px; padding: 10px; background-color: #ef9a9a ;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h2 style=""color:white;text-align:center"">All the performance metrics such as Mean Absolute Percentage Error(MAPE), Mean Fit Time, Mean Score Time can be listed.</h2>
  </div>
</div>",1a53fdd9,0.5
23178,c65f7b375af4ef,d886c7a5,"### Listemizi daha iyi incelemek adına , listeniin basını ve sonunu yeni listede tutucam 
Not : 
1. burda ;
databas = K_basina_ortalama_listesi.head() direk eşitlemek doğru bir kullanım değildir aslında, ancak bana bu şaun bu lazım. (doğru olmamasının sebebi ise K_basina_ortalama listesi değiştiğinde databas ve datason da değişir bazı projelerde canınız yanabilir) 
1. axis ve ignore_index paremetrelerine dikkat edin !!",871d53ca,0.5
23183,8cd6656a65e6e7,fb0d4209,"<h1 id=""real"" style=""color:blue; background:white; border:0.5px dotted cyan;""> 
    <center>Real NVP
        <a class=""anchor-link"" href=""#real"" target=""_self"">¶</a>
    </center>
</h1>",c8e1697a,0.5
23185,0dd3ac2d55efd7,f993c4c7,"Below I have loaded in the dataset uploaded by @anindya2906 . In this dataset we have 50,100,200 and also 300 dimensional glove pre trained word embeddings.
Or else we can find the embeddings at the official website [here](https://nlp.stanford.edu/projects/glove/)",e9aa2cc2,0.5
23192,1011899b959f44,1ce7e991,6. What is the average number of deaths for all the battles in the dataset? (Hint: Use .mean()),0b112382,0.5
23194,6f1481148352e9,2cded71d,"**The largest number of fires was in Mato Grosso - 6.2к, Piau - 2.8к и Paraiba - 2.7к.**",7cfbdb8f,0.5
23195,0e2a23fbe41ca9,3098740f,"Observations:
- There are 24 unique states; not sure if this is US (US has 50 states)
- 3.5% nulls

```state_id``` and ```category_2``` has same number of nulls - 11,887. Let's see if they occur together",64e4762c,0.5
23198,fbb1f9d3818830,02ce4116,"# Model

In this test, I make use of the famous MobileNetV1 model with imageNet weights. You can use any model you like but remember to update the name of the layer whose features you want to visualise, since different models have different naming conventions based on the architecture they have. Those names can be easily found using model.summary()

**Blocks to visualise**

Since MobileNetV1 makes use of depthwise seperable convolutions i.e. depth wise followed by point wise Conv2D forms one proper unit(1 block). In this notebook I only evaluate the activation maps of the point wise Conv2D o/p [that will be seen as 1 block] & also the blocks used will be of earlier layers since the captured local features are easier to understand when compared to the deeper layer's abstract concepts.",c7027f86,0.5
23206,ad26c020235dfc,ec726b28,Define train and test data randomly:,bf766e48,0.5
23209,fae5023faa435f,f2598f18,# Data Preprocessing,b37c893b,0.5
23210,0d59a3e0130db0,9041a5f3,Replace class labels with integers,285f04b2,0.5
23211,b4ecd6e4277e3c,ca82fe6f,### LOAD DATASET FROM DISK,94d79d5f,0.5
23213,52cfd66e9ec908,5edb5ef1,It seems like the two centroids have a somewhat strongly negative correlation and seemingly similar variable distributions. It seems that as such there is a negative correlation between both the variables.,c74adcdf,0.5
23223,47e3a1925754c2,1bb1f07f,## <u>State-Wise Analysis</u>,ecc615b7,0.5
23225,2ca509e51a6e4b,94b635cd,"Now we have a series with a two-level index. We want to convert this into a matrix with `country` on one axis and `category` on the other. To do this, we can use `.unstack`. By default it'll put `NaN`s where data doesn't exist, but we can tell it to fill those spots with zeros.",e63b0ba6,0.5
23228,1a285e4c830f3f,5d6939c4,"### Parameterkurven für Supportvektormaschinen (mit Kernel Trick)
Anstatt des linearen Soft-Margin-Klassifikators nehmen wir nun die Supportvektormaschine mit einem rbf-Kernel. Dies führt einen weiteren Parameter ein, $\gamma$. Angenommen, wir wüssten den bestmöglichen Wert für $\gamma$ bereits: Dann können wir für C und $\gamma$ ebenfalls jeweils eine Parameterkurve zeichnen:",360b50e9,0.5
23231,ea2763c0f6c6a0,59502daf,# Wordcloud: All data,e5812ac1,0.5
23232,d1f92a87a0a1a5,4af3349c,"# Problem
### In years of no huge earthquake, if assumpting small earthquake ocurred once a year, the prediction result would be dominated by its assumption. If assumpting no additional earthquake ocurred, number of data points is not enough for the model.",2cd610b2,0.5
23234,fcb15f03bd0239,256efcbb,"esperanto lack

Q,W,X,Y
",3b9bee3a,0.5
23235,1fac5edd4063ba,f5db9e53,"#I chose United Kingdom of Great Britain and Northern Ireland, according to the charts above they showed one of the higher positions  ",04bc01e0,0.5
23237,a2444ab5d5f147,f5e80994,### Adding Phrase Length as another feature,10617755,0.5
23238,1c5aaf7bea6414,87e09785,# Counting Total Prices,34d8f42d,0.5
23239,0504abe8519634,1629884d,"# 2. without stopwords, without ngrams",46df846a,0.5
23240,98ea617d18c9cc,c67830e3,# Creating data generators for Stratified KFold,e6316d11,0.5
23245,254cccd5145725,75c520fa,# Missing Values,a49b4037,0.5
23252,9169c4e9c33c90,cebf2b4a,"Of those who made the Top 50 more than once, a majority (50.8%) only reached it twice.",725bf880,0.5
23254,36c35f0a9f70f7,a777a63c,## K Nearest Neighbors Classifier,67358bc7,0.5
23263,a76e0e8770b7a0,574b169e,What about all countries?,02863d3b,0.5
23268,c0ddb77bf32e2b,8315ec95,"So far, it matches with [this news(PM2.5達「非常高」 桃園、平鎮和龍潭民眾少外出)](http://news.ltn.com.tw/news/life/breakingnews/1541461)
> (2015-12-16) ...受境外汙染物影響，目前桃園、平鎮和龍潭測得細懸浮微粒（PM2.5）指標已經達「非常高」等級，大園和中壢也達「高」的等級，空氣品質極差...",a0cb45f7,0.5
23271,c5fef7cc592736,dadee13d,"## Training

Let's create the model. We need to change the first layer because it expects an image with 3 channels but we are using a BW image with just one channel and adjust the number of outputs because it is adjusted to 1000 categories by default",d21dc2c1,0.5
23274,842547b2def18c,8851decf,Let us replace Age with ordinals based on these bands.,b8efde6d,0.5
23281,1d1598b6fa2aa7,3fdddcfc,"### Bubble Charts

Also Bubble Charts

More information - https://plotly.com/python/bubble-charts/",e066accf,0.5
23291,e4525eb0c96f28,5ce825ad,"### Sales by Platform

This is important to view because what if the overall negative correlation of sales over time is because certain platforms are more negative than others? Are there any platforms with a positive correlation of sales over time?

First I show the 10 most popular consoles, and then further explore if they could have a positive sales over time.

From the Top 10 Platform sales no conclusion can be drawn yet, but we can take note of how consoles seem to be performing generally better than handheld or PC.

New Columns:
- Platform_Sales: Total sales per platform

New Dataframe:
- **platform_df**: isolates the Platform and Platform_Sales columns.",2093a1f1,0.5
23292,dcb7050d126707,a272d316,"#### Quick prediction

Now you can use pretrained Detoxify models without internet connection.

See https://github.com/unitaryai/detoxify for details",0fe949be,0.5
23299,da199f8fb59439,3c97375d,"> Most number of movies & tv-series are from :-
* United States - *58.4%*
* India - *18.1%*
* United Kingdom - *8.11%*",baaa665d,0.5
23303,268a610bbc64b4,32353a03,# 2. Find the number of users who have booked more than twice on our Web platforms?,8a16f301,0.5
23304,0687cd5c8597db,c5808721,### **Architecture of the model**,4edec76a,0.5
23305,aa46e9376825a5,3ef9d178,"# Aviation Data
Tasks
1. View Aircraft make name
2. View State name
3. View Aircraft model name
4. View Text information
5. View Flight phase
5. View Event description type
7. View Fatal flag
8. Clean the dataset and replace the fatal flag NaN with “No”
9. Find the aircraft types and their occurrences in the dataset
10. Remove all the observations where aircraft names are not available
11. Display the observations where fatal flag is “Yes”",57792d96,0.5
23306,d8d227c158d883,ccaae95d,### Validation,3391b4a7,0.5
23307,d6cbd7160961dc,9fad8f27,"# 5. Application 2: Real data set from Brazilian cities - 2010 (Type of analysis disaggregated)
* 5.1. Preparing the data set
* 5.2. Running the script on the data set
* 5.3.1. Results: First Digit
* 5.3.2. Results: Second Digit
* 5.3.3. Results: Third Digit",36d74664,0.5
23309,fe7360cddc13e5,89ca555c,"Modelin satın alma sürecinin dinamiklerini yakalama performansını değerlendirmek için birbirinden çeşitli satın alma ortamlarını yansıtan simülyasyonlar üzerinde çalışılmış ve parametreler için 3 er farklı değer belirlenerek tüm bu parametre kombinasyonları ile test edilmiştir. 104 haftalık sürecin ilk 52 si train kalan 52 si test için kullanılmıştır. Modelin başarı kriteri olan MAPE (Ortalama mutlak yüzde hata) 'nin ortalaması %2.68 çıkmıştır. MAPE'si en kötü çıkan satın alma ortamında MAPE, gayet kabul edilebilir bir değer olan %6.97 çıktı. 

Gerçek hayat verileri kullanılarak kurulan modelin sonuçları da son derece başarılıdır. Pareto/NBD modelinin sonuçlarına yakın bir performans yansıtmasına karşın biraz daha iyidir.

- Satın alma sıklıkları düşük olan verilerde modelin tahmin gücü görece daha düşük olma eğilimindedir.

- Model ayrı edinme kanallarına ve ayrı çeyrekliklere ayrı ayrı uygulanmalıdır.

- Geçmişte uygulanan pazarlama faaliyetlerinin tahmin edilecek dönemdeki faaliyetlerle benzer olacağı varsayımı altında doğru çalışır.

- Alışveriş tutarı, alışveriş zamanından bağımsız olduğu varsayımında çalışır. ????
",8979e423,0.5
23311,e4c6dd957eb5ce,9d8e6cf4,"
Cool. We can see that only a small part of all users have at least contributor Tier. <br>
Could us infer that 97.9% of Kaggle community is passive? We can't forget that have many people that visit the platform but don't have already registered.  ",2e383665,0.5
23316,a3ae04b78e45b5,e0f805df,**PIECHART**,4195da8b,0.5
23317,9daf8b4a46725e,c6ab8b5d,### Checking Variance,7d9cc411,0.5
23324,d1ff7e10ee0102,46845cac,"# Out liars!

Outliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.

Outliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.",2cc71c3c,0.5
23325,49f2274c1dd516,99c5b20f,"## County Health Rankings

The annual Rankings provide a revealing snapshot of how health is influenced by where we live, learn, work, and play. They provide a starting point for change in communities.",06b0ffee,0.5
23326,df2a7968c08ee4,6b22b92a,"### Visualizing Image

Here we are retrieving one image and label from the training_dataset. If we instantiate the train_dataset again in the cell above we can see that the image created below will be different every time. ",a2ba0a72,0.5
23338,892be0a523578c,6635d4b6,"For merged hourly data (**hourly_cal_intense_steps**, dataframe object created in the prepare step) and minutely data (**minute_cal_intense_mets_steps**, dataframe object created in the prepare step), I selected participants that were used above",b0e8d7c0,0.5
23340,44f6a002ecd033,84880076,## Logarithmic Cleaning,70bbe106,0.5
23342,0cb9adc158b705,707a2e7c,"### Model

Fastai has an awesome class which puts everything together, called `cnn_learner`. Here we are using ResNet50. ",3abf056e,0.5
23343,c0e2a467cf23ee,f5e6cc31,The following functions allow to calculate the route length,6d02cd29,0.5
23348,fdc3afd309b850,3d081c02,"Filling in the addresses without latitude and longitude, searching for the address's latitude longitude on Google",966bde38,0.5
23353,ad121e0531afa4,2bd1562c,<h1 align='center' style='color: #8532a8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px'>2. Dataset Class</h1>,a3492905,0.5
23355,1084376bc4897c,1ba0aa15,"#### We observe some strong correlations from these plots. The following feature values are sharp indicators of heart attack:

    - Number of major vessles equals to zero
    - Thall equals to two
    - slp equals to two 
    - Anginas that are not exercise induced
    - Having ST-T wave abnormality in resting electrocardiographic results
    - Having atypical angina or non-anginal chest pain
#### Moreover, men are more likely to have heart attack than women.",1b598487,0.5
23356,ef6d1e959a873e,411df08a,"replacing values(6,7,8) with value(5).as you can see 6,7,8 didn't have much impact on response.",f11a1f43,0.5
23364,62ae2b200f6b36,b52df215,Data_Visualisation for number of registered bikes.,7da4ea31,0.5
23365,e1fff2f67cbe32,8f6086a4,## Question.csv,c6dfde64,0.5
23366,1d5daeca89f48d,768715e4,"# Term Frequency- Inverse Document Frequency(TF-IDF) 

Weight is used to evaluate how important a word in corpus",48d478bc,0.5
23372,c54ea4523bd49c,562c9aea,"Here I define the data augmenter. I use x,y and rotation as the images were taken from aerial and thus can vary in these ways.",097ccba2,0.5
23377,b066ab2167199c,bc0830d6,"<h2 style=""color:blue"" align=""left""> 3. EDA (Exploratory Data Analysis) </h2>

- EDA is a way of **Visualizing, Summarizing and interpreting** the information that is **hidden in rows and column** format.

- Find Unwanted Columns
- Find Missing Values
- Find Features with one value
- Explore the Categorical Features
- Find Categorical Feature Distribution
- Relationship between Categorical Features and Label
- Explore the Numerical Features
- Find Discrete Numerical Features
- Relation between Discrete numerical Features and Labels
- Find Continous Numerical Features
- Distribution of Continous Numerical Features
- Relation between Continous numerical Features and Labels
- Find Outliers in numerical features
- Explore the Correlation between numerical features",18a1753d,0.5
23378,cb4ad8ed4cb300,f3f5d17a,"*Cosine Similarity*
$$\frac{\vec x \cdot \vec y}{|\vec x|\cdot |\vec y|} = \cos(\sphericalangle(\vec x,\vec y))$$",7c0f3236,0.5
23384,9ec2fb131cf677,0dcbc48d,"## Top 5 Events By Comments
<table style=""border-collapse: collapse; width: 436.0000pt; margin-left: 20.4000pt; border: none;"">
<tbody>
<tr style=""height: 26.5000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; background: #f79646; border: 1.0000pt solid #ffffff;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #ffffff; font-size: 16.0000pt;"">Events</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt solid #ffffff; border-top: 1.0000pt solid #ffffff; border-bottom: 1.0000pt solid #ffffff; background: #f79646;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #ffffff; font-size: 16.0000pt;"">Comments Count</span></strong></p>
</td>
</tr>
<tr style=""height: 26.5000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; background: #ffffff; border: 1.0000pt outset windowtext;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">TED2013</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt outset windowtext; border-top: 1.0000pt outset windowtext; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">24548</span></strong></p>
</td>
</tr>
<tr style=""height: 26.5000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: 1.0000pt outset windowtext; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">TEDGLOBAL2010</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">19251</span></strong></p>
</td>
</tr>
<tr style=""height: 26.5000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: 1.0000pt outset windowtext; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">TED2011</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">19214</span></strong></p>
</td>
</tr>
<tr style=""height: 26.5000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: 1.0000pt outset windowtext; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">TED2009</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">18558</span></strong></p>
</td>
</tr>
<tr style=""height: 27.3000pt;"">
<td style=""width: 165.9500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: 1.0000pt outset windowtext; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""221"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">TEDGLOBAL2013</span></strong></p>
</td>
<td style=""width: 270.0500pt; padding: 3.0000pt 6.0000pt 3.0000pt 6.0000pt; border-left: none; border-right: 1.0000pt outset windowtext; border-top: none; border-bottom: 1.0000pt outset windowtext; background: #ffffff;"" width=""360"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">17715</span></strong></p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>",211ea6bd,0.5
23389,ffc9490c4f6c38,cb2c4433,"## Predict with reduction
Can I drop these cells?
",ae7bbbb3,0.5
23391,aae204e78a48d1,a54f78b5,"# Hypothesis 3: customers who transact less are more likely to attrite
In this hypothesis we want to understand if customers who transact less are more likely to attrite.  First lets look at overall transaction levels",53ab6133,0.5
23392,9eb3c43fec604e,50125842,**The dataset is highly imbalanced**,41e825ff,0.5
23394,27778055896e17,bd9353c7,# Decession Tree,1dbe0165,0.5
23395,1294fb4c86f993,39e490cf,### Research Question3: Which states have the hightest gun registeration in 2020?,4471e513,0.5
23396,450fda47b03baa,11f2fdfd,Veri çerçevemizin ne kadar dengeli dağıldığını sorgulayalım.,62c04adb,0.5
23397,09bac0c221388e,db73ab2a,"WE will be using **spacy**, a fantastic library designed to implement standard NLP pipelines expeditiously and smoothly, we can implement this with a few lines of code.",bea4aa2e,0.5
23403,91eaec994e0c6f,663717f8,- CA_3 is the store having the highest number of sales.,376aef10,0.5
23410,49ac6594c8f5cf,f19517d4,But candidates with work experience earn more :D.,6f19f28a,0.5
23412,a915263bc207da,92028b4a,##### Boxplot to find outliers or exceptionally popular movies,b17ebcda,0.5
23415,f3c6048d1058e3,8c31bf44,"<a id = 12></a>
<h2><font color = MidnightBlue>Bigram Analysis</font></h2>",1d9056b0,0.5
23417,e0f03003a69819,85c26d13,# Correlation check,609ad1f4,0.5
23418,510b8303776bb6,0d2f3d35,"Here in order to make the machine learning model, I have taken the threshold to be 0.15. Thus we will take any columns that have a correlation of greater than 0.15 and discard the rest.",18080db8,0.5
23419,7b59fc0ff0d10b,c33f0d92,"Let us zoom into the Mayaguez Gas plant, chosen for its remote location next to the sea and relatively low population density of the surroundings which would be expected to have no other sources of NOx. We take a look at the NO2 column and cloud fraction at the plant on various randomly chosen days.",c192b94c,0.5
23424,71b75664517244,d54092db,"# Managers

Now let's talk about managers, team won't be a good team without great manager. Let's take a look into our manager dataset.",fc905af5,0.5
23428,117fc0956643d0,90b29678,"<div id=""step3""></div>",68cef9fd,0.5
23434,5ba4207c371899,f2cf6ea7,The thing to notice here is the difference in each protocol type. Our initial impression is that protocol may be useful in being able to identify the type of traffic we are observing. Let's see if flag behaves the same way.,187b1451,0.5
23443,3597174a998d4d,3b2ec135,"Based on these conclusions, I construct a new variable named customer_type_new.",276892ed,0.5
23444,e78e7edae89049,7fc2dfba,# Moving Average model,9cef1d94,0.5
23447,2c8119a4061997,79a752b3,The code below computes the competition metric and recall macro metrics for individual components of the prediction. The code is partially borrowed from fast.ai.,1836a79c,0.5
23449,02b7e38902069e,d41bcfe3,#Set the path so that Python knows where to find these on your computer:,726a03a0,0.5
23450,57740be713cf12,40f13851,## ***4. Split Data***,ac122df5,0.5
23453,de577c910a687d,b4a08094,"# Missing Values
Refer to [TPS Sep 2021 single LGBM](https://www.kaggle.com/hiro5299834/tps-sep-2021-single-lgbm/notebook) by [@hiro5299834](https://www.kaggle.com/hiro5299834)",c3ebadbc,0.5
23461,930cd79ca51204,92717ad2,4. Let's get back to our scatter plot and improve it.,5506779a,0.5
23463,245c89d02f3f5f,04b7b0cf,# Further Training,61a1eacd,0.5
23467,0635781991a885,11845167,"**LightGBM increase the score to 0.88537, which is a lot without any parameter tuning. CatBoost can also get this score but it need to be tuned.**
> VERSION: 03 **Now we will do some tuning this lightGBM to make it work more better.** 

A person in the comments suggested me to try the OPtuna OPtimization. SO i thought why not we give it a try so here. we are trying that.",13ab3e33,0.5
23468,8dd655515e7d18,adebbe04,### Neuroticism Analysis,895f41cf,0.5
23469,b547f0f38f7744,c2f6a622,"### Create Dataset

The dataset should inherit from the standard [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class, and implement `__len__` and `__getitem__`.

The only specificity that we require is that the dataset `__getitem__` should return:

* image: a `numpy.ndarray` image
* target: a dict containing the following fields
    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`
    * `labels` (`Int64Tensor[N]`): the label for each bounding box
    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation
    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.
    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.",b6ba66b3,0.5
23475,312d2a3c7547f1,d1447c75,Handling Categorical Data,8fd1efcd,0.5
23478,6b2776f151ed9c,061360f9,# SelectKBest,40406d5c,0.5
23484,3fb15e6e48aec2,b819bf52,"* Age - mean by Pclass, Sex",9d1f4358,0.5
23490,18c751d4a02149,3997ca78,"For illustration, I'm assuming that sample_submission is my final result dataframe df. In your case, it will be whatever dataframe contains your final classification result.",82cedca3,0.5
23491,6b383ec35229a2,551c5155,"
For each word in our vocabulary, we will assign a pretrained embedding weights from GloVe. But what about words for which we don't have a pretrained embedding?

we can initialize the weights using a normal distribution with 0 mean and some small std. Why 0 mean? because we can expect that the average of the final weights is close to zero (some will be positive and somme negative). But since we can extract those statistics from GloVe, we will use them instead
 
",0c41b61b,0.5
23492,c8e2a9ede45b06,9865c296,"Column headings are as follows
* yyyy = Year
* mm = Month
* tmax = Maximum temperature (Celsius)
* tmin = Minimum temperature (Celsius)
* af = Count of Air Frost days in the given month
* rain = Total rainfall (mm)
* sun = Sunshine duration (hrs)",42448353,0.5
23494,bd0e173abb7b52,83b54f3e,"<h5> Observation </h5>

From above findings we can conclude that it is not true that people who earn more than 50K have at least high school education.
As stated above, we can see that people who have done 1st to 11th school also earning more than 50K",9bce3b0d,0.5
23497,2a377ced98d67a,0ea08aa2,### 3.4. Regplot for showing relation between all features and target,262231a8,0.5
23498,c01049afb6d307,641c7fe3,## Servicetime -- Age & Socialdrinker & Bodymassindex,d37d3b5d,0.5
23499,fc8e0042411c46,454d16b6,## What matters most to you in choosing a course,af476c2a,0.5015673981191222
23502,30fdc4a6e3c1db,a6b7d203,# 4. Time Series Views,6111ddee,0.5029239766081871
23503,5ce12be6e7b90e,87817715,"### Concatenation 
We can concatenate strings using the addition operator `+`.",c0ab62dd,0.5029239766081871
23505,b10bd75889dad9,9363d309,### Dummy variables for the categorical data,ee00ceee,0.5033333333333333
23506,726833f92fb87a,d125eb26,"**It looks like that customers tends to accept more the deposit on march, april, september and october.**",7dc5e1b6,0.5033557046979866
23507,5f32117bcd5255,096999e3,### ALL OBJECTS,85882abf,0.5033557046979866
23508,eda49464dd6d1b,3ebe57e8,"### The following code optimized a random forest for accuracy.  It achieves an AUC of 0.855, which is difficult to improve upon.",8421f81f,0.5034965034965035
23511,b61ab8f81dc03d,019173a4,### Passengers distribution by deck (Survived/ Not survived),64d05394,0.5035460992907801
23512,a566b5b7c374e7,ada1c726,### REM Sleep Time,b3dc5545,0.5035971223021583
23513,b01ee6cb674fa3,079bb46a,"# Virgin Orbit

A british company with HQ in USA

A company witin Virgin Group (British) which plans to provide launch services for small satellites. 

The company was formed in 2017 to develop the air-launched LauncherOne rocket, launched from Cosmic Girl, which had previously been a project of Virgin Galactic.[2] 

**Based in Long Beach, California**, Virgin Orbit has more than 300 employees led by president Dan Hart, a former vice president of government satellite systems at Boeing.[3][4]",a8ffd35e,0.5036231884057971
23516,ba4b3bd184acbb,347f04b5,"### Changing Column Types

Finally we can cast the columns using the `astype` method.

Using the `memory_usage` method before and after casting the columns shows how much memory we save.",0f5de724,0.5037593984962406
23518,ce9ed5e2d601d7,8378a6d6,## Wide & Deep model,f58a2f43,0.5039370078740157
23520,e19e307b3fd188,eba27b24,### Property tax,2173955b,0.5040650406504065
23523,4ae6a182abac64,dc2aed81,### 2.1 Filling missing Values,418676c5,0.5042016806722689
23524,c4386b8a01d66e,4912b231,# Turbidity,dc732bf5,0.5042016806722689
23531,ac1abfe1dfe815,9a1e1f0b,"**Encode the target column**  
from (positive, neutral, negative) to (2,1,0)",6529dbcb,0.504424778761062
23534,c84925c8171900,71e67c25,"<a id=""publisher""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.3 Publisher Wise Analysis
            </span>   
        </font>    
</h3>",e21ff7ec,0.5046728971962616
23536,7454fdc444df16,ce26000d,"## Training & Testing Split

There are a few things that we must consider before splitting our data. The first being the ratio of cancerous to non cancerous images that we have. We determined earlier that our data is not split uniformly. We have many more, non-cancerous images that non-cancerous images, which may possibly bias our model. 

This may also pose a problem when evaluating our model. Let's take an extreme example where we have a dataset consisting of 100 points, 90 of them are cancerous and 10 of them are non-cancerous, let's also assume that our model classified our entire dataset as cancerous, this would give an accuracy of 90%, however this is not really representitive of what is really happening. In reality it is always misclassifying our non-cancerous data.

So how do we go about solving this?

We can use the train-test split function offered in sklearn model_selection, this function has the option to split our data such that we get equally distributed training and testing sets when grouped by the classes.

For more information, check out this [article](https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50), that talks about the considerations that need to be taken before splitting your data.",a7818ef5,0.5047619047619047
23538,bbaa07ad21cf4e,88d7a321,### Gene,3ab6b254,0.5047619047619047
23543,98a6794067932a,6bd2c156,"La cellule de code ci-dessous permet de dégager plusieurs statistiques auprès des ventes de chacun des états tels que le nombre de commandes effectuées, la moyenne du montant de ces commandes, le maximum et le minimum du montant des ces ventes. Ce code sert donc à regrouper les ventes par état et à extraire des statistiques dans un dataframe. Cette analyse pourra être utile pour les dirigeants si ces derniers souhaitent avoir accès à de l'information plus précise par rapport au comportement des ventes des différents états.",08600fe2,0.5048543689320388
23549,840534f2908a9c,4d9d0e6e,*Info : Normally airports pickup or dropoff have fixed prices. We can define non-airport category to see the effect on fare amount.*,8081c3cc,0.5052631578947369
23555,396bc36edb95d3,add9273e,#### Confusion Matrix and Classification Report for Training Data - CART,965e4f8f,0.5055555555555555
23558,5f27526aa6c113,633750fe,more claims are from single people rather than married.,a5c26ab6,0.5056179775280899
23559,e67925694c07d3,4e629f37,"
its interesting to see 3 EXTERNAL FEATURE have the lowest correlation
lets dive a bit deeper on those features",83af4c4a,0.5056179775280899
23560,2ada0305b68956,cc862a71,### 85. Palette = 'bwr',133e26f4,0.5057142857142857
23561,14defffcd250f3,5b15b004,# Data Cleaning,3a683b94,0.5057471264367817
23564,d5f78aa381f58d,979207a6,## Logistic Regression,d60f358f,0.5057471264367817
23566,869a39a3d4dea2,aac9ea49,Open CV Arithmetic,9020daf8,0.5058823529411764
23567,513ce405d7f6a3,4c3c1179,# Base line for a model,8461e086,0.5058823529411764
23569,835a7b4e660d23,759a1028,### Missing Data And Checking With Assert,53bc7a6e,0.5060240963855421
23571,4883314a96dc34,2afc940a,"**Model Types**

**- Linear models:**
* Logistic Regression (LR)
* Linear Discriminant Analysis (LDA)

**- Nonlinear models:**
* K-Nearest Neighbors (KNN)
* Classification and Regression Trees (CART)
* Gaussian Naive Bayes (NB)
* Support Vector Machines (SVM)
* Ridge Regression (RR)

**- Bagging ensemble models:**
* Random Forest (RF)

---",50d36836,0.5061728395061729
23573,fdbbd573ba31c2,062d7465,## Handle Extreme Outliers,f7c28d74,0.50625
23576,241cf32abb22d8,62a4f962,"The performances after feature selection by both Random Forest Importance and F-Score are statistically better than the performance based on the full features, at 5% level of significance. 
Meanwhile, the difference between F-Score and Random Forest Importance is also statistically significant. Therefore, for the further analysis, I continue with the top 10 features selected by F-Score as shown in the below figure. In this figure, it shows that the importance decreases sharply after the top 2 features. The importance becomes very marginalised till the last feature.",47157066,0.5064935064935064
23577,75adb7945ef9bd,eada3e7c,We try to distinguish disaster and non-disaster tweets:,785c5095,0.5064935064935064
23579,2cb457b60dd246,bca21d11,### ***Mobile net***,339367df,0.5064935064935064
23581,4ae464582bac51,3e23f6ec,Most people who have had a stroke are married.A maioria da pesssoas que tiveram AVC são casadas.,ca6a52ce,0.5064935064935064
23585,7e1da639035ac5,a9b11d2d,# <a id='10'>10. Collaborative Teachers analysis</a>,120b6c23,0.5066666666666667
23595,fdc9f4863744b1,aa4ddc92,There is definately strong correlation between Gross Square Feet and Sale Price. Regression line is not that bad in this case so I can use Gross Square Feet as a predictor variable.,b4529365,0.5068493150684932
23602,631cd434fc3aa2,d07230d1,"* _MasVnrType_ and _MasVnrArea_: almost all the missing values in the first column correspond to a missing value for the second, so it should be plausible to assume that a NA measn no Masonry veneer area/type. Therefore, for almost all the missing values we'll set them as zeros and Nones. However, there's an house with a missing value for _MasVnrType_ but a specified value in _MasVnrArea_ (check in the code below). Imputing this observation with a None in _MasVnrType_ could be misleading and we see that the majority of houses with that price (between 196.000 and 200.000) have 'Brick Face' as Masonry veneer type (see code below), so we could impute it with this value.

**NOTE**: I'm feeling like there could some issue with this, but I'm not really sure. I've compute the 'majority' of _MasVnrType_ using also the test data, maybe it would be better to use only the train data to impute this one? Could be data leakage?",2b74febb,0.5070422535211268
23606,a1a31459abf078,9f1e86e1,"In the data description above, ```prior_question_had_explanation``` is defined as - Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between.

We notice that whenever users saw explanation to prior question bundle, their correct answer rate improved substantially. Also for more than 90% of the question bundles, users referred to the explanation after answering the questions.

Also the ```prior_question_elapsed_time``` i.e. time spent by users on question bundle does not have any impact on likelihood of users reading explanation of bundle after answering it.

## Question Tags
",66fc0f54,0.5072463768115942
23609,17a24d566ffa59,77b0d613,1. ![](https://s3.amazonaws.com/nlp.practicum/svd4.png),89049e56,0.5072463768115942
23611,7e89d387feb9f5,cc2119d3,## 3. Виды кухни,989e3a1b,0.5072463768115942
23618,7f74a04ae75792,3fe37533,The missing values are now replaced with mean,d01e91da,0.5073529411764706
23619,c65a65d4041018,b7718b96,"Quite interesting!
* Kaggle forums and Medium seem to be universally popular;
* ArXiv is quite popular, but Russia likes it the most!;
* Twitter, Knuggets and Reddit are also widely used;
* In America https://fivethirtyeight.com is very popular, even though it is almost unknown in other countries;
* And of course Siraj is popular in India with his famous teaching style;",824fb229,0.5073529411764706
23620,ba655a261cc09e,22048075,"The distribution has not changed, so we leave the combined feature.
We can also notice that people who traveled with 1-3 relatives had a higher chance of survival than those who traveled alone or in large groups
Let's highlight another trait

Распределение не изменилось, поэтому оставляем комбинированный признак.
Также можем заметить, что люди, путешествовавшие с 1-3 родственниками, имели выше шанс выжить, чем те, кто путешествовал один или в больших группах.
Выделим еще один признак",48cc549a,0.5074626865671642
23626,03048e86a6d806,5c75628f,"Respondents from US, India, South Africa, and some European countries countries mostly work in the companies that employ 20+ people for data science roles. From this dataset, respondents from other countries work for companies that have smaller data science team.",1285c231,0.5076923076923077
23628,f2f2db16a2f86c,5fef3b9e,Added more features.,ffc6a115,0.5076923076923077
23629,a8c042af6b7245,1b0e800f,"Only ps_car_11_cat has many distinct values, although it is still reasonable.

",2487ac62,0.5076923076923077
23630,c115e287523aab,3a27834a,"# Data Split
* Data is splited using **Pawpularity** distrubtion.
",feb1288b,0.5076923076923077
23632,d07915a6e6992e,d88c1ba7,**Fare**,2b912140,0.5076923076923077
23643,06ecf7a304c309,902d1222,- Before adding noise,714de627,0.5079365079365079
23644,8985a124d4b657,34ea5705,"We can try out different optimizers to see which minimizes loss and maximizes accuracy. Stochastic gradient descent (SGD), Adam, AdaBoost, RMSProp are few of them. lr = learning rate, decay = by how much to decay the learning rate, momentum = how much should the gradient descent be accelerated to dampen oscillations, nesterov = whether to use nesterov momentum. Nesterov has stronger convergence for convex functions. And then we compile using MSE (mean squared loss) as our loss function.",586d1846,0.5079365079365079
23649,0858e1bb3cbaca,906bd139,# Sort,78548374,0.5081967213114754
23652,37b09262279764,1c06238c,### Training the model,37c4c417,0.5083333333333333
23653,62487bcd70b199,e7421843,## <a id='6.1.'>6.1. KNN Classifier</a>,f6ae50af,0.5083333333333333
23654,ed8009f482b380,806044bf,First we separate the target and the features,e99941fa,0.5084745762711864
23656,9169c4e9c33c90,e406b4ca,"<a id=""User_Rating""></a>",725bf880,0.5084745762711864
23658,c4bca5d86a38c3,f7052bd0,Haciendo OneHotEncoding de la columna Embarked,e23d297c,0.5084745762711864
23660,bb0905d33ae417,6b4fafdd,"We will now train the model on the same dataset, except we are using images of higher resolution. Intuitively, the 'concepts' learnt by the neural network will continue to be applied in training with the new set of images.",25fd1965,0.5084745762711864
23663,a077820f7ab459,00faa1e0,### Compile model,05a43104,0.5084745762711864
23665,2a56d6b0e153f2,25b8ae4a,"OVER HERE, WE CAN SEE PERCENTAGE IN RANGE 62% - 90% ARE LIKELY TO BE PLACED.",8dc315e6,0.5084745762711864
23666,a81661cc35d8d2,6eaef672,Making three copies of the data set,3331f113,0.5084745762711864
23669,81712ee7510ac5,ece5657a,**Nested Dictionary**,c4685e79,0.5085714285714286
23673,30fdc4a6e3c1db,b5072204,### 4.1 Across Years,6111ddee,0.5087719298245614
23682,9e27af2600925c,4faa8051,"### 4.4 - Optimization
- You have initialized your parameters.
- You are also able to compute a cost function and its gradient.
- Now, you want to update the parameters using gradient descent.

**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate.",9b556435,0.5087719298245614
23684,fe7360cddc13e5,7be95fc9,"Tüm bu bilgiler ışığında görülmektedir ki model, standart modellerden(rfm), bir müşterinin gelecekteki işlemlerinin beklenen sayısını ve hayatta kalma olasılığını değerlendirerek geleceğe dair stokastik gibi görünen bir süreci matematiksel olarak açıklama çabasıyla ayrılmakta ve modelin çıktıları kullanılarak t zamanda bir müşterinin CLV'su hesaplanabilmektedir. ",8979e423,0.5087719298245614
23689,2f0f808765fc67,36112c0d,*The 10-Fold Cross-Validation Error 2*,fd1f6494,0.5092592592592593
23691,ab6da5994949a3,a44b517a,## Visualising the Logistic Regression Test set results,fae6b91d,0.5092592592592593
23694,0ad8d416b89b78,c62e14ec,"Due to the highly skewed nature of the 'native.country' with the disproportionate majority of individuals located in the United States, it was decided to collate all the Non-United States countries into a single value. This will assist with dimensionality reduction when the dataset is binarised down the line.",0b0562f0,0.5094339622641509
23698,614ba9f0c62677,c7574ec9,"<a id=""8""></a>
### Flattening
* <a href=""https://imgbb.com/""><img src=""https://image.ibb.co/c7eVvU/flattenigng.jpg"" alt=""flattenigng"" border=""0""></a>",b8551335,0.5094339622641509
23709,fa02c409161192,e4f8d067,### Basic test to see what percentage of the trained data is predicted correctly. ,e97077f7,0.5098039215686274
23710,917957c6c4065f,f28cdcb2,"조회수 100만 이상의 데이터 570건을 제외하니,  
평균 조회수는 125,664건으로 나타나고,  
조회수 0~20만 이하에 대부분의 영상이 있는 것을 확인할 수 있었습니다.",55b8ed68,0.5098039215686274
23712,52cfd66e9ec908,a8cfab14,"### extent_x, extent_y and extent_z",c74adcdf,0.5098039215686274
23713,d0080e3a39bc5c,d48f67fd,**TRAINING THE MODEL**,2fcde4cf,0.5098039215686274
23714,64169805aacf17,57947cec,## Prepare the Canvas,1f12ded0,0.5098039215686274
23717,523123dad03177,7b28f68b,Male students score more in maths. ,48a5e4e6,0.5098039215686274
23718,32ddc45133f77b,923abc51,The model has over-fitting due to lack of data,3c0d6831,0.5098039215686274
23721,83df814455f06c,383299df,We can see that all  the variables are ordinal categorical data type.,c9cff71a,0.51
23722,726833f92fb87a,c8397f8c,## Poutcome vs campaign success,7dc5e1b6,0.5100671140939598
23725,2343dc02ffb96a,aed95d14,# Histogram + Scatterplots,29aa95a4,0.5102040816326531
23739,56785caebaa256,c7eb5edb,## Latest Cases,a792961a,0.5106382978723404
23742,f6648e47713411,b1c01c41,"### Quan sát:
Kênh màu xanh lam có sự phân bố đồng đều nhất trong số ba kênh màu, với độ lệch tối thiểu (lệch một chút sang trái). Kênh màu xanh lam cho thấy sự thay đổi lớn giữa các hình ảnh trong tập dữ liệu.",f4af4d1c,0.5106382978723404
23746,b01ee6cb674fa3,910579a1,"# Russian Aerospace Forces - VKS RF  

Воздушно-космические силы (Russian Federation)

Born in August 2015 with merge of:
- Russian Air Force (VVS) 
- Russian Aerospace Defence Forces (VVKO)

HQ in Moscow, RF
",a8ffd35e,0.5108695652173914
23750,49ac6594c8f5cf,31c909e6,**Box plot to get the Outliers**,6f19f28a,0.5111111111111111
23752,d6cbd7160961dc,255b6223,"For this application we will use a real data set for cities of São Paulo, Brazil. During 2010 the 644 prefectures of São Paulo State (excluding its capital) made several expenses. We selected 300 expenses from each city, which generated a data set of 193195 rows (Some cities did not made the 300 expenses).  

The column ""unidade"" represents the city name, while the column ""valor"" represents its expenditure in some point of the year 2010.

Now, let's analyze if the data informed by São Paulo cities comply to Benford's Law.",36d74664,0.5111111111111111
23753,c8bf959b9608cf,296e9598,"## Loss Function
Now we will define the loss functions for the style loss and the content loss. For the style loss, we will define the Gram matrix.

### Define Gram matrix
How do you capture relationship between 2 vector? By correlation! Correlation is a statistical measure that captures the relationship between 2 varibles. Correlation indicates the extent to which change in one variable increase or decrease the other variable.


We may see an image as distribution over different feature vectors. Each feature vector in a layer captures some aspect of image. For example, in the initial layer, feature vectors captures edges. Gram matrix captures correlation between different feature vectors, not the presence or absence of specific features. Gram matrix is the product between a matrix and its transpose.

![gram.PNG](attachment:gram.PNG)
",155e3672,0.5111111111111111
23754,d58491f2896fc1,86f761fe,**fitness_score En iyi ebeveynleri uygunluk skorlarıyla birlikte döndürmektedir.**,514bfdff,0.5111111111111111
23760,4fd4b6a80d40e3,a8a07f30,"## Perform Neural Network Calculation with Product of Matrices

![image.png](attachment:image.png)",f6913cc3,0.5111111111111111
23761,b0c2805cd5c087,c7beb1a7,Image Kaggle.com,0446f327,0.5111111111111111
23762,6fad63bfd45ef9,97bd88ba,# Prepare data,b3c6f1d6,0.5111111111111111
23768,7341f069d9b2ee,0e1847cd,Experimenting with Attribute Combinations,e0a49e62,0.5111111111111111
23773,d1ff7e10ee0102,0c65e0b9,### Univariate analysis,2cc71c3c,0.5113636363636364
23774,2ada0305b68956,b4cbec79,### 86. Palette = 'bwr_r',133e26f4,0.5114285714285715
23775,ee23a565163388,030285d0,"**Inference**
- This graph suggests that the failure rate is equal among both the genders.",88aacbc4,0.5114503816793893
23777,72d393488311b6,5a570111,# Feature/Label,80663df0,0.5116279069767442
23778,22ba3a8149c2f1,3e06a64e,"# 2 models, loss, optimizer",19c82be5,0.5116279069767442
23780,743ae010f5e875,244981bb,# Transform data to MLM format,02c54445,0.5116279069767442
23781,2e40928927c0d4,ecad5f6d,"**Obervations:**
The differences between the classes are very minute and intricate in *some cases*, which is difficult to detect by human eyes. So to capture the intricacies we can consider using Inception Network as it combines the information from different scales of the image and the 1x1 convolution helps to detect the complex functions as well as it helps to reduce dimension. Let's see how it goes.... I have taken help from the following link for the inception module architecture:
https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b",b6385ef2,0.5116279069767442
23782,22bd95f4807a23,f6cc7700,* There is not much variation in the average ratings given by each age group.,c05d356f,0.5116279069767442
23785,1660daf8867980,c5db2ccf,"**Theory**
* Like Policy Iteration, we can back up state-action values from the successor state action without waiting for the episode to end. 
* We update our state-action value in the direction of the successor state action value.
* The algorithm is called SARSA: State-Action-Reward-State-Action.
* Epsilon is gradually lowered (the GLIE property)",42d7cffc,0.5116279069767442
23790,87e94f864d74be,e8603270,"#### Targets based on ""rating""",294bfe9f,0.5119047619047619
23798,8cefb86a675e5d,152b9010,# Split the Dataset into Training and Test Datasets,79f9e69b,0.5121951219512195
23800,74a03887600114,7d7d3d15,Let's find the average rating of each movie,c0ffb2f0,0.5121951219512195
23803,9c26c5dcd46a25,d906db0b,Puis nous allons spliter nos données en 1 jeu d'entrainement et un jeu de test *(30%)* :,1bbbb677,0.5121951219512195
23806,0b01138ad120fc,edbe3f16,"**RNNs require 3 dimensional inputs, so we have to reshape it**",0b4b72e6,0.5121951219512195
23808,2f47abddfd1928,f8b06b94,"The correlation between Sex and Survived is very strong.

The males survived in a very small percentaje, almost 5 times more died than survived.

In the case of females, it is just the opposite, more than double females survived than died.",ae33cc0b,0.512396694214876
23811,254cccd5145725,0e02f5bc,One of the most basic yet important step in EDA is to find the missing values in the data.,a49b4037,0.5125
23819,80ad12f326ab70,15846cbf,"The top 3 school district are Connecticut, Utah and Massachusets",da404a16,0.5128205128205128
23821,4d91e84c564cbe,8b7ae0cb,"To actually call it, we add parentheses:",355a43e3,0.5128205128205128
23823,897ca904b74a98,b1feb2d5,## Encode categorical variables,c5844ad4,0.5128205128205128
23827,c8c4705cca1ebb,3e3440b0,Sales veri seti için date formatına göre gruplandırıyoruz,6d9d7107,0.5128205128205128
23831,d4c5aaa4b36810,57f7dcf9,Need to raplace the country names in the index so that we can join the data sets on country names.,65441f28,0.5128205128205128
23833,7705fc44251194,cc934636,# Xception Net,bd2cebe8,0.5128205128205128
23836,52ee792e228d54,c253b114,### Let us use the model to predict Experience and then replace negative Experience values by 0,5096094e,0.5131578947368421
23839,c9b4e282e4e2c1,b7e8f474,"* It seems like linebackers get much more plays playing as outside linebackers (OLB). 
* Defensive Linemans usually play at DE and DT positions.
* Safety players usually play at FS and SS positions.
* Offensive Linemans play at T,G and C positions, in that order of priority.
* Kickers usually stay at K position but they can play at P position too.
* The most typical plays for every type of player are Pass and Rush, except for kickers. Kickers participate in Extra Point, Kickoff, Field Goal, Kickoff Not Returned and Kickoff Returned plays.",f44d339f,0.5132743362831859
23843,bbb3f4b76a4559,9573ec59,"### Optuna : tuning hyperparameters
The next cell, recode uuid of every trial of optuna. You can change range of hyperparameters to improve output. This is not suit this dataset.  
And at first, you should set small number at NUM_BOOST_ROUND. Titanic is small dataset (and also we have only 2 dims) so it doesn't take long time. But in other dataset, I recommend to run small NUM_BOOST_ROUND and narrowing the parameter width sometimes. And then set NUM_BOOST_ROUND what you want.

And this cell print a lot of things...",75185823,0.5135135135135135
23851,63b44c85e32c1f,9abed6f9,Alternative to **remove** function but with using index value is **del**,fb9b9562,0.5135135135135135
23860,ccabe7a86825ce,f8ba84a6,**LeaveOneOutEncoder for nominal features with high cardinality**,d766cbf9,0.5135135135135135
23861,fdc9f4863744b1,ebe57a42,I will explore more around the neighboorhood and boroughs. ,b4529365,0.5136986301369864
23865,166a62ebb4fc3a,169ce192,"First, drop all ""worst"" columns",db48a079,0.5138888888888888
23871,c84925c8171900,151bf223,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.3.1  Publisher Wise Video Game Release (Overall)
            </span>   
        </font>    
</h4>",e21ff7ec,0.514018691588785
23872,fc8e0042411c46,efd2d595,- Most entries are 'Better Career Prospects'. No Inference can be drawn with this parameter.,af476c2a,0.5141065830721003
23876,0fa9979b5690e9,c5b917c7,"Para esse método de aprendizagem, o conjunto de dados wine parece ser mais difícil de classificar. Diferente da Iris, que ficava em torno de 98%, esse conjunto dificilmente passou dos 80%.
Vamos aplicar a visualização desses dados para verificar se eles são muito aglomerados ou linearmente separáveis.",c26eea94,0.5142857142857142
23879,04bac111ffbe9c,7666cc14,"##### LOOKS PRETTY OKAY.
##### OUR TRAINING DATASET IS READY. LET'S CHECK OUR TESTING DATASET",82576b17,0.5142857142857142
23880,80f86fa2d88ff1,7b3e952f,Try several models using grid search and evaluate them,f5cead1f,0.5142857142857142
23881,f4b603905215b7,438cbcff,# 2. Predict Proba,efe1d587,0.5142857142857142
23886,38b79494ac749e,44a9979f,Let's inspect our regression model coefficients:,39162a40,0.5142857142857142
23892,3cea0f929a2035,63ef6169,"In the data provided, the number of suicides between the genders and the age groups is as follows:",04cfbade,0.5142857142857142
23894,171494b45650a2,746812cb,### Topping vs Price,9c8cc578,0.5142857142857142
23896,7454fdc444df16,534ea77b,"The function below returns the percentage of unique items we have in a class so that we can gain a better understanding of our target variables. In our case, the data will be split between 0 and 1 values. These values represent the following:

* 0 -> Tissue does not contain IDC 
* 1 -> Tissue contains IDC",a7818ef5,0.5142857142857142
23897,55a5e31d03df9f,a0976aa3,"Amazing! We have now increase the train accuracy to almost 100% and test accuracy to almost 30% that is so much better than our previous model, but as you can see the accuracy from training and validation differs by a lot, event the losses are increasing in our validation set. This is a symptom of the overfitting we discussed previously and it means that our current model is memorizing the LEGO rather than generalizing.

How about if we introduce some data augmentation and see if this symptom it's better? 

**📝 Note:** Here we already identify what is a technique that is worth spending some time optimizing, we can now call our inside scientist and start experimenting 👨🏽‍🔬.",06dce00f,0.5142857142857142
23898,2b36742b49c7bc,21797510,Multi-class training хийх гэж байгаа тул label-аа 0-indexed байхаар тохируулах:,c8f8a96d,0.5142857142857142
23904,6b65d81a5743dd,689373f6,We Understand from this that on average players' career who played more than 50 games are more likely to be equal to or greater than 5 years.,4080a2d2,0.5142857142857142
23907,0e2a23fbe41ca9,42930d20,"Yup, nulls in both the cols - ```state_id``` and ```category_2``` - occur together. We'll investigate this slice after we are done with the remaining columns in the merchant table.",64e4762c,0.5144927536231884
23912,5ce12be6e7b90e,dfadd527,"### Strings as sequences

Strings are text but can represent other things, too. For example, DNA sequences.

Again we can concat strings:",c0ab62dd,0.5146198830409356
23913,30fdc4a6e3c1db,f6c3fe4e,### Plotting daily sales time series,6111ddee,0.5146198830409356
23914,99821bc6a45be6,9fc8cfc7,## Analysis,b9d59346,0.5147058823529411
23916,eb0ecd6bebeb15,723b4975,Üç çiçek türü için üç farklı keman grafiğini sepal.length değişkeninin dağılımı üzerine tek bir satır ile görselleştirelim.,d7b93a60,0.5147058823529411
23917,156bbcff05dcea,f3f7b5ae,# Evaluation for Classification Model,66ad1fe9,0.5147058823529411
23918,7f74a04ae75792,d9e63b43,"#### Handling Missing Value for `Age` column
",d01e91da,0.5147058823529411
23919,e4c6dd957eb5ce,78791c47,## How many time in mean to get the tiers?,2e383665,0.5147058823529411
23923,c65a65d4041018,d7dde41f,And now let's see what other resourses people use. I'll show data for resources which were named by at least 10 responders.,824fb229,0.5147058823529411
23933,b42180a6a5b42f,f9f08207,"#### Temos aproximadamente 73 dias de dados inseridos, desde as primeiros óbitos confirmados Covid-19 no Brasil. 
 **Todavia, temos 9881 inputs.**
 **Isto ocorre porque os dados são atualizados diariamente e de forma cumulativa. **
 
**Para tanto, precisaremos realizar o agrupamento pela data ('date'), bem como agregar os números de mortes (new_deaths_covid19') aplicando a soma de eventos (óbitos) repetidos no mesmo dia.**",987cea5f,0.5151515151515151
23937,dbd96dd275dc60,8f1b6695,"# Now that all of our data is numeric, as well as dataframe has no missing values, we should be able to biuld a ML model",1ed493a8,0.5151515151515151
23945,225b4fe5d3894a,46504b74,"<a id=""6c""></a>
### c. Column Transformer

For regular transformation of columns as we did while experimention with features, we can define a column transformer",4b4197b3,0.5154639175257731
23947,ff3a8ce61fab6a,5410b7ef,"<hr>
### Exampel 2 ",9afe1654,0.515625
23948,c85c94076e9c3a,d380b199,## Year_Birth,3ea0c443,0.515625
23950,f91f58d488d4af,8a71c1b0,"x is the image, with all of the rows stacked up end to end into a single long line.And we are assuming that the weights are a vector w. 

If we have this function, then we just need some way to update the weights to make them a little bit better. 

With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.

We want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. ",5df1bbf3,0.5157894736842106
23952,840534f2908a9c,fe6ea30a,**3. Feature engineering**,8081c3cc,0.5157894736842106
23959,098fedfcd07456,76858cb9,"# One Hot Encoding 
1. ytrain and ytest has the output columns or labels 
1. For all CNN output columns with Multiclass this step is same 
1. This is not as same normal one hot encoding in machine learning the encoding will Create columns equal to no of Class . 
1. The steps are boiler plate code and can be copied for all cnn problem with multi class. ",052ece26,0.5161290322580645
23963,0caaec057f7184,adeb4a6e,"There are three conditions of data for the test data.
- old items already luanched are the test data that have history data in training.
- old items new luanched are the test data that have item history data 'in other shops' in training.
- new items new luanched are items in test data that don't have any history data in training.
Here, we'll take a closer look into 'old items new launched' and 'new items new launched'.",b875533e,0.5161290322580645
23964,57070ad5e0f94f,cafb82d2,# **Completing Missing Values With Others' Median**,d97edc41,0.5161290322580645
23965,16862cb02d73d5,c6be01e5,"Now as we see the 3D point the anomaly points are mostly wide from the cluster of normal points,but a 2D point will help us to even judge better.
Lets try plotting the same fed to a PCA reduced to 2 dimensions.",d7ffa1a6,0.5161290322580645
23969,b05ee1ea1c8269,dd5695fb,# Omicron perc_sequences change in 6 countries,19e4d303,0.5161290322580645
23972,f98996de281488,5a2c3dfb,SEARCHING ALGORITHMS,9e59b70a,0.5161290322580645
23974,c0ddb77bf32e2b,a38824c1,"# EDA again with more detail with univariate and biavariate analysis#
## univariate analysis##
",a0cb45f7,0.5161290322580645
23980,917957c6c4065f,fd385225,"views에서 한가지 의외인 점은, 최소 조회수를 봤을 때 2,050의 조회수로도 인기 동영상이 될 수 있다는 것 입니다.  
좀 더 자세히 살펴보겠습니다.  ",55b8ed68,0.5163398692810458
23981,979f1e99f1b309,2601fdfb,## Feature Selection using SelectKbest with f_regression as score funcation,d1bfebbf,0.5163934426229508
23982,5f4ae633cfd090,e56773ce,There does seem to be somewhat of a linear correlation in significant striking accuracy and winner of the bout,a30a16e2,0.5164835164835165
23988,62487bcd70b199,9e987765,## <a id='6.1.1.'>6.1.1.Finding best K value</a>,f6ae50af,0.5166666666666667
23992,37b09262279764,82cb70d4,- We'll use many models to train on and choose the one which gives the best accuracy,37c4c417,0.5166666666666667
23995,c18267b203f28a,73543885,"You can also modify the above code to look at your `validation` and `test` data, like this:",09ca8efb,0.5166666666666667
24001,04ff2af52f147b,911eb523,"**Create FamilySizeCat Feature:**

We can see in the 'Survived Counts by Family Size' plot that there seems to be significant trends in survival based on *FamilySize*.  Those who are alone have comparably low chances of survival compared to small families of 2-4.  For families of size >=5, we see that people are again less likely to survive than die.  As a result, we will group the values of *FamilySize* together into the following categories and create a new feature called *FamilySizeCat*.

- 1: Alone
- 2-4: Small
- 5-11: Large",d5f37be9,0.5168539325842697
24003,1294fb4c86f993,0f324f23,Figure above suggests that <b>Kentucky</b> has the hightest gun registeration in 2020,4471e513,0.5169491525423728
24004,9169c4e9c33c90,08577bf4,"# User Rating

[Back to top](#Top)",725bf880,0.5169491525423728
24005,2ada0305b68956,afe49089,### 87. Palette = 'cividis',133e26f4,0.5171428571428571
24006,6903d3f38c6a66,85ffbb3d,Or You can use **plt.add_axes()** to create an ax where you want.,6067ce5e,0.5172413793103449
24009,00001756c60be8,63c26fa2,"Выбросы наблюдаются в: HouseYear, KitchenSquare.

Признаки с аномально высоким значением, которые нужно будет ограничить: HouseFloor, LifeSquare, Rooms, Square.",945aea18,0.5172413793103449
24014,18a96bb5711ed9,76b3ab78,"# Timeline <br>

Next, it will be good to check the frequency of reported incidents by year. Most of the reported incidents in the dataset is from 2016-2018. ",e79768db,0.5172413793103449
24016,f3c6048d1058e3,3084d328,"<a id = 13></a>
<h2><font color = MidnightBlue>Trigram Analysis</font></h2>",1d9056b0,0.5172413793103449
24018,434f930cb58aee,f2db5579,We will use the pretrained  Resnet50 model as the base of our model. ,0e1d3554,0.5172413793103449
24019,fb5c6021d127ef,884cdf4c,"## 4) Exercise: Model evaluation

Now that you have a model, evaluate it's performance on data from 2018. 


> Note that the ML.EVALUATE function will return different metrics depending on what's appropriate for your specific model. You can just use the regular ML.EVALUATE funciton here. (ROC curves are generally used to evaluate binary problems, not linear regression, so there's no reason to plot one here.)",dd05cbd3,0.5172413793103449
24022,20e1ba19eb9b5e,f0887bf2,**Join train and test datasets in the same dataframe**,4569bfc1,0.5172413793103449
24024,eb800c50fcfbb2,be31d230,# Training,e7173f4d,0.5172413793103449
24025,1750367e54f407,c92a2ce3,I wanted to try the new `CosineDecay` function implemented in `tf.keras` as it seemed promising and I struggled to find the right settings (if there were any) for the `ReduceLROnPlateau`.,a8e655b2,0.5172413793103449
24032,5fc2f23dfbeeb1,cae9bff9,### Counter Vectorizer,f37b4110,0.5172413793103449
24034,5ea840754577e3,62a3293b,Number of people not survived seems to be disproportionately high in the age group of 20-30 against the number of people who survived. Where as the only age group where more survived are the group from 0-10. This can again be attributed to the fact that women and children were given higher priority. Most of the passengers were in the age group 20-40,9cf9b73f,0.5172413793103449
24037,84127ade6fde87,5b26f6aa,"Note that word2index_dict is now a dictionary with words as keys and an integer as a value. We will use it to efficiently find the index of a word as we one-hot encode it. Let’s now focus on our sentence: we break it up into words and one-hot encode it — that is, we populate a tensor with one one-hot-encoded vector per word. We create an empty vector and assign the one-hot-encoded values of the word in the sentence:",f55d05b6,0.5172413793103449
24041,fc8e0042411c46,7c44f2a3,## Search,af476c2a,0.5172413793103449
24049,eda49464dd6d1b,3e9b33c7,## Save model,8421f81f,0.5174825174825175
24052,fe7360cddc13e5,c1d7a562,"kaynak : “Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD model - 2015 

http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf",8979e423,0.5175438596491229
24053,e93a41c03638fe,f5d51ac3,# 3.2 Text classification with stacked LSTMs :,7363527b,0.5176470588235295
24060,3cd78d8d6d56e4,def1b810,## Model Checkpoints,9f632e94,0.5178571428571429
24061,585c280865b46e,ba67c78b,# Expression of histone genes,4d6056f1,0.5178571428571429
24065,f13534449a3750,ef93b78e,"**Results** (considering only the images with ships):


Ship: 0.005 (127777104)


No ship: 0.995 (24972773040)",8b7f3332,0.5178571428571429
24068,4daf6153275cbf,36f99de6,"This time, we cannot see a Mediterranean similarity, but the similarity in Nordic countries persists, with, of course, Switzerland.",51db1961,0.5180722891566265
24069,b01ee6cb674fa3,42f849e3,"# Mitsubishi Heavy Industries

a private japanese company with HQ @ Tokyo",a8ffd35e,0.5181159420289855
24071,3c2033cc99c12c,060e80bd,"### T-SNE method of the dimension reduction 
+ The Similarity Measure 
+ The analysis  of the  T-SNE method ",dfa22a54,0.5182481751824818
24072,ac04ba639d1c93,4404abea,Let's get the distance between atoms first.,748059d5,0.5185185185185185
24077,c968dbd8d49ae6,c863d8d7,# **Personal Atributes of Churn costumers**,dfb2684d,0.5185185185185185
24087,faa8e6c8ab9246,a0780ee6,Lets do Data Visualization for train dataset and test dataset,2bea1419,0.5185185185185185
24089,613bf7bfdcb9e3,dc8ea4e6,### axis = 1 (left->right direction common value),32beb65d,0.5185185185185185
24090,c6f8ff61a5fa87,ecc9b30f,## 4.LightGBM Regression,3eea586b,0.5185185185185185
24093,1883198d6d8c3c,fcc6bf8a,"<h3>1. Data Types - dataframe.dtypes</h3>
<p>Main type stored in pandas dataframe are: <b>object, float, int, boolean, datetime64</b> </p>",69a1d458,0.5185185185185185
24096,4883314a96dc34,c700ed28,Split dataset into train/test set:,50d36836,0.5185185185185185
24097,24e550b8226932,ad29c79e,##### test:,0caee953,0.5185185185185185
24102,b9328fe3b0cefc,a0222a2c,"we see(可以看到)：
- Connecticut,Stanford,Notre Dame,Baylor all had 10 times in the top 32.(进入32强最多的球队是Connecticut,Stanford,Notre Dame,Baylor, 均为10次)
- Connecticut,Stanford,Notre Dame,Baylor all had 10 times in the top 16.(进入16强的最多的球队与32强一致，还是那四只球队)
- Connecticut had 10 times in the top 8.(进入8强最多的球队是的Connecticut的10次)
- Connecticut had 10 times in the top 4.(进入4强最多的球队是Connecticut的10次)
- Notre Dame had 9 times in the top 2.(进入2强最多的球队是Notre Dame的6次)",3a35eb23,0.5185185185185185
24103,8106640e2f9c7e,b0a16fee,## Baseline #2: Самые популярные товары в Target для самого популярного товара из Data,faea9b8e,0.5185185185185185
24107,b809d07ddd17ed,cdc45146,"## Define MONAI transforms, Dataset and Dataloader to pre-process data",e32bf3b1,0.5185185185185185
24110,135122550b6483,05fa8edb,<h2> Data Cleaning and Feature Engineering,6592d6d8,0.5185185185185185
24113,55ce731a138ca7,65c527ec,# Inference,4996250b,0.5185185185185185
24114,aa7db7b023d0a2,5f7f68a2,#AutoKeras,ec912af3,0.5185185185185185
24115,fdbbd573ba31c2,3e16a4ae,"It's obvious that 'atmospheric_temperature(°C)', 'shaft_temperature(°C)', 'blades_angle(°)', 'windmill_body_temperature(°C)', 'resistance(ohm)', 'rotor_torque(N-m)', 'blade_length(m)' have extreme outliers like -99.",f7c28d74,0.51875
24117,510b8303776bb6,5ced8c06,## Plotting a correlational matrix for all purely categorical fields.,18080db8,0.5188679245283019
24121,ee23a565163388,ff2f3ad9,## **Impact of smoking on the functioning of heart**,88aacbc4,0.5190839694656488
24125,44f6a002ecd033,a65d1c3d,"The columns that we are replacing with logarithm values are:
* ApplicantIncome
* CoapplicantIncome
* LoanAmount

First we will start again by merging the log_train and log_test and then perform the transformations.",70bbe106,0.5192307692307693
24130,95efc1ad1d3e26,a3a1a2fa,## 1.4. Sampling,79de1120,0.5192307692307693
24132,90691864eb68c7,8476ef7d,"Now we can delete the variable dist1, dist2, dist3 and dist4",3555ef9b,0.5194805194805194
24137,722cd844dfbe8f,08a027e3,"# <span style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"" id=""section_3"">Development of supervised models</span>

We will test several models of neural networks and test the main metrics to determine the best model.

- **CNN** from scratch (Baseline).
- **Learning transfer**.
- **Fine Tuning**.

The metrics tested will be the **accuracy** in train and validation.",0cedb385,0.5194805194805194
24140,842547b2def18c,7987d7a6,We can not remove the AgeBand feature.,b8efde6d,0.5196078431372549
24143,7cfd96218dd933,d0423d05,"#### **ATTENTION**
* HERE IS ANTALYA",7c34d96c,0.5196078431372549
24146,37e461081e47c5,5e040fd2,#### Function to store output file as CSV,b3e6549e,0.52
24150,cb570c7b7f0501,e887e977,No significant relation between one of the diseases and the so-show cases. ,a200a0ec,0.52
24152,7e1da639035ac5,62ee9bda,### <a id='10.1'>10.1 Collaborative teachers % distribution</a>,120b6c23,0.52
24155,10c5a39a87c47e,c26f16bd,## Step 10: Setting Callbacks<a id='step-10'></a>,09c7337a,0.52
24164,91eaec994e0c6f,476dc48b,- Let's see Sales' correlations between different Stores.,376aef10,0.52
24167,cee088a6840708,c17e4499,"# Step 7 -> Do some normalization on the graph

Connect each node to itself ... why ? I assume that if I am affected by others in the network, I must be affected by myself in the first place, huh ?

Do some square root stuff... just normalization .... not a big deal ... like scaling columns we used to do with pandas dataframes...huh ?",55463e1c,0.52
24168,7dd46c750653eb,fe7dc95f,"**Inference**

* Birth rate seems to be always greater than Death rate over the years except for year 2010 & 2020

* Birth rate was highest in 2016 and was lowest in 2011

* Death rate was highest in 2010 and was lowest in 2010",c2644713,0.52
24178,519e936017c30a,f301382a,"En este apartado, vamos a estudiar cuales han sido las 20 mejores ventas de videojuegos a lo largo de los años y quienes ha sido los encargados de la realización de dicho proyecto.",dc34915d,0.52
24180,4cd25e50c7e007,6744de8f,## Rescaling the Features,ceb0c525,0.52
24183,e19e307b3fd188,4f7cdfa4,There doesn't seem to be much correlation between the **property tax** and the **rent price**.,2173955b,0.5203252032520326
24187,5ce12be6e7b90e,6083d8f4,We can find the length of a string using the command `len`:,c0ab62dd,0.52046783625731
24191,91473a39b85068,90f88b8d,"### Observations:

""c#"", ""java"", ""php"", ""android"", ""javascript"", ""jquery"", ""C++"" are some of the most frequent tags.",6e3d91c2,0.5205479452054794
24195,e9b9663777db82,c8cdb992,#### 9.Quantitative analysis for Available for Loan feature,648e8507,0.5206611570247934
24196,eb0854a6601407,c06eaee7,"The average of mean target by asset show a bell-shaped distribution, beware that there are outliers, anyway, because there are some assets with quite negative average target (-0.4 area) and some quite positive ones (+0.8 area). Overall the average mean target by asset is slightly negative (-0.0231)",6d107747,0.5208333333333334
24199,e82462cdc998a7,1b6f76ad,"### 4.7 Smoothing<a class=""anchor"" id=""4.7""></a>

[Back to Table of Contents](#0.1)",b39bf244,0.5208333333333334
24202,3b5903412fe741,58369106,"`iloc` is conceptually simpler than `loc` because it ignores the dataset's indices. When we use `iloc` we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. `loc`, by contrast, uses the information in the indices to do its work. Since your dataset usually has meaningful indices, it's usually easier to do things using `loc` instead. For example, here's one operation that's much easier using `loc`:",ad231969,0.5208333333333334
24204,64a336ac34d95c,9481f2fc,# Data Preprocessing,be73a990,0.5208333333333334
24205,8c7e00ca3dc5a7,c6323a7e,"## Outliers

Based on the orignal [Ames data defintion](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt) there are outliers present into the data. Especially in the GrLivArea column. So need to find out those observations and remove it if possible.",c83346e4,0.5208333333333334
24206,386c42a7fb27a4,ca9c0c5e,#### Categorical Columns,9e9f6974,0.5208333333333334
24209,4ae6a182abac64,cd8e4c3e,### 2.2 Bining Categorical variables,418676c5,0.5210084033613446
24213,3d77c1560bd16e,d1ed331e,"<a id='5'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Vaccine Doses Allocated</div>
This section presents the allocated vaccine doses across counties based on the weekly report. So far there are 11 weekly reports published by Texas Department of State Health Services starting from 12-Dec-2020",87c141ca,0.5211267605633803
24217,f6648e47713411,2606941a,## Tổng hợp các kênh,f4af4d1c,0.5212765957446809
24225,ea4e559a86d613,d64d3361,**Add image path**,eff47843,0.5217391304347826
24234,90ead00a8ee283,6ca5814b,"**Exercise 4**: Read the following `csv` dataset on wine reviews into the a `DataFrame`:

![](https://i.imgur.com/74RCZtU.png)

The filepath to the CSV file is `../input/wine-reviews/winemag-data_first150k.csv`.",612efa48,0.5217391304347826
24235,0e2a23fbe41ca9,f74ae3d1,"### 4. Merchant ID cols

merchant_id, merchant_group_id, merchant_category_id, subsector_id",64e4762c,0.5217391304347826
24237,9d9da6c439b96b,ea8d5c0f,## Annual Sales of Games,361cc7d9,0.5217391304347826
24239,73ca9abcc2034e,1c8d8c81,# Most popular words used in title,cec3446c,0.5217391304347826
24243,f6c1eb62cceb70,fc032fb7,Let us have a look see into our features,90a1b790,0.5217391304347826
24246,8336d84cf3ff6b,eb01119c,# Creating Various Models for Ensembling ,b96b58a0,0.5217391304347826
24251,17a24d566ffa59,44f522e6,"##### SVD approximation equation
Equation 1 implies that one can get a rough approximation to A by taking the product of the first singular value with the matrix formed from the outer product of the first column of U with the first column of V . The matrix formed, A1, will be m × n but will be of only rank one. Of all possible matrices, B, of rank one, ||A − B||2 will be smallest when B = A1 holds. One can improve the approximation by forming the product of the second singular value with the outer product of the second columns of U and V , and then adding this result to A1. The resultant matrix, A2, will be the the best rank-two approximation to A. The approximations can be successively improved by repeating the process until k = r holds and the original matrix is produced.",89049e56,0.5217391304347826
24256,77f958b3f41a70,4823775e,# Communicating with high-risk populations,2ad9bb69,0.5217391304347826
24260,7f74a04ae75792,74cfc616,Changes for Age is omitted since it is not relevant to change missing value of age to 0.,d01e91da,0.5220588235294118
24261,ac1abfe1dfe815,171eac94,---,6529dbcb,0.5221238938053098
24262,c9b4e282e4e2c1,a058c5ac,"B)Relation between FieldType and StadiumType
",f44d339f,0.5221238938053098
24263,a5a419dc7245b0,314a5c64,### Feature Selection,4279726e,0.5221238938053098
24264,d6cbd7160961dc,45c305be,## 5.1. Preparing the data set,36d74664,0.5222222222222223
24265,892be0a523578c,f72378a4,"For **minuteSleep_merged.csv**, I think it may be useful to transform it to a data frame recording each entire sleep for each participant, which includes the time when participants go to bed, the time when they get up, total minutes asleep and total time in bed. In this case, each entire sleep record is not splitted into different days. 
  We can use the **logId** to separate each sleep. ",b0e8d7c0,0.5222222222222223
24272,a4aa36df07fd53,fc0288e7,"---
# Exploratory Data Analysis 

Sekarang mari kita coba plot informasi informasi yang ada didalam dataset ini. Kernel asli sudah menyediakan banyak sekali bantuan untuk mempermudah kita dalam membuat grafik yang cantik. Fungsi fungsi yang disediakan menggunakan plotly sebagai library untuk plotting.

### Finding the Top 20% most well paid",d2f42b6d,0.5223880597014925
24273,20b372b6e4e276,661e25db,Text from a single word almost always processes correctly,ec8b0860,0.5223880597014925
24274,21413205980558,62b85706,"# It can be seen that the higher the education level, the more tend to have deposit,and the greater proportion in the banking business.<p>
# 这里可以看出，受教育程度越高，越趋向于拥有存款，且在银行的业务中占比也越重。",84197de0,0.5223880597014925
24275,1a222fee3089d2,7f3b5031,## **Encoding**,59ab8894,0.5223880597014925
24279,d1ff7e10ee0102,6c933cd6,"The primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.",2cc71c3c,0.5227272727272727
24280,be2f4d8a6b73ca,ea2180ce,**visualising continuous features with each other using scatter plot**,5d8ce40a,0.5227272727272727
24281,73d8e56bc709b1,e65067e7,"It's surprised that there are huge amount of young English players play in top european teams!   
That must be a good sign for English football for they have so many talented and potential players.  
They don't have much players over 28, this may indicate that there was a time when England didn't have many top players.",78ec3cce,0.5227272727272727
24282,a0b321057e7402,cc56d675,"**Panads diff()**

Pandas library provides an option for automatic differencing with diff().",5f73fb91,0.5227272727272727
24284,0a918602a04693,a8f6bf59,"Now, lets use the 'one hot encoding' technique for the categorical columns which have more than 2 values.",c1ef0e95,0.5227272727272727
24290,5e1d001f8764e0,dbce5e7d,# CREATING OUR LSTM MODEL,62be464c,0.5227272727272727
24292,32e04b08ff52eb,e4a1be91,Prior Calculation,8d5b86e0,0.5227272727272727
24293,2ada0305b68956,bc865e2b,### 88. Palette = 'cividis_r',133e26f4,0.5228571428571429
24300,09751c520b0616,f30530af,#### (iv) TotalSF : Sum of : 1stFlrSF and 2ndFlrSF and basement area,a4d0c7e9,0.5230769230769231
24307,03048e86a6d806,4c7430a1,### Yearly Compensation by Job Title,1285c231,0.5230769230769231
24309,c09fac3c943d51,fcbf87be,Looking to future features (from https://www.kaggle.com/ashishpatel26/future-is-here):,678d076d,0.5232558139534884
24310,d96642860ab3dd,6421c979,#### 2.1.3 Embarked column Missing values,98419d48,0.5232558139534884
24315,726833f92fb87a,db382afd,**We can see that customers who previously accepted the deposit tends to accept the deposit by a large margin.**,7dc5e1b6,0.5234899328859061
24317,55a5e31d03df9f,d13303fd,"# <a name='transferlearn'> Transfer Learning </a>

🤔 **What is transfer learning?**

Transfer learning is why deep learning can solve complex problems without much data, we use the architecture and weights of a typically large model that was pre-trained on a large amount of data, and then we fine-tuned the model to a highly relevant data set.

In that sense, when a model created for one job is utilized as the basis for a model for a different purpose.

❓ **Why this works?**

Most of the time the first layers of a neural network understand basic things, in an image, for example, are vertical lines, horizontal lines, edges, borders, etc. so we can leverage that understanding from one domain and apply it to our specific use case. This allows the network to use the features it previously learned, mix and match them in new combinations.

🤔 **Why use it?**
1. You can leverage an already proven to work architecture on problems similar to your own (like what we did with the TinyVGG).
2. You don't have much data, so you can leverage layers with weights already tuned in much more data than what you own.
3. It's faster, you don't need to start to train from 0.

❓ **How we can use transfer learning in TensorFlow?**
You can use two options:
1. [Using TensorflowHub](https://www.tensorflow.org/hub), which is a open-source repository from TensorFlow already trained deep learning models. For using it you need to install `tensorflow_hub` and find a model that suits your use case [here](https://tfhub.dev/).
2. [Using Keras Applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications), which are deep learning models that are made available alongside pre-trained weights. For using it you just need to call any of the models in `tf.keras.applications` from the tensorflow library, you can find the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications). 


You can read more about in [this resource](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) and check Andrew NG trasnfer learning explanation [here](https://youtu.be/yofjFQddwHE)


",06dce00f,0.5238095238095238
24318,15eb884262ba09,149c21e3,"Not too bad, though not quite the continents we recognize on a proper world map..",d703bdab,0.5238095238095238
24320,2473d004f92592,f1c83923,"**Extraction & CountVectorize**


*The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.*",18d3b6ee,0.5238095238095238
24321,a758983a68c014,59dc1d4e,![caption](https://www.researchgate.net/publication/322905432/figure/fig1/AS:614314310373461@1523475353979/The-architecture-of-Skip-gram-model-20.png),ab89f181,0.5238095238095238
24330,1084376bc4897c,7e2f83d9,# 4. Building models,1b598487,0.5238095238095238
24336,066c5ee1ef39e6,879316a9,## Train Pipeline,0f394e1b,0.5238095238095238
24341,e16860fce156b0,5a1a174b,"#If a user is interested in one or two specific columns, it provides a more detailed plot for the specific columns by passing column names as the parameter.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.5238095238095238
24349,fda19edaf5c621,e000e1d6,**Looks like the highest participation are from Millenials and Gen Z**,5cfff76e,0.5238095238095238
24351,7a058705183598,748d7346,3. Random Forest Classifier,b0ead917,0.5238095238095238
24360,b6e698d389d0d3,26f084c4,# model variables,f02f68b5,0.5238095238095238
24364,bbaa07ad21cf4e,849cc0a2,### Variant,3ab6b254,0.5238095238095238
24365,898d18d501f68d,8a66b92f,"Now with continous variable we are going to do the plotting. means we are going plot normal univariate distribution for the 1st 10 columns means excluiding Soil, wild, id and cover type columns in cl:",d8bdea2d,0.5238095238095238
24367,9ceb7278784462,6846b677, ## <a id='18'> 14.KNN</a>,3768a567,0.5241935483870968
24369,78fcac7baa760f,46c185a2,- percentage of females who smoke and don't smoke of the total number of females,c96cafcc,0.5242718446601942
24370,98a6794067932a,f1b5ad23,"**2.5 Expédition des commandes selon les états**

Lors de la section précédente, nous avons analysé l'importance des différents états avec l'aide des niveaux de ventes. Toutefois,bien que nos ventes en dollars par état soient intéressantes à analyser, nous aimerions orienter nos décisions stratégiques plutôt sur l'aspect du nombre d'expéditions effectuées par état. Cette section sera donc consacrée à l'analyse de cet aspect plutôt axé sur la logistique et non sur les ventes et marketing.",08600fe2,0.5242718446601942
24371,fd4017c1514157,79788f28,"Here,

0 : Morning

1 : Afternoon

2 : Evening

3 : Night",fd8f0896,0.524390243902439
24379,918040fad252ec,d43600f5,Konversi model h5 ke TFlite,966fcd8f,0.5245901639344263
24380,0858e1bb3cbaca,5b892ffd,"Now I want to know which are the sales that generated most net profits.
We can sort the data by using

**.sort_values()**

The default order of this function is ascending. To reverse it, we can add

**ascending = False** 
![](http://)to the bracket.

",78548374,0.5245901639344263
24381,957e035ba5b9d5,c5aff530,"SVG(model_to_dot(model, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))",778ab3d3,0.524822695035461
24383,56785caebaa256,b8542cbf,"## 4.3. Set initial values for tuning<a class=""anchor"" id=""4.3""></a>

[Back to Table of Contents](#0.1)",a792961a,0.524822695035461
24394,5626e84c4e6bf8,a970f6dc,## Distribution of outlier neurons,e2ecb669,0.525
24399,9a040a4f21091e,26cc1425,"> # TF-IDF

Now, let's do a very similar classification, but instead of CountVectorizer we will be constructing a matrix of Tf-Idf features.
",f591b57d,0.525
24400,37b09262279764,47c6ce5d,#### GradientBoostingClassifier,37c4c417,0.525
24401,5ffe6aa38958a1,ba9badec,### 2.3.3  Cabin - Use as category,11f5412e,0.525
24402,a566b5b7c374e7,075b390d,### Average HRV,b3dc5545,0.5251798561151079
24404,b01ee6cb674fa3,9d81d011,"# Islamic Revolutionary Guard Corps - IRGC

Public company bellonging to the Iranian Republic

The IRGC was founded in 1979 within the context of the Islamic Revolution in Iran
",a8ffd35e,0.5253623188405797
24405,1294fb4c86f993,d28925ec,### Research Question 4: What is the trend of total guns registeration over time?,4471e513,0.5254237288135594
24407,a44368590e878a,667727d8,### Daily confirmations,77743ba8,0.5254237288135594
24412,b9bc7dc9f582e5,3ed3b1bd,# Train test Split,15cc4d28,0.5254237288135594
24417,f2e5e9fb9eaaf7,139d2e62,"[back to top](#table-of-contents)
<a id=""5""></a>
# 5 Target

<a id=""5.1""></a>
## 5.1 Distribution
Target variable has a value of `0` to `1` which indicate people that not claim and claim the insurance. Let's see how the distribution of the `claim` variable.

**Observations:**
- The number of people that not claim and claim (`0` and `1`) are almost the same of `480,404` and `477,515`, respectively.
- In term of percentage both of people that claim and not claim are around 50%.",048e0d08,0.5254237288135594
24418,bb0905d33ae417,ee09bffd,## 64x64 images,25fd1965,0.5254237288135594
24423,163ceeb80d6923,a42be40e,## Evaluation,4adfbb90,0.5256410256410257
24434,29437539745aa5,5b53f9fb,"<a style=""text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"" id=""helper_functions"">3&nbsp;&nbsp;HELPER FUNCTIONS</a>",c17b490a,0.5263157894736842
24435,a1dcd92986bc84,d6b13b1b,"## Train the dual encoder model

In this experiment, we freeze the base encoders for text and images, and make only
the projection head trainable.",730acaaa,0.5263157894736842
24438,9f3710be6aea65,242d4f15,## Data Visualization,ae9bda88,0.5263157894736842
24443,31b564f11ef638,daf1881e,## Train Model and Measure Model Performance,424f9692,0.5263157894736842
24446,54004b32784b68,99d8e087,# 3- Feature Engineering,27213ca9,0.5263157894736842
24451,caa0ce2715bf34,09dfb517,## Scaling of Data using z-score method,78a5dc51,0.5263157894736842
24453,99afe9f3af6dbc,91484ac4,"Membuat dataframe yang terdiri dari title, type_show, description dan cluster. Hanya dibatasi 200 data.",cdec9b3a,0.5263157894736842
24455,30fdc4a6e3c1db,03662ec4,"What we see:
* We see an increasing overall trend
* We also see a monthly seasonality peaking at August.

Now these things will be even clearer when we look at the time series decomposition.

Before that a little context:

A given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.

These components are defined as follows:

    Level: The average value in the series.
    Trend: The increasing or decreasing value in the series.
    Seasonality: The repeating short-term cycle in the series.
    Noise: The random variation in the series.",6111ddee,0.5263157894736842
24458,fe7360cddc13e5,aa6bd236,<font color='blue'> **PRATİK KISIM**,8979e423,0.5263157894736842
24462,5c884c77130c59,15937bd3,#Neural Network CNN,8625c60e,0.5263157894736842
24464,f35ee6e9fab592,32cd911b,Review scores and their frequency count...,b15f7073,0.5263157894736842
24468,169177b6e9edea,bd38cea1,<h2><b>Fare,ca42152f,0.5263157894736842
24471,c950cff74e51ac,17a7f925,From the boxplot above we can conclude that Entire home/apt is the most expensive compared to private room and shared room,d59bf323,0.5263157894736842
24472,3fb15e6e48aec2,8bb81cdb,# Create Dummies,9d1f4358,0.5263157894736842
24477,038abade89e59f,d4fe2c53,# Loading Models,cb32a3fe,0.5263157894736842
24478,f91f58d488d4af,feb55538,"To turn this function into a machine learning classifier:

1. Initialize the weights.
2. For each image, use these weights to predict whether it appears to be a 3 or a 7.
3. Based on these predictions, calculate how good the model is (its loss).
4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss
5. Step (that is, change) all the weights based on that calculation.
6. Go back to the step 2, and repeat the process.
7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer).",5df1bbf3,0.5263157894736842
24479,ed83604018af52,40d4bdeb,# Conv2d Model,838163a4,0.5263157894736842
24481,4c47839b067546,eefc8bfa,"train.engineDisplacement nan только у электромашин, заполним 0",1f517b02,0.526595744680851
24489,63b44c85e32c1f,6b7b14ba,The entire elements present in the list can be reversed by using the **reverse()** function.,fb9b9562,0.527027027027027
24490,27d5291d6365ba,aef3e6fe,# Debit-Credit Transaction Mean analysis By Gender,96b30229,0.527027027027027
24492,e4525eb0c96f28,1a59be3e,"### Sales by Platform over time

Next we plot the sales of platforms over time to see how each platform performs. Some observations:
- Majority of platforms other than Xbox360 (and Genesis, but it has too few data points to be considered) have a negative/constant slope. 
- A general trend for sales is that in the first few years the sales of a console goes well, after which it falls low.
- PC as a platform is the most consistent by years, yet is seemingly constant/slightly negative in sales over time. We will explore PC later.
- All of the best selling platforms (other than Xbox360) are clearly decreasing in sales over time. A possible conclusion to this is because they are consoles that have been out for a while, so it's only natural that they are decreasing. However, what about newer consoles, such as the Nintendo Switch, that are less affected by the dying console hype?",2093a1f1,0.527027027027027
24493,5a8c553e21c70f,ae3eaa0b,"Upsampling and downsampling should be mild, because too much upsampling or downsampling have side effects like increasing the correlation between features.",9ebd9d8f,0.5272727272727272
24495,016abae0483764,59fb8333,Shortness of breathing provides a strong response here. We can see that the positiveness of the test is majorly affected by the attribute of breathlessness,bc9f289b,0.5272727272727272
24498,738bfced935b69,a38d46d8,Six clusters by tax between 126-145.,2d3c592d,0.5273972602739726
24504,0f5085b162bd9f,66bd06c5,# Mean Shift,a3d989ee,0.5277777777777778
24517,dd3721cb49c1fd,eee4745d,"<a id='7'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">7. <b>Plotting the Backtest</b></h1>
  </div>
</div>",1a53fdd9,0.5277777777777778
24518,593d1d3d1df05a,faefa361,# Libraries for Model Training,bc682ffe,0.5277777777777778
24520,ab6da5994949a3,6d946dcb,"# Support Vecor (SVC) Classification Model
## Fitting SVC to the Training set",fae6b91d,0.5277777777777778
24523,c01049afb6d307,4cf13ee6,"
* Work experience is directly proportional to age.
* Social drink is higher in former and new employees.",d37d3b5d,0.5277777777777778
24524,cf39cde80e66b7,be8a0b53,"# <div class=""h3"">RMSE: Root mean square error</div>
<a id=""m3""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.5277777777777778
24532,5f27526aa6c113,53cfc067,most claims are from people with no responsibilites maybe because of freedom they meet more accidents,a5c26ab6,0.5280898876404494
24534,614ba9f0c62677,ade85ec3,"<a id=""9""></a>
### Full Connection
* Neurons in a fully connected layer have connections to all activations in the previous layer
* Artificial Neural Network
* <a href=""https://ibb.co/hsS14p""><img src=""https://preview.ibb.co/evzsAU/fullyc.jpg"" alt=""fullyc"" border=""0""></a>",b8551335,0.5283018867924528
24536,43e60eb1362f5c,09d94f1a,# The Figure shows that Atlanta has the highest count of flight from origin city ,87934234,0.5283018867924528
24537,4dd47072617594,10f51c79,# 4. Sentiment Modeling,44ff1d11,0.5283018867924528
24539,510b8303776bb6,84c2a195,### One hot encoding of all purely categorical data columns,18080db8,0.5283018867924528
24540,f015d0147e8fbf,a9cf1649,### Drop Unhelpful Features,518954fb,0.5283018867924528
24541,f3c8651cb08234,0cb5a7bc,# Randomized Search CV,37f86e36,0.5283018867924528
24543,a070fd03ae8ed2,5fa26fc7,## 5.2 Расчет бинарного вектора прогноза (targets_pred) из вероятностного прогноза (pred_proba),c0ec4138,0.5283018867924528
24544,e19e307b3fd188,d7e9738c,### Area,2173955b,0.5284552845528455
24545,675b60eaf415a6,78e9bf8e,### **Fine tune Inception Pretrained model using Food 101 dataset**,68c0b725,0.5285714285714286
24547,2730840089c8eb,3364b20e,"### Building strings with `.format()`

Python lets us concatenate strings with the `+` operator.",34d27dac,0.5285714285714286
24550,2ada0305b68956,8ff659ee,### 89. Palette = 'cool',133e26f4,0.5285714285714286
24561,2f47abddfd1928,0d3adc34,"Both sex travel mostly alone, with less frequency they travel with 1 or 2 parents/children and less frequently but mostly women travel with more than 2 parents/childre.

We can translate this information into:

- People traveling with 1-2 parents/children are a mix of adults with children and children with their parents. Although I guess more of the second option.
- In the case of more than 2, it is likely that are couples with children or a women with its children. Being more frequent the second option.",ae33cc0b,0.5289256198347108
24566,02b7e38902069e,5f489361,"#Splitting input text into sentences

""Indic NLP Library supports many basic text processing tasks like normalization, tokenization at the word level, etc. But sentence level tokenization is what I find interesting because this is something that different Indian languages follow different rules for.""

""Here is an example of how to use this sentence splitter:""

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.5294117647058824
24569,21bce4ec54b3fa,6e3e4d14,"# In-built feature importance

Random Forest was chosen for another nice property - it has in-built feature importance feature after fitting it, which saves considerable amount of coding.
Let's use already fitted benchmark to get the important features.",35546e30,0.5294117647058824
24571,917957c6c4065f,ec9307c3,"조회수 3000미만의 영상 3개 입니다.  
1071행의 데이터는 11월17일에 게시하여 11월19일에 조회수 2,050 좋아요 3, 댓글 0 으로 인기동영상이 되었네요.  
3일 동안 조회수 2,050에 좋아요 3이 인기동영상이 될 정도인가..? 하는 생각이 듭니다.  
이 정도 수치를 달성하는 영상들이 전체 영상 중에서 적지 않을 것 같은데 말이죠.  

17414행의 데이터는 당일에 게시하여, 조회수 2919에 좋아요 10, 댓글 1로 인기동영상이 된 것과 비교해봐도..의구심이 생깁니다.  
시간당 조회수로 따져봐도 1071은 2050/(24*3) = 28.47 , 17414는 2919/24 = 121.63 인데..

유튜브에서 발표한 인기영상의 기준을 봤을 때,  
![image.png](attachment:image.png)

조회수, 온도, 업로드기간에서 크게 강점이 없는 것 같은데, 그러면 유입소스에서 강점이 있는걸까요?  

*혹시 영상 내 광고 유무가 영향을 줄까하여 해당 영상을 확인해봤지만, 광고는 없었습니다.  
",55b8ed68,0.5294117647058824
24575,7868594a8135f4,4f59ee22,# Modeling,e15d2167,0.5294117647058824
24577,6014a2010722f2,558dc23d,"### The End (of the data extraction)

Next, we will do some cleanup.",abd38dca,0.5294117647058824
24579,a871419285588a,40bbfc45,### Visualize some slices with their tumor mask,5e08e15f,0.5294117647058824
24580,869a39a3d4dea2,196c44d3,"## Bitwise operations <a id=""bitwise""></a>",9020daf8,0.5294117647058824
24583,7cfd96218dd933,c0764b9f,### DATE BASED ANALYSIS,7c34d96c,0.5294117647058824
24586,52cfd66e9ec908,0b8056e1,"It seems both the distributions of extent X and extent Y are heavily right skewed, as is centroid X. However, I have left out extent Z is order for readability of the plot, let's look at it now.

Try to smooth the data and get:",c74adcdf,0.5294117647058824
24588,4ae6a182abac64,86e96910,"
* Before we fit the data into a machine learning algorithm, there is a step very crucial is that we make sure to encode categorical variables

  correctly We will change Sex to binary, as either 1 for female and 0 for male. We do the same for Embarked. We do this same process on  

  both the training and testing set to prepare our data for Machine Learning.",418676c5,0.5294117647058824
24590,513ce405d7f6a3,65a6213d,# Naive bayes moldes without data preprocesing,8461e086,0.5294117647058824
24592,55c34673c1f760,3c1b6d53,## Perceptron Architecture,2663c47f,0.5294117647058824
24595,c65a65d4041018,c140cdc5,First place is ods.ai - a slack community of Russian DS which is open for everyone wishing to diccuss DS.,824fb229,0.5294117647058824
24597,d0080e3a39bc5c,6ec44406,**DATA AUGMENTATION**,2fcde4cf,0.5294117647058824
24598,64d96b5ce09ab0,fac7162e,**TESTING THE MODEL**,0aecaff2,0.5294117647058824
24601,36d0d4cb9c7993,7dad0596,## visualization of decision surface,34b93e27,0.5294117647058824
24604,6cac6d4743088f,9ca7e6c8,"### Questão 1 - Item C - Representação Gráfica
Para cada uma das variáveis, produza um ou mais gráficos, usando matplotlib, que descreva seu comportamento / caracteristica. Lembre-se que estes gráficos precisam ser compatíveis com a classificação da variável. ",55fb02fa,0.5294117647058824
24605,907f08f9a2c6cf,3b54a075,### Train the Model,aa84c325,0.5294117647058824
24611,156bbcff05dcea,3b3e92d5,# Accuracy,66ad1fe9,0.5294117647058824
24612,fa02c409161192,bd1e049b,"Here we will test the NN to see if how well it makes predictions. Here we begin with a simple analysis, we will compare the outputs of the trained NN with their 'true values' given by the label. We will calculate what percentage of the test data produces the correct output, we will define this as the performance. I will potentially see if I can calculate the confusion matrix.

Additionally I see three obvious stratagies which will allow for me to gain useful insight on NN. They are as follows;

1. Test to how the size of the training set effects performance
2. Test to how the number of hidden nodes effects performance
3. Test to how the learning rate effects performance (~ Hyperparameter tuning)

The analysis is below but first we need to define a metric that we can use to quantify the 'performance' of the NN. We will just say that the NN performed well if it was able to predict a large percentage of the test data's labels. The percentage of correct predictions will be define as the performance. ",e97077f7,0.5294117647058824
24616,a0a5baa6c7e12a,4c795d4b,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Initial Preparations</div>

We are going to start with the essential pre-requisites as follows

- installing *AutoViz* into this notebook
- importing the standard Python packages we need to use down the road
- programming the useful automation routines for repeatable data visualizations we are going to draw in the Advance Analytical EDA trials down the road",551d41de,0.5294117647058824
24619,1a0bd2f72bbe36,76b7ee09,"## There are around 96 books who have MORE THAN ONE Edition: 
  ",2fa311dc,0.5294117647058824
24620,6aaee7fdbc7945,6dc63fb2,# **Question 2**,dae653ae,0.5294117647058824
24622,fc8e0042411c46,6f63e5cc,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.5297805642633229
24623,20b372b6e4e276,8fe07c35,"## 5.2. WordCloud <a class=""anchor"" id=""5.2""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.5298507462686567
24624,cddd881ce4b8ef,4c7ee4bf,"titanic.loc[(titanic['sex'] == ""male"", ""sex"")]=1
titanic.loc[(titanic['sex'] == ""female"", ""sex"")]=0",5294325c,0.5299145299145299
24630,4daf6153275cbf,cadf973c,#### 3. Ratings,51db1961,0.5301204819277109
24633,726833f92fb87a,14b7c51f,## New_cust VS campaign success,7dc5e1b6,0.5302013422818792
24637,f1e162ddd14f11,c7262d2c,we can see that the first feature and fifth feature (present price and fuel_type_diesel) has the most importance,cdb2e771,0.5306122448979592
24639,e69a496109e7d8,2eb4e0b0,The above plot shows that people aged 35-60(approx) are likely to survive. There is equal rate of survived and non-survived at the age range of 50-60,1c640591,0.5306122448979592
24646,ffd1df95ca5289,5ee30db3,There is not much difference w.r.t response,db00c338,0.5306122448979592
24647,a4f8ad33c823c5,4e03d912,The above plot shows that a higher proportion of males suffer from diabetes compared to females.,fcd48307,0.5307692307692308
24648,d07915a6e6992e,9d8cacad,**Age**,2b912140,0.5307692307692308
24653,a5a419dc7245b0,58eb31c6,#### Stats Model for Logistic reg.,4279726e,0.5309734513274337
24654,ac1abfe1dfe815,5b79e68e,# Cleaning Tweets,6529dbcb,0.5309734513274337
24656,5e02999ca74e7e,5b572527,# **Model Evaluation**,b69da28e,0.53125
24657,51a46d0a7597f5,b2c43cb7,"One hot encoding is chosen above, because we want to retain the value of each column and all values are to be treated equally, it seems appropriate over other methods like 'label encoding' which gives different ranking to values",e9e25b17,0.53125
24660,bd0e173abb7b52,4e304cd1,**7. Display age statistics for each race (*race* feature) and each gender (*sex* feature). Use *groupby()* and *describe()*. Find the maximum age of men of *Amer-Indian-Eskimo* race.**,9bce3b0d,0.53125
24662,69130a37583a06,60496065,"### addr1 & addr2 Distribution :

* At quantile feature i  think we are not able to get any insight because value for these column are numerical labels like pincode of any area. These values dont have any numerical significance .

",65a4de1c,0.53125
24663,2a724fb7835cdc,4326fc05,"**Splitting Data into folds**

If we split the data totally random, we may bias our validation set because the phrases in the same sentence may be distributed to train and validation sets. We need to guarantee that all phrases of one sentence is in one fold. We can assume that SentenceId%NUM_FOLDS preserves this while splitting the data randomly.",c38ac61d,0.53125
24664,96c4c0e36b8ec0,86fece41,**Age distribution between people who Survived or Perished**,4dd6de8c,0.53125
24666,6f05f4ea9addbf,fac2f209,# Feature Extraction,dfb04c84,0.53125
24667,49f2274c1dd516,be42e925,"## COVID 19 Canada Open Data Working Group
A working group collecting publicly available information on confirmed and presumptive positive cases during the ongoing COVID-19 outbreak in Canada. Data are entered in a spreadsheet with each line representing a unique case, including age, sex, health region location, and history of travel where available. Sources are included as a reference for each entry. All data are exclusively collected from publicly available sources including government reports and news media.",06b0ffee,0.53125
24670,117fc0956643d0,b44724d0,"## Step 3. Extract Excerpt from Abstract Using Fine-Tuned ALBERT
We utilize the fine-tuned ALBERT model to get answers relevant to each COVID-19 questions. The abstract of each article is the context we use for extracting relevant answers to questions. The answer is a span of text from the abstract. The workflow can be broken down into the following steps. 

1. Tokenizing the question and the abstract of each article  
2. Feeding the fine-tuned model with the tokenized text
3. Calculating the confidence score of the start and end index for each token. 
4. Getting tokens with the maximum confidence score for start and end index in the abstract. 
5. Detokenizing and getting an answer (i.e. a span of text from the abstract).

After calculating the maximum start and end confidence score of each article, we average the two confidence scores. The average of the beginning and end index of the abstract is the corresponding article's confidence score. ",68cef9fd,0.53125
24675,2c3a6969252dc0,96a1b667,Students with better LOR tend to have Higher CGPA.,d30f10ce,0.53125
24683,c7e5f658090347,5d1aabc5,<a id='LogReg_pipeline'></a>,43c78e7d,0.5319148936170213
24687,3f25b363afec54,36a241d9,So the number of patients per medical camp in train set ranges from 6543 for campid=6543 to 44 for campid=6558,bbdaae25,0.5319148936170213
24693,5ce12be6e7b90e,24a3fd96,Augmented assignment also work with numbers and other operators:,c0ab62dd,0.5321637426900585
24694,ad26c020235dfc,d1f4ac82,Grid Search with XGB:,bf766e48,0.532258064516129
24701,c13f73168789c2,0382ff0a,"## 6. Conditional selection of columns<a id='19'></a>
Syntax 1 : `df[df.column_name < value]`

Syntax 2 : `df[df.column_name > value]`

Syntax 3 : `df[df.column_name == value]`

Syntax 4 : `df[df.column_name <= value]`

Syntax 5 : `df[df.column_name >= value]`",16175052,0.5324675324675324
24702,722cd844dfbe8f,9d686d55,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_3_1"">Multimodal inputs CNN from scratch</span>

As part of this modeling, it is necessary to develop the models on each of the 4 types of scans available to patients:

- Fluid Attenuated Inversion Recovery (FLAIR).
- T1-weighted pre-contrast (T1w).
- T1-weighted post-contrast (T1Gd).
- T2-weighted (T2).

As a reminder, these models are stored in the variable:
```Python
scan_categories = [""FLAIR"", ""T1w"", ""T1wCE"", ""T2w""]
```

The arrays of the patient images will be loaded into the variable X, the labels of MGMT_value into the variable y. To obtain a binary classification, we will use a global **sigmoid activation layer** to the results of our 4 models *(classifier layer)*.

We will test the models with **a sequence of images** *(as for a video)* by taking the **middle 24 images** *(not entirely black)* of each patient for each type of scan :",0cedb385,0.5324675324675324
24703,241cf32abb22d8,f6f2935c,"## Train-Test Splitting

The dataset is split into train and test at a 70:30 partition ratio by stratification:

* Training (70%): X_train (descriptive), y_train (target)
* Testing (30%): X_test (desciptive), y_test (target)

Meanwhile, I created X_train_10 and X_test_10, which have the same sample rows as X_train and X_test, but only have the top 10 features selected by F-score from the previous process.",47157066,0.5324675324675324
24704,4ae464582bac51,e8fa208b,"People who did not smoke had a higher incidence of stroke than others who smoke or have already smoked. However, there is a large part that is unknown and that may - or not - change this scenario.",ca6a52ce,0.5324675324675324
24705,2cb457b60dd246,a0a7a9f1,# **Dense net**,339367df,0.5324675324675324
24706,663bbc9eaf267b,6c0a6100,## Model,32445529,0.5324675324675324
24707,75adb7945ef9bd,46432bb3,"Findings:
- Top two words in disaster tweets: 'fire' and 'news', don't make the top 20 on unreal disaster tweets.
- Words are more specific for real disaster tweets (e.g. 'califonia', 'hiroshima', 'fire', 'police', 'suicide', 'bomb').",785c5095,0.5324675324675324
24709,b01ee6cb674fa3,18d068d8,"# Arianespace


An european multintional company founded by 1080 and is the world's first commercial launch service provider

Its HQ is located in France",a8ffd35e,0.532608695652174
24712,c80939c7c626cf,c35c4945,"# Now we need to convert Age to categorical variable
child: 0
young: 1
adult: 2
mid-age: 3
senior: 4",b9ac31e2,0.5328467153284672
24713,3c2033cc99c12c,8158e0a3,"As is shown from the above three methods, T-SNE has the best performance. For better visualization,I choose to create a 3D dynamic plot.",dfa22a54,0.5328467153284672
24714,fc8e0042411c46,6470ed44,## Magazine,af476c2a,0.5329153605015674
24715,62487bcd70b199,ff8d7eda,"## Inference:
at k=5 we have better score and hence we will use this in our model",f6ae50af,0.5333333333333333
24717,d96e03a9e7c030,be8357d2,"#### Socrata data - Discretionary funding and number of crimes occurring at a school
As you would expect, the higher number of crimes at a school, the less likely that school has a higher SHSAT participation rate. 

Also as you would expect, the higher amount of discretionary funding from the government to the zip code that shares a school, the higher the SHSAT participation rate.

The `model`s with `_proportion` appended are actually variables that are a combination of crimes reported at the school / average number of crimes of all buildings in the same ""group"" as defined by the dataset found [here](http://www.kaggle.com/new-york-city/ny-2010-2016-school-safety-report).

> However, looking at the `model_r2` and `median_absolute_error` columns of these variables, they are not as predictive as some of the other variables previously discussed.",d2b72ced,0.5333333333333333
24718,e25c0f830df3f4,b74340e0,# Displaying highly negative reviews,fdcf7189,0.5333333333333333
24719,04bac111ffbe9c,45f49e94,"### So as we can see, we have to repeat all steps we did on the training data to make it an appropriate testing dataset.",82576b17,0.5333333333333333
24722,d905cde3391d2b,1e9967d1,"## Interquartile Range (IQR)

**Interquartile range** or **IQR** is the amount spread in middle 50% of the dataset or the distance between *first Quartile* (Q₁) and *third Quartile* (Q₃)

* First Quartile (Q₁) = Median of data points to left of the median in ordered list (25th percentile)
* Second Quartile (Q₂) = Median of data (50th percentile)
* Third Quartile (Q₃) = Median of data points to right of the median in ordered list (75th percentile)
* IQR = Q₃ - Q₁

![Interquartile range](https://raw.githubusercontent.com/nowke/nowke.github.io/gh-pages/src/pages/stats/images/iqr.png)

```
[41, 48, 58, 60, 60, 67, 69, 71, 75, 78, 81, 83, 89, 89, 91, 92, 94, 94, 96, 98]
 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
             first half                              second half
        median = (60 + 67) / 2 = 63.5        median = (91 + 92) / 2 = 91.5

Q₁ = 63.5
Q₃ = 91.5
IQR = Q₃ - Q₁ = 91.5 - 63.5 = 28
```

Let's calculate IQR for `win_by_runs`",067dba39,0.5333333333333333
24726,4bada947d597ac,855e7dd5,# Defining the model structure,eab5094a,0.5333333333333333
24728,b57862be79c695,8fc60218,## Train Test Split,989222cb,0.5333333333333333
24731,b10bd75889dad9,d283f1d6,#### Splitting data into X and y,ee00ceee,0.5333333333333333
24732,3597174a998d4d,b56f89d5,"For agent and company variables, their values ranges are large. And their number of booking and ratio of cancelation vary from values. I conctruct two new variable named agent_new and company_new.",276892ed,0.5333333333333333
24736,cb570c7b7f0501,ea9ba6ac,"### Research Question 3 ( Are people received messages likely to attend !? )
",a200a0ec,0.5333333333333333
24737,55a5e31d03df9f,7ff9d426,"## <a name='tfhubcreate'> Creating models using TensorFlow Hub </a>

Using the TFHub webpage filtering by Image Classification - Fine Tunable models and TF2.0 we found EfficientNet V2 with input size 240x240 which perfectly suits our data (😉 this was intended). You can find more information about this model [here](https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/classification/2)",06dce00f,0.5333333333333333
24739,9276fa5cc2fef6,b0541e6b,Let's have a glance at new features ... ,24aa6a52,0.5333333333333333
24752,6998861ff6ff01,4eb58639,"# Select just the day of the month from our column
___

""Ok, Rachael,"" you may be saying at this point, ""This messing around with data types is fine, I guess, but what's the *point*?"" To answer your question, let's try to get information on the day of the month that a landslide occured on from the original ""date"" column, which has an ""object"" dtype: ",ea9e72cf,0.5333333333333333
24762,7454fdc444df16,b1549066,"### Shuffling Our Data
Below we shuffle our data such that the patient order is shuffled, however the individual patient order is maintained.",a7818ef5,0.5333333333333333
24763,91eaec994e0c6f,8bb6ccac,"- The highest correlation is between CA_1 and CA_3 stores, in the same state.
- The lowest correlation is between WI_1 and WI_3 stores, in the same state!",376aef10,0.5333333333333333
24765,4c55891bcb068d,8ed8c096,# Learning,01b9cd67,0.5333333333333333
24766,061d6757dfbce0,be96dbba,"We can see that meter readings by meter type are very different. Steam in particular, lives on a much higher range than all other meters.",c0c2915a,0.5333333333333333
24767,d77e6d61ad2e8b,da43f4dd,# Feature Importance Plot,03fd0e96,0.5333333333333333
24769,f7436bc492474c,89bf3757,"## Building the model

We will start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.",328fd235,0.5333333333333333
24770,b547f0f38f7744,ef651d5f,Create `train` `valid` datasets:,b6ba66b3,0.5333333333333333
24774,70c1c70437ce86,0fd39274,## Decision Tree Classifier,e94552a4,0.5333333333333333
24781,cb6f349e54c2a1,c57502ba,"# Functional API

The Keras functional API provides a more flexible way for defining models.

It specifically allows you to define multiple input or output models as well as models that share layers. More than that, it allows you to define ad hoc acyclic network graphs.

Models are defined by creating instances of layers and connecting them directly to each other in pairs, then defining a Model that specifies the layers to act as the input and output to the model.",962bade8,0.5333333333333333
24782,ff9142eb631dd5,f8104d74,"## Get the current best features and model from:

https://www.kaggle.com/jazivxt/physically-possible",453131ac,0.5333333333333333
24783,864302b10e7730,d60ee32a,# 2. KDE,e9dd1d2d,0.5333333333333333
24784,d6ddbe57f59cf7,3af4766c,# Box Plot,504a3cda,0.5333333333333333
24785,396bc36edb95d3,bfd9a6ec,#### Confusion Matrix and Classification Report for Testing data,965e4f8f,0.5333333333333333
24787,2bd6c370695ea7,6a005316,## Images,cbe6aec8,0.5333333333333333
24790,1c7dacc7f36c8c,680f0e3c,## 풀이,871a833c,0.5333333333333333
24800,98a6794067932a,8a55005f,"Le code ci-dessous permet d'évaluer le nombre de commandes effectuées auprès des cinq villes les plus populaires dans la base de données. Le code calcule dans un premier temps le nombre total de commandes effectuées pour chacune des villes présentes dans la base de données et retourne ensuite un tableau démontrant le nombre de commandes effectuées auprès des cinq villes les plus populaires de la base de données. D'un point de vue logistique, ces informations vont être très importantes pour les dirigeants étant donné qu'elles permettent de constater les cinq points de vente principaux où les commandes de l'entreprise doivent être expédiées. Il pourrait donc être intéressant de considérer placer leurs centres de distribution à proximité de ces villes. ",08600fe2,0.5339805825242718
24802,73d8e56bc709b1,78b6c31a,# # 3. Germany,78ec3cce,0.5340909090909091
24805,1eb62c5782f2d7,bccb1300,"### Interpretasi

Ketika $200$ pengunjung memasuki toko, kamu akan mendapati $200 \times 0.7333 = 146.66$, atau sekitar $147$ pengunjung berada di toko diantara menit $24$ dan $54$.",bb69f147,0.5342465753424658
24808,91473a39b85068,cb89a466,### Bar plot of top 20 tags,6e3d91c2,0.5342465753424658
24810,2ada0305b68956,4a0c36e5,### 90. Palette = 'cool_r',133e26f4,0.5342857142857143
24816,f3c6048d1058e3,fffb1e7b,"This N
gram analysis showcase words which are
occurring together in IMDB data base.
From Uni
gram we see that ‘Movie’ word occurs more
than 1L times in dataset, which is quite obvious.
From Bi
gram, we see that ‘look like’ and ‘ever seen’
words have occurred more than 2500 times together
From Tri
gram, we see that ‘movie ever seen ’ has
observed maximum number of times together.",1d9056b0,0.5344827586206896
24824,1dd9c6aa74d289,d281f504,### Median Value,5ef9a1be,0.5344827586206896
24826,8539260444e6b5,4e98b0a9,# Building a machine learning model,0369463f,0.5348837209302325
24827,2e40928927c0d4,5c081763,**Defining the inception network**,b6385ef2,0.5348837209302325
24831,22bd95f4807a23,187b8180,## How does the ratings vary across ages?,c05d356f,0.5348837209302325
24832,806ce45c8fa303,8f6b7a8a,## Building Autoencoder Model,3e5c34dc,0.5348837209302325
24833,743ae010f5e875,f3b8f50e,### Load model and tokenizer,02c54445,0.5348837209302325
24835,1660daf8867980,6f3591f0,**Implementation**,42d7cffc,0.5348837209302325
24836,22ba3a8149c2f1,fa191c81,## 2.1 model 1 - resnet,19c82be5,0.5348837209302325
24837,fe7360cddc13e5,829a638e,## Kütüphanelerin Import Edilmesi,8979e423,0.5350877192982456
24842,d8ff894670d506,41f1d4f9,**More advanced analysis of the states and their Donations:**,eb0fb7de,0.5352112676056338
24843,06c7ba9203293f,f0a37988,# Analysing 'rating' series,1e1a2b48,0.5352112676056338
24844,9bcfa825c8b2e6,c1ca63fb,Zaten aralarında bir sıralama olduğundan bu şekilde bırakılır.,220f36e4,0.5352112676056338
24848,df51d4c54fbb91,68a214a4,"## Convolution
",4226dd72,0.5357142857142857
24851,99f84fa59cb1da,7511fa5b,### Sanity Check,41e95f63,0.5357142857142857
24854,98fd05fcc5c3e3,15f8144a,## Label Encoding,55fe7ece,0.5357142857142857
24856,7686f42e1f28d2,db07d009,"# GradientBoostingClassifier
In this kernel, we'll be trying to predict a user's rating. <br>I always find tree-based algorithms really helpful just because the data does not need much preprocessing (no scaling and centering needed)<br>In this section, we are using a [Gradient Boosing Decision Tree](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4). It is one of the most powerful machine learning models and is frequently used for competitions. I've written a wrappers (and a method) for GDBT. You can look through them if you wish but they are really nothing special, just an implementation of cross-validation and a bit of visualization.",6c128859,0.5357142857142857
24863,8dd655515e7d18,98ecf31b,"**Analysis:**

The 'Neuroticism' is uniformly distributed between min-1 & max-24",895f41cf,0.5357142857142857
24865,ff653816a337fb,3c46b53a,"we need to label the year, month, day column now
",e5373696,0.5357142857142857
24867,87e94f864d74be,02766be1,### Let's fix the datatype ,294bfe9f,0.5357142857142857
24868,f13534449a3750,93bed3b7,"The class imbalance is reduced, but it is still very high: 5‰. So in the images with ships only 0.5% of the pixels are ships while 99.5% are no-ships. This condition of extreme class imbalance of the dataset is a problem and so we had to take this into account when deciding which strategy was best for solving the ship detection task.",8b7f3332,0.5357142857142857
24870,6e9b4020644836,af5b4e11,"<a id=""3""></a>
# <p style=""background-color:#627D78;font-family:newtimeroman;color:#D5CABD;font-size:150%;text-align:center;border-radius:20px 60px;"">Data Cleaning</p>",5ad41fc6,0.5357142857142857
24871,409e2142d880d2,ce4ce4e7,"* female Not survied (full)
* male not survied (25%)
* male survied (full)
* female  survived (50%)",2bb7b414,0.5357142857142857
24874,92e9fc3a0ff5c0,ebaef721,## **Sentiment Polarity on Trump's dataset**,d53da425,0.5357142857142857
24875,f4514ec092a771,c3882c5c,"So the most important part is how to create your own phoneme list. For me, I list all the label and search the phoneme on this [website](http://www.speech.cs.cmu.edu/tools/lextool.html).  
My *lexicon.txt* is here:  
> bed	b eh d  
bird	b er d  
cat	k ae t  
dog	d ao g  
down	d aw n  
eight	ey t  
five	f ay v  
four	f ao r  
go	g ow  
happy	hh ae p iy  
house	hh aw s  
left	l eh f t  
marvin	m aa r v ih n  
nine	n ay n  
no	n ow  
off	ao f  
on	aa n  
on	ao n  
one	w ah n  
one	hh w ah n  
right	r ay t  
seven	s eh v ah n  
sheila	sh iy l ah  
six	s ih k s  
stop	s t aa p  
three	th r iy  
tree	t r iy  
two	t uw  
up	ah p  
wow	w aw  
yes	y eh s  
zero	z iy r ow  ",3739ab1e,0.5357142857142857
24877,912bb73358069c,79279942,### Evaluations of  keras outputs and pytorch outputs are 0.24+. This is significantly worse than the val_loss  in keras training. Why?,0cf9db82,0.5357142857142857
24878,b8849a04581d32,e615087a,"#### Let's look at the numerical features for each Punt outcome. To do this, we will build violin graphs for numerical tables, and categorical we will group into compact tables according to the number of characteristics for each outcome.
#### These actions will allow you to evaluate how much each outcome for different features differs and whether it makes sense to use these features. ",b8a568cd,0.5357142857142857
24880,917957c6c4065f,fd3ca4f5,"![image.png](attachment:image.png)

분석시점 2020년 02월 25일 기준 해당 영상의 수치입니다.  
컨텐츠는 앱 광고영상이네요.  
인기영상에 간 시점에서 2년이 지났는데 조회수 767, 좋아요 1이 증가했습니다.  
전체 조회수의 약 70% 동영상 게시 2일 동안 발생했다는 이야기인데 구독자도 1인 상황이네요.
유튜브 알고리즘이 정상적으로 작동했다는 가정하에, 가능성은 두 가지입니다.  
첫번째: 해당 영상을 광고로 사용(궁금증: 광고에서 영상을 봐도 조회수가 오르는가?)  
두번째: 봇을 사용하여 해당 영상의 조회수를 조작(가능성이 낮다고 생각합니다.)  

첫번째 경우에 해당될 것 같습니다.  
만약 광고 영상으로 조회수가 오르는 것이 사실이라면, 
저는 인기 동영상에 게시되는게 목표이기 때문에, 동영상 게시 후 약 3일 동안 광고하여 조회수 2,000을 넘어가게끔 하겠습니다.",55b8ed68,0.5359477124183006
24884,8ec771f5600a61,360f3a13,# MODELING ON TRAIN DATA,48364c1f,0.5360824742268041
24887,e3fb4c6300cb56,8be91c98,"<a id=""7""></a> 
## Violin Plot",8ebbdf89,0.5362318840579711
24888,8336d84cf3ff6b,69868c64,"## Dummy Classifier 

To see how much we can improve over a baseline ",b96b58a0,0.5362318840579711
24892,7e89d387feb9f5,3eaf23a4,"#### Как видим, данные в этом Series представляют собой строковые представления списков в соответствии с синтаксисом языка Python.",989e3a1b,0.5362318840579711
24897,7e275c8d5ff2a0,f81a2ef7,"<div style=""color:white;
           padding:8px 10px 0 10px;
           display:inline-block;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana"">
    <h1 style='color:white;'>6. Making Predictions (India)</h1>
</div>",b3afcc98,0.5362318840579711
24900,514d8de15cb7ef,1fdac9ef,### Machine Learning Algorithms ,cfe111b2,0.5365853658536586
24901,9c26c5dcd46a25,35b13eb1,"#### <font color=""#114b98"" id=""section_2_1"">2.1. Baseline</font>

Nous avons donc à notre disposition un jeu d'entrainement et un jeu de test pour nos prédictions du Nutriscore. Comme dans toute approche de modélisation, nous allons devoir vérifier que notre modèle est réellement performant. La régression linéaire étant un modèle relativement simple, nous allons devoir **réaliser une baseline**. Nous utiliserons la méthode `DummyRegressor` de la librairie Sklearn :",1bbbb677,0.5365853658536586
24903,0b01138ad120fc,c04e3b9e,**I'll create a graph function to plot future models**,0b4b72e6,0.5365853658536586
24905,0e09587faffa8f,415ec04d,The maximum number of tickets were issued for **overspeeding near school premises** violation,0d563d61,0.5365853658536586
24908,47b2c9be5e31cb,75147c13,Let's take a quick look at what the data looks like:,7d4afe56,0.5365853658536586
24909,5169abdc647412,b9742f75,### can i use accuracy score for train data,28efc68d,0.5365853658536586
24910,8d0aebab1e5914,94e8c9f1,"# Computing Eigenvectors and Eigenvalues
Eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: 
* Eigenvectors (principal components) determine the directions of new feature space
* Eigenvalues determine their magnitude <br>
In other words,`eigenvalues explain variance of data along new feature axes`

Eigenvectors and Eigenvalues of covariance matrix will give the principal components and a vector that we can use to project high-dimensional inputs to lower-dimensional subspace",084e671f,0.5365853658536586
24911,fd4017c1514157,02fd07a9,### <font color = 'red'>Rating</font>,fd8f0896,0.5365853658536586
24912,8d70dcae7f40a3,c1411280,"#### **Confusition matrix**
#### + Có 773 true positive (Số người dự đoán chính xác có bệnh)
#### + Có 11 true negative (Số người được dự đoán chính xác không có bệnh)",472c71ce,0.5365853658536586
24913,62582b8036fbfe,53ad6b5f,### Converting text to number format - (Tf-idf),6c2160db,0.5365853658536586
24916,c65a65d4041018,6950007b,### Most popular IDE,824fb229,0.5367647058823529
24918,f91f58d488d4af,9ece56ba,"The magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating **gradients**.",5df1bbf3,0.5368421052631579
24923,233cb23d9e01b9,6156489f,# Build the model with the optimal hyperparameters and train it on the data,ffa56c19,0.5370370370370371
24924,135122550b6483,b0fad964,"<h4>
So obviously there ia a Trend of Norway > Sweden > Finland <br>
So the Feature Engineering further will require a row of GDP Like how a lot of people have done <br>
So we will have to bring the GDP Dataset as an added feature for the Model <br>
",6592d6d8,0.5370370370370371
24930,2f0f808765fc67,f8c9c68f,# **D. Lasso Regression**,fd1f6494,0.5370370370370371
24939,21413205980558,b42cc3d4,# Section Ⅳ Marketing analysis,84197de0,0.5373134328358209
24942,20b372b6e4e276,fcfb0d86,Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert,ec8b0860,0.5373134328358209
24943,fdbbd573ba31c2,ff79b8f1,## Handle Missing Values,f7c28d74,0.5375
24944,3dd4294f903768,bd4073ac,"We can see that the 'price range', 'votes' and the 'new cost' correlated with our target, so we will use them in our model. ",0d89d098,0.5375
24945,5ffe6aa38958a1,7380df45,Many values of Cabin are undefined. This may be passengers who had no cabin. We can fill all unassigned values with 'U',11f5412e,0.5375
24946,254cccd5145725,cff8d7ad,The above value displays all the missing values in the data. Now we need to fill this with appopriate values that are derived from a concrete hypothesis.,a49b4037,0.5375
24947,0caaec057f7184,cd4533a3,"## Old items new launched in shop items

For old items new launched in shop items, the real condition can be
- new launched in the shop 
- launched with 0 selling records
- launched with missing selling records

Few questions to ask for this kind of condition:
- How's the item saling in other shops? (hotness of the item)
- How's the category saling in the shop? in other shops? 

We'll use eda techniques to help decide how to deal with this pipe of condition.",b875533e,0.5376344086021505
24950,4ae6a182abac64,763e8dac,* **Preprocessing Sex**,418676c5,0.5378151260504201
24953,30fdc4a6e3c1db,2f10767d,"What we see:
* The first graph shows the actual time series
* The second graph shows the trend. The overall trend seems to be increasing over the years
* The third graph shows the seasonality. We see a strong weekly seasonality
* We also see lots of residuals in the last graph",6111ddee,0.5380116959064327
24956,4f69b7bb1ca287,5d66cb14,## Data Modelling and Validation,34abc6a1,0.5384615384615384
24958,cf08b03b002c13,759b4e41,"Similarly to score, on average, number of comments tends to be 4 times higer for NSWF content. ",104d416f,0.5384615384615384
24959,c115e287523aab,7d16222f,"# Data Augmentation
Used simple augmentations, some of them may hurt the model.
* RandomFlip (Left-Right)
* No Rotation
* RandomBrightness
* RndomContrast
* Shear
* Zoom
* Coarsee Dropout/Cutout",feb1288b,0.5384615384615384
24960,80ad12f326ab70,f3524324,"The Subhurb locale has the highest distribution of collected data and Town, the least",da404a16,0.5384615384615384
24963,9eed0fae1c7958,d873f82d,# Compile the model,3fb1438e,0.5384615384615384
24964,4d91e84c564cbe,c3deef99,"> **Aside:** You've actually been calling methods already if you've been doing the exercises. In the exercise notebooks `q1`, `q2`, `q3`, etc. are all objects which have methods called `check`, `hint`, and `solution`.

In the same way that we can pass functions to the `help` function (e.g. `help(max)`), we can also pass in methods:",355a43e3,0.5384615384615384
24968,03048e86a6d806,f1c916d1,"For this part, the measurement is using median. Let's see how much each job title earns for a living, globally.",1285c231,0.5384615384615384
24970,d07915a6e6992e,8eae5eab,"There are 2 ways of handling the missing age values.

* Fill the age with median age of similar rows according to Sex, Pclass, Parch & SibSP
* or use a quick machine learning algorithm to predict the age values based on Age, Title, Fare & SibSP

I used both of them to test which one works better. One of the code will be markdown to avoid confusion.",2b912140,0.5384615384615384
24971,d0f6276d5b628c,a5296e22,"# Behavorial Analysis of consumer :

As the company should produce best content as per the rating also they should find out the consumers which are not showing healthy behaviour ( giving ratings without visiting and others).",c64f5ce5,0.5384615384615384
24975,21122355e39af4,70e339ae,CASE STUDY,88b95e2b,0.5384615384615384
24977,d78988cb5a1b02,8f5ff937,# **Ada Boost Classifier**,233f3a92,0.5384615384615384
24978,dd00690b4a2f1f,26b3cd3d,**<h3> Definition model and training + prediction**,e8b7ce8a,0.5384615384615384
24984,33398ae40da63d,724abe05,Now we want to examine the various publications in each csv file,7bd02b88,0.5384615384615384
24989,6f1481148352e9,60907c52,"**The largest number of fires was in Mato Grosso - 3.6к, Piau - 3.5к и Paraiba - 3.4к.**",7cfbdb8f,0.5384615384615384
24991,e424c111c44669,4968024f,![4-Figure2-1.png](attachment:4-Figure2-1.png),d9fccfba,0.5384615384615384
24993,af6556ced704f6,60d1048c,"***Converting data types***
* We have 5 data types: **int**,**float**,**boolean**,**object**,**category**
* Category is important for machine learning

* We can convert data types using **astype()** function",881577c0,0.5384615384615384
24998,9b42412e75d640,425ed3c4,"# Part 3 
# Building classification models and tuning hyperparameters",b616570a,0.5384615384615384
25005,897ca904b74a98,4673f205,## Feature engineering,c5844ad4,0.5384615384615384
25008,3f451680b1857b,35c4b028,# Copying Files,56c45a1b,0.5384615384615384
25011,2facf256353117,d0c2f933,## get image header,18f579be,0.5384615384615384
25018,8e9d63e1f6319e,243647a6,## Proof,4743b346,0.5384615384615384
25020,3536195ad632ee,60639a71,"**Controlling the process of training**
",3c26aafc,0.5384615384615384
25026,50b03ce5b1a286,70ffe68f,#Check if the Label Encoding worked,d49896a5,0.5384615384615384
25029,020c28a360b0cd,3ff65d2c,on paper,2ba397f0,0.5384615384615384
25032,5f4ae633cfd090,678066b7,"When B_ev is increasing in value, the winners are Red and when R_ev is increasing, the winners are Blue.",a30a16e2,0.5384615384615384
25040,0932046e1f485d,7ffb4655,Let us check again without the absurd prices.,218cc7a3,0.5390625
25043,842547b2def18c,60baf2fd,"### Create new feature combining existing features

We can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.",b8efde6d,0.5392156862745098
25046,71b75664517244,2ea544e3,"This dataset has 4 features, name, club, nationality, and season. now let's merge this dataset with standings dataset.",fc905af5,0.5392156862745098
25048,04ff2af52f147b,3284811e,"**Create Title Feature:**

The *Name* feature appears to have titles ('Mr', 'Mrs', 'Master', etc.) for all passengers aboard.  We will take this part from *Name* and make it its own feature.",d5f37be9,0.5393258426966292
25049,312135b445bd23,3d3612a3,"# **Topic Modeling & Keyword Extraction**
In order to understand the content of the corpus and how the text might relate to each task, we extracted relevant topics with the Latent Dirichlet Allocation (LDA) algorithm. We used the [Gensim](https://radimrehurek.com/gensim/) package on the clean version of the sentences within the filtered subset of the corpus.

[Notebook with code for topic modeling](https://github.com/Hazoom/covid19/blob/master/notebooks/Taxonomy/Topic_Model_LDA.ipynb)

Steps taken:

1. Read in cleaned sentences
2. Build Gensim dictionary with id2word
3. Structure corpus with doc2bow
4. Calculate term document frequency
5. Train the LDA model
6. Using the full filtered subset of articles did not result in very distinctive topics, below is a snippet with keywords from the 10 topics which were derived:",8ced381f,0.5393258426966292
25050,e67925694c07d3,d40617d1,EXT_SOURCE_3 seems interesting because for target 1 and 2 they skewed differently,83af4c4a,0.5393258426966292
25051,5f27526aa6c113,5b778aa9,## feature engineering,a5c26ab6,0.5393258426966292
25054,06ecf7a304c309,8d8cc8f2,- After adding noise,714de627,0.5396825396825397
25060,c9b4e282e4e2c1,bccb1452,I'm going to consider the stadiums with open roof as outdoors.,f44d339f,0.5398230088495575
25061,ac1abfe1dfe815,96b691af,"**Edit the stpowords, flight is the most common word in all classes, and `no`,`doon't`, etc. Because are more common in the negative class**  
flight and get are common in all classes",6529dbcb,0.5398230088495575
25062,b01ee6cb674fa3,b79f0e04,"# Iranian Space Agency - ISA

A public space agency of the iranian government.

Founded by february 2004",a8ffd35e,0.5398550724637681
25064,2ada0305b68956,3613bffd,### 91. Palette = 'coolwarm',133e26f4,0.54
25065,b10bd75889dad9,4c72bf6b,#### Test Train split,ee00ceee,0.54
25066,91eaec994e0c6f,850b77e1,### 2.3.3 Catgeory Level Analysis,376aef10,0.54
25072,7dd46c750653eb,9130ec08,Comparing Death Rate and Birth Rate Month Wise,c2644713,0.54
25073,0687cd5c8597db,05d05376,### **Adding callbacks to save the best model and to stop training when there is no significant change in the Validation Accuracy**,4edec76a,0.54
25082,a6c34cd514e30e,ed56b9bb,Distribution graphs (histogram/bar graph) of sampled columns:,bf603ddd,0.5405405405405406
25084,b7b1057764fa02,7b7abbd9,"We see that we have successfully one-hot encoded the labels. The length is 29, to make room for the 29 labels `A` to `nothing` in our data.

# 5. Preprocessing - Normalize RGB values

Now let us look at how the image data is stored. There are three components for each image - one component each for the Red, Green, and Blue (RGB) channels. The component values are stored as integer numbers in the range 0 to 255, the range that a single 8-bit byte can offer. 

If, however, we divide by 255 the range can be described with a 0.0-1.0 where 0.0 means 0 (0x00 in hex) and 1.0 means 255 (0xFF in hex). Normalization will help us remove distortions caused by lights and shadows in an image. This is a good idea for our dataset as we have seen that our images have a lot of different light and shadow areas.

The following image is an example of what this normalization of RGB values does:

<img src=""http://aishack.in/static/img/tut/normalized-rgb.jpg"" alt=""RGB vs Normalized RGB"" style=""width:300px"">

A more detailed explanation can be found [here](http://aishack.in/tutorials/normalized-rgb/).",5053a192,0.5405405405405406
25086,bdf23d2d396916,85baf6b7,"## Random Forest - Modelling and Model Performance
",b0e45a49,0.5405405405405406
25097,63b44c85e32c1f,b116cf2c,"Note that the nested list [5,4,2,8] is treated as a single element of the parent list lst. Thus the elements inside the nested list is not reversed.
",fb9b9562,0.5405405405405406
25098,d76896b30cebd3,ce241fa7,"Campaign Distribution over Years

",1b4e8f34,0.5405405405405406
25102,bcd7e398c4d0ec,679ff334,"## Description Sentiment
The length of description maybe is the significant feature for model training. ",77a143f6,0.5409836065573771
25104,2105f2c5132866,589ea722,Build the Models,bfe8023d,0.5409836065573771
25107,fdc9f4863744b1,56e5b530,Grouping the dataset based on Boroughs will help segmenting my analysis.,b4529365,0.541095890410959
25109,869a39a3d4dea2,5fb4ea49,"Some of the bitwise operators are AND, OR, XOR and NOT, which may seem simple operations but useful for masking. Bitwise operations turn off when the values are zero and turn on when values are greater than zero",9020daf8,0.5411764705882353
25112,ba4b3bd184acbb,eb49579a,Wow! The Apps DataFrame uses about half the memory after casting and the Setiment column uses about 1/8th!,0f5de724,0.5413533834586466
25114,7ba63a2d9abb58,ad21491e,"The scatter plot below shows US, Brazil, and India as clear outliers with high cases and deaths ",821a261f,0.5416666666666666
25115,fdc3afd309b850,a351a146,"<a id=""dttbd""></a>
## 7.2  Distance to the Brasília Downtown
",966bde38,0.5416666666666666
25116,56a583a039b57c,f074827e,### Aggregate on Maximum value during the timeframe,c0526ea5,0.5416666666666666
25117,1d5daeca89f48d,6cf1145a,## How to calculate Tf-idf weight,48d478bc,0.5416666666666666
25120,6a80f915608fc2,4b47cbd9,"### Which g-vectors have variation over the 23 targets we'd like to detect

The ""tSNE-9"" set of targets are very detectible with the gs_to_use_22 but
two other sets of targets are not well detected with just those, the sets are:
i) the targets with a > 0.01, and ii) expected to be highly detectable (with > 69 counts.)
Look for any g-vectors that are sensitive to these...",636938eb,0.5416666666666666
25123,a69d41047fdd3e,b6d27087,"For a hint or the solution, uncomment the appropriate line below.",b1f28647,0.5416666666666666
25133,02773bdc5d3c7a,f62eaac2,*4. Create an empty dataframe for storing the performance metrics for multiple algorithms*,86245f35,0.5416666666666666
25135,62487bcd70b199,894a50b9,## <a id='6.1.2.'>6.1.2. KNN Model</a>,f6ae50af,0.5416666666666666
25141,923e97b05be00b,6d0cab15,"## 3. Training the model
Now, we will **train the model**. This is the cool part.

We will be using a technology called **resnet34** to look at our images. This is a model that Google created to do image analysis, and has released to the general public. It's pretty smart -- let's see how smart it is on our images.",3a4a22dd,0.5416666666666666
25149,166a62ebb4fc3a,e024fdbb,"Then, drop all columns related to the ""perimeter"" and ""area"" attributes",db48a079,0.5416666666666666
25150,2a377ced98d67a,ccd522b2,### 3.5. Lineplot for showing relations among all features,262231a8,0.5416666666666666
25153,c01049afb6d307,7c98023e,## Age -- Bodymassindex,d37d3b5d,0.5416666666666666
25155,1452c9aef2c2a7,7bbd64be,"### To find flattening dimension

def __init():<br>
    x=torch.randn(33,108).view(-1,1,33,108) <br>
    self._convs(x)<br>
    <br>
def _convs(self,x):<br>
        x=self.conv1(x)<br>
        x=self.conv2(x)<br>
        x=self.conv3(x)<br>
        x=self.conv4(x)<br>
        print(x.shape)<br>
        x=self.flatten(x)<br>
        print(x.shape)<br>
        
 #### else use AdaptiveMaxPool2d
       ",10b8c047,0.5416666666666666
25157,c84925c8171900,8bbadf05,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.3.2  Publisher Wise Video Game Sales (Overall)
            </span>   
        </font>    
</h4>",e21ff7ec,0.5420560747663551
25161,b660910fcc2954,32309ee5,## Target distribution,80b74f88,0.5423728813559322
25162,1294fb4c86f993,8a8cfb6a,"<a id='Q5'></a>
### Research Question 5: What is the trend of guns registeration in the state Louisiana and Texas?",4471e513,0.5423728813559322
25165,a077820f7ab459,11f88cff,### Train the model,05a43104,0.5423728813559322
25167,a81661cc35d8d2,50b3cb57,Transformation 1: Transformation of variables,3331f113,0.5423728813559322
25171,9169c4e9c33c90,9dab212a,"Unsurprisingly, the User Rating distribution is skewed left, as most books in the top 50 have high ratings.",725bf880,0.5423728813559322
25172,2a56d6b0e153f2,a1398f28,"HERE, MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 58% - 65% IN THEIR MBA RESPECTIVELY.",8dc315e6,0.5423728813559322
25173,dac3c8204a2d1b,974de396,We can see here we extract top 10 Authors who got highest ratings.,b0d2d0dc,0.5423728813559322
25175,917957c6c4065f,8fb0d0ca,### 2.2. likes,55b8ed68,0.5424836601307189
25176,4c47839b067546,c960f775,### fuelType,1f517b02,0.5425531914893617
25178,73893f0467d5e3,9218716b,# Extracting features (Independent and Dependent variables),279787c6,0.5425531914893617
25181,6b65d81a5743dd,18b17185,#### It's obvious that more game will lead to more in every other feature else so let's introduce some new features,4080a2d2,0.5428571428571428
25183,38b79494ac749e,7ece1e9b,"Hmm... it looks like high degree polynomials are coming with much bigger regression coefficients. 

We are going to plot the mean absolute value of $w_i$ as a function of degree to reveal the relationship:",39162a40,0.5428571428571428
25186,9c044fa3072552,d704fd9a,### Hour Start Time,1362842e,0.5428571428571428
25188,675b60eaf415a6,e300595d,"* Keras and other Deep Learning libraries provide pretrained models  
* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  
* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  
* This helps in faster convergance and saves time and computation when compared to models trained from scratch",68c0b725,0.5428571428571428
25189,867a9f977fa945,2cb292fc,# Similerity,2740fcca,0.5428571428571428
25194,bbaa07ad21cf4e,33a4540b,### Text,3ab6b254,0.5428571428571428
25196,1823d096209b96,68f013fb,### Bridge and Train a Ridge linear Regression,cb2a79e0,0.5428571428571428
25198,81712ee7510ac5,e0396051,**Boolean**,c4685e79,0.5428571428571428
25200,04bac111ffbe9c,4696ab21,##### We know which columns to drop. We drop them without further analysis,82576b17,0.5428571428571428
25206,f4b603905215b7,a69e4bf0,predict_proba gives you the probabilities for the target (0 and 1 in this case) in array form. The number of probabilities for each row is equal to the number of categories in target variable. ,efe1d587,0.5428571428571428
25212,f3c6048d1058e3,8f71ad8e,"# Word Embedding: 
- Many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. A Word Embedding format generally tries to map a word using a dictionary to a vector.

## Frequency Based Vectorization",1d9056b0,0.5431034482758621
25213,4883314a96dc34,36bb0280,## Iteration (1),50d36836,0.5432098765432098
25214,faa8e6c8ab9246,ee3c58c3,"From the above figure, we can observe that survived people are less compared to not-survived people.",2bea1419,0.5432098765432098
25217,7e89d387feb9f5,7f990a35,### Добавленный числовой признак №11. Отсутствующие данные в столбце 'Cuisine Style',989e3a1b,0.5434782608695652
25223,9b5de3823ad5ab,f72b87f3,"### Image data generators
Given the time available for the task, I didn't considered using data augmentation (DA), because I didn't know much about the underlying details of the problem and what methods were appropriate. So, the top layers were trained without DA.

I added DA. after peeking the work of the competition's winner, so I'm using almost the same method as them. This means that I'm using DA in the training of all layers (just to be clear, this step wasn't taken for the top layers' training).

The images are resized to 224x224 px because it's the input size for EfficientNet-B0. Batch size is 512 in order to try and accelerate training and also because there is enough memory for the task (1024 works, but raises warnings). Once again we're seeding the RNG, so experiments should be consistent between runs.",33e48774,0.5434782608695652
25224,0e2a23fbe41ca9,47af5a8f,"Since ```merchant_id``` is not uniquely identifying each row in the table, I tried to find some compound key. Unfortunately, no such key is there. Data will need to be deduped on the ```merchant_id``` as we discussed during the sanity check part.",64e4762c,0.5434782608695652
25226,72d528df923403,8fd3ab63,"- The days that occur before a purchase have gradually decreased. Which means that the average quantity of consumption has decreased, but people come more often to make their purchases.",d51c8e8e,0.5434782608695652
25227,726833f92fb87a,eb36edcf,**We can see that customers who had previously been contacted tends to accept the deposit**,7dc5e1b6,0.5436241610738255
25232,fdbbd573ba31c2,a4e75e4f,"Replacing the missing values of numeric features with `mean`, and `mode` for categorical features.",f7c28d74,0.54375
25239,30fdc4a6e3c1db,0dbfbd48,### Plotting monthly sales time series across the 3 states,6111ddee,0.543859649122807
25241,c3498779cda661,e92cb6ea,"# Manejo de Nulos

Dado que la cantidad de nulos es mucho menor que el total de datos se va a eliminar los que tengan nulos.",0f531b65,0.543859649122807
25245,5ce12be6e7b90e,b8e5a1d2,"### Access: Indexing

We can acces specific characters (sequence items) in a string using square brackets `[i]`:",c0ab62dd,0.543859649122807
25247,3d905ce4828057,1b84a982,# ** GAMMA-GAMMA Modelinin Kurulması**,5b006cc3,0.5441176470588235
25250,7f74a04ae75792,b101fca6,#### Handling Missing Value for `Cust_Ann_Income` column ,d01e91da,0.5441176470588235
25252,eb0ecd6bebeb15,463a6e2b,"Hangi çiçek türünden kaçar adet gözlem barındırıyor veri çerçevemiz?

50 x 3 olduğunu ve dengeli olduğunu value_counts ile zaten görmüştük, ancak bunu görsel olarak ifade etmek için sns.countplot() fonksiyonuna variety parametresini vereilm.",d7b93a60,0.5441176470588235
25256,9cec5ddf8b6f49,d6fd7e11,# 4. Evaluate Algorithms,d39fc8e7,0.5441176470588235
25264,892be0a523578c,74fddd39,"For **sleepDay_merged.csv**, I don't do much cleaning except for changing the data type of ""SleepDay"" attribute to pandas datetime data type, which was already done in the proceess step. 
Also, there is not too much we can do for the **weightLogInfo_merged.csv** dataset, except for changing the ""Date"" attribute to the right data type, which was already done in the process step. 
After taking all the efforts, we can now walk into the analysis part!",b0e8d7c0,0.5444444444444444
25265,b0c2805cd5c087,1eef386e,Image slideshare.net,0446f327,0.5444444444444444
25266,e19e307b3fd188,5ee0386d,There doesn't seem to be much correlation between the **area** and the **rent price**.,2173955b,0.5447154471544715
25268,d1ff7e10ee0102,578f38db,"How 'SalePrice' looks with her new clothes:

* Low range values are similar and not too far from 0.
* High range values are far from 0 and the 7.something values are really out of range.

For now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.",2cc71c3c,0.5454545454545454
25269,25c2f1ef13b402,afa0b654,**GETTING THE MODELS CREATED BIASES AND VARIABLES AND  CALCULATING PREDICTIONS MANUALLY- FASTER WAY THAN METHOD MODEL.GET_PREDICTS - WITCH IS TOO SLOW TO USE LATER ON THE PRIVATE TEST SET**,b028c35a,0.5454545454545454
25272,016abae0483764,3d0467b3,**Preproccesing** the dataset as some of them are not in numerical value and eventually also we need numerical value for tarining the model...,bc9f289b,0.5454545454545454
25276,2f47abddfd1928,adab8812,"The fare feature per sex shows a quite skewed distribution for both sex.

As expected male is much closer to zero as most of the males traveling where workers and poor people. Although there are high cost tickets for males but its percentage is very low.

Female passengers, even though it is skewed towards zero as well, it covers a higher range of ticket costs.",ae33cc0b,0.5454545454545454
25278,4ae464582bac51,fd62e3d0,"## Numerical Attributes
",ca6a52ce,0.5454545454545454
25282,90964081c7faab,45fe64fc,## 5) Data Modeling,b423b0c3,0.5454545454545454
25284,70193f0c034b98,93325c8f,Validation dataset,f8cacd26,0.5454545454545454
25286,b066ab2167199c,3196e82d,### 1. Find Unwanted Columns,18a1753d,0.5454545454545454
25288,2f964d08c25d93,1acf90f3,Let's take a quick look at what the data looks like:,1f2e4468,0.5454545454545454
25290,eda49464dd6d1b,82fe1340,# Evaluate Model ,8421f81f,0.5454545454545454
25291,dd02a9b545f742,c6fd8ece,# Phase 1 | Object Building - Object 2: Feature Analytics Project,7116cd2d,0.5454545454545454
25293,efd44ce2c08541,874aab82,### Graph Features,ebc2d00c,0.5454545454545454
25298,4b64dc653fb7eb,39220e6c,"<a id=""5""> Step 5 - Split each sentence using delimiter </a>

Converting each sentence to list of words. We are doing it to keep necessary words in the upcoming steps and descarding the rest.",57675cc2,0.5454545454545454
25300,adf419444a59df,e286c50e,## 2. Defining the Loss function,3a275e7f,0.5454545454545454
25304,f0fab078f8533b,87542ae3,########################################################################################################################################,bdb5ea32,0.5454545454545454
25307,b1684dfa49524a,f2487e68,"- Ridge regression is not accurate enough, so  I write a polynomial regression",60a2599b,0.5454545454545454
25309,0475899eec1ffe,16352184,"After observing that our data set is too much data, we are reducing our data set considering that we will evaluate the data after 01/01/2018.",d825dc37,0.5454545454545454
25313,5083d7a61f2426,c3ed94c8,"Scalling back to the original data, to visualize the results",541a0fec,0.5454545454545454
25315,a2444ab5d5f147,b010d0d3,### Let's analyze 1 word phrases,10617755,0.5454545454545454
25318,6d66ced0028dea,c6aca798,### Заполнение NaN,f50aae52,0.5454545454545454
25319,fc8e0042411c46,e6c5981d,- All entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.5454545454545454
25320,dbd96dd275dc60,85be397d,"**Question:** WHy the above value, does not hold water/true/reliable?
Because we have done our scoring/evaluation on the same datset on which we trianed our model",1ed493a8,0.5454545454545454
25321,f166950fa915f8,8374e975,### Embedding layer,a7f6ca5e,0.5454545454545454
25323,a26032553265a6,d9fb830c,> ,f489744e,0.5454545454545454
25325,5a8c553e21c70f,9bfe3e0b,"## Standardization

StandardScaler is only fit to training data to prevent data leakage.",9ebd9d8f,0.5454545454545454
25326,18ce858f90966d,c67d4ee6,# **Hierachitical**,09e9caed,0.5454545454545454
25330,da199f8fb59439,d06824ee,#### **Release of movies & tv-series per year**,baaa665d,0.5454545454545454
25332,37360278c19104,3e4fbad8,# Separate features and targets,21473a41,0.5454545454545454
25334,450fda47b03baa,84636732,"Keman grafiği çizdirerek Rating değişkeninin dağılımını inceleyelim.

Söz konusu dağılım bizim için ne ifade ediyor, normal bir dağılım olduğunu söyleyebilir miyiz?",62c04adb,0.5454545454545454
25337,132fa9714f2046,1d62e2e7,"** Now use x,y, and z arrays to recreate the plot below. Notice the xlimits and y limits on the inserted plot:**",3bb1775f,0.5454545454545454
25342,347c7b0f48c53f,207a3600,"**Another important library that we need to parse XML and HTML is the lxml library. Execute the following command at command prompt to download lxml
******",c58305ba,0.5454545454545454
25343,a35cdce61f4059,4133d101,"While our test results show that we have 99.9% percent accuracy, around 25% of all frauds where undetected<br>
and almost all non-fraudulent cases where predicted correctly. this is because of **extreme imbalance in our dataset**<br>
for ratio of two classes compared to each other.",acc8eab6,0.5454545454545454
25344,bb3d1b4b9f1248,1cd54886,"**Observations:**
1. Missouri PerCapita consumption came back to same in July as when it started
2. Kentucky PerCapita consumption is the highest among the 16 states

Above tke key observations",bf7de324,0.5454545454545454
25349,663bbc9eaf267b,2e074538,Number of cars for each category,32445529,0.5454545454545454
25350,2ada0305b68956,c5700c9f,### 92. Palette = 'coolwarm_r',133e26f4,0.5457142857142857
25353,b61ab8f81dc03d,eb5f6583,"The last item is ""Fare"" on testing data to be mapped. In this specific case that we have just one (1) item missing we can use the median to fill out it.",64d05394,0.5460992907801419
25355,d07915a6e6992e,cd0a069d,"This section of code is for missing value treatment for age. Instead of directly replacing the missing values with the median value of complete data, the script looks for nearby data which is similar in terms of Sex, Pclass, Parch and SibSp and then takes the median values of those observations.

For example, if my salary information is missing, you can't simply replace the missing value with median salary of my whole organization. You will look for people with similar experience, type of work, department, etc and then will use the median salary of such a group.",2b912140,0.5461538461538461
25356,09751c520b0616,f6e4eddf,- Final numerical dataset,a4d0c7e9,0.5461538461538461
25357,a4f8ad33c823c5,4acb5b0b,The above plot shows the distribution of the bmi values between the females and males diagnosed with diabetes.,fcd48307,0.5461538461538461
25360,fdc3afd309b850,f0554c45,"To use the geolocation in our model we're going to use the Haversine formula to determines the great-circle distance between the latitude and longitude (lat1, lon1) of the apartaments and the Brasília downtown latitude longitude -15.794228, -47.882165 (lat2, lon2).

To do that, I was lucky to find this [answer](https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula) from Salvador Dali on Stack OverFlow. It seems that he is coding now. Shout-out to Mr. Dali. You are a very talented man!

[Haversine on wiki](https://en.wikipedia.org/wiki/Haversine_formula)",966bde38,0.5462962962962963
25362,ab6da5994949a3,f44bcfe4,## SVC Training Results,fae6b91d,0.5462962962962963
25365,225b4fe5d3894a,5413da1a,or we can use FunctionTransformer that easily defines above class based on your function,4b4197b3,0.5463917525773195
25367,d96642860ab3dd,166f4342,#### 2.1.4 Cabin column Missing values,98419d48,0.5465116279069767
25368,c09fac3c943d51,485ba3e2,"Paired categories from ""teach-lightgbm-to-sum-predictions"" kernel",678d076d,0.5465116279069767
25369,67b7354e96113a,a91f32f9,"**Dropping Columns**

**Name**        -     Unique so not needed

**Age**         -     Since we have AgeGroup,we'll delete this.

**Ticket**      -     Unique so not needed

**Fare**        -      Since we have FareCat,we'll delete this 

**Cabin**       -       Many Nan so imputing might lead to bais

**PassengerId** - Cannot be categorised",dca94250,0.5466666666666666
25370,7e1da639035ac5,99143415,### <a id='10.2'>10.2 Collaborative teachers ratings statistical analysis</a>,120b6c23,0.5466666666666666
25374,37e461081e47c5,c32601fd,#### Test Function for Modeling (calculate RMSE for various tests),b3e6549e,0.5466666666666666
25378,a566b5b7c374e7,84deaa27,### Average Resting Heart Rate,b3dc5545,0.5467625899280576
25381,ff3a8ce61fab6a,2da0ba4d,"<hr>
<div>
    <b>Alert</b><br><br>
    You must make sure that two numbers you will add have the same             <b>datatype</b><br>
    If you add <code>5 + 10.5</code> you will get error because 5 is <b>int</b> but 10.5 is <b>float</b>,<br> To avoid this problem you must add <code>5.0 + 10.5</code> to get 15.5 
</div>
<br>

## String concatination<hr>

### Exampel 1 ",9afe1654,0.546875
25383,49ee86d074de69,77f0ee8a,### Select Inputs,71ccc6d3,0.5470085470085471
25386,b01ee6cb674fa3,57289bf6,"# Blue Origin

Blue Origin Federation is an American privately funded aerospace manufacturer and sub-orbital spaceflight services company 

HQ: Kent, Washington, USA. 

Founded in 2000 by Jeff Bezos",a8ffd35e,0.5471014492753623
25387,510b8303776bb6,aa2600eb,### Plotting the correlational matrix.,18080db8,0.5471698113207547
25391,0ad8d416b89b78,fe578c5f,Binarising the dataset for use in model development and training.,0b0562f0,0.5471698113207547
25394,614ba9f0c62677,e36bd923,"<a id=""10""></a>
## Implementing with Keras",b8551335,0.5471698113207547
25397,63b44c85e32c1f,845cfc3c,"[](http://)For lists containing string elements, **sort( )** would sort the elements based on it's ASCII value in ascending and by specifying reverse=True in descending.",fb9b9562,0.5472972972972973
25398,f91f58d488d4af,7402d6f4,"Let's illustrate with a simple case. Define a simple function, the quadratic and let's pretend this is our loss function, and x is a weight parameter of the function.

And a simple function *plot_function* which plots a graph for the given function.",5df1bbf3,0.5473684210526316
25401,3c2033cc99c12c,78643021,"<b>The advantages and disadvantages of T-SNE<b>  
+ The dimension reduction method with best performance 
+ The t-sne costs large memory and long running time 
+ It could not plays a good role when we are not clear whether the dataset is separabl 
",dfa22a54,0.5474452554744526
25404,2d40f383473fa4,56ad43bc,Let's standardize `Age` and `Fare`.,1da1eff0,0.5476190476190477
25405,066c5ee1ef39e6,6dc2c7c9,### Toxic Data Train TD-IDF + Ridge,0f394e1b,0.5476190476190477
25408,a76e0e8770b7a0,dc6103f8,This plot looks horrific.,02863d3b,0.5476190476190477
25409,a758983a68c014,7718775d,"It's time to define the Loss function. In Skip-Gram we want to predict context by given word. So we want to maximize the following equation:

\begin{align} 
& max \prod_{center} \prod_{context} P(context | center; \theta) 
\end{align}
<br />
<br />
We want to maximize it, because we're interested in maximizng of $P(context|center)$ for each `context` `center` pair. But neural nets don't like to maximize, but rather minimize. So equation transforms to:

\begin{align} 
& min -\prod_{center} \prod_{context} P(context | center; \theta) 
\end{align}
<br />
<br />
Adding logrithm before the equation helps to use it's useful property, concretely:

\begin{align} 
& min -\prod_{center} \prod_{context} log\;P(context | center; \theta)
\end{align}
<br />
<br />
\begin{align} 
& log(a * b) = log(a) + log(b) 
\end{align}
<br />
<br />
\begin{align} 
& min -\sum_{center} \sum_{context} log\;P(context | center; \theta) 
\end{align}
<br />
<br />
It's left to define $P(context|center; \theta)$, here Softmax function is used:

\begin{align} 
& P(context|center) = \frac{exp(u^T_{context}v_{center})}{\sum_{\omega\in vocab} exp(u^T_{\omega} v_{center})} 
\end{align}
<br />
<br />
where $u^T_{context}v_{center}$ is a scalar product of vectors $u$ and $v$ (`context`, `center` respectively). Summarizing, the cost or loss function looks like this:

\begin{align} 
& min -\sum_{center} \sum_{context} log\;\frac{exp(u^T_{context}v_{center})}{\sum_{\omega\in vocab} exp(u^T_{\omega} v_{center})} 
\end{align}
<br />
<br />
Thanks to PyTorch developers it contains CrossEntropyLoss funtion which is exactly the funtion above. [See details in PyTorch documentation](https://pytorch.org/docs/stable/nn.html).",ab89f181,0.5476190476190477
25410,1fac5edd4063ba,3d34f361,![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQrJn2D0YN0Qeqqz6tYxcGac1OsFIK7yZmASQ&usqp=CAU)amazon.com.br,04bc01e0,0.5476190476190477
25411,1084376bc4897c,9fa047a3,"## 4.1 Importing libraries.  We are going to use Logistic regression, Random Forest and Boosted Trees to make predictions.",1b598487,0.5476190476190477
25416,916ccf243827f1,cdd3cde2,## 8. Forward Propagation with Dropout,5147f4d2,0.5476190476190477
25417,0d59a3e0130db0,1433ae13,"Now split the dataset into training and test data and determine the size of the vocabulary by counting unique words in the training set.
I count words only in the training set to avoid an obvious data leakage",285f04b2,0.5476190476190477
25420,31268b33de97b5,69211be6,# Distribution Plots for continuous Features ,1e6f7d14,0.5476190476190477
25428,1eb62c5782f2d7,4d962329,### Solusi Q2,bb69f147,0.547945205479452
25432,99bf357eaf61f1,b0e6e3bb,### Deleting Columns,9d92fafe,0.5480769230769231
25434,44f6a002ecd033,a2ef39ec,"The log transformation will not be insightful if we have values equal to zero, so if that is the case we will need to drop them.",70bbe106,0.5480769230769231
25441,0d9a2067267ba1,02b5d312,The most insteresting correlation to retain is the `energy_star_rating` : **buildings with low energy star rating have higher energy intensity**,abc194fb,0.5483870967741935
25443,921fff7d3040db,2127a234,# 5. Decision tree model,5f36ced9,0.5483870967741935
25444,0925f172b5eb74,2f3b88e5,# Building the learning rate function,ec34cd72,0.5483870967741935
25450,0cb456a5456cf9,15261d03,"# **Q1 Answer**<br> For all numerical features, we compute the correlation of them. The top 5(except for is_canceled) features are important.<br> 我们得出相关系数矩阵并将其排序，发现排在前五个(is_canceled 除外)的数值型特征与is_canceled具有较强相关性。<br> 
    lead_time
    total_of_special_requests
    required_car_parking_spaces
    booking_changes
    previous_cancellations.",5701729c,0.5483870967741935
25452,a3e8d6ef4c5188,d21db668,### Splitting the dataset,7c8212dd,0.5483870967741935
25453,f15eac23fbcc9d,e9098f31,Separate independant and depandent data using fast.ai utility proc_df.,ea46d8af,0.5483870967741935
25463,fc8e0042411c46,62b99d9a,## Newspaper Article,af476c2a,0.54858934169279
25470,ba4b3bd184acbb,5062ebee,"### Removing Null Values

The next cleaning process to undertake is to remove null values.

First we use the `isna` method to count the null values in each column to see how we should address them.",0f5de724,0.5488721804511278
25471,523123dad03177,7c56354a,Group E scores more in Maths.,48a5e4e6,0.5490196078431373
25473,64169805aacf17,d9cce575,# Let's Paint,1f12ded0,0.5490196078431373
25478,32ddc45133f77b,77640ea7,The model has accuracy of around 76%,3c0d6831,0.5490196078431373
25480,52cfd66e9ec908,592a105a,"Once again, we have a right-skewed distribution as is the same with all the `extent` variables. ",c74adcdf,0.5490196078431373
25482,d0080e3a39bc5c,f56505e8,"Real-Time Data Augmentations like 
* Flipping
* Rotation
* Lighting Changes
* Warps

were all carried out to make the model learn better",2fcde4cf,0.5490196078431373
25483,4fa553c2b837d4,935a3a2b,"If some columns has some useful values i.e the columns with missing values. Then your model may result in an error.

So, this is not always the best solution. However it can be helpful when most values in the columns are missing.",c65a23e9,0.5490196078431373
25486,9bcfa825c8b2e6,1bacacf4,BloodPressure kan basıncının yüksek olması da etkiliyor,220f36e4,0.5492957746478874
25487,3d77c1560bd16e,55779b93,"## Type of vaccine allocated

Vaccines are allocated in batches from producers (Moderna, Pfizer). Some providers are allocated with combination of Moderna and Pfizer so exact count of those allocations are not reported.",87c141ca,0.5492957746478874
25488,bddd799cdbbae8,81ccee48,**Spliting data**,b44e3c08,0.5492957746478874
25490,631cd434fc3aa2,f2147433,* _MSZoning_: replacing this with the most frequent value.,2b74febb,0.5492957746478874
25500,2bace980aeb34c,ae76b2a5,"### Part B

Run the code cell below without changes.

To pass this step, you need to have defined a pipeline in **Part A** that achieves lower MAE than the code above.  You're encouraged to take your time here and try out many different approaches, to see how low you can get the MAE!  (_If your code does not pass, please amend the preprocessing steps and model in Part A._)",dc05ef6c,0.55
25504,541d0fa0e26b80,00f878c7,# Groupby and Heatmaps,a29e0f29,0.55
25515,cf4d1c1ad1476c,08d4335a,## Creating a Tokenizer,768c1a59,0.55
25520,3dd4294f903768,169bc0e1,"Now we will try to do some **Exploratory data analysis** in order to get a better understanding of the connection between our features, and between the features and the target.",0d89d098,0.55
25524,bfe6c7096b1ad0,4dfc483c,"## Проверяем данные
### Некоторое количество выбросов - хорошо...",fffd95e0,0.55
25526,edc19e349fe80a,5682cb7b,"## Prediction and submission
### 1. Define a prediction function",7882221a,0.55
25531,6cacdcf8daf400,42f433dc,**Preprocessing - Cropping**,83939b53,0.55
25534,9f0ccf5b9e8f03,95edb778,"#The Study findings

The findings have important implications for public health. The association between SARS-CoV-2 and Kawasaki-like disease should be taken into account when it comes to considering social reintegration policies for the paediatric population. However, the Kawasaki-like disease described there remains a rare condition, probably affecting no more than one in 1000 children exposed to SARS-CoV-2. This estimate is based on the limited data from the case series in this region.

The study has the limitations of a relatively small case series, requiring confirmation in larger groups. Genetic studies investigating the susceptibility of patients developing this disease to the triggering effect of SARS-CoV-2 should be done. Nonetheless, they reported a strong association between an outbreak of Kawasaki-like disease and the SARS-CoV-2 epidemic in the Bergamo province of Italy. Patients diagnosed with Kawasaki-like disease after the viral spreading revealed a severe course, including KDSS and MAS, and required adjunctive steroid treatment. A similar outbreak of Kawasaki-like disease is expected in countries affected by the SARS-CoV-2 pandemic.https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31103-X/fulltext",66691203,0.55
25537,1011899b959f44,9455ef8d,7. Now let's say I want to combine Select and Summarize and find the average number of deaths for battles in only the year 299. How can I specify this? (Hint: Use conditional select and .mean() together),0b112382,0.55
25542,f0faf9e7ac5abd,bdec0775,#More than Half (a friend). Less than half (friend). Maybe their friendships aren't reliable so they count halves friends.,794edb83,0.55
25546,726833f92fb87a,a65deb7c,Is there a relationship between balance and age?,7dc5e1b6,0.5503355704697986
25552,7e275c8d5ff2a0,94a6fb3d, Data forecasting is done by using Prophet library .Prophet is an open source library published by Facebook which is good for Time Series Forecasting. We will predict the coronavirus cases till ** Mid August  2020**.,b3afcc98,0.5507246376811594
25553,ea4e559a86d613,3f4c7505,**Performing one hot encoding**,eff47843,0.5507246376811594
25558,17a24d566ffa59,6ce2a2a9,1. ![](https://s3.amazonaws.com/nlp.practicum/svd_truncated_equation.png),89049e56,0.5507246376811594
25560,0e2a23fbe41ca9,0f75e75e,"### 5. Most recent sales and purchases

most_recent_sales_range, most_recent_purchases_range

Thses categorical variables have the following values: A > B > C > D > E


most_recent_sales_range: Range of revenue (monetary units) in last active month  
most_recent_purchases_range: Range of quantity of transactions in last active month",64e4762c,0.5507246376811594
25561,548f961125248d,07ed2bda,"* The default model gives an **AUC score of 0.90373**. Not bad for the first model!! 
* The winning submission has a score of 92%. Lets see if we can tune the model to squeeze out the last 2%. 


* We will study the paramaters of the default model and try to provide a sensible range for hyperopt to tune on. ",d8c5e8b8,0.5507246376811594
25564,1294fb4c86f993,1643a97c,Defining function to plot state trend over time,4471e513,0.5508474576271186
25565,9169c4e9c33c90,fe4ffe8d,Let's look at which books received less than a 4.0-star rating:,725bf880,0.5508474576271186
25567,ffd1df95ca5289,e3eae6d5,## Data Preprocessing,db00c338,0.5510204081632653
25570,2343dc02ffb96a,a95bf581,# Heatmap,29aa95a4,0.5510204081632653
25573,f35bf4df70d310,2cc9ca27,## 4. Experiment,10bb859a,0.5510204081632653
25574,f1e162ddd14f11,3f537cf0,let's see this by visualization,cdb2e771,0.5510204081632653
25577,ce9ed5e2d601d7,8c661286,"## Classification with Gated Residual and Variable Selection Networks
From [TPS-12] G-Res & Variable Selection NN (Keras) https://www.kaggle.com/mlanhenke/tps-12-g-res-variable-selection-nn-keras",f58a2f43,0.5511811023622047
25581,d4c5aaa4b36810,3c227a38,# Exploritory Data Analysis,65441f28,0.5512820512820513
25584,2ada0305b68956,10b02b5b,### 93. Palette = 'copper',133e26f4,0.5514285714285714
25595,6a1d04e8153df3,4ed32fd6,"**Observation(s):**
- Density of Both the survival rates are almost the same as you can see .
- On common note PDF peak is high so there are more data of survival .
- Similarly as we can see that or also saw above that death is less that's why the peak is down.

This pdf graph used to see the density of the survival rate.",38572b05,0.5517241379310345
25596,858da4bb312f67,d57aa21a,Some images look healthy comparing with the original images.,9cca4391,0.5517241379310345
25597,a1ba5ffd30dbde,d78eff11,- 92% accuracy On Test Data,48e57546,0.5517241379310345
25598,6b54e39f86bdb5,f95923fb,"## Cross validate the model on an unknown set

Now that our model is trained with a good accuracy on the training dataset. I improved the performance on the dev set thanks to the added regularisation, droupout layers, as suggested in the previous notebook.",198084bc,0.5517241379310345
25600,45921c50ac56fa,1a0a1e5a,Tuples of tokens and their corresponding POS tag,465973eb,0.5517241379310345
25601,9535bb04ae042c,d7384005,## vii) Modeling,165b6fae,0.5517241379310345
25603,fb5c6021d127ef,ccda46d9,Write your query below:,dd05cbd3,0.5517241379310345
25605,00d295edcd117e,4c0b9ddf,## 定义损失函数和优化器,f5810f4b,0.5517241379310345
25608,cd10f3afd970b3,16454b3e,### Let's check 2nd file: ../input/boxes_split2.csv,2db3c8e4,0.5517241379310345
25609,ee9ddc756b2d4a,0bdc4684,# Definzione della metrica da utilizzare,e367eab3,0.5517241379310345
25613,d42518f6cb0995,415d70b3,"Different questions have different popularity and complexity, and it can also be used in the baseline.",26913a9b,0.5517241379310345
25614,5ea840754577e3,fbd6f153,## Feature: Sibsp and parch,9cf9b73f,0.5517241379310345
25616,401338428b2d1c,5f67647c,## Training the Random Forest Classification model on the Training set,e4b768be,0.5517241379310345
25618,00001756c60be8,3e5f860d,"Признаки Rooms, KitchenSquare, HouseFloor имеют в некоторых наблюдениях нулевые значения",945aea18,0.5517241379310345
25620,fb9296ecd0cb2a,758788d0,"## Implement mape scorer

Since lb uses weighted mape (all targets are > 0), we will implement mape scorer to pass into GridSearchCV",aa66d98c,0.5517241379310345
25621,4b7039cb44a54c,3afb6d41,# Dataset,24e806af,0.5517241379310345
25623,bb8f5d7807718b,1faa63c2,"# 6. Timelines

Creating a timeline plot with Matplotlib. Here is a timeline showing the Android version history using the code provided in the official [documentation](https://matplotlib.org/3.1.3/gallery/lines_bars_and_markers/timeline.html).",181ec286,0.5517241379310345
25625,87e96e14f8f5ce,826b7d05,# Cleaning of Data,f9f7a3a2,0.5517241379310345
25626,84127ade6fde87,68065dc4,"At this point, tensor represents one sentence of length 12 in an encoding space of size 10,160, the number of words in our dictionary. Figure 4.6 compares the gist of our two options for splitting text (and using the embeddings we’ll look at in the next section).",f55d05b6,0.5517241379310345
25627,c18c37441caa8d,a016a05a,> ** Machine Learnign Models**,ca98414d,0.5517241379310345
25630,21413205980558,1a3d8d5e,# 4.1  Deposit business user age（存款业务用户年龄）,84197de0,0.5522388059701493
25633,ba655a261cc09e,ee048ecf,## data pre-processing,48cc549a,0.5522388059701493
25637,55a5e31d03df9f,7d4f6455,"When we use the tf.hub most of the times we get a Keras Layer object back, then we add it to the Sequential API (or Functional, but it is more complicated) and add the last layer our own output layer. 

**Note:** We specify the input shape in the `hub.KerasLayer` method and we set the trainable parameter to false, this is mandatory if you wish to fine-tune to your case and not start training all the layers.",06dce00f,0.5523809523809524
25645,c950cff74e51ac,8d7854b7,**Average price of room types in each neighbourhood**,d59bf323,0.5526315789473685
25648,52ee792e228d54,dc12c7e3,"### These negative values are predicted by the model. We will not be able to impute the Experience for people with age below 25 using the model as it will predict negative experience (As they say, ""There's no perfect relationship"").
### Let us instead replace leftover negative Experience values by 0",5096094e,0.5526315789473685
25650,31b564f11ef638,e55ca38a,### Evaluation score(RMSLE) function,424f9692,0.5526315789473685
25651,bef2347846e476,61d5dc76,Also we can see the names of the columns with the command data.columns  .,cb93bf51,0.5526315789473685
25652,6cade0b6a41ba2,2a4e3013,## 3.10. BMI,e6110293,0.5526315789473685
25658,e19e307b3fd188,92e804fb,# Testing ML models,2173955b,0.5528455284552846
25660,513ce405d7f6a3,f635770b,"Naive Bayes is the simplest Machine learning model that can use to do classifications. Even without pre-processing the text, Naive Bayes got an accuracy of 61%",8461e086,0.5529411764705883
25663,f6648e47713411,1519adb1,"### Kết luận
Phân tích kênh màu thì thấy kênh màu xanh chủ yếu có phân bố ở vùng có intensity mạnh ( vì nó tập trung ở phần cuối đồ thị) ==> Dễ hiểu vì ảnh toàn lá. Ví dụ như ảnh toàn lá mà thấy kênh màu đỏ lại mạnh hơn thì sẽ phát hiện bất thường. Ở đây kênh màu đỏ lại phân bố ở vùng giữa mạnh hơn kênh màu xanh dương, chứng tỏ màu đỏ nó cũng có xuất hiện nhiều là do một số lá cây bị sâu bênh hay có màu đỏ",f4af4d1c,0.5531914893617021
25665,56785caebaa256,74eb6f99,"## 5. Tuning Prophet model and holidays parameters<a class=""anchor"" id=""5""></a>

[Back to Table of Contents](#0.1)",a792961a,0.5531914893617021
25669,c7e5f658090347,48c0cc8a,"## Define the Initial Testing Pipeline with Logistic Regression

I will try two approaches to handle the class imbalances present for the 4 train_test datasets above. After the over/undersampling I will scale the data prior to model building.

**Version 1:** Undersample the Dataset to get a 1 to 1 match between the classes. 

**Version 2:** Oversample the Dataset with Borderline-SMOTE to get a 1 to 1 match between the classes. 

**Version 3 + 4:** After completing the above two versions and observing no notable difference with over sampling. I decided to try a different ML model (random forest) which should be more ""data hungry"" and see if this also is insensitive to under vs oversampling.

Borderline-SMOTE is a special type of oversampling technique that only makes new versions of minority class members that are rather ambiguous/uncertain to classify. So the extra examples generated are more likely to be useful to the model (we do not need to upsample in regions that are easily classified as 1 or 0 already). 

It is [recommended to combine oversampling with undersampling](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/), and this will also notably reduce model training time. So we will upsample the fraud examples to 1:20 and then downsample the major class afterwards to get a 1:1 ratio. 
",43c78e7d,0.5531914893617021
25670,3f25b363afec54,4024b5c2,## Test data exploration,bbdaae25,0.5531914893617021
25671,5f674175839b32,46b7cf76,*IN year 2008 and 2009 most number of games had been relased.*,53a2e343,0.5531914893617021
25676,98a6794067932a,ab00d903,"La suite des cinq cellules de code suivantes a été créée de la même manière pour chacune des cellules, cependant elles permettent de représenter des informations différentes. De manière générale, ces cinq tableaux permettent de représenter sous forme de pourcentage la proportion des expéditions qui sont effectuées pour chacune des sous-catégories de produit et ce, pour une ville précise. Le premier tableau présente l'information pour la ville de ""New York City"", le deuxième tableau présente l'information pour la ville de ""Los Angeles"", le troisième tableau présente l'information pour la ville de ""Philadelphia"", le quatrième tableau présente l'information pour la ville de ""San Francisco"" et finalement le cinquième tableau présente l'information pour la ville de ""Seattle"". Tout d'abord, le code permet de représenter le nombre d'expéditions par sous-catégorie de produits. Ensuit, le code permet de sélectionner seulement la ville désirée. Le code suivant ajoute une colonne représentant le pourcentage des expéditions selon la sous-catégorie de produits par rapport au total des expéditions de cette ville. Finalement, le dernier code permet de trier les pourcentages sous forme décroissante. Ces tableaux permettront donc aux dirigeants d'observer le comportement des cinq principales villes desservies par l'entreprise face aux sous-catégories de produits qui sont favorisées par les clients de ces villes.",08600fe2,0.5533980582524272
25679,f13534449a3750,ca2c44d4,"## Images Visualizations
<a id=""subsection-two-2""></a>

In this section we will investigate better the dataset, in particular the images and the masks that will be used in the training part. 
Firstly we plot only the maks, while in after we merge the images with the mask obtaining the real satellite image with the presenceof the ship detected. ",8b7f3332,0.5535714285714286
25680,8dd655515e7d18,58617e23,### Word Count Analysis,895f41cf,0.5535714285714286
25682,3cd78d8d6d56e4,eb3d1e44,## Model Training,9f632e94,0.5535714285714286
25685,2f47abddfd1928,818113df,"## 3.3. Age

Seems that Pclass has a good correlation with Pclass (already covered), SibSp, Parch, Fare and Cabin_Letter.

Lets inspect these relations.",ae33cc0b,0.5537190082644629
25686,e9b9663777db82,22409ac6,#### 10.Quantitative analysis for From Who feature,648e8507,0.5537190082644629
25690,a4f8ad33c823c5,7db1a734,#### Ethnicity Description,fcd48307,0.5538461538461539
25702,62037c5832129c,e37d47b3,# Looking at different performance evaluation metrics,61474350,0.5540540540540541
25703,e4525eb0c96f28,db75f10a,"### Nintendo Switch sales analysis

As shown, even a newer console like the switch has sales that fall off after the first year. This may not be a fair statement to make however, since the PS2 was the console that thrived in the age of console excitement. Meanwhile, the Switch is a console that came years afterwards.

New Dataframe:
- **switch_pc_df**: For visualizing PC and Switch data. Removes outlier year 1970.",2093a1f1,0.5540540540540541
25705,4daf6153275cbf,5a423afa,"While Belgium distinctively has low ratings, people in Slovenia seem to be happy with what they are paying for their own food.",51db1961,0.5542168674698795
25709,b01ee6cb674fa3,b1418cde,"# Indian Space Research Organization - ISRO

Space agency of the Government of India, public

HQ: Bangalore, India

founded by august, 1969
",a8ffd35e,0.5543478260869565
25710,4ae6a182abac64,0f7d25d4,* **Processing Embarked**,418676c5,0.5546218487394958
25712,0932046e1f485d,a7312994,## <a id=genres>Genres & Categories</a>,218cc7a3,0.5546875
25714,3c2033cc99c12c,c0dacf20,### A brief comparison between three dimension reduction methods  ,dfa22a54,0.5547445255474452
25727,7ff97196d5db8c,46e9eb39,# Preparing a model,3d82be43,0.5555555555555556
25732,cf39cde80e66b7,e72a869e,"
![](https://miro.medium.com/max/650/0*at-j68ROeSmiruDE.png)
source: https://www.includehelp.com/ml-ai/root-mean-square%20error-rmse.aspx
> RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation.",aed4bc9b,0.5555555555555556
25741,c1984e64b35234,14854931,"The subject is heterozygous in rs4778241 and rs7495174, which if homozygous C and A respectively, form a hapotype for blue-eyes association. Thus the subject likely does not have blue- eyes.",1811225b,0.5555555555555556
25747,593d1d3d1df05a,cf752e05,# Training and Validation Data Processing,bc682ffe,0.5555555555555556
25748,db5a369894fef6,a5ee7994,"Alternatively use `covid_data` and sum up the counts for each country but I don't like the messy country labels like ""('St. Martin',)"" or occupied Palestinian territory. So I choose to use the combined time series data. ",065aaf61,0.5555555555555556
25752,c8bf959b9608cf,d3529677,"### Define Style Loss
The style loss is sum of L2 distances between the Gram matrices of the representations of the generated image and the style image, extracted from different layers of CNN(here, VGGNet). The general idea is to capture style (color/texture/shapes/edges etc) at different layers.

Let $A^l$ and $G^l$ style representations of generated(artistic) and style image in layer $l$. The contribution of that layer to the total loss is:
![sLoss.PNG](attachment:sLoss.PNG)

and the total style loss is:
![lLoss.PNG](attachment:lLoss.PNG)",155e3672,0.5555555555555556
25754,36efe086c3f23c,864ae9b2,"''' Now We'll tell you why?
 Why can't these plots capture the essence of milk prices even when they look to have some significant pattern on yearly basis?
 The significant pattern seems to be of monotonically increasing.
 But is that so?
 Let us answer these questions one by one.
 '''",d0676cd0,0.5555555555555556
25755,30fdc4a6e3c1db,d2bd1f0d,"What we see:
* CA sales has always been the highest. The peaks in August are the most evident at CA in comparison to other states. Seasonality impacts CA sales the most
* WI have shown the highest increase in sales over the years.The sales were lower than TX before 2013. TX and WI sales were similar in 2013 to 2015 August. It has shown higher sales after 2015 August than TX. It will be interesting to look at what has caused WI to increase significantly over the years.",6111ddee,0.5555555555555556
25758,b3e0b7e9ff6849,65638073,"## 3. Deriving the RFM Metrics

<p><img style=""float: right;margin:-10px 20px 20px 5px; max-width:380px"" src=""https://www.retailreco.com/blog/wp-content/uploads/2018/11/RFM-Analytics.jpg""></p>

The predictive CLTV models are built around 4 key metrics. These are:

**<p>Recency:**  The age of the customer at the time of their last purchase.
**<p>Monetary:** The average total sales of the customer.
**<p>Frequency:** Number of purchases/transactions.
**<p>Age (T):** The age of the customer since the date of a customer's first purchase to the current date.",f6e4bb0d,0.5555555555555556
25759,f4b9042e693b6c,db776f8a,"I now create my folds. Of course, you can replace with your own CV setup here.",676cacc9,0.5555555555555556
25760,917957c6c4065f,1ca837c0,"간략하게 살펴보면 
likes는 평균 7,552, 범위는 0 ~ 4,470,923  
상위 3개의 동영상을 살펴보니 모두 뮤직비디오이고, ibighit 채널에서 게시한 동영상이네요.  
29969 항목은 조회수 1위에 해당하는 동영상입니다. ",55b8ed68,0.5555555555555556
25762,69ac33d79f5130,b8c486f8,### Start Time,9d760d2a,0.5555555555555556
25769,2e0fd6e937bf79,93182157,# build model,6acb965d,0.5555555555555556
25773,ac04ba639d1c93,6d27d7d7,# Create Dist Features,748059d5,0.5555555555555556
25778,0dfa3e758551c8,2f0d4bb4,survived men where mostly from pclass 1 and 3. dead men where mostly from class 3.,7adbb44c,0.5555555555555556
25782,1fcf9c261518d6,ad124b00,# IV. Feature Scaling and Model Fitting,be646fb0,0.5555555555555556
25783,d58491f2896fc1,328e35b2,**selection fonksiyonu en iyi ebeveynlerin seçilmesini sağlamaktadır**,514bfdff,0.5555555555555556
25786,08b5b53e94599f,d1f5d313,Let's look at the data types.,31609f4d,0.5555555555555556
25789,2259c048379b36,7fe35571,### Measuring Variability by using percentage change and standard deviation ,43558d11,0.5555555555555556
25790,f18e737fcc4b06,8cb708a2,"# Basic Data Analysis
* Pclass - Survived
* Sex - Survived
* SibSp - Survived
* Parch - Survived",087b8637,0.5555555555555556
25791,892be0a523578c,3922d134,"<div>
    <h1 id=""Analysis"" style=""color:#4e79a7"">
        Analysis
    </h1>
 </div>",b0e8d7c0,0.5555555555555556
25795,4fd4b6a80d40e3,96018f34,"## 3-Layer Neural Network

![image.png](attachment:image.png)",f6913cc3,0.5555555555555556
25796,80664f474fe2ef,9858c14c,**Hyperparameters**,bd82c7eb,0.5555555555555556
25801,2500c5fe8497ee,53bf2376,# Q1) Total meat consumption WorldWide,855355f0,0.5555555555555556
25802,df2a7968c08ee4,934eca4f,"### Defining CNN Class

For image classification CNN Models are very effective.

We use the following layers in the CNN Model. 

- Convolutional Layers - Building blocks of CNNs and what do the ""heavy computation""
- Pooling Layers - Steps along image - reduces parameters and decreases likelihood of overfitting
- Batch Normalization Layer - Scales down outliers, and forces NN to not relying too much on a Particular Weight
- Dropout Layer - Regularization Technique that randomly drops a percentage of neurons to avoid overfitting (usually 20% - 50%)
- Flatten Layer - Flattens the input as a 1D vector
- Output Layer - Units equals number of classes (predicted probability of each class)
- Linear Layer - Fully connected layer which performs a linear operation on the layer's input


Note: I attempted to build the same model as I did in Tensorflow here --> [MNIST Tensorflow Notebook](https://www.kaggle.com/brendanartley/mnist-keras-cnn-99-6)",a2ba0a72,0.5555555555555556
25803,c6f8ff61a5fa87,28cb238d,## 5.CatBoost Regression,3eea586b,0.5555555555555556
25807,f998cece696659,4fc33198,"<a id=""3""></a><br>
# Multiple Linear Regression",7964297e,0.5555555555555556
25814,9ad9a97e628bfa,31e97270,"이외에도 SibSp, Parch 는 이니셜과 어느정도의 관계를 가지는 것으로 보임",0a7e1136,0.5555555555555556
25815,9085cba2265204,48dc312b,### Converting categorical data into numerical data  ,de766eb3,0.5555555555555556
25819,56cc8fb47bef6a,d64a6914,"<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">The target column &quot;label&quot; contains text data which is &quot;ham&quot; or &quot;spam&quot;,&nbsp;</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">we need to convert this to numerical value for processing.&nbsp;</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">get_dummies will do this for us, creates two columns for each category.</span></span></p>

<p><span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">We can choose any one of them using iloc</span></span></p>
",652d6670,0.5555555555555556
25830,d128317750d689,5eeaf6a9,"After the network architecture is designed and defined, we need to create an instance of it. We can ""print"" the network to take a look at the structure one more time and to make sure everything is fine. ",d87f7428,0.5555555555555556
25832,7baeb0ffc6659e,7361896e,**Fare**,8cbebba9,0.5555555555555556
25834,5d6d539f8e7121,e07a5956,"## Hint
- 그룹 합계 ",79340a85,0.5555555555555556
25835,b6c0ad74f95b8c,2736861d,Create my distnace function (using squared distance and absolute value functions to ensure we don't get negative values). Also notice that I've treated the distnce for each player like the components of a vector (and so we take the sqrt of the squares to determine the distnace from the origin). ,5de5b241,0.5555555555555556
25836,95d896e75f9a50,82904a39,"**My take:** This looks very interesting - following patterns occur:

* Pattern 1: Features *17 through 40* are extremely good at separating `feature_0`. Looking at the tags file, this does not seem to match clearly any of the given tags :/ 

* Pattern 2: Features 65-68 seem to have some relation to `feature_0`. None of these feature have any tag.

* Pattern 3: Features 72-106 seemt to have a repeating pattern of relation to `feature_0` which matches the the frequency (6) of a similar pattern seen in 6 tags for the same feature (tags 0-5), however in the tags file the pattern extends all the way to feature 118, so it's not a complete match. 

* Pattern 4: Even though the figure of model scores based on individual features indicate that some features by themselves are not related to `feature_0`, when all these ""poor"" features are combined, they are able to classify `feature_0` very well, indicating that the value of `feature_0` truly influences the distribution of a lot of the other features.

## Experiment 4: Inspecting feature patterns
With the different patterns identified above, let's look at the actual values of the features as split by `feature_0`, to see if we can actually observe the differences in distribution.

### Pattern 1 - Good Predictors",2721b6f5,0.5555555555555556
25840,0c452d3a0b9339,29d8bbfa,"**Nucleoitides**
1. Count(A) > Count(G) > Count(U) > Count(C)
2. Count(U) ~ Count(C)

**Structures**
1. BasePairs and NonBasePairs had the Similar Counts.
2. Count(A-U) < Count(G-C)



**LoopTypes: Highly Skewed**
1. High Frequency --> Stem, HairPin, Dangling Ends,
2. Low  Frequency --> Bulge , Multiloop",5d857385,0.5555555555555556
25841,20e523830aab51,a76110b0,# Splitting/Scaling,810b8785,0.5555555555555556
25842,b9328fe3b0cefc,22cf93ad,### The score gap in every season(各个赛季随时间推进胜负分差的趋势),3a35eb23,0.5555555555555556
25844,245c89d02f3f5f,748fbccb,## NoFrameskip-v4,61a1eacd,0.5555555555555556
25846,efbcfe95cd7fde,026ef427,"Esta variable no aporta a la predicción del target, es indiferente en ser fbs 0 o fbs 1¶",54be281a,0.5555555555555556
25853,9ceb7278784462,41b986de,## Model Tuning,3768a567,0.5564516129032258
25854,71c3c1eab0377d,a27a6932,### Missing Value Treatment of the Column Embark in the Training Set.,52b4e360,0.5565217391304348
25864,d1ff7e10ee0102,1fd06578,### Bivariate analysis,2cc71c3c,0.5568181818181818
25866,5d2a3e82679cf3,9e136275,Standard Scaler(df3),9e60b1e3,0.5569620253164557
25873,2ada0305b68956,253524a6,### 94. Palette = 'copper_r',133e26f4,0.5571428571428572
25875,2730840089c8eb,19910e95,"If we want to throw in any non-string objects, we have to be careful to call `str()` on them first",34d27dac,0.5571428571428572
25877,675b60eaf415a6,73d97bc9,"* We currently have a subset of dataset with 51 classes 
* Use the below code to finetune InceptionResNetV2 pretrained model",68c0b725,0.5571428571428572
25878,ee23a565163388,045c5607,"**Inference**
- 31% of the smokers have faced heart failure.",88aacbc4,0.5572519083969466
25880,979f1e99f1b309,db48c951,## Scaling the data using StandardScaler,d1bfebbf,0.5573770491803278
25881,0858e1bb3cbaca,8ba742ae,"For clarity, we can also write a long code like this",78548374,0.5573770491803278
25884,918040fad252ec,d3fd9c28,Menampilkan squential tabel,966fcd8f,0.5573770491803278
25886,a5a419dc7245b0,163ccf6e,#### Feature Selection using random Forest Classifier,4279726e,0.5575221238938053
25890,6f1481148352e9,b6ffaf91,"**Now let's check the number of fires in each month. But first, let's study the distribution of the number of fires.**",7cfbdb8f,0.5576923076923077
25898,169177b6e9edea,33c665b0,<h3><b>Mutual Information,ca42152f,0.5578947368421052
25899,840534f2908a9c,bd1d5373,Here is the means for computing earth surface distance in km base on longitude and latitude of pickup and dropoff points.,8081c3cc,0.5578947368421052
25907,8539260444e6b5,f86e5740,# Bag-of-words,0369463f,0.5581395348837209
25913,72d393488311b6,ba215fd4,# Test&Train Split,80663df0,0.5581395348837209
25916,62487bcd70b199,231e332a,## <a id='6.2.'>6.2. LogisticRegression Model</a>,f6ae50af,0.5583333333333333
25918,c13f73168789c2,3901bd0c,"# **Slicing**<a id='20'></a>
Slicing in Python is a feature that enables accessing parts of sequences like strings, tuples, and lists. You can also use them to modify or delete the items of mutable sequences such as lists. Slices can also be applied on third-party objects like NumPy arrays, as well as Pandas series and data frames.

Slicing enables writing clean, concise, and readable code.",16175052,0.5584415584415584
25919,75adb7945ef9bd,d8eb0354,"Findings:
- Most top bigrams in disaster tweets show certain kinds of catestrophe (e.g. suicide bomber, oil spill, california wildfire); for non-disaster tweets, only 'burning buildings' as top bigram look like a disaster;
- Top bigrams in disaster tweets have a more casual tone;
- 'youtube' appears in three of the twenty bigrams for non-disaster tweets; none in disaster tweets",785c5095,0.5584415584415584
25927,52cfd66e9ec908,c50886b5,### yaw,c74adcdf,0.5588235294117647
25929,9cec5ddf8b6f49,90c06957,## 4.a) Split-out validation dataset,d39fc8e7,0.5588235294117647
25931,e170d33ee1da8c,742ee545,I'm going to log runs to Weights&Biases. This may be of greate use for further analysis of the results. As you shell see with fastai it doesn't require much extra work at all.,253cee3c,0.5588235294117647
25935,842547b2def18c,827557c4,We can create another feature called IsAlone.,b8efde6d,0.5588235294117647
25936,395ed8e0b4fd17,01b18fca,### Litecoin(LTC) Candelstick Chart for last 2500 rows,7573ea31,0.5588235294117647
25941,8f50c9c16db95f,c021434c,"We also confirmed that the foot speed and direction is nearly uncorrelated (~0.16 is negligible). Thus it is expected that when we add kicking direction to the model, the foot speed coefficient is unaffected.",26cc763a,0.5588235294117647
25943,1d1598b6fa2aa7,70bc071d,"### Displot

And displot! More information - https://plotly.com/python/distplot/",e066accf,0.5588235294117647
25945,156bbcff05dcea,724d29b5,# Area Under Curve AUC,66ad1fe9,0.5588235294117647
25946,71b75664517244,eece4d49,"## Best Managers

Let's see who is the best manager ever",fc905af5,0.5588235294117647
25948,02b7e38902069e,afa04b0f,#perform transliteration using the Indic NLP Library: Transliterating from Hindi to Tamil,726a03a0,0.5588235294117647
25952,0caaec057f7184,37c79698,"For the kind of data, there are around 87K shop+item combinations, with around 4.7K items in the test data. For each item, the number of other shops having sales records in training data can range to around 60 shops, while the number of shops not having sales records in testing data can also range to around 45 shops. 

Most of the inputs in this kind of data (old items new launched) have the phenomenon with testing data less than training data, with some having very extreme ratio (test num / train num more than 40). Therefore, we have to predict the sales of the items with most of the conditions that the items have lesser data of other shops in traning data compared to testing data.

We'll then take a detail looks to some of the examples in the case. From the perspective of items, we'll take 3 items and do further analysis from the view of item and category.
- item_id: 11286  shop_numbers_intest: 41  shop_numbers_intrain: 1  
- item_id: 13818  shop_numbers_intest: 18  shop_numbers_intrain: 25
- item_id: 4240  shop_numbers_intest: 2  shop_numbers_intrain: 57
",b875533e,0.5591397849462365
25957,a81661cc35d8d2,32c4f821,"The following studies were helpful in getting intuition as to how we can better use some of the features in our dataset

1. Serum creatinine - According to a study (https://www.ahajournals.org/doi/full/10.1161/01.str.28.3.557) published in 1997 done on middle-aged men, elevated serum creatinine level is linked to a higher risk of CVD. Higher risk of CVD was observed where serum creatinine levels are higher than 130 µmol/L, or 1.47 mg/dL*

    <font size=""1"">*Conversion units taken as 1 mg/dL = 0.0113 µmol/L - http://www.endmemo.com/medical/unitconvert/Creatinine.php</font>


2. Serum Sodium - A 2018 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6224129/) study links heart failure to Hyponatremia (serum sodium concentration of less than 135 mEq/L)",3331f113,0.559322033898305
25961,f2e5e9fb9eaaf7,9a6e0c7d,"<a id=""5.2""></a>
## 5.2 Target & missing value
As missing value constitute of `62%` of the observations, it's a good idea to explore if higher numbers of missing values in a observation relates to higher probability of claim. An assumption that customers that disclose every information required may be less likely to cheat and they may be more honest.

**Observations:**
- An observation that has no missing value has the lowest probability to claim with only `14%`.
- Observation that has a missing value (`1`) increased the probability to claim to `58%`.
- A missing value between `2 to 13` has probability to claim above `70%`, while it drop for a missing value of `14` to `50%`.
- This may be used for `target encoding` in `feature engineering`",048e0d08,0.559322033898305
25964,c4bca5d86a38c3,2928ef37,Juntando columnas SibSp y Parch en una sola columna FamilyMembers,e23d297c,0.559322033898305
25965,a44368590e878a,f5bb0262,### Confirmed count,77743ba8,0.559322033898305
25966,dac3c8204a2d1b,f0bb5bcf,# Top 10 Books with highest reviews,b0d2d0dc,0.559322033898305
25976,639e8aae4e046e,7607f92e,**Classification: LGBM**,77deb4cb,0.56
25977,5cb7f999fd1ecb,2b2d5163,# Seaborn - Bar Plot,88b54f70,0.56
25979,bbad077c274022,28c4fa2f,*I find it more sinusoidal. :p*,3c2e3dea,0.56
25983,cb570c7b7f0501,a645a4a3,oops. sounds like the more you give attention the more you get ignored :D ,a200a0ec,0.56
25984,9395559895004f,d00318ac,## Compiling and Training...,b5a0494b,0.56
25985,8854f72e7e9be0,48cd9bb7,**Simple Model with 2 inputs and 1 output**,2a1031b7,0.56
25987,caaa6793391520,d37f6f8c,Now we test our utilities on the scikit-learn datasets. Let's try our approach on the famous titanic dataset. Let's see if any of the columns contain nans,1e79f342,0.56
25989,10c5a39a87c47e,789a5bfc,## Step 11: Model Fitting<a id='step-11'></a>,09c7337a,0.56
25991,83df814455f06c,c70ded37,We now have training and test set ready for model building. ,c9cff71a,0.56
25993,81712ee7510ac5,8c51b5a4,**Tuples**,c4685e79,0.56
25994,50d4ddf1953997,98769037,"What about directors and actors? There are no directors listed for TV-shows, so these will only be analysed for movies. The .explode() method will be used to separate the directors in separate entries where multiple ones are listed. We are now considering all entries in the database for the next two analyses.",90bdddd6,0.56
25995,ce7abd85d777b5,96ef0466,"Considering the boxplot of Occupancy and Light, no light seems to mean no person, and any light seems to mean that there are someone in the room. 
Light and Occupancy correlation have strong positive correlation 0.91.

It means that any light shows that in many cases, the room is occupied. But it may cause target leakage when light is on, off and tuned by people. And for security use, in many case, thief does not turn on the light.
So I think that it will be better for me to try modeling without 'Light'.",0a340dbb,0.56
25999,274b32da3b19a8,3bd99df5,## Prepare dataset,408f7268,0.56
26011,957e035ba5b9d5,2deec6c8,## Visualize training history,778ab3d3,0.5602836879432624
26014,5f4ae633cfd090,7f792447,"As the chances of Red winning increases (i.e. R_odds decreases), the profit i.e. B_ev increases.
This indicates that B_ev is the profit on Red rather than blue and the other way round would be the same for R_ev",a30a16e2,0.5604395604395604
26019,63b44c85e32c1f,980181c5,To sort based on length key=len should be specified as shown.,fb9b9562,0.5608108108108109
26024,74a03887600114,db277b0e,Let's find the number of rating a particular movie has received,c0ffb2f0,0.5609756097560976
26027,9c26c5dcd46a25,c14e7a91,Nous allons calculer plusieurs métriques pour cette baseline :,1bbbb677,0.5609756097560976
26028,514d8de15cb7ef,a2246ab2,### A. Naive Bayes Algorithm,cfe111b2,0.5609756097560976
26031,8cefb86a675e5d,fd2c1a3a,# **~ Linear Regression: Fit a model to the training set**,79f9e69b,0.5609756097560976
26032,0b01138ad120fc,dd875b02,"#### Defining the Model.  
**Our model is a Recurrent Neural Network with 32 recurrent units and 1 hidden layer with 8 neurons  
Relu is used as the activation function.  
Mean Squared Error is used as Loss Function with Adam optimizer.**",0b4b72e6,0.5609756097560976
26037,fc8e0042411c46,0cb22cc0,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.5611285266457681
26040,fe7360cddc13e5,4566386f,## Veriyi Yükleme ve Yapısını İnceleme,8979e423,0.5614035087719298
26047,e03eb63c1f725d,4cca59a6,"<a id=""6""></a>
<font size=""+2"" color=""blue""><b>Passive Aggressive Classifier for CountVectorizer</b> </font><br>",e204b7e3,0.5614035087719298
26049,5ce12be6e7b90e,4568723c,"Python uses **zero-count** indexing: the first element has index 0.

In addition, there is also support for **reverse indexing** using negative numbers:",c0ab62dd,0.5614035087719298
26050,9e27af2600925c,08cbf547,"**Expected Output**: 

<table style=""width:40%"">
    <tr>
       <td> **w** </td>
       <td>[[ 0.19033591]
 [ 0.12259159]] </td>
    </tr>
    
    <tr>
       <td> **b** </td>
       <td> 1.92535983008 </td>
    </tr>
    <tr>
       <td> **dw** </td>
       <td> [[ 0.67752042]
 [ 1.41625495]] </td>
    </tr>
    <tr>
       <td> **db** </td>
       <td> 0.219194504541 </td>
    </tr>

</table>",9b556435,0.5614035087719298
26054,09751c520b0616,d5304483,### Encoding categorical feature,a4d0c7e9,0.5615384615384615
26057,d07915a6e6992e,67b48bde,**2nd approach to treat the Age feature**,2b912140,0.5615384615384615
26058,b01ee6cb674fa3,5f6a2c4c,"# Exos Aerospace Systems & Technologies 

A private USA company, founded by 2014


",a8ffd35e,0.5615942028985508
26059,3d08ca7656dec0,0ce81447,"***From numerical data he identify that is outliers is there,removing of outliers is best for ML model accuracy***",bd3f87e3,0.5616438356164384
26060,91473a39b85068,c64ea9a8,"Key Points:

i. Majority of the most frequent tags are programming language.

ii. C# is the top most frequent programming language.

iii. Android, IOS, Linux and windows are among the top most frequent operating systems.",6e3d91c2,0.5616438356164384
26061,738bfced935b69,9f68d2f4,By tax between 146 to 580 we notice all engine sizes are found.,2d3c592d,0.5616438356164384
26065,04ff2af52f147b,23a82509,We can see that almost all the titles have too small of a sample size for the model to extract information from.  We will use the strategy that Peter Begle used in an article on [Medium](https://medium.com/i-like-big-data-and-i-cannot-lie/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9) to normalize these titles and give our model something to work with.,d5f37be9,0.5617977528089888
26066,312135b445bd23,9a2d9b9e,"The lack of distinctive topics is likely due to the corpus range of content which contained a lot of noise. If we run the same LDA on the output from the semantic search for relevant sentences, hopefully clearer topics will emerge. This is an interesting research direction for the future.",8ced381f,0.5617977528089888
26067,e67925694c07d3,2b4a6ebe,## Categorical features,83af4c4a,0.5617977528089888
26069,7454fdc444df16,a9dc080a,"Now that we know the initial, split of the classes, let's split our data into training and testing sets. We will split our data as follows:

* ***Training Data*** : Data that we will use to train our model
* ***Testing Data*** : Data that we will use to test our model
* ***Out of Sample Data*** : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.

We will be using sklearn's train_test_split to split our data. 

Shuffle is set to False, this is because we already manually shuffled our data in the previous cell.",a7818ef5,0.5619047619047619
26071,04bac111ffbe9c,d7b122b0,##### Filling in the missing values in Age,82576b17,0.5619047619047619
26077,c80939c7c626cf,1fa92bc0,"# 6 Embarked / Boarding Place
Find the missing values
",b9ac31e2,0.5620437956204379
26078,917957c6c4065f,0a72ba42,### 2.3. dislikes,55b8ed68,0.5620915032679739
26082,eb0854a6601407,76a4b827,"Also the average of mean standard deviation (std) by asset presents some interesting patterns. First of all, it is skewed toward the right, with some assets having more std (up to 2.5). On the other side there are also some few assets with std almost at zero.",6d107747,0.5625
26084,8c7e00ca3dc5a7,7dd879c3,"In the above plot we can see the 2 dots in the bottom-right. They are above 4000 sq ft. but has been sold very less amount. Whereas other sellings like top right corner are sold at much more higher prices. So those 2 are outliers and should be deleted. Otherwise the models will try to capture those points and resulting overfit. The data definition also tells there're more outliers but we can keep it. By looking at the plot, it seems like all other points are following trend.",c83346e4,0.5625
26085,3dd4294f903768,b3abcde5,We will first add new feature from our target to understand it better.,0d89d098,0.5625
26087,e82462cdc998a7,9f8a01f9,"## 5. Modelling<a class=""anchor"" id=""5""></a>

[Back to Table of Contents](#0.1)",b39bf244,0.5625
26089,f5ca8fb6a465f3,4a844904,# Plot,56c45a1b,0.5625
26092,0635781991a885,ec0a0dfc,[Thanks to Raj Gandhi for K_FOLD_CV Function](https://www.kaggle.com/rajgandhi/tps-march-lgbm-7-fold-cv),13ab3e33,0.5625
26097,c85c94076e9c3a,5988e85e,## Education,3ea0c443,0.5625
26107,5ffe6aa38958a1,7ed763f6,We will take the first letter from Cabin number and convert that into a feature. It turns out there are 8 different classess for cabins. ,11f5412e,0.5625
26118,49f2274c1dd516,6b267532,"# COVID Tracker Canada
An independent project that compiles daily reports of covid-19 cases as reported by Canadian news outlets.

Copyright © COVID19Tracker.ca 2020 // COVID19Tracker.ca reports both presumptive and confirmed cases in near real-time // contact@covid19tracker.ca",06b0ffee,0.5625
26119,3b5903412fe741,62ec2117,"When choosing or transitioning between `loc` and `iloc`, there is one ""gotcha"" worth keeping in mind, which is that the two methods use slightly different indexing schemes.

`iloc` uses the Python stdlib indexing scheme, where the first element of the range is included and the last one excluded. So 0:10 will select entries 0,...,9. `loc`, meanwhile, indexes inclusively. So 0:10 will select entries 0,...,10.

Why the change? Remember that loc can index any stdlib type: strings, for example. If we have a DataFrame with index values `Apples, ..., Potatoes, ...`, and we want to select ""all the alphabetical fruit choices between Apples and Potatoes"", then it's a heck of a lot more convenient to index `df.loc['Apples':'Potatoes']` than it is to index something like `df.loc['Apples', 'Potatoet]` (`t` coming after `s` in the alphabet).

This is particularly confusing when the `DataFrame` index is a simple numerical list, e.g. `0,...,1000`. In this case `df.iloc[0:1000]` will return 1000 entries, while `df.loc[0:1000]` return 1001 of them! To get 1000 elements using `loc`, you will need to go one lower and ask for `df.iloc[0:999]`. Earlier versions of this tutorial did not point this out explicitly, leading to a lot of user confusion on some of the related answers, so we've included this note here explaining this issue.

Otherwise, the semantics of using `loc` are the same as those for `iloc`.",ad231969,0.5625
26126,13c7672da1b571,f10bf867,"Next, four loops are run to find the optimal values of the hyperparameters of the preprocessing pipeline. 
Note that XGBRegressor is run with early stopping rounds to speed up the evaluation.",002d3ec0,0.5625
26130,0932046e1f485d,251b25a8,Most frequent categories.,218cc7a3,0.5625
26131,3cc097a5859dc1,34bf5231,# **Data Transformations**,14380d73,0.5625
26133,386c42a7fb27a4,e74ca043,### Numeric Columns,9e9f6974,0.5625
26137,2ada0305b68956,25b9481e,### 95. Palette = 'cubehelix',133e26f4,0.5628571428571428
26158,726833f92fb87a,8fdd56cb,"This plot looks messy, we will create a categorical column for 'age'.",7dc5e1b6,0.5637583892617449
26161,4c47839b067546,972b6a66,### modelDate ,1f517b02,0.5638297872340425
26162,f6648e47713411,cadc6bba,"## 2.2 Tạo đường dẫn cho ảnh và phân chia tập huấn luyện, tập thẩm định",f4af4d1c,0.5638297872340425
26174,9b42412e75d640,a8778a67,I will first try Naive Bayes,b616570a,0.5641025641025641
26175,897ca904b74a98,20d1a805,## Imputation,c5844ad4,0.5641025641025641
26176,0a1fcda859252c,2d132f6a,"We will initialize the weights of first two convolutions with imagenet weights,",13a38774,0.5641025641025641
26180,80ad12f326ab70,0d9eae45,Percentage of students in the districts eligible for free or reduced-price lunch has the most Schools provide about 20% - 40% of aid for their students.,da404a16,0.5641025641025641
26181,49ee86d074de69,62db0eb9,"<a id = ""13""></a><br>
## Standardize The Data
* Omit Dummy Features",71ccc6d3,0.5641025641025641
26182,4d91e84c564cbe,c1d23e75,"<!-- TODO:
dir?
A useful builtin method for interacting with objects is `dir`. `dir` asks: what are the names of all the things (methods, and attributes) that this object is carrying around?
help(x)?
-->

The examples above were utterly obscure. None of the types of objects we've looked at so far (numbers, functions, booleans) have attributes or methods you're likely ever to use.

But it turns out that lists have several methods which you'll use all the time.",355a43e3,0.5641025641025641
26183,fc8e0042411c46,a9eaa48e,## X Education Forums,af476c2a,0.5642633228840125
26184,57070ad5e0f94f,77bc03b9,"# **Dropping(Returning then removing) ""Survived"" Column because It's Our Label That We Will Fit Into ML Algorithms**",d97edc41,0.5645161290322581
26186,ad26c020235dfc,0e324f4e,Grid Search with Linear Regression:,bf766e48,0.5645161290322581
26192,ab6da5994949a3,1f510d5b,## SVC Test Results,fae6b91d,0.5648148148148148
26196,ed5c03987493eb,6aced4e6,"Now, we use priors to generate never-seen-before digits:",bea97744,0.5652173913043478
26197,17a24d566ffa59,c74a3bba,"##### Coursera Course on SVD

SOURCE: https://www.coursera.org/learn/matrix-factorization/lecture/K5NBy/singular-value-decomposition",89049e56,0.5652173913043478
26198,598b6228760590,d87f1296,"- There are some columns with greater relevance, and we will not consider excluding them for the time being. The first correlation calculation uses spearman, which is aimed at categorical and continuous variables, and may not be interpretable.
- !!!!!!!!
- After subsequent model calculations, I found that the recall rate and accuracy are basically 74%-78%. Now I decided to delete the feature of the structure: Cabin_type, and finally see if the effect is improved.",be30ab66,0.5652173913043478
26199,4913b61a68d355,13158d54,## Fitting,6e269c6a,0.5652173913043478
26201,33d736abb432d0,2413e410,"## 可以搭建分词pipeline
### use_stopwords参数为是否使用停用词
### keep_eng_and_dig参数为是否保留英语数字（因为第2条文本中有LSTM那段英文存在）",d64052a2,0.5652173913043478
26203,a6b9837940ee38,476e8b50,"* In fact, the image pixels we are training should be scaled to 0-1, which is just divided by 255, but I'm not going to do this in data processing. Simply add a Rescaling layer at the bottom of the model, and you don't have to manually scale the data when making predictions.",52d2acc7,0.5652173913043478
26205,cfcb3bdee4f1e4,11e9be71,"The approach that we are taking is to feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.

So this is what we are going to do:

1.     Tokenization - break down the sentence into words.
1.     Indexing - add token into dictionary and index them For eg, {1:""Natural"",2:""language"",3:""understading"",4:""with"",5:""deep_learning""}
1.     Index Representation- represent the sequence of tokens/words in the comments in the form of index, and feed this chain of index into our Long short-term memory (LSTM) . For eg, [1,2,3,4,2,5]",eb4ed2ae,0.5652173913043478
26206,fe118026267a88,61033faa,"## 4.

Read the following csv dataset of wine reviews into a DataFrame called `reviews`:

![](https://i.imgur.com/74RCZtU.png)

The filepath to the csv file is `../input/wine-reviews/winemag-data_first150k.csv`. The first few lines look like:

```
,country,description,designation,points,price,province,region_1,region_2,variety,winery
0,US,""This tremendous 100% varietal wine[...]"",Martha's Vineyard,96,235.0,California,Napa Valley,Napa,Cabernet Sauvignon,Heitz
1,Spain,""Ripe aromas of fig, blackberry and[...]"",Carodorum Selección Especial Reserva,96,110.0,Northern Spain,Toro,,Tinta de Toro,Bodega Carmen Rodríguez
```",612efa48,0.5652173913043478
26210,8336d84cf3ff6b,b98345a6,# XGB Classifier Baseline ,b96b58a0,0.5652173913043478
26211,a1a31459abf078,8949cad9,"I also took a look at different tag values associated to different questions. In the above two visualizations -
1. I extracted the count of tags associated to a question and then plotted correct answer rate next to it to understand if more number of tags have an impact on successful answer rate on a question
2. We also extracted first value of a tag across each question and compared the total count of questions along with correct answer rate across each of the tags

Here are some findings - 
* A very high percentage of questions only had a single tag associated to them
* **Multi tag questions have a higher correct answer rate than single tag questions**
* For each of the count of tags, number of questions associated with a tag count seems to be inversely proportional to correct answer rate for a tag count
* **Most tags have a correct answer rate in the range of 0.4-0.8


As students when we practice over our learning material we tend to get better and better with time. We would check for the same in our dataset, we would look at whether a user's correct answer rate improve with two factors - **total number of questions answered and total timestamp**

### User Activity",66fc0f54,0.5652173913043478
26215,9d9da6c439b96b,118dd652,"in 2009, Annual published game reach the peak. in contras, Global sales got the top sales in 2008 and started to drop in 2009, however both graph has similarity of flow.",361cc7d9,0.5652173913043478
26216,2b434130adf886,a9e17dc3,# Train our model,0c4afeca,0.5652173913043478
26225,57bad3860b0fa4,77ad16fc,"I found out that the optimizers have their own parameters, so I made a code block to test with them to try and get the best from them.",05138a5e,0.5652173913043478
26227,69d50f5e1373f1,3737a7a3,"## Function to plot the first train/test input/output pairs of a task

You can use this function to plot the first `train` and `test` grids. The color aligns with what is found on the ARC app. Note though, the ARC app presents the grids to scale, where these display the grids in the same size, regardless of their dimension.",ec7545ee,0.5652173913043478
26228,73ca9abcc2034e,e8fe4c1e,# Let's remove the stop words,cec3446c,0.5652173913043478
26239,43e60eb1362f5c,3e942a77,American Airlines Inc has the highest Arrival Delay.,87934234,0.5660377358490566
26240,614ba9f0c62677,134e5933,"<a id=""11""></a>
### Create Model
* conv => max pool => dropout => conv => max pool => dropout => fully connected (2 layer)
* Dropout: Dropout is a technique where randomly selected neurons are ignored during training
* <a href=""https://ibb.co/jGcvVU""><img src=""https://preview.ibb.co/e7yPPp/dropout.jpg"" alt=""dropout"" border=""0""></a>",b8551335,0.5660377358490566
26241,23df07a474aaae,d683b72d,# Visualising Predicted and Actual Streams,0ea40276,0.5660377358490566
26244,a070fd03ae8ed2,eacbd226,"## 5.3 Метрики, матрицы ошибок и кривые",c0ec4138,0.5660377358490566
26246,f015d0147e8fbf,abd3e581,### Executing All Preprocessing and Feature Engineering:,518954fb,0.5660377358490566
26249,7f74a04ae75792,eeffce2f,"#### Handling Missing Value for `Gender` column
",d01e91da,0.5661764705882353
26254,c9b4e282e4e2c1,4bf56949,"Now I'm going to merge the two datasets InjuryRecord and PlayList to study the possible relation between parameters like type of injuries and roster position, betweem type of injuries and weather, etc.",f44d339f,0.5663716814159292
26258,712198370d5521,fb0025b5,"<a id=""5""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">DIMENSIONALITY REDUCTION</p>
In this problem, there are many factors on the basis of which the final classification will be done. These factors are basically attributes or features. The higher the number of features, the harder it is to work with it. Many of these features are correlated, and hence redundant. This is why I will be performing dimensionality reduction on the selected features before putting them through a classifier.  
*Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables.* 

**Principal component analysis (PCA)** is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss.

**Steps in this section:**
* Dimensionality reduction with PCA
* Plotting the reduced dataframe

**Dimensionality reduction with PCA**

For this project, I will be reducing the dimensions to 3.",5882e04c,0.5666666666666667
26265,37b09262279764,0e9b3374,#### RandomForestClassifier,37c4c417,0.5666666666666667
26266,63d0d9b9a8c7d2,370d9e2a,***Logistic Regression***,e32e5933,0.5666666666666667
26267,bc058fe14d3d1b,bb83a6cb,discount rate,d0273670,0.5666666666666667
26271,c91c137284976f,07dcd4f4,#### 1.4. Transforming data,c6888c0a,0.5666666666666667
26272,b547f0f38f7744,4906fd5b,Show images with `transforms`:,b6ba66b3,0.5666666666666667
26273,be616f0785c32d,1d434791,"[Go Top](#top)


###### C.1.3 Nested CV to prioritize drug candidates 
For each round, we randomly selected 80% of the example as training, and 20% as testing. The prediction scores for the test set are recorded in each round. We repeated this process for 20 times ensuring all examples occurred in the test set (100 experiments in total). Then the average of each example was taken as the final prediction score.",b78e18aa,0.5666666666666667
26275,061d6757dfbce0,ecf2891f,"## Examine Missing Values

Next we can look at the number and percentage of missing values in each column. ",c0c2915a,0.5666666666666667
26278,07f5853e4db8f8,df96791e,# visualizing district data,d13c2c32,0.5666666666666667
26280,e78f177ca86768,7ae85418,## Polynomial Features,120e25c1,0.5666666666666667
26282,892be0a523578c,f9d6d851,"First, let's answer the first question, **how many customer groups can we identify from the data?** This question may be helpful for precision marketing.",b0e8d7c0,0.5666666666666667
26283,8d575f495686ab,a9195c3c,## Prediction - Using LSTM model ,0fc16499,0.5666666666666667
26288,3597174a998d4d,9d731389,### 2.2.2 The behavior,276892ed,0.5666666666666667
26293,2a123b4e8f9433,6d01b48e,Distribution of validation scores,0a082218,0.5670103092783505
26299,e58e68e4eeefe5,30382f28,"If I consider the ones which are highly correlated to the output label,
**features = {ejection_fraction, serum_creatinine, time}**

* Logistic Regression --> 90%
* Random Forest --> 90%
* Gradient Boosting --> 95%",a87662ce,0.5671641791044776
26300,1a222fee3089d2,a4d191b0,# Feature correlation,59ab8894,0.5671641791044776
26301,30fdc4a6e3c1db,cc1e8b07,"What we see:
* CA and WI show a gradual increasing trend while TX grew up quite fast and then became a bit stagnant
* CA and TX show a similar seasonality peaking at July, August and dipping at Dec ,Jan
* WI shows a different seasonality peaking at March and dipping at April and then peaking again at August",6111ddee,0.5672514619883041
26312,27d5291d6365ba,d52b7f17,# Debit-Credit Transaction Sum analysis By Age,96b30229,0.5675675675675675
26325,62037c5832129c,df5214b0,"**We have used accuracy for most of the model evaluations so far.**
Other Metrics
* Precision
* Recall
* F1-score",61474350,0.5675675675675675
26326,ac9b48d531bad9,5c584ba6,**COMPARING CONFUSION MATRIX FOR 5 DIFFERENT CLASSIFICATION MODEL**,95965e35,0.5675675675675675
26329,1294fb4c86f993,44a240ae,#### Examining Louisiana,4471e513,0.5677966101694916
26330,4883314a96dc34,ee4ff8a0,"### Optimize Models by Fitting Parameters (1)

Train models on train set to find the best parameters with cross validation & get the first performance measures on the validation set

Find different model performance metrics part of *scikit learn* here: https://scikit-learn.org/stable/modules/model_evaluation.html",50d36836,0.5679012345679012
26331,faa8e6c8ab9246,4759b1e9,males are more compared to females,2bea1419,0.5679012345679012
26332,a35cdce61f4059,c466932c,* **Random Forest**,acc8eab6,0.5681818181818182
26337,d1ff7e10ee0102,4d904fac,"We already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'.",2cc71c3c,0.5681818181818182
26340,d83e5b44d1b80d,9a1b6407,"***Except Nigeria, most countries durtion on Survey response is between 0 to 2000 seconds), There are outliers based on the dataset***",62845930,0.5681818181818182
26341,90964081c7faab,682fe13c,"#### Classification models used:
- Random Forest Classifier
- Decision Tree Classifier
- Support Vector Classifier
- K-Nearest Neighbors
- Logistic Regression Model
- Gausian Naive Bayes
- XGBoost
- Gradient Boosting
",b423b0c3,0.5681818181818182
26343,f269d2fbd5f1be,22d41f82,# Visualization,1264c440,0.5681818181818182
26345,a0b321057e7402,c3209174,Now lets plot the seasonal decomposition and AC & PAC on the stationary data so we can see the results.,5f73fb91,0.5681818181818182
26346,73d8e56bc709b1,e7dc7e2b,"The distribution of German players looks balanced, for they got players with almost all ages.  
They got peak at 22,29, that means they always have best players these years.  There's only one thing they need to pay attention to, training young and talented players.",78ec3cce,0.5681818181818182
26349,a566b5b7c374e7,830466f0,### Sleep Score,b3dc5545,0.5683453237410072
26356,2ada0305b68956,b111db60,### 96. Palette = 'cubehelix_r',133e26f4,0.5685714285714286
26358,fa02c409161192,6ec24911,### 1.4.1 How the training data's size effects preformance.  ,e97077f7,0.5686274509803921
26365,523123dad03177,0045a1e2,# 7. Reading Scores,48a5e4e6,0.5686274509803921
26366,64169805aacf17,8d5081a3,## Tell the machine what to paint,1f12ded0,0.5686274509803921
26368,7cfd96218dd933,760a5927,#### JULY 28,7c34d96c,0.5686274509803921
26369,32ddc45133f77b,e7d83c26,**Retraining the model with full dataset**,3c0d6831,0.5686274509803921
26370,4fa553c2b837d4,3ca8aa27,"## Soln 2 : Imputation
Imputation fills in the missing value with some number i.e the mean value. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.",c65a23e9,0.5686274509803921
26371,fdbbd573ba31c2,1328a072,## Convert Datetime,f7c28d74,0.56875
26373,b01ee6cb674fa3,266cc2b1,"# International Launch Services - ILS

International US Russian private joint venture, founded in 1995 

Joint venture with exclusive rights to the worldwide sale of commercial Angara and Proton rocket launch services. 

Founded by Lockheed Martin (US), Khrunichev (Ru) and Energia (Ru)

",a8ffd35e,0.5688405797101449
26374,f3c6048d1058e3,821f24c0,"### **1) TF-IDF-** 
- In TF-IDF which based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. Common words like ‘is’, ‘the’, ‘a’ etc. tend to appear quite frequently in comparison to the words which are important to a document. Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.",1d9056b0,0.5689655172413793
26376,84127ade6fde87,d3998d64,"The choice between character-level and word-level encoding leaves us to make a trade-off. In many languages, there are significantly fewer characters than words: representing characters has us representing just a few classes, while representing words requires us to represent a very large number of classes and, in any practical application, deal with words that are not in the dictionary. On the other hand, words convey much more meaning than individual characters, so a representation of words is considerably more informative by itself. Given the stark contrast between these two options, it is perhaps unsurprising that intermediate ways have been sought, found, and applied with great success: for example, the byte pair encoding method starts with a dictionary of individual letters but then iteratively adds the most frequently observed pairs to the dictionary until it reaches a prescribed dictionary size. Our example sentence might then be split into tokens like this:",f55d05b6,0.5689655172413793
26377,1dd9c6aa74d289,eb40775e,### Simple Gaussian fit,5ef9a1be,0.5689655172413793
26385,1cd8be6e679620,ad4e5ce7,"### 🤷‍♂️ unlike training data 'E' is not highly correlated with '(' , ')'.",3ce15a43,0.5689655172413793
26386,1750367e54f407,166f9d8b,# Verification of the training process,a8e655b2,0.5689655172413793
26388,e19e307b3fd188,fa5bc5d5,"As this is a continuous value forecast, I will use regression models.",2173955b,0.5691056910569106
26395,09751c520b0616,c1c4a199,- To encod categorical feature using Label Encoding technique,a4d0c7e9,0.5692307692307692
26398,f2f2db16a2f86c,7be012c8,**One Hot Encoding**,ffc6a115,0.5692307692307692
26399,c115e287523aab,f86a81f1,"## Data Pipeline
* Reads the raw file and then decodes it to tf.Tensor
* Resizes the image in desired size
* Chages the datatype to **float32**
* Caches the Data for boosting up the speed.
* Uses Augmentations to reduce overfitting and make model more robust.
* Finally, splits the data into batches.
",feb1288b,0.5692307692307692
26401,a8c042af6b7245,40a16a0b,"## Exploratory Data Visualization

### Categorical variables

Let's look into the categorical variables and the proportion of customers with target = 1",2487ac62,0.5692307692307692
26403,3c2033cc99c12c,26de6179,"<b>Summary:<b>  
*As seen above, the PCA and SVD method's effeciency is much shorter than T-sne method, but the performance of T-SNE method is a bit better. In reality, the T-sne method is mainly used in the field of visualization, for T-SNE determines the local neighborhood size of each data point based on the local density of the data (by forcing each conditional probability distribution to have the same degree of confusion). The PCA and SVD method is of great similarities in algorithm, both utilizing the eign vectors related decomposition. In most of the time, the running time of SVD is less than PCA, but since there are only 800 rows of data after the under sampling, so the ruuning time are approximative*",dfa22a54,0.5693430656934306
26405,c01049afb6d307,b3d57fbe,## Education -- Socialdrinker & Bodymassindex,d37d3b5d,0.5694444444444444
26407,166a62ebb4fc3a,e96a51db,"Drop all columns related to the ""concavity"" and ""concave points"" attributes",db48a079,0.5694444444444444
26409,fdc3afd309b850,d9ff840a,"<a id=""pci""></a>
## 7.3 Per Capita Income (PCI)",966bde38,0.5694444444444444
26417,4cd25e50c7e007,fc58ad2c,### Dividing into X and Y sets for the model building,ceb0c525,0.57
26418,83df814455f06c,f3c759f0,"# **13. Decision Tree Classifier with criterion gini index** <a class=""anchor"" id=""13""></a>

[Table of Contents](#0.1)",c9cff71a,0.57
26419,c84925c8171900,d9b5a8a1,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.3.3 Top 5 Publisher - Yearwise
            </span>   
        </font>    
</h4>",e21ff7ec,0.5700934579439252
26424,2f47abddfd1928,a1575c58,"To get a better visualization I have remove the values of SibSp over 4, as they are very little.

Similarly as Parch, most of the passenger travels alone and they are middle age, mostly between 25-35.

The next biggest group is passenger traveling with 1 SibSp, and we can understand that in this case they are mainly couples covering a bigger range than the people traveling alone. Although they are also middle age, 20-40. In this case there are some children traveling with just one sibling.

The passengers traveling with more than 1 Sibling/Spouse are younger, basically the higher the Sib/Sp the younger the passenger. Basically this groups are children and some young people traveling in family.",ae33cc0b,0.5702479338842975
26430,f35bf4df70d310,ecb2667f,"Now let's experiment with using multiple dimension input and see which 
PCA settings is the most optimal for reducing fit times while retaining high accuracy",10bb859a,0.5714285714285714
26431,870a7144fa75ad,8232b8e0,**Apply train_test_split**,2a8c3427,0.5714285714285714
26432,1fac5edd4063ba,10e357b5,"#Ada Lovelace

Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She is believed by some to be the first to recognise that the machine had applications beyond pure calculation, and to have published the first algorithm intended to be carried out by such a machine. As a result, she is often regarded as the first to recognise the full potential of computers and as one of the first to be a computer programmer.https://en.wikipedia.org/wiki/Ada_Lovelace",04bc01e0,0.5714285714285714
26435,241cf32abb22d8,5c046c2d,"# Hyperparameter Tuning<a class=""anchor"" id=""3""></a>

In this section, I train and fine-tune the models based on the 276 rows of training data. I also compare the performance of models with the full features and with the top 10 features.

Each model is evaluated by 5-fold stratified cross-validation with 3 repetitions for hyperparameter tuning.",47157066,0.5714285714285714
26437,585c280865b46e,203f0931,"# Histone genes and cell cycle

###  Histones are subdivided into canonical replication-dependent histones that are expressed during the S-phase of the cell cycle and replication-independent histone variants, expressed during the whole cell cycle. ",4d6056f1,0.5714285714285714
26442,06ecf7a304c309,bc5085b7,"이제 오토인코더를 위한 모델을 만들어봅시다. 어떤 종류의 네트워크가 필요한지 알아봅시다.

**Encoding Architecture**

인코딩 구조는 3개의 Convolutional Layer과 3개의 Max Pooling 레이어를 하나하나 쌓아 구성됩니다.
Relu를 활성화함수로 사용하고, `same` 매개변수로 이미지 크기를 패딩을 통해 유지합니다.

Max pooling layer의 역할은 이미지 차원을 다운샘플링하기 위해 사용됩니다.
이 레이어는 초기 표현의 겹치지 않는 부분 영역에 최대 필터를 적용합니다.

**Decoding Architecture**

디코딩 구조에서도 거의 유사하게 3개의 Convolutional Layer를 사용합니다. 하지만 Max Pooling layer 3개 대신에 unsampling layer 3개를 사용합니다. 활성화함수와 패딩은 인코딩과 동일합니다.

Unsampling layer의 역할은 입력 벡터를 더 높은 차원으로 업샘플링하기 위해 사용합니다.
Max pooling 연산은 비가역이지만, 각 풀링 영역 내에 최대 값의 위치를 기록함으로써 근사 역을 구할 수있습니다. Umsampling 레이어는 이 속성을 사용하여 낮은 차원의 특징 공간에서 재구성합니다.
",714de627,0.5714285714285714
26443,be357c1e2c975d,4dd66159,### Task 4: Create CNN Model,2486a061,0.5714285714285714
26444,6f4795cfdc96c7,572abbe6,Let's take a quick look at what the data looks like:,1f3ab82f,0.5714285714285714
26446,e78e7edae89049,5cc06831,# ARMA Model,9cef1d94,0.5714285714285714
26449,87e94f864d74be,9924c600,"# <span style=""font-family:serif; font-size:28px;""> 4. Data Visualization </span>",294bfe9f,0.5714285714285714
26458,ada107575a1bb1,eddd991e,Merge,9816c26f,0.5714285714285714
26459,bbaa07ad21cf4e,bc9b316d,### Stacking all the 3 vectorized forms and making dataframes,3ab6b254,0.5714285714285714
26461,c818250dd720eb,079be33e,"# Data Labels

Each image in our training data comes with a corresponding 'mask' tiff file of same size providing us with pixelwise labelling in the red channel, other channels are set to zero.


The Radboud labels were semi-automatically generated by several deep learning algorithms, contain noise and can be considered weakly supervised, while the Karolinska labels were semi-automatically generated based on a pathologist's annotations. Each data provider labels the data slightly differently so with the help of a custom colour map we can display some images alongside their labels.",68ee40de,0.5714285714285714
26462,3cea0f929a2035,ac2bf739,In the figure below we see the recorded number of years taken from the table.,04cfbade,0.5714285714285714
26464,8985a124d4b657,2b70b485,Let's make our predictions using model.predict.,586d1846,0.5714285714285714
26465,15eb884262ba09,ac66790a,**The World as we know it (Predifined Centroids)**,d703bdab,0.5714285714285714
26468,2b36742b49c7bc,489bb192,"5-fold [Stratified Cross Validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) ашиглаж үр дүнгээ offline дүнэх болно.
",c8f8a96d,0.5714285714285714
26469,a758983a68c014,a1d7b3d3,Let's write some supportive functions.,ab89f181,0.5714285714285714
26471,3cd78d8d6d56e4,f53ac340,"### Randomized Search
Finding best hyperparameters with Randomized search",9f632e94,0.5714285714285714
26472,898d18d501f68d,12b205b0,"we have seen plotting are skewed, So normalization is done w.r.t cover type and thn plot it",d8bdea2d,0.5714285714285714
26479,76d94f2011a1cb,4f3689e9,"## Training Resnet
Densenet takes quite a lot of time considering this problem is trivial. Bit of an overkill to be honest.
Let's try Resnet.",9820aca8,0.5714285714285714
26481,324c699253abc2,d1c17f9d,Principal component analysis the goal is to find a lower dimensionality representation of the variables maintaining most of the variance with a linear combination of them to create principal components which are not correlated.,7e81e44e,0.5714285714285714
26482,fcb15f03bd0239,9cdcd740,![](https://www.omniglot.com/images/writing/esperanto.gif),3b9bee3a,0.5714285714285714
26486,4ae464582bac51,718b51dc,## Categorical Attributes,ca6a52ce,0.5714285714285714
26491,0c57e3132ae184,7873e1a3,"### IV. Extract Features from Amenities Columns. For each amenity provided, create a new binary variable representing if the amenity is present",f6bac298,0.5714285714285714
26492,99f84fa59cb1da,aef16489,"You wanna make sure this split makes sense. We can do that by checking the stratification
",41e95f63,0.5714285714285714
26494,55a5e31d03df9f,5c926bae,"As we can see we got around 8.2 Million parameters! We also in the summary can check the output shape of our feature extraction, with this information we concluded that it wasn't neccesary to add a Flatten layer. 

**Note:** In the summary we can also check the number of trainable parameters, we will just train ~36K this is the 1000 inputs from our feature extraction layer all connected to the output layer (36x1000) + 36 bias terms",06dce00f,0.5714285714285714
26496,1c381451c17150,59d2b7b3,"
# Load Model
Load models or weights here. For following up on partially trained models saved by checkpoint.",e79b530f,0.5714285714285714
26497,60d500d196eb42,be56bbf0,Let's take a quick look at what the data looks like:,2ad55f3f,0.5714285714285714
26498,adb8441ad28019,3ae1ba8e,"<div class=""alert alert-success"">
    <h1 align='center'>Random Forest Classifier</h1>
</div>",d89de993,0.5714285714285714
26506,6b65d81a5743dd,7a249388,"After some google searching we can intoduce 2 new features 
> Player Efficiency Rating : (FieldGoalsMade + Rebounds + Assists + Steals + Blocks + Turnovers)/ MinutesPlayed

> Participation : MinutesPlayed/GamedPlayed",4080a2d2,0.5714285714285714
26508,ffdb3fe29f4755,f82641a0,Now let see see factual numerical difference betwee first and last measurements of gold in every currency.,019b2e69,0.5714285714285714
26511,12f4d16fc21645,1b8d4bdd,<h3 style='color:Red'>Correlation between different features </h3>,c7752038,0.5714285714285714
26515,f3d5d8917ce5df,bd61bcf7,"# Transform! <img src = ""https://upload.wikimedia.org/wikipedia/commons/3/3d/Fesoj_-_Papilio_machaon_%28by%29.jpg"" align = ""right"" width = 200>
So now we'll do some transforms to get all the data in the DFs numeric and remove nulls... i.e. we're putting the data in the format that you need to have the data in for any ML/DL model.",e45112f8,0.5714285714285714
26516,e16860fce156b0,504a2923,"#<b><mark style=""background-color: #9B59B6""><font color=""white"">plot_correlation(): analyze correlations</font></mark></b>

They provide an API plot_correlation to analyze the correlation between columns. It plots the correlation matrix between columns. If a user is interested in the correlated columns for a specific column, e.g the most correlated columns to column ‘A’, the API can provide a more detailed analysis by passing column names as the parameter.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.5714285714285714
26520,c4386b8a01d66e,e87d8414,# ph_random,dc732bf5,0.5714285714285714
26523,0fa9979b5690e9,ecf12392,"Os dados estão bastante aglomerados em algumas regiões, dificultando o funcionamento de métodos de aprendizagem baseados em distância e funções lineares. Os dados naturalmente têm essa característica, mas é sempre importante verificar se essa aglomeração de informação não é devido a escala dos atributos. Isso, inclusive, é extremamente prejudicial para métodos baseados em distância.",c26eea94,0.5714285714285714
26526,72e098fe5b2a04,719116b8,# 4 layer deberta 460,5399eebd,0.5714285714285714
26530,ba4b3bd184acbb,e1ff2cf1,"Some of the columns only have a few null values so we will remove the rows containing those.

However, the Rating column has numerous null values so we will create a seperate DataFrame with these values removed.",0f5de724,0.5714285714285714
26532,b211c8c107f56d,0a701c46,"## Best model  <a name=""step3""></a>

Now sort the models of the grid from decreasing order on the *auc* criteria, and keep the first. You can check what are the parameters that were used to reach the best score. A detailed history on the different metrics is informative, especially for the *AUC* of the Precision Recall curve since target column is not balanced. ",805b90f3,0.5714285714285714
26533,2b97b399158701,246ff3e3,"Out of the four models used in ensembling, two of the models use EfficientNetB7 and two others use DenseNet201.",04bd0060,0.5714285714285714
26534,3879ef16f5eb28,756df02d,# Pie plot,784b8d8e,0.5714285714285714
26538,38b79494ac749e,e3c5798e,### Summary,39162a40,0.5714285714285714
26539,c13f73168789c2,595300bd,"## 1. Slicing rows and columns using labels<a id='21'></a>
You can select a range of rows or columns using labels or by position. To slice by labels you use **loc** attribute of the DataFrame.",16175052,0.5714285714285714
26542,b4ecd6e4277e3c,3e69b407,"### Load Embeddings

Two embedding matrices have been used. Glove, and paragram. The mean of the two is used as the final embedding matrix",94d79d5f,0.5714285714285714
26543,5d5c9480b5a0a3,b974286e,# Visualization for the purpose of understanding,04d82e2d,0.5714285714285714
26547,b74076b2f8ba1d,b34e1f1b,Let's take a quick look at what the data looks like:,9ace22d4,0.5714285714285714
26550,c6673ce23495fc,e237a6e3,## Visualization,43f1ea4a,0.5714285714285714
26558,4ae6a182abac64,bea1b78b,### 2.3 Creating New Features,418676c5,0.5714285714285714
26561,75adb7945ef9bd,47bfab50,"## 8. Encoding and Vectorizers

As part of feature generation, we will:
- Apply target encoding to keyword and location (cleaned)
- Count Vectorize cleaned text, links, hashtags and mentions columns",785c5095,0.5714285714285714
26565,4b4117cf42ef8d,b851b7d9,# Ridge Regression ,457cd6f4,0.5714285714285714
26566,659f5f3ef8aa0e,b4f3dda6,Let's take a quick look at what the data looks like:,3654c2d0,0.5714285714285714
26568,c54ea4523bd49c,0d8d1161,Starting with the simple stacked 3x3 conv net from Towards Data Science,097ccba2,0.5714285714285714
26571,84d1ef55b89e17,05ae52ba,**Main() Function** ,2d0b9d51,0.5714285714285714
26572,5f4ae633cfd090,e9ef21ac,The _bout features would have more or less similar relationship with one another,a30a16e2,0.5714285714285714
26574,b290039151fb39,8a0a5182,The code below computes the competition metric and recall macro metrics for individual components of the prediction. The code is partially borrowed from fast.ai.,1836a79c,0.5714285714285714
26575,171494b45650a2,cb0082d9,### Company vs Price,9c8cc578,0.5714285714285714
26576,9c044fa3072552,209ae481,### Days of the week Start Time,1362842e,0.5714285714285714
26578,f4514ec092a771,99fc17b1,## Running script on Kaldi,3739ab1e,0.5714285714285714
26580,6471597c5d2f66,3cfc210a,Let's take a quick look at what the data looks like:,a41b4abe,0.5714285714285714
26582,396bc36edb95d3,fdc235a9,### Conclusion for CART Model,965e4f8f,0.5722222222222222
26585,7e89d387feb9f5,53b46ccf,"### Добавленный числовой признак №12. Количество различных кухонь, представленных в ресторане",989e3a1b,0.572463768115942
26586,ee23a565163388,117d107d,# **Feature Engineering**,88aacbc4,0.5725190839694656
26596,312135b445bd23,a82b3e5e,"# **Search Engine for Seed Sentences**
Now, our goal was to find the best (accurate and informative) sentences for each sub-task for each task in the CORD-19 challenge.
For that goal, we did the following:
1. Created a search engine for finding relevant sentences using input keywords. The search engine transforms the input to phrases using our phrases model above and performs Query Expansion technique by adding synonyms (above certain similarity threshold) to the input keywords and ranks sentences by the number of keyword matches and the date of the article the sentence came from (the newest will be ranked higer). In order to focus on articles about COVID-19, the search engine can get optional keywords that boost the sentences containing them. We've tried to use TF-IDF and some other weighting techniques, but our simple method worked best for us.
2. For each sub-task we created list of keywords the retrieves for us the best result set of sentences that answer the resaerch question in the sub-task.
3. Picked 1-3 sentences (out of 10) that summarizes the answer in the most accurate, consice and informative manner. Those sentences are considered ""seed sentences"" and will serve us in the next module of sentence similarity. This part was done partially manually and we belive this approach can be automatic with better keyword extraction and a better search engine. This is an interesting area for future research.

All the examples are listed in the `notebooks` folder in our repo.",8ced381f,0.5730337078651685
26599,5ce12be6e7b90e,bb610c42,"Here, the last element is accessed using -1 index, and so on.",c0ab62dd,0.5730994152046783
26601,fd4017c1514157,ae368cd2,"So most of the recordings have rating greater than 3.0 whcih is good. However, there are around 3.3k recordings which have 0 rating.",fd8f0896,0.573170731707317
26603,7e1da639035ac5,b57eb0fe,# <a id='11'>11. ELA and math proficiency analysis</a>,120b6c23,0.5733333333333334
26604,cb570c7b7f0501,ecee1f40,"### Research Question 4  ( is it about the place where the patients come from !? )

",a200a0ec,0.5733333333333334
26608,91eaec994e0c6f,47116c22,"- FOODS is the category having the highest number of sales, HOBBIES having the lowest one.",376aef10,0.5733333333333334
26609,b10bd75889dad9,cc38ec8e,## Lets build a Predictive Model Using PCA and Random Forest Classifier,ee00ceee,0.5733333333333334
26610,37e461081e47c5,a68ecc4b,"# Model 1: Naive Model
Use the most recent value of sales to predict future sales",b3e6549e,0.5733333333333334
26613,99821bc6a45be6,fee55f0c,"# Deep Model

We saw that the linear model performs well, but we wanted to see if adding more hidden layers could improve the performance. The following model has the same base model, ResNet152, and 3 hidden layers instead of none,",b9d59346,0.5735294117647058
26617,e4c6dd957eb5ce,8d970246,"Cool! <br>
As I said I don't know exactly if it's really correct. but show us interesting and similar patterns. <br>
Only in Gran Master Tier we can see a clear difference in mean days.
<br>
I'm trying to do a correctly handling to avoid any type of leakage.<br>
We can see that when the time to a person do all things to become a Contributor is in median <br><br>

How can I be sure that it is correctly?! The Grandmaster of scripts seem to be wrong but I think it is because the outlier guys like Chris Deotte, MH Bahamani and nananshi that have rushed very fast. <br>I will investigate it further.  ",2e383665,0.5735294117647058
26621,c65a65d4041018,c9ec0e2b,"* Most popular IDE is Jupyter Notebooks. Well, obviously it is one of the best tools for fast EDA and modelling in Python;
* RStudio is quite popular in USA - so lots of R-DS there? And it seems that in Russia the interest in R is quite low;
* In Russia most people prefer to use Pycharm and other countries tend to use Notepad++. Different styles of coding?
* Interesting that Spyder is quite popular in India;
* Among other IDE Eclipse and Emacs are most widely used. A lot of people from Java-development?",824fb229,0.5735294117647058
26622,eb0ecd6bebeb15,638baaf5,"sepal.length ve sepal.width değişkenlerini sns.jointplot ile görselleştirelim, dağılımı ve dağılımın frekansı yüksek olduğu bölgelerini inceleyelim.",d7b93a60,0.5735294117647058
26627,979f1e99f1b309,1a156d6a,## Split the data into train set and  test set,d1bfebbf,0.5737704918032787
26636,9ad9a97e628bfa,83ccee21,"**Cabin 살펴보기**

Cabin은 많은 수의 데이터가 없지만, 탑승자가 위치해있던 좌석/호실의 위치와 생존 여부가 관련이 있을 수 있으므로 살펴보도록 하고, Null Data를 채울 수 있는 실마리를 찾아보도록 하겠다. ",0a7e1136,0.5740740740740741
26638,fdc3afd309b850,b2c26468,Web Scraping the Wikipedia getting the AR and PCI,966bde38,0.5740740740740741
26640,2ada0305b68956,489de341,### 97. Palette = 'flag',133e26f4,0.5742857142857143
26641,63b44c85e32c1f,772a23ee,### Copying a list,fb9b9562,0.5743243243243243
26645,4c47839b067546,6e37e281,"корреляция очень высокая, поэтому удалим modelDate",1f517b02,0.574468085106383
26650,957e035ba5b9d5,c870e1c2,## Evaluate test data,778ab3d3,0.574468085106383
26655,d5f78aa381f58d,9739b803,## Naives Bayes,d60f358f,0.5747126436781609
26662,62487bcd70b199,e1b48e79,## <a id='6.3.'>6.3. GaussianNB Model</a>,f6ae50af,0.575
26667,0d58c434c7db1e,e2223ae0,Blitz rating - For games over 3 minutes but under 10 minutes.,517e01d3,0.575
26668,e3c0b55ed519e2,bed6c54b, # Locations with *Lowest* Hospital beds per projected infected individuals who need to be hospitalized.,9f51352e,0.575
26669,9a040a4f21091e,6772b66d,Markedly better than CountVectorizer. Let's see which comments it gets wrong...,f591b57d,0.575
26674,fdbbd573ba31c2,d4e2ca79,Year and second are no that important in this case. ,f7c28d74,0.575
26676,917957c6c4065f,a3f9dd3f,"dislikes는 평균 278, 범위는 0 ~ 884,965  
상위 3개의 동영상을 살펴보니, 모두 조회수 1~3위에 해당하는 동영상들입니다.  ",55b8ed68,0.5751633986928104
26678,ac1abfe1dfe815,bea25b6f,## Negative Words,6529dbcb,0.5752212389380531
26679,c9b4e282e4e2c1,3de0247c,1-Preprocessing the data,f44d339f,0.5752212389380531
26683,91473a39b85068,41099de5,### Text preprocessing,6e3d91c2,0.5753424657534246
26684,738bfced935b69,94c4ea67,## Label Encoding,2d3c592d,0.5753424657534246
26686,510b8303776bb6,f9928eb6,"Here in order to make the machine learning model, I have taken the threshold to be 0.15. Thus we will take any columns that have a correlation of greater than 0.15 and discard the rest.",18080db8,0.5754716981132075
26689,7c89a32e3562ca,8ea5a679,# Logistic Regression,32dd8913,0.5757575757575758
26693,3793c438a71b52,9d88202f,**Data UnderStanding**,13eb76df,0.5757575757575758
26699,adf419444a59df,47eccb58,![Screenshot 2021-08-22 231028.png](attachment:95ec6d0e-93aa-49b6-a6ca-6865df67daf3.png),3a275e7f,0.5757575757575758
26702,b01ee6cb674fa3,a5d23cbf,"# i-Space - 星际荣耀

Private chinese space launch company founded by 2016
",a8ffd35e,0.5760869565217391
26703,b660910fcc2954,10ee0d84,## Continous features distribution,80b74f88,0.576271186440678
26708,9169c4e9c33c90,31297cca,"5 books received less than a 4-star rating, two of which made a repeat appearance in the Top 50.

J.K. Rowling's *The Casual Vacancy* received the lowest rating of all the books.

Interestingly, all these books are of the fiction genre.",725bf880,0.576271186440678
26709,b9bc7dc9f582e5,6f8d0706,# Random Forest Regressor,15cc4d28,0.576271186440678
26710,a077820f7ab459,a8f05f0c,### Visualize accuracy and loss,05a43104,0.576271186440678
26713,2a56d6b0e153f2,2283c812,"IN THIS, MBA % IN RANGE 52% - 75% ARE HIGLY LIKED TO BE PLACED",8dc315e6,0.576271186440678
26714,ed8009f482b380,846ebc76,Import SMOTE and RandomUnderSampler,e99941fa,0.576271186440678
26717,513ce405d7f6a3,1096a8a8,# Gradient Boosting Classifier,8461e086,0.5764705882352941
26718,e93a41c03638fe,59fef9f7,# 3.3 Text classification with GRUs :,7363527b,0.5764705882352941
26719,869a39a3d4dea2,ca0df89d,"## Masking <a id=""masking""></a>",9020daf8,0.5764705882352941
26721,3c2033cc99c12c,c3268804,## Model Training ,dfa22a54,0.5766423357664233
26722,c80939c7c626cf,86ac2bc9,"more than 50% of 1st class are from S embark

more than 50% of 2nd class are from S embark

more than 50% of 3rd class are from S embark",b9ac31e2,0.5766423357664233
26723,b10bd75889dad9,281563b5,#### Lets use PCA and find high variance components,ee00ceee,0.5766666666666667
26724,fc8e0042411c46,18315911,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.5768025078369906
26729,4d91e84c564cbe,0b8c9a9a,## List methods,355a43e3,0.5769230769230769
26732,8ac70416723897,7682a05a,Building the model while one-hot encoding the classification features,d32fd8f6,0.5769230769230769
26735,e7237da7cbec10,c881d3ff,"4. Standard Scaler 
",5fcf5e3d,0.5769230769230769
26744,aae204e78a48d1,3d6e6621,We can clearly see that the attrition base is transacting far less than existing customers.  But is this because of a decline in transactions or did they always do less transactions?,53ab6133,0.5769230769230769
26747,a915263bc207da,45b1edd1,"Here I would take the outliers values because those are the insanely most rated movies and are better recommendations. The value is 159.
So we will calculate corelation with movies which are at least rated by 159 users.",b17ebcda,0.5769230769230769
26750,44f6a002ecd033,1c505700,"Now we have a lot less rows to work with, but we are able to get log transformations that will be useful. Let's verify that we have a approximately normal distribution now by taking a look at the plots.",70bbe106,0.5769230769230769
26753,1bd6cc83c02681,f5ee04a0,# Removing sparsity,17ff92f0,0.5769230769230769
26757,cf08b03b002c13,a1075fbd,Does number of comments help get a higher score?,104d416f,0.5769230769230769
26764,5f32117bcd5255,8187101e,# SPECTRAL TYPE ANALYSIS AND OBSERVING IMAGE,85882abf,0.5771812080536913
26765,e19e307b3fd188,d1e82447,"In this first moment, I will remove the outliers from the data using the interquartile range.",2173955b,0.5772357723577236
26766,063a35f644e3c5,36612f8b,### Draw certain conclusions by creating visuals to communicate the change in the cars in span of 10 years.,1c30fb0a,0.5773195876288659
26768,225b4fe5d3894a,dce5bdd6,"<a id=""6d""></a>
### d. Transformation Pipelines",4b4197b3,0.5773195876288659
26771,3d77c1560bd16e,5a29fd83,"<a id='6'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Weekly trend</div>

> Let's look at the weekly vaccine progress in Texas and plot the trendline to look at how Texas is progressing vs rest of USA",87c141ca,0.5774647887323944
26773,9bcfa825c8b2e6,a917e682,İnsülin değerleri de risk durumuna göre 3 kategoriye ayrılır.,220f36e4,0.5774647887323944
26775,bddd799cdbbae8,61d6f484, # <a id='6'> 6. Features Exctractions</a>,b44e3c08,0.5774647887323944
26776,631cd434fc3aa2,5cea9504,"* _Utilities_: three unique values where one is NaN, two observations with NaN are in the test set, almost all values are 'AllPub' except one 'NoSeWa' which is in the train set and hence this feature will not help in the predictive modelling. We can safely remove it.",2b74febb,0.5774647887323944
26781,e25c0f830df3f4,e99732ad,# Distribution of Polarity,fdcf7189,0.5777777777777777
26784,4fd4b6a80d40e3,74c97a4a,"### Signal Transmission from Input Layer to One Layer

![image.png](attachment:image.png)",f6913cc3,0.5777777777777777
26786,d6cbd7160961dc,e066bed2,## 5.2. Running the script on the data set,36d74664,0.5777777777777777
26787,e0a041e5e2372f,c4217223,* The final dataset contains 1898 samples and 12 attributes,7c4357b2,0.5777777777777777
26791,d77e6d61ad2e8b,ef5ec483,## Confusion Matrix,03fd0e96,0.5777777777777777
26792,b0c2805cd5c087,109d3594,Image biztechclub.com,0446f327,0.5777777777777777
26793,396bc36edb95d3,e86c9feb,"<b>Train Data:</b>  
    AUC: 83%        
    Accuracy: 78%             
    Precision: 66%        
    f1-Score: 64%       
            
<b>Test Data:</b>      
    AUC: 78%      
    Accuracy: 75%         
    Precision: 64%       
    f1-Score: 60%     
  
Training and Test set results are almost similar, and with the overall measures high, the model is a good model.  
  
Agency_Code is the most important variable for predicting claim status. Decision Tree root node split has been done based on Agency_Code. ",965e4f8f,0.5777777777777777
26794,d96e03a9e7c030,4d6d5dd9,"## Step 4: Build our final model
Finally, we can build our final model. As mentioned, we'll be using a linear regression model to predict the percentage of SHSAT testtakers at every school. We can use this to determine the difference between the number of testtakers we would expect at a school versus how many actually took the SHSAT.

Another methodology consideration is that the data set is small, in terms of machine learning models -- only ~550 rows (one row for each school). Partly because of this small data set size, I wanted each school to be used in a test set for a model built on data from other schools. There are a few ways to do this, but I chose to divide the data set into two halves, train a model on each half while using the other half as the test set. Combining the results from each school when it was used in the test set, we can get a sense of model performance without worrying about determining model performance based on observations that *were actually used to train the model*.

If you click on the expandable `Code` button, you'll see the function that builds the model `create_final_model`, along with the variable that standardizes each variable. The reason we didn't standardize the variables when we built our single regression models is that standardization only comes into play when there are multiple variables included in a model. If a variable isn't standardized, more weight may be put on the variables that just happen to have larger units of measurement. For example, `Average ELA Proficiency` has a range of roughly 0-4. Percentage of students with high school credit, `pct_8th_graders_w_hs_credit_2017_city_diff` tends to a range between -0.5 and 0.5. If we were to include both variables in their raw form in the same model, more weight may be given to `Average ELA Proficiency` just because its unit of measurement is larger. Standardizing around the mean and using the standard deviation takes care of this issue. 

I went through many iterations going through potential predictors to include in the final model before building out this notebook, but the below code is what split both models into train/test sets, then combined the respective test sets into one comprehensive data set we can use for our visualizations we took a peek at above. I decided to include`min_dist_to_big_three` variable as there may be some effects that this variable has when interacting with others, as opposed to looking at the `min_dist_to_big_three` variable in a model by itself.",d2b72ced,0.5777777777777777
26795,3597174a998d4d,d2d1bcb7,#### 2.2.2.1 customers' previous booking,276892ed,0.5777777777777777
26796,d905cde3391d2b,d2a0aa1a,"### Percentiles

**Percentile** is a number where certain percentage of numbers fall below that number.

Taking the above example,
* 25th percentile = 11 &rarr; 25% of the matches are won by less thant 11 runs.
* 75th percentile = 38 &rarr; 75% of the matches are won by less than 38 runs.

Percentile can be calculated using `scipy.stats.scoreatpercentile`

To calculate 95th percentile,",067dba39,0.5777777777777777
26797,42e0005bed28aa,24104660,# Creating Model,5616d451,0.5777777777777777
26800,ff3a8ce61fab6a,230b2b9e,"<hr>

### Exampel 2 ",9afe1654,0.578125
26803,4daf6153275cbf,049d9eb0,"Looks like Europe is agreed on how they rate their own food, except for Belgium. 

One comment on Greece I can make they have local food everywhere, the food is relatively cheap and seems like they are enjoying it by looking at the ratings.",51db1961,0.5783132530120482
26804,835a7b4e660d23,dfa480f2,### Adding new column,53bc7a6e,0.5783132530120482
26807,842547b2def18c,28acf108,"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.",b8efde6d,0.5784313725490197
26808,629f2918807a9b,7ebc8f3c,"#### Observations::

There is no transparent correlation between any city and order returned or cancelled, However it can be concluded as:

1. Karachi, which has highest no. of book buying record, it has too not a good record in cancellation and returning of orders. Karachi has 10 cancelled orders while 376 returned orders.


2. Lahore and Islamabad has also not very good record in terms of returning/ cancelling orders, Although Guftugu publications earned good from these cities.


3. Cities termed as others (Cities which have very low cardinality of buying books) have highest no.'s (562) in returning of orders and 2nd heigst in (7) in cancellation of orders.


4. Which means Guftugu publications can take a moment to think before sending of the books to the cities termed as 'other' categories.",be56dc84,0.5784313725490197
26809,52cfd66e9ec908,9581a23b,"So yes it seems like this distribution has several ""protrusions"" as I shall call them. We can now move on to exploring the frames data to check how feasible it is for our tabular purposes.",c74adcdf,0.5784313725490197
26813,f91f58d488d4af,3612ffef,"we can change our weight by a litle in the direction of slope, calculate our loss and then adjust again, and repeat this a few times. Eventually, we will go to the lowest point on the curve.",5df1bbf3,0.5789473684210527
26817,826ccb616bd2a8,b854fb69,"## Features of Importance
`Na_to_K` this feature being the strongest makes sense as it was found in a <a href=""https://www.nih.gov/news-events/nih-research-matters/sodium/potassium-ratio-linked-cardiovascular-disease-risk#:~:text=A%20high%20sodium%2Fpotassium%20ratio,the%20incidence%20of%20cardiovascular%20disease."">2009 study</a> that there is a strong correlation between Na and K when it comes to cardiovascular disease. 

`High Blood Pressure` this feature being strong makes sense as generally people with high blood pressure are at increased risks of heart disease (ex: Heart attacks). 

`Gender` appears to bear no significance when recommending a drug",4d7df2ec,0.5789473684210527
26818,c2a9f2fb3e1594,ea11023d,"<a id=""ch7""></a>
# Step 5: Model Data
Data Science is a multi-disciplinary field between mathematics (i.e. statistics, linear algebra, etc.), computer science (i.e. programming languages, computer systems, etc.) and business management (i.e. communication, subject-matter knowledge, etc.). Most data scientist come from one of the three fields, so they tend to lean towards that discipline. However, data science is like a three-legged stool, with no one leg being more important than the other. So, this step will require advanced knowledge in mathematics. But don’t worry, we only need a high-level overview, which we’ll cover in this Kernel. Also, thanks to computer science, a lot of the heavy lifting is done for you. So, problems that once required graduate degrees in mathematics or statistics, now only take a few lines of code. Last, we’ll need some business acumen to think through the problem. After all, like training a sight-seeing dog, it’s learning from us and not the other way around.

Machine Learning (ML), as the name suggest, is teaching the machine how-to think and not what to think. While this topic and big data has been around for decades, it is becoming more popular than ever because the barrier to entry is lower, for businesses and professionals alike. This is both good and bad. It’s good because these algorithms are now accessible to more people that can solve more problems in the real-world. It’s bad because a lower barrier to entry means, more people will not know the tools they are using and can come to incorrect conclusions. That’s why I focus on teaching you, not just what to do, but why you’re doing it. Previously, I used the analogy of asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible; or even worst, implements incorrect actionable intelligence. So now that I’ve hammered (no pun intended) my point, I’ll show you what to do and most importantly, WHY you do it.

First, you must understand, that the purpose of machine learning is to solve human problems. Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. Unsupervised learning is where you train the model using a training dataset that does not include the correct answer. And reinforced learning is a hybrid of the previous two, where the model is not given the correct answer immediately, but later after a sequence of events to reinforce learning. We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target. We then hope to present it a new subset from the same dataset and have similar results in prediction accuracy.

There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals. We'll save clustering and dimension reduction for another day, and focus on classification and regression. We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. One side note, logistic regression, while it has regression in the name, is really a classification algorithm. Since our problem is predicting if a passenger survived or did not survive, this is a discrete target variable. We will use a classification algorithm from the *sklearn* library to begin our analysis. We will use cross validation and scoring metrics, discussed in later sections, to rank and compare our algorithms’ performance.

**Machine Learning Selection:**
* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)
* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)
* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)


Now that we identified our solution as a supervised learning classification algorithm. We can narrow our list of choices.

**Machine Learning Classification Algorithms:**
* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)
* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)
* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)
* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)
* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)
* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)
* [Discriminant Analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)


### Data Science 101: How to Choose a Machine Learning Algorithm (MLA)
**IMPORTANT:** When it comes to data modeling, the beginner’s question is always, ""what is the best machine learning algorithm?"" To this the beginner must learn, the [No Free Lunch Theorem (NFLT)](http://robertmarks.org/Classes/ENGR5358/Papers/NFL_4_Dummies.pdf) of Machine Learning. In short, NFLT states, there is no super algorithm, that works best in all situations, for all datasets. So the best approach is to try multiple MLAs, tune them, and compare them for your specific scenario. With that being said, some good research has been done to compare algorithms, such as [Caruana & Niculescu-Mizil 2006](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf) watch [video lecture here](http://videolectures.net/solomon_caruana_wslmw/) of MLA comparisons, [Ogutu et al. 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3103196/) done by the NIH for genomic selection, [Fernandez-Delgado et al. 2014](http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf) comparing 179 classifiers from 17 families, [Thoma 2016 sklearn comparison](https://martin-thoma.com/comparing-classifiers/), and there is also a school of thought that says, [more data beats a better algorithm](https://www.kdnuggets.com/2015/06/machine-learning-more-data-better-algorithms.html). 

So with all this information, where is a beginner to start? I recommend starting with [Trees, Bagging, Random Forests, and Boosting](http://jessica2.msri.org/attachments/10778/10778-boost.pdf). They are basically different implementations of a decision tree, which is the easiest concept to learn and understand. They are also easier to tune, discussed in the next section, than something like SVC. Below, I'll give an overview of how-to run and compare several MLAs, but the rest of this Kernel will focus on learning data modeling via decision trees and its derivatives.",53411c04,0.5789473684210527
26819,5ce12be6e7b90e,98f7eb71,"### Access: Slicing
We can extract subsets of a string by using _slicing_, with the corresponding indexes.  
Remember: indexes start from **0**!",c0ab62dd,0.5789473684210527
26820,d93a87fdbdb3d2,fd147fd5,#### Word Cloud,30d079c3,0.5789473684210527
26821,a1dcd92986bc84,d4a5400e,"Note that training the model with 60,000 image-caption pairs, with a batch size of 256,
takes around 12 minutes per epoch using a V100 GPU accelerator. If 2 GPUs are available,
the epoch takes around 8 minutes.",730acaaa,0.5789473684210527
26832,f35ee6e9fab592,9c09b0e9,The following are the only video games to get a perfect score in the last year (as of time of collection of the dataset),b15f7073,0.5789473684210527
26836,d81d3830152f88,bcd7587e,"## A/B Test
Assuming I have to consider the question whether higher team 3pt-fg percentage wins more games based on the observation data we obtained only at a Type I error rate of 5%, what would the null hypothesis be?

### Hypothsis Test
- $H_0$ = Higher team 3pt-fg percentage does not win more games. `P_higher_3pt_win - P_not_higher_win = 0`
- $H_1$ = Higher team 3pt-fg percentage wins more games. `P_higher_3pt_win > P_not_higher_win`

For this A/B test, under the assumption that $H_0 = true$, we want to know whether higher team 3pt-fg percentage wins more games?",9551eac9,0.5789473684210527
26837,169177b6e9edea,8a800fe5,"<p> Referências:</p>
https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb",ca42152f,0.5789473684210527
26844,757fa8de4edc4c,b64bf1d3,"We can clearly see that distance is almost fixed for 1J type since the atoms are neighours of each other. But for 2J & 3J, distance cannot be the only field to predict scaler coupling constant.",87211008,0.5789473684210527
26849,30fdc4a6e3c1db,4d3d8754,### Plotting monthly sales time series across different stores,6111ddee,0.5789473684210527
26852,c3498779cda661,6b125664,# Codificar Datos Categóricos,0f531b65,0.5789473684210527
26855,9e27af2600925c,6afff342,"**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:

1. Calculate $\hat{Y} = A = \sigma(w^T X + b)$

2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). ",9b556435,0.5789473684210527
26856,9d27afa9ca3f96,9254c023,"create labels files, convert from COCO format to YOLO format
must be normalized",2d86a18d,0.5789473684210527
26857,54004b32784b68,8a26fbb6,**Using 2 column for creating a new column**,27213ca9,0.5789473684210527
26862,0a918602a04693,e96072fa,Lets see if there are any string data types in the dataframe,c1ef0e95,0.5795454545454546
26864,73d8e56bc709b1,ddc5a2b3,# 5. Attributes of top 5 strikers,78ec3cce,0.5795454545454546
26866,e3fb4c6300cb56,f197173a,"<a id=""8""></a> 
## Heatmap",8ebbdf89,0.5797101449275363
26868,0e2a23fbe41ca9,92a4b5cc,"Observations:
- Both the columns have 5 unique values
- No nulls are present
- most_recent_sales_range give the range of revenue (monetary units) in last active month  
- most_recent_purchases_range gives the range of quantity of transactions in last active month
- the above plots show for each transactions bucket, the distribution across the revenue bucket
- The lowest transactions bucket (E), highest is the lowest revenue bucket (E) showing that most of the merchants are small in size and most of the transactions happening with them are low
- similarly, A corresponds to A, B to B, C to C and D to D for sales and revenue in the highest values
- Can I say these columns are correlated?",64e4762c,0.5797101449275363
26870,17a24d566ffa59,dfae1678,"##### When is the SVD Technique Appropriate?
For most Text Mining problems, the SVD will be entirely appropriate to use. Without a data reduction technique, there will be more variables (terms) available than one can use in a data mining model. Some method must be applied to select an appropriate set from which a text mining solution can be built. Unlike term elimination, the SVD technique allows one to derive significantly fewer variables from the original variables. There are some drawbacks to using the SVD, however. Computationally, the SVD is fairly resource intensive and requires a large amount of RAM. The user must have access to these resources in order for the decomposition to be obtained.

##### How Many Dimensions Should be Used?
The choice for the number of dimensions k to use can be a crucial aspect of many text mining solutions. With too few dimensions, the model will fail to explain prominent relationships in the text. On the other hand, using too many dimensions will add unnecessary noise to the model and make training an effective model nearly impossible. In practice, there is an upper bound of at most a few hundred dimensions from which to build a model. So the user should not need to consider more than this",89049e56,0.5797101449275363
26871,9d9da6c439b96b,75562ac3,## Annual Sales Market For Each Country,361cc7d9,0.5797101449275363
26872,ea4e559a86d613,e42e2371,"
Transforming all categorical features un numerical.
> Note1: `sex`, `anatomy`, `diagnosis` need to be encoded.

> Note2: `benign_malignant` column will be dropped, as the information is already in the `target` column.",eff47843,0.5797101449275363
26875,548f961125248d,a2f7e3d7,"
## Hyperparameter tuning <a class=""anchor"" id=""sixth""></a>",d8c5e8b8,0.5797101449275363
26879,4ae6a182abac64,1c4cc9f9,"- Then, introducing new features as Family size (to join these Parch and SibSp)",418676c5,0.5798319327731093
26880,fc8e0042411c46,dfd064f0,## Newspaper,af476c2a,0.5799373040752351
26881,0687cd5c8597db,c87a2fb6,### **Training the CNN model with the train datasest**,4edec76a,0.58
26882,2ada0305b68956,70993a4e,### 98. Palette = 'flag_r',133e26f4,0.58
26883,7dd46c750653eb,fbd176cf,"**Inference**

* Except the month of January over the years death rate was always less than birth rate.

* Birth rate is highest in month of July and lowest in month of February.

* Death rate is highest in month of January   and lowest in month of June.",c2644713,0.58
26886,2bd6c370695ea7,8dd6ce06,## Metrics,cbe6aec8,0.58
26895,eda49464dd6d1b,5c01b53f,"### Because the model predicts 0 for all customers, it is necessary to find another way to evaluate it.  This is where ""predict_proba"" comes in.  ""Predict_proba"" assigns each customer a probability that they will choose to buy more insurance, rather than a simple 1 or 0.  In the sorted data below, the highest probability in the dataset is 49.7%.  Your results may vary.",8421f81f,0.5804195804195804
26900,56e58d53ac9c57,84b5d100,now let us first see graphs of Others to get some information,90e2ab8e,0.5806451612903226
26910,0d9a2067267ba1,2ac3255b,### Feature Engineering,abc194fb,0.5806451612903226
26913,0cb456a5456cf9,91f35b0e,# **Q2** <br>**Predict cancel using logistic regression**<br> 使用Logistic 回归预测退订情况,5701729c,0.5806451612903226
26915,0caaec057f7184,cc1f111d,"The figure shows the item counts in group of category. In this kind of task, there are lots of items between the category 14-75.",b875533e,0.5806451612903226
26918,535da6591a3246,ffb1affb,# Preprocessing,9ef8d0c7,0.5806451612903226
26919,16862cb02d73d5,10fda675,"So a 2D plot gives us a clear picture that the algorithm classifies anomalies points in the use case rightly. 

Anomalies are highlighted as **red edges** and normal points are indicated with **green points** in the plot. 

Here the **contamination** parameter plays a great factor.
Our idea here is to capture all the anomalous point in the system. 
So its **better to identify few points which might be normal as anomalous(false positives) ,but not to miss out catching an anomaly(true negative)**.(So i have specified 12% as contamintion which varies based on use case)
",d7ffa1a6,0.5806451612903226
26922,c65a65d4041018,00619971,### Hosted notebooks,824fb229,0.5808823529411765
26924,7a058705183598,8245e55e,4. SVM & Gridsearch,b0ead917,0.580952380952381
26925,7454fdc444df16,08495c0d,Let's ensure that our classes split is maintained after the shuffling and splitting.,a7818ef5,0.580952380952381
26928,04bac111ffbe9c,b4418011,"##### Extracting and handling Title, simultaneously dropping Name",82576b17,0.580952380952381
26930,62037c5832129c,93a55901,"## Reading a confusion matrix
* Confusion Matrix layouts the performance of a learning algoirthm for classfication models
* Square matrix which reports the counts of true positives, false negatives, false positives and true negatives predictions of the classifier.
",61474350,0.581081081081081
26932,63b44c85e32c1f,e733f4cf,"Most of the new python programmers commit this mistake. Consider the following,",fb9b9562,0.581081081081081
26938,1660daf8867980,d274478e,**Demonstration**,42d7cffc,0.5813953488372093
26941,c09fac3c943d51,478b8e9b,User-aggregating features:,678d076d,0.5813953488372093
26944,743ae010f5e875,3c18ceff,### Auxiliary functions,02c54445,0.5813953488372093
26947,8539260444e6b5,742b12ed,# Divide into training and test sets:,0369463f,0.5813953488372093
26950,957e035ba5b9d5,b179bde4,"test_loss, test_acc = model.evaluate_generator(test_generator, steps=32)
y_hat_test = model.predict_generator(test_generator, steps=32)

print('Generated {} predictions'.format(len(y_hat_test)))
print('Test accuracy: {:.2f}%'.format(test_acc * 100))",778ab3d3,0.5815602836879432
26953,917957c6c4065f,d95b6dd6,### 2.4. comment_count,55b8ed68,0.5816993464052288
26954,5a8c553e21c70f,7b402f17,## Correlation Analysis,9ebd9d8f,0.5818181818181818
26956,f0fab078f8533b,55ac1b31,# 3. Visually Analysing the data,bdb5ea32,0.5818181818181818
26962,21413205980558,baed98ad,"# It can be seen that the main deposit business crowd is 20-60 years old
# 可以看出主要的存款业务人群集中在20~60岁",84197de0,0.582089552238806
26963,e58e68e4eeefe5,c72b84ab,# Model Building using Sampling,a87662ce,0.582089552238806
26965,fdc9f4863744b1,d15c41af,"Manhattan has the most expensive houses in averege, followed by Brooklyn, Queens, Bronx and Staten Island. The median gap between Manhattan and other boroughs is huge.",b4529365,0.5821917808219178
26968,5d2a3e82679cf3,e7763e35,Normalize(df7),9e60b1e3,0.5822784810126582
26969,5f4ae633cfd090,44cde0a5,Let's move to _Stance variables,a30a16e2,0.5824175824175825
26972,a2176d4653ef60,c9a806c0,# Split Training Data,ac908675,0.5825242718446602
26974,ce9ed5e2d601d7,2aaa5a37,"# Performance Measures
## StratifiedKFold
Perform stratified sampling to produce folds that contain a representative ratio of each class. Soft voting is used from the predicted y between each fold.
* Hard Voting: In hard voting, the predicted output class is a class with the highest majority of votes.
* Soft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class",f58a2f43,0.5826771653543307
26979,268a610bbc64b4,57d7a9fa,# 3.What is trend for Advance Bookers,8a16f301,0.5833333333333334
26981,9ab7ebae0bfedb,eb1e04b2,"外れ値があるので外れ値の処理をする

関数は以下を使用

https://qiita.com/trami/items/b501abe7667e55ab2c9f",ad87076d,0.5833333333333334
26985,95656e8d666b16,84365b87,Using StandardScaler on 'X',65e88599,0.5833333333333334
26986,0800c019d227f2,4392ea1e,"## Tuning
'LightGBMTunerCV' is a hyperparameter tuner for LightGBM with cross-validation.",9cd3ffa1,0.5833333333333334
26987,98ea617d18c9cc,0694c8ba,# Creating the Model,e6316d11,0.5833333333333334
26991,0119199286f381,0c1e4f27,Using **KNN Classifier**,0db72675,0.5833333333333334
26997,593d1d3d1df05a,edb9e307,# Took These F1-Score from my other projects,bc682ffe,0.5833333333333334
27000,df7b0c10ec94fd,23d828a0,**create a generic user dataframe to keep Client and new segmentation scores**,ec1a6e07,0.5833333333333334
27004,30c8dc87ce52ca,67ffb8f7,"**MODELING USING KERAS AND ANN**

",805e9d67,0.5833333333333334
27005,1c5aaf7bea6414,d29ea20c,# RFM Process,34d8f42d,0.5833333333333334
27010,3b5903412fe741,cf2bd541,"## Manipulating the index

Label-based selection derives its power from the labels in the index. Critically, the index we use is not immutable. We can manipulate the index in any way we see fit.

The `set_index` method can be used to do the job. Here is what happens when we `set_index` to the `title` field:",ad231969,0.5833333333333334
27011,7650e0ac081e94,1fd18869,"### Define evaluator 
Generates lines like this in the training output:
`[10/27 18:31:26 d2.evaluation.testing]: copypaste: MaP IoU=0.2192638391201311` 

See here for definition: https://www.kaggle.com/c/sartorius-cell-instance-segmentation/overview/evaluation

I've made some modifications, because LIVECell coco data is not bitmask but polygon.",0081cee5,0.5833333333333334
27014,d46508f983e086,43a87192,"**Defining Train, Validation and Test**",454138b8,0.5833333333333334
27015,999258a81ba32a,0777dc89,# Ball 6 Frequency Chart,48cd3d21,0.5833333333333334
27023,b01ee6cb674fa3,7708a0a4,"# OneSpace

Private chinese space launch company founded by 2015
",a8ffd35e,0.5833333333333334
27024,dd3721cb49c1fd,87d9c110,"<div style=""margin: 0px; padding: 10px; background-color: #ef9a9a ;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h2 style=""color:white;text-align:center"">Remarks: The model fails to recognise the sudden changes in the trend.</h2>
  </div>
</div>",1a53fdd9,0.5833333333333334
27031,9ca9a30fc69d9b,04e8388e,## Country Wise data,f715c2e5,0.5833333333333334
27034,57740be713cf12,5614f6b0,## ***5. Modelling***,ac122df5,0.5833333333333334
27038,c349ee5a821411,cf73b5e0,"Here we can see something very interesting. In Europe there is a big correlation between Happiness and Freedom, and less correlation between Happiness and Health something that is not seen in the general correlation matrix",572b269d,0.5833333333333334
27040,a3ae04b78e45b5,4cdc7e45,**LM PLOT**,4195da8b,0.5833333333333334
27044,02773bdc5d3c7a,53401907,"Here the empty data frame contains:
    1.  8 rows corresponding to the class imbalance handling techniques
    2.  6 columns corresponding to algorithm name, method and the 4 performance metrics - Accuracy, Precision, Recall and F1_Score.",86245f35,0.5833333333333334
27056,5b92c712910a11,1474bbce,# Snow ball stemmer,e1d17100,0.5833333333333334
27057,71d5f1925c2b4b,05561ce4,#### Likes and Views are highly correlated (0.85).,63ee03ac,0.5833333333333334
27062,396bc36edb95d3,d29d987e,### Building Random Forest Model,965e4f8f,0.5833333333333334
27063,ab6da5994949a3,6670469b,## Visualising the SVC Training set results,fae6b91d,0.5833333333333334
27064,cf46cd6f7c55c0,8c9f6ad6,# Step 3 & 4 - Add pseudo label data and build second model ,191b86b8,0.5833333333333334
27071,7c7a7db391c517,4f903c81,"# 3 Data Visualization
## 3.1 Draw a geograph to show the Top 2 PM2.5 area in Seoul (Considered all the time period in the data source)",f53450dc,0.5833333333333334
27072,1d5daeca89f48d,7c6499da,"sent = ['Deep Learning is fascinating', 
        'I am loving Deep Learning'
       ]

Total Docuemnt = 2 

Sentence 1=> Total words = 4
Sentence 2=> Total words = 5

TF =>  how frequently a term occurs in a document

Formula:
TF(t) = No.of times a terms t occurs in the document/ total no.of terms in a document

Sentence 1:
TF(Deep) = 1/4 = 0.25
TF(is)   = 1/4 = 0.25

Sentence 2: 
TF(Deep) = 1/5 = 0.2

IDF => which measures, how important a term by providing lower weights to very common terms

Formula:
IDF(t) = log(total no. of documents \total no of documents with term t in it)
IDF(Deep) = 2/2 = 1, 
log(IDF(Deep))=> log(1) = 0
IDF(is)   = 2/1 = 2, 
log(IDF(is))=> log(2) = 0.313
 
TF-idf(t) = TF(t)*IDF(t) 
TF-idf(is) = TF(is)*log(IDF(is))
TF-idf(is) = 0.25*0.3 = 0.075


TF-idf(Deep) = TF(Deep)*log(IDF(Deep))
TF-idf(Deep) = 0.25*0  = 0",48d478bc,0.5833333333333334
27073,1d73d04c3aaae8,cbe3b552,"An overall prediction percentage of 64.8% seems quite good. How well does it compare to industry experts?

This year, Trey Wingo of ESPN has predicted 158 of 240 NFL games so far. This is 65.8%, almost the same as Elo. How did Elo do in 2019?",cd43d0aa,0.5833333333333334
27074,b3681fd423741d,94e533df,# 4. Berapa banyak mobil yang memiliki total jarak pemakaian di bawah 100.000 kilometer?,1aec06ce,0.5833333333333334
27076,6b383ec35229a2,fc9f265a,"We found previously some optimal values for batch size and learning rate

lr = 1e-2 seems to be the best learning rate
bs 512, 1024 and 2048 have similar performance, let's choose 2048 to make runtime faster.

Let's first run the model without transfer learning to have a benchmark to compare with",0c41b61b,0.5833333333333334
27078,2a377ced98d67a,98d6792e,#### Note: all the features plotted above seem to be positively correlated to each others,262231a8,0.5833333333333334
27079,726833f92fb87a,600486aa,From this plot we can see that older customers tends to have a higher bank balance and that customers who accepted the deposit tended to have a higher balance compared to the customers who refused the deposit.,7dc5e1b6,0.5838926174496645
27082,3c2033cc99c12c,115a861f,"**Note:** *In this part, i will test a few models on this dataset and have a brief view of the accuracy of the model*   

<b>The models will be divided into a few parts<b><br> 
+ The Logistic Regression   
+ Support Vector Machine  
+ Deep Learning Method ",dfa22a54,0.583941605839416
27085,ac1abfe1dfe815,2c464029,"We can see some of the most common words from the negative tweets, dely, customer service, help, (don, don't), issue, cancel, etc ",6529dbcb,0.584070796460177
27086,04ff2af52f147b,80798708,"Now that they are organized succinctly, we check to see the survival rates by title.",d5f37be9,0.5842696629213483
27094,722cd844dfbe8f,9099751f,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_3_2"">Define loaders for images sequences 4 MRI types</span>",0cedb385,0.5844155844155844
27095,c13f73168789c2,f5928f72,"### 1.1 Slice row by label<a id='22'></a>
Syntax : `df.loc[starting_row_label : ending_row_label, :]`",16175052,0.5844155844155844
27096,663bbc9eaf267b,f77e2704,## Year,32445529,0.5844155844155844
27099,1645979263c148,f625f15f,"## Feature Selection

1. seaborn.pairplot()",fa11663e,0.5846153846153846
27100,03048e86a6d806,0dfebfa4,"Data Scientists & Data Engineers have the highest median yearly compensation (among the working level with similar job title). 
Now, let's see the yearly compensation in 2019.",1285c231,0.5846153846153846
27101,a4f8ad33c823c5,f58df96b,#### Age Distribution,fcd48307,0.5846153846153846
27104,d07915a6e6992e,d1fdf2c3,**Sex**,2b912140,0.5846153846153846
27109,f2f2db16a2f86c,05a9f538,"As mentioned earlier, **ocean_proximity** needs to be encoded as it is a categorical data.",ffc6a115,0.5846153846153846
27111,9169c4e9c33c90,d5eecddf,"<a id=""Reviews""></a>",725bf880,0.5847457627118644
27112,1294fb4c86f993,b1fb6967,"<B>an obvious peak can be seen the year 2005 and can be related to hurrican Katrina as per 
__[New York Magazine](https://www.nytimes.com/interactive/2015/12/10/us/gun-sales-terrorism-obama-restrictions.html)__ ",4471e513,0.5847457627118644
27113,5ce12be6e7b90e,f71a85f7,We can access specific indexes of the list (_starting from 0_),c0ab62dd,0.5847953216374269
27117,43e60eb1362f5c,6e663c33,# Tableau Visualization to give more clear Insight and Inferences,87934234,0.5849056603773585
27119,0ad8d416b89b78,791bfa31,"# Model Development and Training
The Model was split into a training and test set with the target column removed for the test set. An initial Decision Tree Classifier was implemented to show a 'baseline' for model training. Further hyperparameter tuning and development should aim to improve upon this initial classification methodology.",0b0562f0,0.5849056603773585
27122,23df07a474aaae,e378889f,**Linear Regression**,0ea40276,0.5849056603773585
27125,73893f0467d5e3,d79c7252,# Feature Selection ,279787c6,0.5851063829787234
27126,4c47839b067546,17589d34,"### productionDate
заменим на возраст",1f517b02,0.5851063829787234
27130,8d0aebab1e5914,b16a93fc,**`projecting the original data sample on the plane formed by multiplication of two principal eigen vectors with transposed sample_data`**,084e671f,0.5853658536585366
27134,e19e307b3fd188,fa07423a,### rent price (R$) with outliers,2173955b,0.5853658536585366
27136,786475feda0190,a06ed6a7,**There's a lot of class imbalance.**,e4663d97,0.5853658536585366
27138,5169abdc647412,0d05f34e,### predictions for test.csv,28efc68d,0.5853658536585366
27139,47b2c9be5e31cb,bba77d9d,Distribution graphs (histogram/bar graph) of sampled columns:,7d4afe56,0.5853658536585366
27143,0e09587faffa8f,6d677bea,"From the above table, it is quite evident that the maximum number of violations occured **between 8 AM to 12 PM**",0d563d61,0.5853658536585366
27146,2ada0305b68956,ced4d322,### 99. Palette = 'gist_earth',133e26f4,0.5857142857142857
27147,38b79494ac749e,2a918953,"We observe the following:

1. **Underfitting** (degree < 5): The model is not able to fit the data properly. The fit is bad for both the training and the validation set.

2. **Fit is just right** (degree = 5): The model is able to capture the underlying data distribution. The fit is good for both the training and the validation set.

3. **Overfitting** (degree > 5): The model starts fitting the noise in the dataset. While the fit for the training data gets even better, the fit for the validation set gets worse.

4. As the order of polynomial increases, the linear model coefficients become more likely to take on **large values**.",39162a40,0.5857142857142857
27151,b4e238fbc6464c,c63d0ccc,"## Encoding
",fd1a6cba,0.5857142857142857
27155,84127ade6fde87,5e231c4d,"?Im|pos|s|ible|,|?Mr|.|?B|en|net|,|?impossible|,|?when|?I|?am|?not|?acquainted|?with|?him

This is from a SentencePiece tokenizer trained on a machine translation dataset.",f55d05b6,0.5862068965517241
27157,20e1ba19eb9b5e,0b8cbcf9,"Data description says NaN means: ""No Pool"" in case of 'PoolQC', no misc feature for ""MiscFeature"", no alley, no fence and no fireplace etc..",4569bfc1,0.5862068965517241
27160,00d295edcd117e,81486754,我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。,f5810f4b,0.5862068965517241
27162,858da4bb312f67,4a66a00a,### HEALTHY to CBSD,9cca4391,0.5862068965517241
27163,1cd8be6e679620,5505ff17,"---

## RNA Detailed Analysis 🧬 ",3ce15a43,0.5862068965517241
27168,5fc2f23dfbeeb1,ed6bd956,### Changing data type into array and int64,f37b4110,0.5862068965517241
27172,d42518f6cb0995,62cf7277,Let's try to use discovered features and use them in model to predict the right answer probability.,26913a9b,0.5862068965517241
27177,a09e20bb9b5259,fd471752,# Size 299,99475ae5,0.5862068965517241
27179,18a96bb5711ed9,ec557927,"**There are a couple of things to notice**: 

* Most of reported incidents is from Mediterranean, North Africa, and US-Mexico Border regions
* Starting from April 2017, the number of reported incidents from the Sub-Saharan region dramastically increased 
* The highest number of incident by month/year is from North Africa (Just in one month, Feb 2016, there are 62 reported incidents in North Africa <br>

**Based on my research, In Feb 2016**:

* Abnormally dry weather across North Africa [(source)](https://www.reuters.com/article/ozatp-uk-africa-drought-morocco-idAFKCN0VD0Z1)
* A 200-kilometer (125-mile) trench was completed in February 2016, which runs along the Tunisian–Libyan border [(source)](https://carnegieendowment.org/sada/77053)",e79768db,0.5862068965517241
27187,ee9ddc756b2d4a,9c6ccf30,# Training e model evaluation,e367eab3,0.5862068965517241
27188,a1ba5ffd30dbde,bbea245b,<h2 align='center'> Decision Tree </h2>,48e57546,0.5862068965517241
27189,a4f0a3e1316ff9,b4d0ac29,"# Correlations with Australia

Let's look at correlations specific to Australia cutting out the minor ones",53bf0160,0.5862068965517241
27191,ef6d1e959a873e,42e2ceec,Let's check the distribution of Reco_Policy_Premium,f11a1f43,0.5862068965517241
27193,6a1d04e8153df3,803d3a4d," **BOX PLOT AND WHISHKERS**
 > This plot will help to recognize the exact survival rate of women according to the axillary nodes.",38572b05,0.5862068965517241
27194,6903d3f38c6a66,3b802415,"Another way is to use gridspec. This allows you to use **add_subplot together**, similar to subplots to grid.

This approach allows you to take advantage of the concept of list to use a developer-friendly grid",6067ce5e,0.5862068965517241
27196,1750367e54f407,0f3226c4,"First, we will check that we perform on similar level on both the training and validation. The training curve will also tell us if we stopped training too early or may have overfitted in comparison to the validation data.",a8e655b2,0.5862068965517241
27202,e5dd725b8fa422,7f5ad83d,# [Plotting ACF & PACF](http://),14675d8b,0.5866666666666667
27209,7e1da639035ac5,5476cc00,### <a id='11.1'>11.1 ELA and math proficiency distribution</a>,120b6c23,0.5866666666666667
27210,67b7354e96113a,0f60279e,**Label encoding categorical columns**,dca94250,0.5866666666666667
27211,2f47abddfd1928,1d947ca7,"In this case I have removed Parch values under 4 because of the same reason as before.

Again, most of the people travels alone, and they are mostly between 25 and 35 years old.

The passengers traveling with just one parent or children covers a huge range, from less than 10 to more than 40. It means that there is a mix of single parents traveling with their child or viceversa.

The case of passengers traveling with 2 parents or children are most likely to be children with their parents or young people with children.

3 or more parents/children means mainly parents traveling with their children because the age range is mostly between 20 to 50.",ae33cc0b,0.5867768595041323
27215,72d528df923403,bedf5d95,"- In the middle of the year between June and August the number of units sold is higher and at the end of the year until the beginning of the next, sales reach their lowest peak, in all departments.
- The is more distance at the highest and lowest peaks of CA_3.
- Since 2015 CA_2 has started to rise in terms of quantity sold.
- TX_3 outnumbered units sold to TX_1 since 2014.
- Since July 2013 WI_2 is the store that sells the most units in the state of WI.
- WI_3 units sold have declined from 2012 through 2014, where they reached their lowest peak. After that sales gradually increased again.
- While WI_2 sales decreased, W_1 and W_3 sales increased considerably in 2012.",d51c8e8e,0.5869565217391305
27217,a6b9837940ee38,3eda295e,# Build model,52d2acc7,0.5869565217391305
27221,9b5de3823ad5ab,5054a11d,"### Network instatiation

The chosen architecture was EfficientNet-B0. Since I'm submitting this work as an assignment for an university course, it was chosen in order to minimize training time (as I only had a week to dedicate to this assignment). This means that EfficientNet-B0 is the smallest network of its family and should not have a great classification performance.

In summary, the EfficientNet architecture was proposed by Tan and Le who are (or were) both researchers at Google. Its idea was to study the growth of a CNN parameters (number of layers, filters and input size) as an optimization problem. This can be used with more traditional architectures such as ResNet, Inception, GoogLeNet, etc. in order to optimize them, or, to create a brand new architecture - EfficientNet - and scale its growth towards better performance with a reduced number of trainable parameters. For more information, check the link to the paper hosted in arXiv below:

https://arxiv.org/abs/1905.11946

tl;dr: EfficientNet-B0's performance is close to DenseNet-201 and ResNet-152 in ImageNet, while having way less parameters (about half of DenseNet's, and 1/6 of ResNet's).",33e48774,0.5869565217391305
27222,0e2a23fbe41ca9,bc76523c,"### 6. lag_3 columns

- ```avg_sales_lag3```: Monthly average of revenue in last 3 months divided by revenue in last active month
- ```avg_purchases_lag3```: Monthly average of transactions in last 3 months divided by transactions in last active month
- ```active_months_lag3```: Quantity of active months within last 3 months
",64e4762c,0.5869565217391305
27231,5ffe6aa38958a1,299e96e8,"### 2.3.4 Ticket Number - Use as category
---- Not for now ---- ",11f5412e,0.5875
27235,8ec771f5600a61,759abb93,# feature scaling,48364c1f,0.5876288659793815
27236,2a123b4e8f9433,854a1032,# Build and train a Random Forest model,0a082218,0.5876288659793815
27237,063a35f644e3c5,d68fcda4,"Example: 

Are more unique models using alternative fuels in 2018 compared to other years? By how much?

How much have vehicle classes improved in fuel economy (increased in mpg) per year?

What are the characteristics of SmartWay vehicles? Have they changed over time? (mpg, greenhouse gas)What features are associated with better fuel economy (mpg)?",1c30fb0a,0.5876288659793815
27239,6cade0b6a41ba2,be88c789,"##### Count of missing values is bit high enough to drop respective value. for the same, we will use he mean value to impute these null values.",e6110293,0.5877192982456141
27241,ee23a565163388,80857615,Let's split up the features and then scale them since the features are of different scales.,88aacbc4,0.5877862595419847
27245,156bbcff05dcea,36b6571e,# Confusion Matrix,66ad1fe9,0.5882352941176471
27247,4ae6a182abac64,6187b3d7,* **Family size**,418676c5,0.5882352941176471
27249,02b7e38902069e,f6b4b7ce,"#Phonetics for Indian Sub-Continent languages Alphabet

""The Indian Sub-Continent languages have strong phonetics for their alphabet and that’s why in the Indic NLP Library, each character has a phonetic vector associated with it that defines its properties.""

""An example where we take the simple Hindi character ‘उ’ :

https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.5882352941176471
27251,b779c3ce7b671a,18d02ce3,"**Oribit animation** \
To view the animation in this notebook, use ``![movie](./movies/movie.gif)`` in a **new** markdown cell.",ca778770,0.5882352941176471
27258,6b7c80ed7bd03d,2881fad5,"# 3. Visualization
We conduct the visualization task to confirm the DICOM file has been converted as numpy correctly.  ",7bba27db,0.5882352941176471
27259,c3dfa835621ac4,9e79e54d,"# Visualization & Animation

Now we have created all the functions and classes for logic. Next we are going to visualize and animate the process. 

First plot the input and ouput of a pair of sample.

Then generate an animation to show how the state changes when a CA is applied onto the input. Here three matrix will be shown: the first is the state; the second is the memory; the third is the indices of CA rules applied in this step.

Here some tasks solved in [Cellular Automata as a Language for Reasoning](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning) as examples are presented here for demostration. If you don't understand the rules please read this article first.",0126bdad,0.5882352941176471
27260,869a39a3d4dea2,fe816f7e,"Masking ia an important part of image processing, which can be used to extract the useful portion of the image ignoring the others.
In this we are going to build the mask and apply it over the image",9020daf8,0.5882352941176471
27263,130af16914b3f8,c2b6eb6b,# Import data from a database to pandas,4b36d3e3,0.5882352941176471
27264,fa02c409161192,151d1d60,"In this section, we will see how the size of the training set effects the performance of the NN. The NN will have a constant structure (equal number of hidden layers, same activation functions, etc) and we will vary the size of the training set and see how this effects the performance. 

We will plot the results, I expect that a larger training set will have a positive effect on the performance. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that a larger data set takes longer to train. ",e97077f7,0.5882352941176471
27271,629f2918807a9b,11d3812c,"### **Now Lets Start with our new question: ""Find a correlation between date and time with order status"":**",be56dc84,0.5882352941176471
27275,16ca1123840e9f,3843231d,"#### The China Aerospace Science and Technology Corporation (CASC) has the maximum number of active space missions followed by Arianespace. 

#### Though USSR has the maximum launches, it was active from the 1930s until the state's collapse in 1991. Hence, none of their missions are active. [Click Here](https://www.forbes.com/sites/startswithabang/2019/07/11/this-is-why-the-soviet-union-lost-the-space-race-to-the-usa/#650c524e4192) to read an interesting article on USSR.",e8b8f086,0.5882352941176471
27278,9cec5ddf8b6f49,6b740e33,"#### 'train_test_split' function is used to split the train data into train and test where we use the test to validate our model's performance before finally predicting on the test set
#### 'random_state' is used to reproduce the results and 'test_size' will specify the fraction of train data we should use as validation",d39fc8e7,0.5882352941176471
27283,e4c6dd957eb5ce,e85eedfb,"# Kernels
What do we will analyze here?
- Total of Kernels created through the time
- How many kernels are created by month? Does this quantity is growing through time?",2e383665,0.5882352941176471
27288,7cfd96218dd933,1b8c3942,"#### **ATTENTION**
* SHARP INCREASES ARE OBSERVED ONLY IN TURKEY",7c34d96c,0.5882352941176471
27289,71b75664517244,12de025a,"Alex ferguson has the most premier league champion title here, it is exactly the same total amount of the team itself, Manchester United.
Now we can take a conclusion here if MU won only when Alex Ferguson is their managers.",fc905af5,0.5882352941176471
27297,410285582f4f7e,61631846,**Yearly growth of number of accounts created**,d026266b,0.5882352941176471
27300,81712ee7510ac5,0146a73d,**Then whats the difference between tuples and list???**,c4685e79,0.5885714285714285
27301,957e035ba5b9d5,d00b5f21,"## TODO

* Is the shape for the created label arrays correct? Doesn't look like it. Investigate.
* Evaluate thoughts errors.",778ab3d3,0.5886524822695035
27302,56785caebaa256,f925e2c0,"## 5.1. Stage 1 - Tuning holiday parameters<a class=""anchor"" id=""5.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.5886524822695035
27303,b61ab8f81dc03d,9fbc1b40,"<a id=""converting_types""></a>
## Converting types",64d05394,0.5886524822695035
27305,c84925c8171900,756fa2c7,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.3.4 Top 5 Publisher - Reagion Wise
            </span>   
        </font>    
</h4>",e21ff7ec,0.5887850467289719
27309,3597174a998d4d,c4693b79,"In the dataset, is_repeated_guest, previous_cancellations and previous_bookings_not_canceled can be used to describe customers' previous booking. It can be concluded that:
* Most bookings come from new customer, and new customers' ratio of cancelation is much higher than repeated customers.
* If the previous_cancellations variable is equal to 0, the ratio of cancelation is low. While if the previous_cancellations_not_canceled is equal to 0, the ratio of cancelation is high.",276892ed,0.5888888888888889
27314,1eb62c5782f2d7,1c542363,"### Interpretasi

Ketika $200$ pengunjung memasuki toko, kamu akan mendapati $200 \times 0.6915 = 134.3$, atau sekitar $138$ pengunjung berada di toko lebih dari $39$ menit.",bb69f147,0.589041095890411
27315,fdc9f4863744b1,d2252d01,"Let's examine ""Manhattan"" in more detail.",b4529365,0.589041095890411
27318,8dd655515e7d18,707677d7,"**Analysis:**

The Word Count is uniformly distributed between min-5 & max-217",895f41cf,0.5892857142857143
27327,f91f58d488d4af,dafcf91b,"#### Calculating the gradients.

The one magic step is to calculate the gradients. The gradients will tell us how much we have to change each weight yo make our model better.

**The derivative of a function tells you how much a change in it's parameters will change in it's result.**

The derivative calculates the change,rather than the value.

You can calculate the derivative with respect to one weight, and treat all the other ones as constant, then repeat that for each other weight. This is how all of the gradients are calculated, for every weight.",5df1bbf3,0.5894736842105263
27337,897ca904b74a98,da2b40b9,## Preprocessing,c5844ad4,0.5897435897435898
27344,4d91e84c564cbe,b0283b2c,`list.append` modifies a list by adding an item to the end:,355a43e3,0.5897435897435898
27345,80ad12f326ab70,48284372,"The maximum per-pupil total expenditure is about8,000 to 10,000. ",da404a16,0.5897435897435898
27346,9eed0fae1c7958,b06dce0a,# Model summary,3fb1438e,0.5897435897435898
27348,b86bda7afe3ac3,56993256,Ruddit ponderation,16197934,0.5897435897435898
27349,a566b5b7c374e7,36f4fb61,### Restfulness Score,b3dc5545,0.5899280575539568
27353,918040fad252ec,4f3dadff,Menampilkan *Confusion Matrix*,966fcd8f,0.5901639344262295
27356,0858e1bb3cbaca,409ec8d7,"Alternatively, if I only want to know the one sale with the max net profits, we can use

**.max()** 

to access the max among our data",78548374,0.5901639344262295
27357,bcd7e398c4d0ec,df2b097c, # 2. Feature Extraction,77a143f6,0.5901639344262295
27360,4daf6153275cbf,0dea0c31,#### 4. Reviews,51db1961,0.5903614457831325
27364,55a5e31d03df9f,52fadb50,🤯 Wow! As you can see the results are amazing with just 5 epochs we already beat all our previous work that is the power of transfer learning! Now let's visualize the losses curves. ,06dce00f,0.5904761904761905
27365,bbaa07ad21cf4e,4bf28a65,"## 7. Log Loss evaluation for feature importance <a class=""anchor"" id=""8""></a>
",3ab6b254,0.5904761904761905
27368,b01ee6cb674fa3,a58c3571,"   # LandSpace
   
   Private chinese space launch company founded by 2016
",a8ffd35e,0.5905797101449275
27369,5f32117bcd5255,34847bec,### GETTING IMAGE DATA,85882abf,0.5906040268456376
27370,726833f92fb87a,156f4219,# Data Preparation for ML algorithms,7dc5e1b6,0.5906040268456376
27372,30fdc4a6e3c1db,6d041558,"What we see:
* In CA, CA_1 and CA_3 shows the highest seasonality in sales and have an increasing trend overall.CA_2 shows a stark decrease in 2015 but has then peaked up in 2016. CA_4 shows a similar increasing trend. Overall, CA stores show the same trend
* In TX, TX_2 was really performing well till 2014 but has dipped after that drastically which is bad.TX_1 shows a moderate increasing trend. The good news in TX is TX_3 is consistently showing an increasing trend
* In WI, WI_1 and WI_2 shows drastic increase in 2012 and 2013 from 50k sales to 100k sales. WI_3 which was the best store in 2012 dipped a lot during  2014 and 2015 but has increased a bit in 2016 ",6111ddee,0.5906432748538012
27376,da199f8fb59439,aa3577d3,> *Highest number of Movies & Tv-shows were released in the year 2018*,baaa665d,0.5909090909090909
27377,a2444ab5d5f147,6e84d139,### Some data cleaning (Removing rows with only whitespace in `phrase_without_stopwords`),10617755,0.5909090909090909
27383,c2be02442e8cfd,46300b43,# PDF and CDF,2d364acc,0.5909090909090909
27384,f269d2fbd5f1be,27e39cef,# Scatterplot of Rating & Salary by Age Group,1264c440,0.5909090909090909
27385,f166950fa915f8,6fcfab74,### Build Model,a7f6ca5e,0.5909090909090909
27387,0cb9adc158b705,6b32807a,"We are using `accuracy_multi` and `f1score` metrics, because its a multi-label problem and the evaluation metric for the competition is *F1Score*.

Finally, lets train (technically, fine-tune 🤯) our model.",3abf056e,0.5909090909090909
27389,6d66ced0028dea,76708cae,### 1.2 Feature engineering,f50aae52,0.5909090909090909
27393,bb3d1b4b9f1248,c9727734,# ** Comparing the PerCapita for 2019 and 2020**,bf7de324,0.5909090909090909
27395,d1ff7e10ee0102,1cd9e70a,"What has been revealed:

* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.
* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.",2cc71c3c,0.5909090909090909
27399,930cd79ca51204,4dfd5086,"Let's replace the tick values 1000, 10000 and 100000 with 1k, 10k and 100k. ",5506779a,0.5909090909090909
27400,69e2428808c415,b7cec199,# Data Pre-processing,ae798c16,0.5909090909090909
27403,450fda47b03baa,8b0c20b4,Normal dağılım yoktur,62c04adb,0.5909090909090909
27405,5083d7a61f2426,e1584978,"Performing predictions with the test data
",541a0fec,0.5909090909090909
27406,32e04b08ff52eb,a0aa8aaa,Prediction Function,8d5b86e0,0.5909090909090909
27408,e323e594ef918f,71c736e3,Now let's double check the class predictions. It looks good!,6e829ab6,0.5909090909090909
27410,b066ab2167199c,f4726f59,"- There is no unwanted column present in given dataset to remove.

     EX: ID, S.No etc",18a1753d,0.5909090909090909
27411,d83e5b44d1b80d,550909e6,# Age,62845930,0.5909090909090909
27414,3c2033cc99c12c,57866ab2,### Logistic Regression In Sklearn ,dfa22a54,0.5912408759124088
27417,2ada0305b68956,870c439d,### 100. Palette = 'gist_earth_r',133e26f4,0.5914285714285714
27424,62487bcd70b199,3f4a9537,## <a id='6.4.'>6.4. DecisionTreeClassifier Model</a>,f6ae50af,0.5916666666666667
27434,2343dc02ffb96a,e69055b4,"# Scipy: Pearson's Correlation
# Because of the deprecation error that ""import statsmodels.api as sm"" gave, the following will not run in Kaggle.",29aa95a4,0.5918367346938775
27435,e69a496109e7d8,1df445e8,The above plots shows that operation which had done from 1962-1965 tend to be failures,1c640591,0.5918367346938775
27444,fc8e0042411c46,941484d2,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.5924764890282131
27447,4392956f62c040,becdce3c,## TRAINING TAMIL MODEL,c3ed519d,0.5925925925925926
27450,c6f8ff61a5fa87,99922fbf,"# <span style=""color:blue;""><strong>7.Prediction</strong></span>",3eea586b,0.5925925925925926
27452,fce6f1b02867e3,3c7851a1,## loc retrieve data points using the label of rows/columns and accept boolean arguments.,3fb572c2,0.5925925925925926
27455,0c452d3a0b9339,3e917af1,# Visualize the Reactivity & Degradation Stats of the basePair & nonBasePairs,5d857385,0.5925925925925926
27457,1667a100fc8b42,7afeb4ac,"## Feature reduction ##

There are many features, so we are going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio.",6c8cd6b6,0.5925925925925926
27461,ac04ba639d1c93,a878ada0,# Create Group Features,748059d5,0.5925925925925926
27462,6d29650083cbde,4b8f6682,"I filter to select the top 20% of the players with high potential (i.e. I exclude Messi & co who have been performing incredibly well over the past), and *voilà!*",e65fd993,0.5925925925925926
27463,24e550b8226932,1e2d8ef9,# Preprocessing:,0caee953,0.5925925925925926
27464,ffc3bb768dcf97,fb9128b4,Split Dataset,15ce84be,0.5925925925925926
27465,c1984e64b35234,c67cbfbd,"The rs1799971(G) allele in exon 1 of the mu opioid receptor OPRM1 gene causes the normal amino acid at residue 40, asparagine (Asn), to be replaced by aspartic acid (Asp). In the literature this SNP is also known as A118G, N40D, or Asn40Asp.

Carriers of at least one rs1799971(G) allele appear to have **stronger cravings for alcohol** than carriers of two rs1799971(A) alleles, and are thus hypothesized to be more at higher risk for alcoholism. [PMID 17207095]",1811225b,0.5925925925925926
27469,4883314a96dc34,2b430642,### Evaluate Model Performance (1),50d36836,0.5925925925925926
27472,07544ba83da480,47be6e32,# Question 3: What time should we display advertisiments to maximise likelyhood of the customer buying the product?,dc2f52b1,0.5925925925925926
27476,1883198d6d8c3c,263d1214,* <h3>2. Statistical summery - dataframe.describe()</h3>,69a1d458,0.5925925925925926
27482,c968dbd8d49ae6,024bf791,# **Service Atributes of Churn costumers**,dfb2684d,0.5925925925925926
27486,613bf7bfdcb9e3,7f128941,### axis = 1 (left->right direction common value count),32beb65d,0.5925925925925926
27492,d96642860ab3dd,bbfd1f9d,### 2.2 Feature Engineering familySize = SibSp + Parch,98419d48,0.5930232558139535
27494,149cb8d3489224,a45eb82a,### Hashtags analysis,116858e7,0.5932203389830508
27495,1294fb4c86f993,2c2835d7,#### Examining Texas,4471e513,0.5932203389830508
27497,c4bca5d86a38c3,8df1160b,Contando número de valores null en columna Age,e23d297c,0.5932203389830508
27499,a44368590e878a,c6a111eb,### Infection networks,77743ba8,0.5932203389830508
27500,2a56d6b0e153f2,d87b9362,# DATA PREPROCESSING,8dc315e6,0.5932203389830508
27501,b9bc7dc9f582e5,584bbb00,Training data is combination of Continues and Categorical features.Random Forest regressor handles both types of data in regression.,15cc4d28,0.5932203389830508
27504,a81661cc35d8d2,0c7e0064,Splitting into testing and training data sets,3331f113,0.5932203389830508
27506,f2e5e9fb9eaaf7,9b313a87,"[back to top](#table-of-contents)
<a id=""6""></a>
# 6 Model
Evaluate the performance of base model. Models will be evaluated using five cross validation without any hyperparameters tuning. *(to see the packages used, please expand)*",048e0d08,0.5932203389830508
27507,9169c4e9c33c90,6da06b10,"# Reviews

[Back to top](#Top)",725bf880,0.5932203389830508
27508,2bd6c370695ea7,f73f3af9,## DataLoader,cbe6aec8,0.5933333333333334
27514,c5fef7cc592736,1e363c23,Once we have the model created we use the standard procedure with fastai leaner,d21dc2c1,0.59375
27515,49f2274c1dd516,44cac54b,"# COVID Tracking Project
The COVID Tracking Project collects and publishes the most complete testing data available for US states and territories",06b0ffee,0.59375
27516,96c4c0e36b8ec0,8d21e832,"What the graph tells us:

* Very young children had a very high survival rate 
* Greatest amount of people to perish were between the ages of 20 and 30 (this is also the most common ages of all passengers are in this range so this may not be statistically significant
* Everybody between the age of 65 and 75 perished 
* Everybody between the age of 75 and 80 survived
",4dd6de8c,0.59375
27517,c85c94076e9c3a,873452d0,## Marital_Status,3ea0c443,0.59375
27519,2c3a6969252dc0,e4ea489a,Students with better LOR tend to have Higher TOEFL Scores.,d30f10ce,0.59375
27520,fae5023faa435f,67d7a497,# LSTM,b37c893b,0.59375
27521,117fc0956643d0,d1e43d9c,"## Step 4. Get Top 10 relevant articles
We have sorted the articles in descending order based on the confidence score calculated from the above steps. After sorting the articles, we extract the top 10 articles.

Due to the time constraint, we have run the code using GPU in Google Colab and uploaded the output in ""../input/covid19-top-articles"". Remove the output files from the directory if you prefer running on your local machine.",68cef9fd,0.59375
27524,51a46d0a7597f5,970a70eb,"Only 0.26% values are Null, lets go ahead and drop them.",e9e25b17,0.59375
27534,17a24d566ffa59,a38195b4,"### SVD in comparision to PCA
Using the below image displaying several documents that contain only two terms (A&B)

**SVD:** draw a line through the points in such a way that the sum of the distances from each point to the line is minimized. The documents can then be perpendicularly projected onto this new line. The circles indicate the locations of the projected documents. This line and the new locations for the documents, can be obtained by using the SVD

**PCA:** Spread the points out as much as possible on this new line to maximize the variance of the points that are projected onto the line. This line is formed by performing Principal Component Analysis

Although based on equivalent procedures, since PCA and TM’s SVD approach operate on different data, they do not produce the same results. Depending on whether the raw data is used or the covariance matrix is used, different vectors will be found as basis vectors for the reduced space

While the PCA component maximizes the variance, the SVD finds the best fitting line in the least-squares sense. Depending on the nature of the data, these two lines may or may not be fairly close to one another. ",89049e56,0.5942028985507246
27536,7e89d387feb9f5,ef8d2009,"#### Количество типов кухонь, которые предлагаются только в одном ресторане.",989e3a1b,0.5942028985507246
27537,7e275c8d5ff2a0,4d57740d,"<div style=""color:white;
           padding:8px 10px 0 10px;
           display:inline-block;
           border-radius:5px;
           background-color:#EC2566;
           font-size:90%;
           font-family:Verdana"">
    <h1 style='color:white;'>6.1. Confirmed Cases in India</h1>
</div>",b3afcc98,0.5942028985507246
27538,8336d84cf3ff6b,0370ab4d,"
import lightgbm as lgb

clf = lgb.LGBMClassifier(boosting_type='gbdt', class_weight='balanced', colsample_bytree=0.5,
                importance_type='split', learning_rate=0.1, max_depth=4,
                min_child_samples=20, min_child_weight=0.001, min_data_in_leaf=5,
                min_split_gain=0.0, n_estimators=650, n_jobs=-1, num_class=3,
                num_leaves=63, objective='multiclass', random_state=42,
                reg_alpha=2, reg_lambda=0, silent=True, subsample=0.7,
                subsample_for_bin=200000, subsample_freq=0, verbose=1)

lgbm_prediction=[]

kfold=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=101)

for train_idx,val_idx in kfold.split(X=X,y=y):
    clf.fit(X.loc[train_idx,:],y[train_idx])
    predict=clf.predict(X.loc[val_idx,:])
    lgbm_prediction.append(metrics.accuracy_score(y[val_idx],predict))
    

print('Accuracy of LGBM Baseline is {}'.format(np.mean(lgbm_prediction)*100))

#predict=clf.predict_proba(test.drop(columns='ID',axis=1))

#XGB_baseline=pd.DataFrame(predict,columns=['XBG_baseline_0','XGB_baseline_1','XGB_baseline_2'])",b96b58a0,0.5942028985507246
27544,ea4e559a86d613,e3aed964,# Folds,eff47843,0.5942028985507246
27549,510b8303776bb6,f6e3bca9,# Prepaing training and testing sets,18080db8,0.5943396226415094
27553,bbb3f4b76a4559,6be62f9d,"This is not so bad... But not good.  
Let's see data.",75185823,0.5945945945945946
27557,b7b1057764fa02,8b0983dc,"Having loaded, understood and pre-processed our data, it is finally time to turn to the model.

# 5. Define and run the model

I will define a deep learning model, a convolutional neural network (CNN) to classify the data. CNNs are a specific kind of artificial neural network that is very effective for image classification because they are able to take into account the spatial coherence of the image, i.e., that pixels close to each other are often related. This network accepts an image of an American Sign Language letter as input. The output layer returns the network's predicted probabilities that the image belongs in each category.

Building a CNN begins with specifying the model type. In our case, I'll use a Sequential model, which is a linear stack of layers. I'll then add two convolutional layers. To understand convolutional layers, imagine a flashlight being shown over the top left corner of the image and slowly sliding across all the areas of the image, moving across the image in the same way your eyes move across words on a page. Convolutional layers pass a kernel (a sliding window) over the image and perform element-wise matrix multiplication between the kernel values and the pixel values in the image.

However, those are not the only layers that we need to perform our task. A complete neural network architecture will have a number of other layers that are designed to play a specific role in the overall functioning of the network. Much deep learning research is about how to structure these layers into coherent systems.

We'll add the following layers:

*     MaxPooling. This passes a (4, 4) moving window over the image and downscales the image by outputting the maximum value within the window.
*     Dropout. This prevents the model from overfitting, i.e. perfectly remembering each image, by randomly setting a percentage of the input units to 0 at each update during training.
*     Conv2D. Add further convolutional layers since deeper models, i.e. models with more convolutional layers, are better able to learn features from images.
*     Flatten. As its name suggests, this flattens the output from the convolutional part of the CNN into a one-dimensional feature vector which can be passed into the following fully connected layers.
*     Dense. Fully connected layer where every input is connected to every output.

<img src=""https://i.imgur.com/6Crxeo5.png"" alt=""Fully connected layer vs Convolutional Layer"" >

To take a look at how it all stacks up, we'll print the model summary. Notice that our model has a whopping `1,660,253` paramaters. These are the different weights that the model learns through training and what are used to generate predictions on a new image.",5053a192,0.5945945945945946
27558,a6c34cd514e30e,89e22aab,Correlation matrix:,bf603ddd,0.5945945945945946
27561,e4525eb0c96f28,0f082e7b,"### New consoles are released regularly, but what about PC?
I assume from looking at the previous plot that console excitement falls off fast with each new release - soon after Switch came PS5 and XBox Series X. However, PCs are a constant Platform that remains throughout the years and only improves over time. Do PC sales grow or remain constant over time?

As shown below even PC has a negative slope. However, it can be seen that the density of PC sales over time grows.",2093a1f1,0.5945945945945946
27567,993966a1cb5eb1,b4242fbf,## Loss Function,5cd181e2,0.5945945945945946
27570,917957c6c4065f,1cc4eed9,"comment_count는 평균 1,250, 범위는 0 ~ 905,925  
상위 2개의 항목은, 조회수 1~3위에 해당하는 동영상이네요.",55b8ed68,0.5947712418300654
27575,e9b9663777db82,def62999,#### 11.Quantitative analysis for m²_gross feature: Find Irregular Data (Outliers),648e8507,0.5950413223140496
27576,6a80f915608fc2,6874cedf,### Which c- features have the most variation across the treatment data?,636938eb,0.5952380952380952
27577,916ccf243827f1,61028ae7,## 9. Cost Function,5147f4d2,0.5952380952380952
27582,2d40f383473fa4,13221c15,And the new Datasets are here.,1da1eff0,0.5952380952380952
27583,a76e0e8770b7a0,e85038b7,IRAq and Pakistan is sad,02863d3b,0.5952380952380952
27585,27778055896e17,1b752bcc,# Ranom Forest Classifier,1dbe0165,0.5952380952380952
27586,1084376bc4897c,d84ba3e4,## 4.2 Prepare the train and test sets,1b598487,0.5952380952380952
27594,066c5ee1ef39e6,c1ba031a,### Toxic Clean Data TD-IDF + Ridge,0f394e1b,0.5952380952380952
27595,87e94f864d74be,e945495c,>  Nearly 2/3rd of the content on netflix are movies while the rest are TV Show,294bfe9f,0.5952380952380952
27599,e67925694c07d3,18684196,### categorical column distribution,83af4c4a,0.5955056179775281
27604,fc8e0042411c46,4b55a102,## Digital Advertisement,af476c2a,0.5956112852664577
27606,b61ab8f81dc03d,6fd88a9c,"Let's transform the ""Fare"" column from float type into integer. ",64d05394,0.5957446808510638
27610,73893f0467d5e3,b63e5f4c,## Variance Threshold,279787c6,0.5957446808510638
27611,3f25b363afec54,f0e7980f,Now let us see the distribution just like train set,bbdaae25,0.5957446808510638
27612,5f674175839b32,7f4bdb2a,*From the above graph it is clear that the maximum sales has happened around 2007 to 2009.*,53a2e343,0.5957446808510638
27613,957e035ba5b9d5,8fd669bf,# train_test_split approach,778ab3d3,0.5957446808510638
27623,95efc1ad1d3e26,ea5f3e08,## Time to make fun!,79de1120,0.5961538461538461
27625,6f1481148352e9,851d2443,"* The number of months studied - 239;
* There is a slight displacement of the data to the right, which is caused by the outlier. The largest number of fires by month and year ~ in the range 1.2k - 4.3k;
* Average number of fires per month - 2.9k;
* Minimum number of fires - 0;
* The maximum number of fires - 7.;
* We also observe one pronounced outburst.",7cfbdb8f,0.5961538461538461
27628,5ce12be6e7b90e,446d1318,"Indexes work from the tail as well, using negative indices:",c0ab62dd,0.5964912280701754
27634,e03eb63c1f725d,9ee3ee7c,"<a id=""8""></a>
<font size=""+2"" color=""blue""><b>TfidfVectorizer</b> </font><br>",e204b7e3,0.5964912280701754
27639,3fb15e6e48aec2,9bbd8411,"# CountVectorizer - Name
* run on Surname and create a dense matrix and concat to df",9d1f4358,0.5964912280701754
27640,fe7360cddc13e5,7586cf72,## Eksik Veri,8979e423,0.5964912280701754
27641,30fdc4a6e3c1db,00d8a9d9,### Plotting sales time series accross categories,6111ddee,0.5964912280701754
27644,b10bd75889dad9,53d897d1,#### Lets build a Random Forest Model with Hyper-parameter tuning with the above reduced train data,ee00ceee,0.5966666666666667
27649,9ceb7278784462,4d17e3df, ## <a id='19'> 15.MLP Regressor</a>,3768a567,0.5967741935483871
27650,1a222fee3089d2,8de5662f,# Selecting the model,59ab8894,0.5970149253731343
27655,a4aa36df07fd53,7ccc22a2,Bagaimana dengan pelajar? Dimana posisi mereka?,d2f42b6d,0.5970149253731343
27657,2ada0305b68956,2debf952,### 101. Palette = 'gist_gray',133e26f4,0.5971428571428572
27664,c01049afb6d307,54aedb7e,"
* There is only one person with an education level of four. Apart from that, education level is inversely proportional to Socialdrinker and Bodymassindex.",d37d3b5d,0.5972222222222222
27665,726833f92fb87a,3534b147,"In this section, we will perform some data preprocessing on the features. First, we will perform some transformations on skewed features.",7dc5e1b6,0.5973154362416108
27666,5f32117bcd5255,651c6a06,#### NEAR INFRARED,85882abf,0.5973154362416108
27668,4ae464582bac51,2988c501,# PRE-PRECEDING THE BASE,ca6a52ce,0.5974025974025974
27673,241cf32abb22d8,84438160,"## 1. K-Nearest Neighbors

I use grid search for hyperparameter tuning in a pipeline and train the KNN model with different k-nearest neighbors and distance types.",47157066,0.5974025974025974
27677,9c26c5dcd46a25,74fbd4f3,"#### <font color=""#114b98"" id=""section_2_2"">2.2. première régression linéaire</font>
Nous avons donc nos métriques de Baseline et nous allons à présent pouvoir réaliser notre premier modèle de régression linéaire multivariée et le comparer aux métriques de base obtenues :",1bbbb677,0.5975609756097561
27681,14defffcd250f3,9a2e950e,Now there are no missing values in any of the columns,3a683b94,0.5977011494252874
27682,b01ee6cb674fa3,ed8e55ad,"# Eurockot

Commercial spacecraft launch provider and was founded in 1995

HQ: Bremmen, Germany",a8ffd35e,0.5978260869565217
27687,7cfd96218dd933,c8acefec,#### JULY 29,7c34d96c,0.5980392156862745
27688,52cfd66e9ec908,a54a0e9a,"So here we have the ego rotations with regards to the centroids, which will be very interesting to consider. It seems like we will require to check multiple of these variables at once:",c74adcdf,0.5980392156862745
27690,71b75664517244,e9e3c37c,"## Longest Career

Longest career Manager on premier league, they just loyal.",fc905af5,0.5980392156862745
27691,842547b2def18c,722fc0bb,We can also create an artificial feature combining Pclass and Age.,b8efde6d,0.5980392156862745
27701,10c5a39a87c47e,c93e7952,## Step 12: Model Evaluation<a id='step-12'></a>,09c7337a,0.6
27703,2ef36514eab722,a46265b3,## Augmentations,759739cb,0.6
27705,72f72525e8a7fb,ec58c250,"Augmented Dickey-Fuller test (ADF test)
H0 : 'The time series is not stationary'
Ha : 'The time series is stationary'",e6380e0b,0.6
27707,f7436bc492474c,95b4c0da,"It turns out that using TF-IDF gives even better priors than the binarized features used in the paper. Its not  mentioned in any paper before as per knowledge , but it improves leaderboard score from 0.59 to 0.55.",328fd235,0.6
27709,519e936017c30a,09e7035c,"Los resultados obtenidos, nos demuestran que la mayoria de los videojuegos han sido publicados por la editorial ""Nintendo"", registrando esta compañía en el año 2006 un ingreso global de 82,74 millones de dólares por su videojuego de ""Wii Sport"", otorgando a este año el mayor nivel de ventas en la historia.

En el siguiente apartado de este estudio, selecionaremos una plataforma al azar, a la cual le realizaremos un análisis del género de videjuegos, cuáles son los ingresos de estos, y las ventas de los videojuegos de dicho género.",dc34915d,0.6
27710,83df814455f06c,31174a73,### Predict the Test set results with criterion gini index,c9cff71a,0.6
27711,55a5e31d03df9f,2b61968d,## <a name='tfhubresults'> Evaluating TF-Hub Model performance </a>,06dce00f,0.6
27713,65245c6e88a2ee,4c343ff0,## Modelling for the dataset,71d6e90e,0.6
27719,b3e48999ed0d00,c7df6d00,## Normalize data,fe9ada0f,0.6
27721,864302b10e7730,72b20020,# 3. JointPlot,e9dd1d2d,0.6
27722,cb570c7b7f0501,7a07f406,"### Research Question 5  ( is it about the day of the week !? )
Do we have a certian day in the week with a high propability of no_show !?",a200a0ec,0.6
27724,b547f0f38f7744,5a18de20,## Prepare Model,b6ba66b3,0.6
27725,169177b6e9edea,439c0a5f,<h3><b>Data Preparation for modeling,ca42152f,0.6
27727,2730840089c8eb,79bf7b54,This is getting hard to read and annoying to type. `str.format()` to the rescue.,34d27dac,0.6
27731,6e472c6c591c7d,ce0ff7c7,"### 2) Identify interesting codes to explore

The last question started by telling you to focus on rows with the code `SE.XPD.TOTL.GD.ZS`. But how would you find more interesting indicator codes to explore?

There are 1000s of codes in the dataset, so it would be time consuming to review them all. But many codes are available for only a few countries. When browsing the options for different codes, you might restrict yourself to codes that are reported by many countries.

Write a query below that selects the indicator code and indicator name for all codes with at least 175 rows in the year 2016.

Requirements:
- You should have one row for each indicator code.
- The columns in your results should be called `indicator_code`, `indicator_name`, and `num_rows`.
- Only select codes with 175 or more rows in the raw database (exactly 175 rows would be included).
- To get both the `indicator_code` and `indicator_name` in your resulting DataFrame, you need to include both in your **SELECT** statement (in addition to a **COUNT()** aggregation). This requires you to include both in your **GROUP BY** clause.
- Order from results most frequent to least frequent.",65532a3d,0.6
27732,4bada947d597ac,3060169e,# Training the model,eab5094a,0.6
27733,6e28c4f557f736,affb92a1,### Data Prep Completed HERE,021fdf75,0.6
27735,fc3adf9d45953e,bf7cc01b,#Code by Alexander Bader https://www.kaggle.com/alexanderbader/perry-knn/data,e3789b90,0.6
27736,675b60eaf415a6,6bd73e8f,### **Visualize the accuracy and loss plots**,68c0b725,0.6
27740,21c1e34efd71b8,8de0402f,"If you look at the Learning curve for Logestic Regression above.. the train and CV curve run in (almost) parallel around 90% accuracy. Which means, providing more training data would not be of any help in improving the performance of the model.",23b2cdd6,0.6
27743,9c044fa3072552,4b58b043,X-axis represents days of the week. It is evident that weekdays have more number of accidents than weekends. Let's take a closer look at how different these 2 distributions are.,1362842e,0.6
27751,b0e401bb7b5943,42659137,"## Finally, the map

And we get a copy to keep as an html file.",ed8abc16,0.6
27753,f0fab078f8533b,ad86ec34,## a. From the below plot we can see that Movies dominate,bdb5ea32,0.6
27754,38b79494ac749e,8805aac9,## Part 2: Regularization,39162a40,0.6
27756,8cd6656a65e6e7,55863bcb,"## Density Estimation using Real NVP

Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations. ",c8e1697a,0.6
27758,f50dc95483c98f,f12addda,"## **Training the Decision Tree model on the Training set**

A Decision Tree is a simple representation for classifying examples. It is a Supervised Machine Learning where the data is continuously split according to a certain parameter.
Decision Tree consists of :
* Nodes : Test for the value of a certain attribute.
* Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf.
* Leaf nodes : Terminal nodes that predict the outcome (represent class labels or class distribution)

![A Good Example for Decision Tree](https://miro.medium.com/max/282/0*ToYXqRes95eMvIKV.png)
To understand the concept of Decision Tree consider the above example. Let’s say you want to predict whether a person is fit or unfit, given their information like age, eating habits, physical activity, etc. The decision nodes are the questions like ‘What’s the age?’, ‘Does he exercise?’, ‘Does he eat a lot of pizzas’? And the leaves represent outcomes like either ‘fit’, or ‘unfit’.

#### Classification trees (Yes/No types) :
What we’ve seen above is an example of classification tree, where the outcome was a variable like ‘fit’ or ‘unfit’. Here the decision variable is Categorical/ discrete.
Such a tree is built through a process known as binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches.

For more study on decision tree Classification, we get the following [Blog to read from GeekforGeeks](https://www.geeksforgeeks.org/decision-tree/)

![Image to explain the concept of Decision Tree Classification](data:image/png;base64,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)
",cd9e9621,0.6
27762,d58491f2896fc1,e7751260,**crossover birinci ebeveyn ile ikinci ebeveyn arasında gen çaprazlanmasını sağlamaktadır**,514bfdff,0.6
27769,b7452d87e4abfe,7c474c76,"**Next, we get the players last names from the table Player. I filter out the None values (if any) from the query and add them back later to the players_names list. I try to keep the name in the same order as the other lists, so as to later map the names to the x,y coordinates**",9c75a86d,0.6
27774,f91f58d488d4af,b482458f,"**PyTorch** is able to automatically compute the derivate of nearly any function!. 


Notice the special method *requires_grad_*? That's the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value.


First, let's pick a tensor value at which we want gradients.",5df1bbf3,0.6
27780,4fd4b6a80d40e3,290e282e,"$a_{1}^{1} = w_{11}^{1}x_{1} + w_{12}^{1}x_{2} + b_{1}^{1}$

$\mathbf{A}^{(1)} = \mathbf{X}\mathbf{W}^{(1)} + \mathbf{B}^{(1)}$

$\mathbf{A}^{(1)} = (a_{1}^{(1)}, a_{2}^{(1)}, a_{3}^{(1)})$

$\mathbf{X} = (x_{1}, x_{2})$

$\mathbf{B}^{(1)} = (b_{1}^{1}, b_{2}^{1}, b_{3}^{1})$

$\mathbf{W}^{(1)} = \begin{pmatrix}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)}\\ 
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}$",f6913cc3,0.6
27786,7454fdc444df16,294e5e77,### Splitting Data to Cancerous and non-Cancerous data,a7818ef5,0.6
27794,c18267b203f28a,74afb3ea,"# Building the model
## Learning rate schedule",09ca8efb,0.6
27796,ce7abd85d777b5,2ba34a17,3. Modeling by PyCaret,0a340dbb,0.6
27803,80ecc4c67a9f54,ea819762,"## Calculating Name Normalite Score

### The more peculiar names score lower",4bbf546c,0.6
27804,c8bf959b9608cf,1d319456,"### Define Content Loss
The content loss is a L2 distance between the features of the generated image and the features of the content image. Let $P^l$ and $F^l$ be feature representation of content image and generated image in layer $l$. We then define the squared-error loss between the two feature representations as:

![content.PNG](attachment:content.PNG)",155e3672,0.6
27807,016abae0483764,8699157b,Here we can see the table is well numerical placed...,bc9f289b,0.6
27811,36c35f0a9f70f7,e65ee3c7,## Support Vector Machine,67358bc7,0.6
27816,c115e287523aab,0bc798af,"## Visualization
* Check if augmentation is working properly or not.",feb1288b,0.6
27819,a8c042af6b7245,df466a40,"As we can see from the variables with missing values, it is a good idea to keep the missing values as a separate category value, instead of replacing them by the mode for instance. The customers with a missing value appear to have a much higher (in some cases much lower) probability to ask for an insurance claim.

#### Interval variables

Checking the correlations between interval variables. A heatmap is a good way to visualize the correlation between variables. The code below is based on an example by Michael Waskom",2487ac62,0.6
27823,0687cd5c8597db,3be39c79,"<img src=""https://ars.els-cdn.com/content/image/1-s2.0-S0893608020301301-gr1.jpg"" 
     style=""display: block;margin-left:auto;margin-right:auto;width:50%;""/>",4edec76a,0.6
27824,d07915a6e6992e,6acd7215,**Creating Family Size variable using SibSp & Parch**,2b912140,0.6
27828,188731d7fa0604,63c56aa3,## 데이터 전처리,7cc543d3,0.6
27832,04bac111ffbe9c,5aeb5a47,##### Handling Pclass,82576b17,0.6
27841,bbad077c274022,a844fb0a,**Let's more fine this pattern. Are my weekdays having any impact on my step count?**,3c2e3dea,0.6
27847,09751c520b0616,9f2694ca,### Split train and test dataset,a4d0c7e9,0.6
27850,7dd46c750653eb,d3dfbd96,# **Marriage and Divorce in Moscow**,c2644713,0.6
27851,8696921d9adc93,8f93243c,**5. Label Encoding **,b8908b23,0.6
27852,91eaec994e0c6f,0b24a896,Let's plot a Sales Calendar Heatmap of the first and last years (by Categroy).,376aef10,0.6
27853,6998861ff6ff01,062b192b,"We got an error! The important part to look at here is the part at the very end that says `AttributeError: Can only use .dt accessor with datetimelike values`. We're getting this error because the dt.day() function doesn't know how to deal with a column with the dtype ""object"". Even though our dataframe has dates in it, because they haven't been parsed we can't interact with them in a useful way.

Luckily, we have a column that we parsed earlier , and that lets us get the day of the month out no problem:",ea9e72cf,0.6
27856,5ffe6aa38958a1,993089cf,"### 2.3.5 Fill missing values

Age: Mean(age)
",11f5412e,0.6
27857,f89f8540df580e,a00d3b1b,"# Predicting Mask
#### The one with the highest score is considered the best response to the **< mask >**",83579ee7,0.6
27858,a4f8ad33c823c5,0b2ba257,"### Summary of the missing data values
Selecting cohort patients who age is greater than 16.",fcd48307,0.6
27867,63d0d9b9a8c7d2,7a88dc15,***Decision Tree Classifier***,e32e5933,0.6
27877,c73e07ad6d25c5,24728c38,## Parch,3ab391fb,0.6
27886,5626e84c4e6bf8,e122239c,"## Visualizing patterns 
*winner_map* contains dominant features/patterns generated by neurons. Some of these features if visualized look like amalgamation of different kinds of clothes.
Like a generated feature may generate images that look like the combination of shirts, coats and pullovers. This may help in clustering",e2ecb669,0.6
27887,e0e19e91579432,7e47425e,# For hyper parameter,0c8a0755,0.6
27888,1011899b959f44,c99f9d3b,"# Sort
Now that we are able to Select and Summarize, we can add Sort as our step 3. After examining average deaths and deaths per year, let's say we want to find the year with the highest number of battles and the year with the lowest.

Sort methods in Pandas include:
* .sort_index() and sort_values(). - Displays values in the table in an order from highest to lowest or vice versa
* .max() and .min() - returning the maximum/minimum value
* .idxmax() and .idxmin() - returning the index of the maximum/minimum value
",0b112382,0.6
27891,9276fa5cc2fef6,7f75fd6c," ## Oh my! A Bad Word

We found a significant bad words in the insincere comments. Here I am using few profanity words list Let's device and see count for each and ratio features for the same. ",24aa6a52,0.6
27894,edc19e349fe80a,2124c17d,### 2. Read test data:,7882221a,0.6
27896,ad121e0531afa4,6c3b6f38,<h1 align='center' style='color: #8532a8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px'>3. Augmentations</h1>,a3492905,0.6
27900,cee088a6840708,b6064606,"# Step 8 -> Define the layers you will need

Regardless of what I am going to do , I will need to define my own layers (Although the GraphConv layer is predefined in dgl, I prefered to write it on my own).
Ignore normalization and baises for now.

To simplify,

1. Remember the most simple classification problem you ever learnt.
2. You have some users for each user you know [age, gender, height, weight, education, etc] and you want to predict his salary.
3. Another example, Imaging some users on twitter writing tweets and you would like to predict if they are happy or not.

4. Imagine the these users are connected in a network
5. Forget about the network and think about the users
6. Remember the very simple dense layer that you always do for any classification problem 
7. You will have input_features (age, gender, height,education) and output_features (salary)
8. Make a simple dense layer that take input_features of each node  and output the output_features

All the above is the same for any network

9. But, 
10. But, this is a network
11. In short, exchange the features between the nodes
Calculate the output features of each node, and, and, and, give each node the summation of the output features of all nodes around it.

        graph.ndata['h'] = feat
        graph.update_all(dgl.function.copy_src(src='h', out='m'),
                             dgl.function.sum(msg='m', out='h'))
        rst = graph.ndata['h']

",55463e1c,0.6
27901,5be39e4e35cec7,cd8450e0,"<a id = ""7""></a><br>
# Outlier Detection ",14d617c9,0.6
27905,917957c6c4065f,85c8c1a0,### 2.5. tag_count,55b8ed68,0.6013071895424836
27906,63b44c85e32c1f,117b586a,"Here, We have declared a list, lista = [2,1,4,3]. This list is copied to listb by assigning it's value and it get's copied as seen. Now we perform some random operations on lista.",fb9b9562,0.6013513513513513
27907,eda49464dd6d1b,27d6fc80,"### The model shows a probability distribution for all customers.  About half of customers are clustered around 0, and most of the rest are clustered around 0.3.  Since all of the probabilities assigned are <0.5, the ""predict"" function predicted 0 for everybody.
* Even if a customer has every trait to maximize his or her probability of buying vehicle insurance, they still have <50% chance of buying it.
* By listing each customer in order of their respective probability, the business can define a cutoff point beyond which they will decide it is not worth the cost to target those customers to offer them vehicle insurance.",8421f81f,0.6013986013986014
27912,0932046e1f485d,1f0b8780,Most frequent genre combinations.,218cc7a3,0.6015625
27913,e19e307b3fd188,8613bc1a,"Maximum rental prices do not usually exceed **15.000**, but with outliers they reach **40.000**.",2173955b,0.6016260162601627
27916,ac1abfe1dfe815,cc258841,## Neutural Words,6529dbcb,0.6017699115044248
27917,a5a419dc7245b0,e81d3943,### Modeling ,4279726e,0.6017699115044248
27920,ab6da5994949a3,9718f7d8,## Visualising the SVC Test set results,fae6b91d,0.6018518518518519
27921,2f0f808765fc67,daf88893,# **Cross Validation Lasso**,fd1f6494,0.6018518518518519
27930,73d8e56bc709b1,7d6f191b,"> Position ST: Acceleration, Jumping, Finishing, Agility, Balance, BallControl [3]",78ec3cce,0.6022727272727273
27933,835a7b4e660d23,8a835d18,### Vısual Exploratory Data Analysis,53bc7a6e,0.6024096385542169
27942,3d08ca7656dec0,6bd33c0a,# Preprocessing data for ML,bd3f87e3,0.6027397260273972
27943,738bfced935b69,b01ee932,## Data Modeling,2d3c592d,0.6027397260273972
27945,1eb62c5782f2d7,d23f1805,# Normal Distributions: Mencari raw value,bb69f147,0.6027397260273972
27949,2ada0305b68956,fef0194d,### 102. Palette = 'gist_gray_r',133e26f4,0.6028571428571429
27956,7f74a04ae75792,5c399d9a,### Explore the Relationship between Numerical Variables & Target Variable,d01e91da,0.6029411764705882
27958,eb0ecd6bebeb15,263e64fd,"Bir önceki hücrede yapmış olduğumuz görselleştirmeye kind = ""kde"" parametresini ekleyelim. Böylelikle dağılımın noktalı gösterimden çıkıp yoğunluk odaklı bir görselleştirmeye dönüştüğünü görmüş olacağız.",d7b93a60,0.6029411764705882
27964,06ecf7a304c309,c0b14d99,모델 정보는 다음과 같습니다.,714de627,0.6031746031746031
27967,2f47abddfd1928,92cfac9a,"I have removed some outliers, mainly in fare feature to get a better visualization. Otherwise a few points makes difficult to see actual distribution.

Comparing Age and fare it is quite difficult to find some insight.

We can appreciate a tendency of the increase of fare with the age. Which makes sense because the older the ""higher"" you can get social-economically.

The age feature follows a normal distribution centered in around 25-30 which also is logical and confirms previous comments.",ae33cc0b,0.6033057851239669
27970,434f930cb58aee,3eac5c2b,"While doing some research, I found out the tensorflow provides a built-in F-score metric in [tensorflow-addons](ttps://www.tensorflow.org/addons) module. Therefore I decided to use that one instead of the custom metric we defined earlier. tesnorflow-addons provides really helpful metric and loss functions. I am also planning to use loss function related to f-score. May be afterwards I will use it.",0e1d3554,0.603448275862069
27971,1dd9c6aa74d289,e7703785,"Acatually, the distributions would be better described by asymmetric Gussian (skewed on the left). So I will seek Cumulative Distribution Functions to tell me more.",5ef9a1be,0.603448275862069
27972,1cd8be6e679620,28b005f0,### Example-1 (from Train data),3ce15a43,0.603448275862069
27974,00001756c60be8,3eebeb87,Значения меньше 1 и больше 250 отсекаем,945aea18,0.603448275862069
27981,84127ade6fde87,a5242d16,![image.png](attachment:image.png),f55d05b6,0.603448275862069
27982,f3c6048d1058e3,013f2c2a,"### **2) Count Vectorizer-** 
- It is tool provided by the scikit-learn library in Python. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.",1d9056b0,0.603448275862069
27987,510b8303776bb6,708ac5af,## 1. Training set,18080db8,0.6037735849056604
27989,614ba9f0c62677,4caca3f8,"<a id=""12""></a>
### Define Optimizer   
* Adam optimizer: Change the learning rate",b8551335,0.6037735849056604
27992,f015d0147e8fbf,a5c4d9b6,## II. Training & Cross-Validation,518954fb,0.6037735849056604
27993,726833f92fb87a,268156d4,# Categorical Features encoding,7dc5e1b6,0.6040268456375839
27998,8c7e00ca3dc5a7,0154fedf,## Categorical Data Transformation,c83346e4,0.6041666666666666
28000,eb0854a6601407,035497fb,"By jointly plotting the distribution of observartions by asset and the mean target value by asset, we may notice that the target value slightly reduces proportionally to the number of observation. The dispersion of values tends to grow with less observations, hence we need to re-plot the scatterplot this time using the standard deviation.",6d107747,0.6041666666666666
28003,2a377ced98d67a,9514ea04,### 3.6. Correlation among features,262231a8,0.6041666666666666
28008,5f4ae633cfd090,f1af1129,Orthodox is the superior stance followed by Southpaw and then Switch,a30a16e2,0.6043956043956044
28012,8539260444e6b5,df6e722b,# Applying logistic regression,0369463f,0.6046511627906976
28013,2e40928927c0d4,1767c32e,"**Image Data Generator**: with Image Data Generator we can use Model.fit_generator() instead of Model.fit(). The 1st one exploits multiprocessing in python, while the 2nd one does not.",b6385ef2,0.6046511627906976
28015,22bd95f4807a23,6ce2d859,* Individuals from the ages 30 to 35 had a large amount of variation in the ratings they gave.,c05d356f,0.6046511627906976
28016,c09fac3c943d51,9a32f7eb,# Encoding features,678d076d,0.6046511627906976
28022,4883314a96dc34,056014e0,Performance metrics on ***train set***:,50d36836,0.6049382716049383
28026,4ae6a182abac64,b4d124d1,- The next option is to cerate IsAlone feature to check wheter a person traveling alolne is more likely to survived or died,418676c5,0.6050420168067226
28027,b01ee6cb674fa3,625a8a66,"# Land Launch

It's a service product of Sea Launch, a multinational company (US, Russia, Norway, Ukraine) 

HQ located @ Nyon, Switzerland",a8ffd35e,0.605072463768116
28036,52ee792e228d54,e9c9548c,"### Observations from the above heatmap:
#### &emsp;&emsp;-> Age and Experience are highly correlated(0.99)
#### &emsp;&emsp;-> CCAVg and Income are correlated (we saw this earlier too).
#### &emsp;&emsp;-> Personal loan is correlated to Income.",5096094e,0.6052631578947368
28038,f05342aabe2b59,91b5f904,## Run the simulation,cfbb391f,0.6052631578947368
28041,d369f200a84c2a,da4c3d43,# Decoder,8fef4d48,0.6052631578947368
28042,31b564f11ef638,8a37fc95,### Linear regression model,424f9692,0.6052631578947368
28043,bd380b97b5c894,d18d5d36,## Scaling,66f2562a,0.6055045871559633
28045,d8ff894670d506,96606b96,***A linear model is used to show the relationship between projects and donations.***,eb0fb7de,0.6056338028169014
28048,631cd434fc3aa2,6949f6ae,"* _Functional_: documentation says ""_Assume typical unless deductions are warranted_"", so we fill the NaN with typical (_Typ_).",2b74febb,0.6056338028169014
28049,bddd799cdbbae8,ed88463c, # <a id='7'> 7. Classification</a>,b44e3c08,0.6056338028169014
28050,3d77c1560bd16e,1ccb12cf,> No new allocations of Johnson & Johnson vaccines since the week of 12-Apr-2021,87c141ca,0.6056338028169014
28057,3c2033cc99c12c,ec4831eb,#### Model Establishment ,dfa22a54,0.6058394160583942
28058,4b64dc653fb7eb,2e7f36eb,"<a id=""6""> Step 6 - Remove Stop Words </a>

Stop words are the most common words in a language and mostly filtered in NLP problems.",57675cc2,0.6060606060606061
28062,dbd96dd275dc60,6ad53c1d,"### Building an evalluation function
",1ed493a8,0.6060606060606061
28064,adf419444a59df,c5e484e0,"What we have here is a bit long loss function but we can understand it by diving it into smaller pieces.

In the first line we see the loss for midpoints. Lets just ignore lambda values for now (Lambda_coord , Lambda_noobj). 
We see a basic mse loss with the indicator function in front of it. In simple words it says; sum up all the distances between the target and prediction midpoints for all grids iff grid i contains an object and bounding box j is ""responsible"" for outputing that object. Now here responsible means that the bounding box with the highest IOU(intersection over union) score with the target box. So which means we ignore the grid i that does not contain an object and also the bounding boxes that are not responsible. In this way we just penalize the model for the distance for true labels. 

In the second line everyhing is the same except now we calculate the distances for width and height. The reason we use sqrt here is we dont want to penalize the model for the difference of w and h for large objects. And the rest is the same for indcator and bounding boxes.

In the third line we calculate the confidance score loss which means if there was an object at grid i and responsible bounding box had a confidence score for that object, how bad our prediction is? And we still just calculate it for responsible bounding boxes and True labeled grids.


The fourth part that is the inverse of part 3 we calculate the same thing for no object at grid i and the responsible bounding box j.

And lastly, we calculate the differences of probabilities of class predictions and ground truth labels if the grid i contains an object. 


The lamdas here helps us to modify the loss we set Lambda_noobj to 0.5 and Lambda_coord to 5 here what we do is, we are pushing the model for better bounding box predictions and we give some room to model for wrong object predictions.",3a275e7f,0.6060606060606061
28065,2f964d08c25d93,0d7eb0da,Distribution graphs (histogram/bar graph) of sampled columns:,1f2e4468,0.6060606060606061
28067,efd44ce2c08541,3445c06c,# Sample weighting according to label group size,ebc2d00c,0.6060606060606061
28074,f6648e47713411,9a46a1e7,"## 2.3 Augumentation
Nhận thấy chúng ta có khá ít dữ liệu, ...",f4af4d1c,0.6063829787234043
28075,4c47839b067546,ca9f6161,### vehicleTransmission,1f517b02,0.6063829787234043
28080,979f1e99f1b309,eec7bdc5,## Try Linear Regression Model,d1bfebbf,0.6065573770491803
28085,2bd6c370695ea7,1e222ac3,## Trainer,cbe6aec8,0.6066666666666667
28086,5f27526aa6c113,9f8060ee,"saturdays and sundays have least accidents, one reason could be that these are common holidays so no office",a5c26ab6,0.6067415730337079
28088,04ff2af52f147b,2a23ed4d,"We can see that title provides us with useful information as the likelihood of survival is drastically different between titles.  As a result, we will later use ordinal encoding to encapsulate the inherent hierarchy in likelihood of survival.",d5f37be9,0.6067415730337079
28091,cddd881ce4b8ef,53b81bf7,"titanic[""cat_Age""] = pd.cut(titanic[""Age""],bins=[0,6,18,60,80], labels=[1,2,3,4])",5294325c,0.6068376068376068
28096,98fd05fcc5c3e3,5c32f34e,## Feature Scaling,55fe7ece,0.6071428571428571
28098,c65f7b375af4ef,1783b80d,"# DİKKAT !!!!!!
yaptığınız işlemleri her zaman kontrol edin !! 
yaptığınız hesaplama da aykırı veriler var mı diye 
bu projelerde en önemli adım  
## veri önişlemedir
yaptığınız programınız doğruluk değerini düşürüz daha da kötüsü yanlış sonuç bile döndürebilir size
",871d53ca,0.6071428571428571
28099,f4514ec092a771,f104566e,"The detail of how to run the script is in the README of Github repository, which I gave on the top of this kernel.  
I just want to note that I'm using HMM + GMM and train with triphone.  
The result of validation set is relatively **5% WER** (Word Error Average) after finish the training.",3739ab1e,0.6071428571428571
28101,173fe4d95da62a,f00259f8,"# Model building

1. Naive Bayes - 69%
2. Logistic Regression - 82%  
3. K Nearest Neighbors - 83%
4. Random Forest - 83%
5. Support Vector Classifier - 83%
6. XGB Classifier - 85%",dd38adb8,0.6071428571428571
28107,0dd3ac2d55efd7,79099306,"1. If you want to know more about [tensorflow's tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer). 
2. pad_sequences is used to convert each sentence to the same length specified either by padding 0s pre sentence or post sentence till max length is achieved or by truncating if sentence is of greater length than max length.",e9aa2cc2,0.6071428571428571
28114,b7298d6aaff625,262175f3,## Building Model ,bdf24bf7,0.6071428571428571
28115,92e9fc3a0ff5c0,695c8d9d,## **Sentiment Polarity on Biden's dataset**,d53da425,0.6071428571428571
28117,912bb73358069c,f3a79e25,### Keras loss fuction returns a list instead of a number. So how is val_loss calculated in keras training? I guess it's the average of the loss list.,0cf9db82,0.6071428571428571
28122,8dd655515e7d18,0e6e512b,## Univariate Analysis - Categorical Column,895f41cf,0.6071428571428571
28123,565ad413cd802f,906b8d7c,"If your kernel runs out of memory here, you might need to reduce your batch size.",397b074e,0.6071428571428571
28126,5d2a3e82679cf3,0be00efe,# PREPROCESSING IS FINISHED HERE.,9e60b1e3,0.6075949367088608
28128,09751c520b0616,fd1e28d3,- Concat Numerical and Categorical dataset,a4d0c7e9,0.6076923076923076
28134,523123dad03177,b7ed14d5,Girls are better at reading than boys.,48a5e4e6,0.6078431372549019
28136,d0080e3a39bc5c,deb72207,"
* First we trained the model with all the **ImageNet** pretrained layers of ResNet having **fixed weights**, i.e, we only **tuned the last layers** in the above training part.


* Now, we can proceed to ""Unfreezing"" the inner layers of the architecture and fine-tuning them for our specific cause - Classifying Ships from each other.",2fcde4cf,0.6078431372549019
28139,52cfd66e9ec908,20729d57,### ego_rotatations,c74adcdf,0.6078431372549019
28141,32ddc45133f77b,1d470861,**Predicting the test data**,3c0d6831,0.6078431372549019
28142,4fa553c2b837d4,558c3332,## Soln 3 : An Extension To Imputation,c65a23e9,0.6078431372549019
28150,27d5291d6365ba,e6d717a3,# Debit-Credit Transaction Mean analysis By Age,96b30229,0.6081081081081081
28151,fc8e0042411c46,aefe4439,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.6081504702194357
28152,5ce12be6e7b90e,38a3e19e,We can get a range of indexes using _\[start:end\]_,c0ab62dd,0.6081871345029239
28153,30fdc4a6e3c1db,9858e784,"What we see:
* Food sales have always remained much higher than household and hobbies
* Hobbies show a preety much flat trend and less seasonality
* Food show the highest seasonality and household tend the show the highest increase in sales overall",6111ddee,0.6081871345029239
28155,8ec771f5600a61,a4528046,# using linear regression,48364c1f,0.6082474226804123
28157,225b4fe5d3894a,2d29b43c,"<a id=""7""></a>
## 7. Select and Train a Model",4b4197b3,0.6082474226804123
28158,37b09262279764,bb0f6891,#### LGBMClassifier,37c4c417,0.6083333333333333
28159,62487bcd70b199,747aa668,## <a id='6.5.'>6.5. RadiusNeighborsClassifier Model</a>,f6ae50af,0.6083333333333333
28160,eda49464dd6d1b,3c0a86b8,## Classification Report ,8421f81f,0.6083916083916084
28161,2ada0305b68956,f7f339e4,### 103. Palette = 'gist_heat',133e26f4,0.6085714285714285
28168,0e2a23fbe41ca9,b593cf82,"Observations:
- Quantity of active months within last 3 months, is 3 in 99.5% of data meaning, they were active during all the 3 months",64e4762c,0.6086956521739131
28169,77f958b3f41a70,ae35a0b8,# Information sharing and inter-sectoral collaboration,2ad9bb69,0.6086956521739131
28173,598b6228760590,8b06e4a5,# Modeling,be30ab66,0.6086956521739131
28179,8336d84cf3ff6b,c5943184,## Random Forest Baseline ,b96b58a0,0.6086956521739131
28183,90ead00a8ee283,4abbec7b,"**Exercise 5**: Read the following `xls` sheet into a `DataFrame`: 

![](https://i.imgur.com/QZJBIBF.png)

The filepath to the XLS file is `../input/publicassistance/xls_files_all/WICAgencies2014ytd.xls`.

Hint: the name of the method you need inclues the word `excel`. The name of the sheet is `Pregnant Women Participating`. Don't do any cleanup.",612efa48,0.6086956521739131
28193,548f961125248d,af585285,"## Model Validation <a class=""anchor"" id=""seventh""></a>",d8c5e8b8,0.6086956521739131
28197,7e89d387feb9f5,10ceb93c,### Добавленный числовой признак №13. Присутствует ли среди кухонь ресторана кухня присущая только ему.,989e3a1b,0.6086956521739131
28200,ba4b3bd184acbb,60161565,"Now we have two DataFrames: 

- apps which only has null values in the Ratings column
- apps_with_rating which has no null values",0f5de724,0.6090225563909775
28202,ee9ddc756b2d4a,ac7ce45e,## Modello 1: uso le features che ho creato con efficientnet e uso svm,e367eab3,0.6091954022988506
28203,14defffcd250f3,0d655f7e,# Categorical Features,3a683b94,0.6091954022988506
28207,ff3a8ce61fab6a,9e1e6b05,"
<p>
    Let's talk about <b>multiply</b> function We can use it to  <b>Number multiplication.</b>
</p>

## Number multiplication<hr>

### Exampel 1 ",9afe1654,0.609375
28217,62582b8036fbfe,60575c61,### Fitting the model,6c2160db,0.6097560975609756
28224,8d70dcae7f40a3,cefaa614,### **Đánh giá cụ thể hơn trên Confusion Matrix**,472c71ce,0.6097560975609756
28225,8cefb86a675e5d,ec0c98ab,**Performing Prediction using Linear Regression Model**,79f9e69b,0.6097560975609756
28226,786475feda0190,05c36e3f,### Reading Audio Samples.,e4663d97,0.6097560975609756
28228,e19e307b3fd188,b1b97db7,### Select quantiles,2173955b,0.6097560975609756
28231,74a03887600114,a0f70c2c,Now we will going to make a datafame in which we will have rating and number of ratings column,c0ffb2f0,0.6097560975609756
28233,56785caebaa256,fa11654b,"## 5.1.1. Model training, forecasting and evaluation<a class=""anchor"" id=""5.1.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.6099290780141844
28235,4cd25e50c7e007,4adab96d,"## Building our model
##### This time, we will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)",ceb0c525,0.61
28240,dac3c8204a2d1b,e1fb61fa,We Extract top 10 books with based upon reviews,b0d2d0dc,0.6101694915254238
28249,1294fb4c86f993,15af3f85,Similare peak could be found in 2005-2006 during hurrican Katrina. another peak can be found at the year end of 2012 and can be correlated to <b>the Sandy Hook school shooting</b> __[see here](https://www.cbs19.tv/article/news/local/national-study-shows-gun-sales-on-the-rise-in-the-us-in-texas/503-0f2b622b-1636-4fc2-83e1-d042c1b459a5)__,4471e513,0.6101694915254238
28252,7f74a04ae75792,fcfc8b21,"The “target variable” is the variable whose values are to be modeled and predicted by other variables.
https://www.dtreg.com/solution/classes-and-types-of-variables#:~:text=Target%20variable%20%2D%2D%20The%20%E2%80%9Ctarget,in%20a%20decision%20tree%20analysis.",d01e91da,0.6102941176470589
28261,c13f73168789c2,31b4ab85,"### 1.2 Slice columns by label<a id='23'></a>
Syntax : `df.loc[:, 'starting_column_name' : 'ending_column_name']`",16175052,0.6103896103896104
28265,a5a419dc7245b0,0870207b,"#### Phase1: Data Preprocessing, Data Balancing",4279726e,0.6106194690265486
28266,ac1abfe1dfe815,53faa5eb,"Some of the words in the neutural are please, help, need, fleet, time, thank, tomorrow, today, etc.",6529dbcb,0.6106194690265486
28268,ee23a565163388,177fe7e7,"**Standardization** is a technique which is aimed at scaling the values in such a way that the standard deviation is around 1. When this is performed over a dataframe, all the features would be rescaled with an unit standard deviation.",88aacbc4,0.6106870229007634
28270,726833f92fb87a,205ba7bc,"First, we encode the target variable.<br>
We note that the target column presents just two possible values: 'yes' or 'no'. Since these binary values appears in more columns, we encode all the 'yes' and 'no' in the whole dataframe with 0 and 1.",7dc5e1b6,0.610738255033557
28272,a4a494c667c673,56f6a034,# Building Pipeline,397d12f8,0.6111111111111112
28276,3597174a998d4d,ed95dd2b,So I construct a new variable named new_previous_cancellations.,276892ed,0.6111111111111112
28279,cf39cde80e66b7,1b44b48b,"# <div class=""h3"">MAPE: Mean absolute percentage error</div>
<a id=""m4""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.6111111111111112
28281,892be0a523578c,9bffc6a8,"It turns out that we only need 2 components to explain the daily activity features. When the cluster number is between 2 and 5, the scores don't vary too much. Considering too much clusters is not good to precision marketing, I choose to use 3 clusters. ",b0e8d7c0,0.6111111111111112
28283,d6cbd7160961dc,a74788ee,## 5.3.1. Results: First Digit,36d74664,0.6111111111111112
28291,cb4ad8ed4cb300,76c6ed15,"# 5. Define Function, which Answers Questions",7c0f3236,0.6111111111111112
28293,e2a94f078e1161,90a6dc97,Result:- The Hypothesis was incorrect Liberal and Lang Studies had less Discussions than,5fc53059,0.6111111111111112
28302,69ac33d79f5130,6869c848,"### High percent of accident happen between 6 am to 10 am.
### Next from 3 pm to 6pm",9d760d2a,0.6111111111111112
28303,dd3721cb49c1fd,4a54e41b,"<a id='8'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">8. <b>Listing the different evaluation metrics</b></h1>
  </div>

</div>",1a53fdd9,0.6111111111111112
28305,c01049afb6d307,09dc4207,## Socialdrinker -- Bodymassindex,d37d3b5d,0.6111111111111112
28308,268a610bbc64b4,2ee73607,"The definition of advance is not given. I am assuming month of booking prior to the departure month would be definition of advance. Also, the metric for which the trend is to be observed is not given. I am assuming #booking made in N months in advance.

Since we have to figure out the trend, so filtering only reqd data.",8a16f301,0.6111111111111112
28310,3ac432b2cac29c,abe885da,Inspect your predictions and actual values from validation data.,a358669e,0.6111111111111112
28311,c9dc8d00773da4,4380bde5,# Remove image_id.,d9aa2f85,0.6111111111111112
28314,fbb1f9d3818830,6c550a0b,# Utility Functions,c7027f86,0.6111111111111112
28316,166a62ebb4fc3a,7fed9bf0,"Lastly, also drop the id column",db48a079,0.6111111111111112
28324,b0c2805cd5c087,07e00bb5,Image semanticscholar.org,0446f327,0.6111111111111112
28326,caee5b3bdf65c1,9fc6b1da,## Test multiple splits to avoid overfitting,a46111dd,0.6111111111111112
28327,df2a7968c08ee4,68f6122e,"### Instantiate Model, Loss, Optimizer, Variables

In the following cell I instantiate the model, loss function, optimizer, varaiables, and lists for the upcoming model training.

",a2ba0a72,0.6111111111111112
28328,e0f03003a69819,991231c2,"# Conclusion :

* Heat map shows as the correlation (i.e proprtionality in layman terms) in this the heatmap 1 , shows us the correlation between attributes between attributes, as its a simple dataset , the results were already quite expected.

* In the second heatmap we took the first 4 elements (vary the values of k and you will be able to see the relation between the attributes and charges column)

* You can also check the multi-collinearity of the attributes. (we will see that in later tries). ",609ad1f4,0.6111111111111112
28329,593d1d3d1df05a,090f54e5,"### Took these too from my other project. It is to save and load a pretrained model. So, that we don't have to re train the model again and again",bc682ffe,0.6111111111111112
28335,4ba67fa2de9e6e,91516d99,### Checking some images from different sources:,8e0dd482,0.6111111111111112
28336,fc8e0042411c46,1966e49c,## Through Recommendations,af476c2a,0.6112852664576802
28339,a566b5b7c374e7,d4d26300,### Start Time,b3dc5545,0.6115107913669064
28343,98a6794067932a,15937ea2,"La suite des quatre cellules de code suivantes a été créée de la même manière pour chacune des cellules, cependant elles permettent de représenter des informations différentes. De manière générale, ces quatre tableaux permettent de représenter sous forme de pourcentage la proportion des expéditions qui sont effectuées pour chacune des catégories de produits dans un premier temps et pour chacune des sous-catégories de produits dans un deuxième temps et ce, pour une région précise. Le premier tableau présente l'information pour la région ""Central"", le deuxième tableau présente l'information pour la région ""East"", le troisième tableau présente l'information pour la région ""South"" et finalement le quatrième tableau présente l'information pour la région ""West"". Tout d'abord, le code permet de sélectionner seulement le segment de client désiré. Ensuite, une colonne représentant le pourcentage des expéditions selon la catégorie de produit par rapport au total des expéditions de cette région est ajoutée. Finalement, le code suivant permet de trier les pourcentages sous forme décroissante. Ces trois étapes sont par la suite répétées pour les sous-catégories de produits de cette même région. Ces tableaux permettront donc aux dirigeants d'observer le comportement des différentes régions face aux catégories et mêmes sous-catégories de produits qu'ils favorisent en fonction des différentes régions visées par les centres de distribution.",08600fe2,0.6116504854368932
28344,a2176d4653ef60,32f94fa2,# Data Modeling - SVC,ac908675,0.6116504854368932
28352,21413205980558,9e330a9e,"# It can be seen that people after the age of 20 are happy to have deposit, while the younger people under 20 are more likely to have no deposit.
# 可以看出20岁之后的人乐于拥有存款，而20岁以下年轻人则更大多数是没有存款的。",84197de0,0.6119402985074627
28353,1a222fee3089d2,4c66054d,"
    Gaussian Naive Bayes
    Logistic Regression
    Support Vector Machines
    k-Nearest Neighbors
    AdaBoost Classifier
    Random Forest Classifier
    Decision Tree Classifier
    Gradient Boosting Classifier
    Extreme Gradient Boosting Machine
    Light Gradient Boosting Machine
",59ab8894,0.6119402985074627
28358,12f4d16fc21645,8006e23b,<h1 style='color:blue'>Declare independent and target variables</h1>,c7752038,0.6122448979591837
28364,f35bf4df70d310,c1ef60d5,"#### Prepare the data

Let's try to reduce the data dimension while retaining **85%, 90%, 95%, and 99%** of the variance",10bb859a,0.6122448979591837
28366,b01ee6cb674fa3,74e12446,"# China Aerospace Science and Industry Corporation - CASIC

Public owned company

Founded by july, 1999",a8ffd35e,0.6123188405797102
28369,3dd4294f903768,3dce81c9,We can see that most of the restaurants have aggregate rating between 3 to 4.,0d89d098,0.6125
28371,098fedfcd07456,6b6d8e7e,"# Building the Fully Connected Neural Network . 
1. The First Layer is sequential layer 
1. Filter is no of Filter using for every layer different or same filter can be used . 
1. Here filter size is choosen as 2*2 we can change this to higher or lower value . 
1. For Higher Accuracy the filter size(Kernel_size) should be lower this would increase the training time which will come at the cost of accuracy . 
1. Activation function is relu which will take either positive value and conver 0 to all negative value . 
1. Flatten layer is pre-requisite for all CNN network where the 2 array is changed to 1 day array none of values are lost in this step.
1. We have added Dense layer this can vary from dataset to dataset .
1. For Output layer the activation function is softmax this is multiclass classification function for binary class sigmoid is preferred choice. ",052ece26,0.6129032258064516
28374,b05ee1ea1c8269,06faf277,# Dataframe for Bar Chart Race,19e4d303,0.6129032258064516
28381,0925f172b5eb74,f3685783,# Building up the model: DenseNet 121,ec34cd72,0.6129032258064516
28396,e8c6480a3122b3,5656770a,We can see that Alone passengers were more likely to survive than others.,40dc4cca,0.6129032258064516
28398,c80939c7c626cf,b0713d81,# 7 Fare/ Ticket Price,b9ac31e2,0.6131386861313869
28401,43e60eb1362f5c,6430c488,# Very High Correlation Between Arrival Delay and Departure Delay,87934234,0.6132075471698113
28403,e5dd725b8fa422,4423d303,"# [Building AR, ARIMA Models](http://)",14675d8b,0.6133333333333333
28405,67b7354e96113a,527d1c89,**Feature Selection**,dca94250,0.6133333333333333
28406,37e461081e47c5,5a50e99e,"# Model 2: Mean Model
Each prediction is the mean of the product for the entire range of data available",b3e6549e,0.6133333333333333
28409,91eaec994e0c6f,5eacf533,### 2.3.4 Department Level Analysis,376aef10,0.6133333333333333
28410,7e1da639035ac5,57013a4d,### <a id='11.2'>11.2 ELA proficiency vs math proficiency distribution</a>,120b6c23,0.6133333333333333
28411,4ae6a182abac64,2b189e24,* **IsAlone**,418676c5,0.6134453781512605
28412,c4386b8a01d66e,2da09dfd,# Sulfate_random,dc732bf5,0.6134453781512605
28413,5e1d001f8764e0,70e6b7e4,# MODEL TRAINING,62be464c,0.6136363636363636
28416,0a918602a04693,f17ed9c4,The TotalCharge column is not in numerical type. Lets change it to numeric,c1ef0e95,0.6136363636363636
28417,be2f4d8a6b73ca,f1ff5cdf,"<div class=""alert alert-block alert-info"" style=""text-align:center""> 📌<b>Insights :</b> Oldpeak and Heart Disease have strong positive correlation, whereas maxHR (maximum Heart Rate) and Age have negative correlation (younger people have more heart rate compared to older people)</div>",5d8ce40a,0.6136363636363636
28421,18ce858f90966d,38d473f2,# **K-Means Clustering**,09e9caed,0.6136363636363636
28423,450fda47b03baa,c3d1af2f,"Daha iyi anlayabilmek için Rating üzerine bir distplot çizdirelim.

",62c04adb,0.6136363636363636
28427,73d8e56bc709b1,80ae37f6,"we can see top 6 attributes of strikers are Acceleration, Jumping, Finishing,Agility,Balance,BallControl. 
Then, we can analyse these 6 features of top 5 strikers.",78ec3cce,0.6136363636363636
28429,6d66ced0028dea,6dd71d54,**DistrictId**,f50aae52,0.6136363636363636
28433,30fdc4a6e3c1db,26cf15f5,Let's see the time series decomposition for each of the categories as well to be more clear,6111ddee,0.6140350877192983
28438,d8fb26c4197325,cfec27ab,# Random forest,b190ac50,0.6140350877192983
28444,ce9ed5e2d601d7,2d581d6b,# Evaluation,f58a2f43,0.6141732283464567
28447,9c044fa3072552,745af6c8,### On Sundays,1362842e,0.6142857142857143
28449,2ada0305b68956,fede4802,### 104. Palette = 'gist_heat_r',133e26f4,0.6142857142857143
28450,38b79494ac749e,770cf6af,"There are two major ways to build a machine learning model with the ability to generalize well on unseen data:
1. Train the simplest model possible for our purpose (according to Occam’s Razor).
2. Train a complex or more expressive model on the data and perform regularization.

Regularization is a method used to reduce the variance of a machine learning model. In other words, it is used to reduce overfitting. Regularization penalizes a model for being complex. For linear models, it means regularization forces model coefficients to be smaller in magnitude.

Let's pick a polynomial model of degree **15** (which tends to overfit strongly) and try to regularize it using **L1** and **L2** penalties.",39162a40,0.6142857142857143
28452,fe6750354fb64f,a2120c16,The Linear Regression Model is absolutely falling aprat. As it is clearly visible that the trend of Confirmed Cases in absolutely not Linear.¶,271741f0,0.6142857142857143
28454,917957c6c4065f,ef2a2af4,"tag_count는 평균 16, 범위는 0 ~ 152  
태그의 개수가 10개 이하인 동영상이 약 10,000개 정도 입니다.",55b8ed68,0.6143790849673203
28456,4daf6153275cbf,e202debb,"Finally I looked up for average number of reviews done on local restaurants, another metric that Italy is leading by far.",51db1961,0.6144578313253012
28467,a915263bc207da,130493ec,### Creating sparse corelation matrix,b17ebcda,0.6153846153846154
28468,e169603b62be56,4c636b1f,**> Finging the alpha parameter fo Ridge **,8c311ec1,0.6153846153846154
28472,cddd881ce4b8ef,bca9c4a0,"titanic[[""age"", ""cat_age""]].head",5294325c,0.6153846153846154
28475,d4c5aaa4b36810,b5d22753,"Most of these features appear to have either no relationship or a linear relationship with happiness, however some such as GDP per capita appear to have a higher order relationship. These non linear relationships will likely not be captured well by linear models. ",65441f28,0.6153846153846154
28478,b27bdd02db1bbd,1f5f41fa,## 풀이,79340a85,0.6153846153846154
28479,aa46e9376825a5,3628d244,### For the first 7 tasks we can create a new dataframe with just the required columns,57792d96,0.6153846153846154
28480,03048e86a6d806,10b17748,"In 2019, median yearly compensation for Data Scientist is the same as Product/Project Manager's. The values of yearly compensation in 2019 was higher than in 2020 for nearly every job title.",1285c231,0.6153846153846154
28485,d78988cb5a1b02,714468a7,# **K Nearest Neighbor Classifier**,233f3a92,0.6153846153846154
28487,e424c111c44669,52d5da87,![3-s2.0-B9780124159365000098-f09-10-9780124159365.jpg](attachment:3-s2.0-B9780124159365000098-f09-10-9780124159365.jpg),d9fccfba,0.6153846153846154
28488,020c28a360b0cd,22126095,# Code,2ba397f0,0.6153846153846154
28492,c970849d1f6da2,37fa99b9,# Util functions,056e3955,0.6153846153846154
28494,758e5bde888d11,4f773247,### Note: The scale of pixel value is not normalized,c4486e8b,0.6153846153846154
28495,6e8f3e8ed1c241,7f3ccacd,The code from below was taken from the code in this [link](https://stackoverflow.com/questions/13405956/convert-an-image-rgb-lab-with-python/13423989)  ,c2cfb626,0.6153846153846154
28497,3f451680b1857b,a711cde6,# Get Class Name,56c45a1b,0.6153846153846154
28501,9a96e1588410ee,33dee252,"## Model Inference
This is one part I struggled a lot to find the solution. All the fastai sample code I found was efficiently using the GPU for faster training through batches. However, when it came to interpretation / inference, all I could find was how to classify a single image. I spent a good amount of time experimenting to get the predictions done in batches. I finally copied the source code of fastai (learner.predict() function to be mroe precise) to acheive that. After that I realized why there was so much emphasis in the fastai course to understand the inner workings rather than treating the libraries as some magic black boxes.

Finally, I was super satisfied when the prediction of 20K images ran under a minute vs 30minute+ projected time when run one at a time.",3fa6cf92,0.6153846153846154
28503,8ddaa0c6c395ec,12c3cf06,## Trainning the model,9fccabdc,0.6153846153846154
28505,669ce946943d60,82b00660,### KFold Training and Inference,0f63c4ce,0.6153846153846154
28506,44f6a002ecd033,a8232014,We can verify that these are now approximately normally distributed. From here we will go ahead and drop the original columns associated with the log transformations and then follow the same steps as with the all_data dataset with how we will clean.,70bbe106,0.6153846153846154
28508,582cb872d19026,cb3b2d4d,## Analysis of Top 20 Games,8d966d69,0.6153846153846154
28510,49ee86d074de69,3be0a668,"<a id = ""14""></a><br>
## Train-Test Split of Data",71ccc6d3,0.6153846153846154
28524,4d91e84c564cbe,e6b22e05,"Why does the cell above have no output? Let's check the documentation by calling `help(planets.append)`.

> **Aside:** `append` is a method carried around by *all* objects of type list, not just `planets`, so we also could have called `help(list.append)`. However, if we try to call `help(append)`, Python will complain that no variable exists called ""append"". The ""append"" name only exists within lists - it doesn't exist as a standalone name like builtin functions such as `max` or `len`.",355a43e3,0.6153846153846154
28526,50b03ce5b1a286,c51dd21b,"#Community QLattice

We are now ready to connect to the QLattice. The feyn module will look in you local configuration file to see if we have a commercial QLattice. If not, it will allocate a community QLattice for us on the Abzu compute cluster.",d49896a5,0.6153846153846154
28530,5f4ae633cfd090,890b871a,"# 3. Feature Engineering, Filling null values and Final Model",a30a16e2,0.6153846153846154
28531,2facf256353117,dd407d40,## get image data with simple itk,18f579be,0.6153846153846154
28532,80ad12f326ab70,bda203d0,"* most states are made up of the suburbs 
* Tennesssee is made up of mainly tow locales
* NorthDakota and New Hampshire has Rural locales
* states like Minnesota,Arizona, District of Columbia, Michigan etc have only one locale",da404a16,0.6153846153846154
28537,9b42412e75d640,2f103bd6,Naive Bayes gave pretty good results. Let's check others,b616570a,0.6153846153846154
28542,34fff8ce731b03,5297d8c7,"## Treinamento e Avaliação do modelo

Nesta etapa iremos treinar o nosso classificador, neste caso uma [árvore de decisão](https://spark.apache.org/docs/2.4.6/ml-classification-regression.html#decision-tree-classifier). Os dados de treinamento estão armazenados em *X_train* (features) e *y_train* (rótulo). A predição é realizada com os dados de treinamento em *X_valid*.",6f9e5b2e,0.6153846153846154
28548,d96642860ab3dd,da7d03aa,### 2.3 Feature Engineering Name feature,98419d48,0.6162790697674418
28553,1eb62c5782f2d7,2bd9a3f6,"- Sebelumnya kita diberi variabel acak terdistribusi normal $x$ dan kita menemukan probabilitas bahwa $x$ akan berada dalam interval dengan menghitung area di bawah kurva normal untuk interval tersebut.
- Tapi bagaimana jika kamu diberikan probabilitinya dan ingin mencari raw value?",bb69f147,0.6164383561643836
28557,712198370d5521,c81bc7b0,"<a id=""6""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">CLUSTERING</p>

Now that I have reduced the attributes to three dimensions, I will be performing clustering via Agglomerative clustering. Agglomerative clustering is a hierarchical clustering method.  It involves merging examples until the desired number of clusters is achieved.

**Steps involved in the Clustering**
* Elbow Method to determine the number of clusters to be formed
* Clustering via Agglomerative Clustering
* Examining the clusters formed via scatter plot",5882e04c,0.6166666666666667
28561,5b92c712910a11,5ebac6a8,# Lemmatization,e1d17100,0.6166666666666667
28564,cf4d1c1ad1476c,cfb6b430,# Encoding the text suitable to transformer Models,768c1a59,0.6166666666666667
28568,07f5853e4db8f8,742d920e,"Observe that  the three datasets have relation. That is in district data set column district_id entries are 
the name of engagment datasets and column lp id is common column for both product and engagment datasets. So I deciede to merge/concatinate three of them
in one and will process it.",d13c2c32,0.6166666666666667
28569,f6488772605bb5,496a2ca6,## Training the Model,068d4697,0.6166666666666667
28571,c84925c8171900,8553b6bd,"<a id=""platform""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.4 Platform Wise Analysis
            </span>   
        </font>    
</h3>",e21ff7ec,0.616822429906542
28573,957e035ba5b9d5,a557181e,## Creating a tuple of all images and its category,778ab3d3,0.6170212765957447
28585,0932046e1f485d,9720001a,"Spliting the ""Genres"" column into two seperate columns.",218cc7a3,0.6171875
28590,5f32117bcd5255,39e08d71,#### HST OPTICAL ONE,85882abf,0.6174496644295302
28592,02b7e38902069e,a4f05dfd,"<h1><span class=""label label-default"" style=""background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px"">Stanza NLP Tookit</span></h1><br>",726a03a0,0.6176470588235294
28595,7cfd96218dd933,553320c5,"#### **ATTENTION**
* SHARP INCREASES ARE OBSERVED ONLY IN TURKEY",7c34d96c,0.6176470588235294
28596,ab657da5329e3f,c2b68d6a,"# Model
Not the best but it converges ...",021526f8,0.6176470588235294
28597,52cfd66e9ec908,564df45e,"First of all, we have nine ego rotation columns corresponding to each. So I would want to do a quick check of the correlation of these variables before moving on to some more high-level analyses.",c74adcdf,0.6176470588235294
28598,99821bc6a45be6,2ba24e03,## Analysis:,b9d59346,0.6176470588235294
28599,8f50c9c16db95f,423b24f4,"# Variable 1 - Foot speed <a id=""speed""></a>
The positive relation between the kicking foot speed and the kick length has also been found in Australian Rules football([5](https://acephysed.files.wordpress.com/2015/01/atricle-1-reference-list.pdf)). Inspired by that paper, I was looking at whether there is a strong association between the foot speed and the last step length. I used the data **dis**, the distance traveled from prior time point, in yards (numeric), as a measure for estimating the length of the last step of each kick. And the outcome is promising. We can clearly see that the longer the last step length, the faster speed of the kick's foot. The foot speed has a very high positive correlation (**95.6%**) with the larger last step length before the kick, which usually indicates a larger angle of the pelvis. 

> So for the NFL professions and fans, if you see a kicker has a very large step before kicking the ball, it usually means he runs very fast towards the ball and the length of the kick shouldn't be short.",26cc763a,0.6176470588235294
28601,1d1598b6fa2aa7,427d1639,"### Box Plot

Box plot - https://plotly.com/python/box-plots/",e066accf,0.6176470588235294
28602,156bbcff05dcea,58fd6906,"> confusionMatrix()[source]
> Returns confusion matrix: predicted classes are in columns, they are ordered by class label ascending, as in “labels”.
> 
Reference https://spark.apache.org/docs/2.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.MulticlassMetrics",66ad1fe9,0.6176470588235294
28604,395ed8e0b4fd17,71b8165f,# <center>OHLC CHARTS</center> ,7573ea31,0.6176470588235294
28607,c65a65d4041018,f8039803,"Most countries have a similar pattern - high rate of using free resources. And many don't use them at all.
Among other notebooks Databricks for Spark is used.",824fb229,0.6176470588235294
28610,9cec5ddf8b6f49,3beae50d,## Tuning LightGBM using optuna,d39fc8e7,0.6176470588235294
28614,55c34673c1f760,e81a6976,## Compilation,2663c47f,0.6176470588235294
28615,21bce4ec54b3fa,f342f8e3,"Ok, so we improved the score somewhat, prediction time is similar, dimensionality is still quite high, around 2/3 of the original input.
Of course, if this was the issue, we could try sacrificing some of the accuracy and choosing less top features, instead of taking all non-random.
Also, importance plot shows exponential decay, so 20/80 principle is applicable here and shouldn't hurt accuracy too much.",35546e30,0.6176470588235294
28616,842547b2def18c,5a0f71eb,"### Completing a categorical feature

Embarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.",b8efde6d,0.6176470588235294
28618,e67925694c07d3,050791ad,# Feature Engineering,83af4c4a,0.6179775280898876
28620,04ff2af52f147b,e454e3b0,"**Create Surname Feature:**

Another feature we can create is a *Surname* feature from the *Name* feature.  This may be useful as it seems likely that if one family member survived, others or perhaps all of the family would have survived.",d5f37be9,0.6179775280898876
28622,5a8c553e21c70f,8d302237,"As can be understood from color saturation in correlation matrix cells, there aren't any serious correlation issues, the correlations are at low and mid-level.",9ebd9d8f,0.6181818181818182
28629,225b4fe5d3894a,c4c5a006,"<a id=""7a""></a>
### a. Training and Evaluating on Training Set",4b4197b3,0.6185567010309279
28632,9169c4e9c33c90,45a962f6,Which titles got people talking the most?,725bf880,0.6186440677966102
28633,1294fb4c86f993,e8f4f1d8,This can lead us to ask a new question <b> Have the gun registeration went up after 11/9/2001 terror attack in the US?</b>,4471e513,0.6186440677966102
28635,fdbbd573ba31c2,717ff490,## Encoding On Categorical Features,f7c28d74,0.61875
28636,e16860fce156b0,3a958b32,"#Top ‘k’ attributes’ correlation heatmap (here, k=1)",2054f1ce,0.6190476190476191
28638,7454fdc444df16,d5723e99,### Creating New Training & Testing Dataset,a7818ef5,0.6190476190476191
28641,04bac111ffbe9c,0ae1102f,##### Combining SibSp and Parch to Fam,82576b17,0.6190476190476191
28642,898d18d501f68d,1e51e360,"cover type 7 is basically found in higher elevation and covr type 3 is found from lowest elevation to a medium elevation.
aspet is same accross all cover type
slope is also same accross all cover type the average is around 35.
horizental distance average for cover type is around 600.
vertical distance average is 250 to 300
",d8bdea2d,0.6190476190476191
28645,565ad413cd802f,cce585e7,## Training the model,397b074e,0.6190476190476191
28651,53f302571cd4ac,7f6d4081,"## Encapsulation:
Encapsulation refers to hiding variables or some implementation that may be changed so often in a class to prevent outsiders access it directly.<br>
They must access it via getter and setter methods.",62c28443,0.6190476190476191
28653,2473d004f92592,7bb4484a,**Modelling & training**,18d3b6ee,0.6190476190476191
28657,8985a124d4b657,61f2ef86,"Below, I have plotted the actual true values (first plot) and preedicted values (second plot). One can visually see that the distribution is almost the same. This says that our predictions are very accurate.",586d1846,0.6190476190476191
28664,c818250dd720eb,d969d7f3,"Radboudumc: Prostate glands are individually labelled. Valid values are: 0: background (non tissue) or unknown 1: stroma (connective tissue, non-epithelium tissue) 2: healthy (benign) epithelium 3: cancerous epithelium (Gleason 3) 4: cancerous epithelium (Gleason 4) 5: cancerous epithelium (Gleason 5)

Karolinska: Regions are labelled. Valid values: 0: background (non tissue) or unknown 1: benign tissue (stroma and epithelium combined) 2: cancerous tissue (stroma and epithelium combined)

We will label apply Karolinska's method of not distinguishing between stroma and epithelium. 

Key: 

    Karolinska: Black = Background, Grey = Healthy Tissue, Purple = Cancerous
     
    Radboud: Black = Background, Grey = Healthy Tissue, Yellow = Gleason 3, Orange = Gleason 4, Red = Gleason 5",68ee40de,0.6190476190476191
28668,55339ceb40d5e9,5b12eed0,"# Univariate Analysis

* numeric variables: histogram
* categorical variables: bar chart
 
 we can surely carry out some feature engineering before ",d0f687dd,0.6190476190476191
28672,a758983a68c014,48f514cd,"We want our words to be represented by vectors consisting of 5 elements. So, `embedding_dims` equals 5. If you want to perform calculations on GPU, just replace `cpu` with GPU id, for example `cuda:1`.",ab89f181,0.6190476190476191
28677,b6e698d389d0d3,552bf7c4,# define model,f02f68b5,0.6190476190476191
28684,31268b33de97b5,71e6993a,# Ploting Missing Values,1e6f7d14,0.6190476190476191
28687,a5a419dc7245b0,934a1262,"##### Creating the subset from existing DatFrame for Analysis.
",4279726e,0.6194690265486725
28693,9bcfa825c8b2e6,0634eb85,Yeni columnlar oluştuğundan tekrar kategorik-numerik ayrımı yapılır.,220f36e4,0.6197183098591549
28694,bddd799cdbbae8,735400d3,**Support Vector Machine classifier**,b44e3c08,0.6197183098591549
28697,2f47abddfd1928,72752f78,"I have removed the values of Other in Cabin because it makes difficult to read the graph and also it won't give us value as it comes from missing values.

As expected, the cabins occupied of first and second class are mainly of middle age passengers (30-50 approx.).

And the cabins expected to be for 3rd class shows younger passengers (workers) more concentrated between 20 to 30 years old.

We can see how almost all features give us socio-economic insights and are theoretically related one with each other.",ae33cc0b,0.6198347107438017
28700,5ce12be6e7b90e,3478ea09,"Notice that the _start_ position is included, but not the _end_ position. We actually take the characters with indexes 2,3,4,5,6,7.",c0ab62dd,0.6198830409356725
28701,2ada0305b68956,958d2052,### 105. Palette = 'gist_ncar',133e26f4,0.62
28704,7dd46c750653eb,923484ec,**Marriages over the years**,c2644713,0.62
28707,83df814455f06c,ee013399,### Check accuracy score with criterion gini index,c9cff71a,0.62
28711,4cd25e50c7e007,8fcdec52,### RFE (Recursive feature elimination),ceb0c525,0.62
28712,5d2a3e82679cf3,d3ae8010,# CLASSIC REGRESSION,9e60b1e3,0.620253164556962
28716,ab6da5994949a3,6f65c911,"# K Nearest Neighbors (K-NN) Classification Model
## Fitting K-NN to the Training set",fae6b91d,0.6203703703703703
28723,20e1ba19eb9b5e,68992af5,"Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.",4569bfc1,0.6206896551724138
28724,1750367e54f407,991039ce,"We load the best weights that were kept from the training phase. Just to check how our model is performing, we will attempt predictions over the validation set. This can help to highlight any classes that will be consistently miscategorised.",a8e655b2,0.6206896551724138
28726,ef6d1e959a873e,e23a3165,"we can see it shifted towards left, i.e.,the distribution is right skewed. so, let's take the log transfomation to make the distribution normal.",f11a1f43,0.6206896551724138
28731,cd10f3afd970b3,5bbb349d,Let's take a quick look at what the data looks like:,2db3c8e4,0.6206896551724138
28735,bb8f5d7807718b,a9d465d1,"# 7. Bar of Pie

Have you ever wanted to further drill down into a pie chart? Maybe you wanted to expand one of its slices and ‘explode’ it into a bar chart? Matplotlib makes it possible through a ‘Bar of Pie’ functionality. It uses a [ConnectionPatch that connects two points (possibly in different axes](https://matplotlib.org/api/_as_gen/matplotlib.patches.ConnectionPatch.html#matplotlib-patches-connectionpatch))). Here is an example from the [official documentation](https://matplotlib.org/gallery/pie_and_polar_charts/bar_of_pie.html?highlight=bar%20pie).

![](https://parulpandeycom.files.wordpress.com/2020/08/pie.png)",181ec286,0.6206896551724138
28738,401338428b2d1c,48aa870d,## Predicting a new result,e4b768be,0.6206896551724138
28741,fb5c6021d127ef,39da90bc,"You should see that the r^2 score here is negative. Negative values indicate that the model is worse than just predicting the mean rides for each example.

## 5) Theories for poor performance

Why would your model be doing worse than making the most simple prediction based on historical data?",dd05cbd3,0.6206896551724138
28742,9535bb04ae042c,97c4bd2f,## viii)  Creating Pipeline,165b6fae,0.6206896551724138
28743,84127ade6fde87,e357e15a,"For most things, our mapping is just splitting by words. But the rarer parts—the capitalized Impossible and the name Bennet—are composed of subunits.",f55d05b6,0.6206896551724138
28747,1dd9c6aa74d289,f9600d54,"### CDF distributions

I will also project several elite atheletes with different heights in the plots.

I deliberately selected one or two Olympic athletes from different height ranges and marked where they are on the plot. This does not mean that the height distribution of athletes is even. Only some athletes' heights were put on the official website, so we can not know the height distribution as a whole. On the other hand, the 8a.nu data contains many excellent climbers, so it is still referenced.",5ef9a1be,0.6206896551724138
28748,5ea840754577e3,3dbff53c,The group of passenger with 0 number of parent/children tolled the most in deaths. The group with 1 and 2 in Parch has a slightly larger proportion of people who have survived. There is not enough data to come to an ideal conclusion for Parch > 3,9cf9b73f,0.6206896551724138
28750,6b54e39f86bdb5,a76d7362,"## Remarks

Now we see that the t dev set accuracy is better than our train set. Thus that mean that we can fit better the training set. There multiple solutions for this:
* Train a deeper network
* Train for longer
* Generate even more data",198084bc,0.6206896551724138
28754,656185a18260be,cd818861,"# Negative Sampling

Some of the contexts are very long, resulting with lots of negative examples that don't have an answer. We will sample negative contexts so they don't dominate our training.",0318cab5,0.6206896551724138
28759,4b7039cb44a54c,08fc5330,# CNN Model,24e806af,0.6206896551724138
28763,9ceb7278784462,c19335c2,## Model Tuning,3768a567,0.6209677419354839
28764,f91f58d488d4af,5cef6929,"Now, we'll tell **PyTorch** to calculate gradients.

The ""backward"" here refers to backpropagation, which is the name given to the process of calculating the derivative of each layer.",5df1bbf3,0.6210526315789474
28765,840534f2908a9c,ba6344b1,*The fare seems to be fixed for trip distances > 80 km; generally they are dropoffs and pickups from and to airports.*,8081c3cc,0.6210526315789474
28768,f166950fa915f8,d63c4599,### Compile model,a7f6ca5e,0.6212121212121212
28775,e4525eb0c96f28,98abaf98,"## Hypothesis Testing and Machine Learning
At this point, we have explored the data to reveal some of the relationships between different variables. But, we still want to be able to construct a model of how good a game will sell. One of the strongest continuous variables that affects the sales of a game is, of course, the year. We can first try to fit a Linear Regression model of video game sales over the years. For this, we will be using a new module called Sci-Kit Learn. Note that we are not going to split the data because we want to take the entire dataset into account. Given the spread of the data, we do not need to worry about overfitting the linear regression model.",2093a1f1,0.6216216216216216
28776,8276973853faa1,300352e9,# Top 5 Non-Veg consumption States in India,88da542b,0.6216216216216216
28779,297cbe4a23c4bf,1ebac15f,# Modelling,a843e619,0.6216216216216216
28780,63b44c85e32c1f,d3791485,"listb has also changed though no operation has been performed on it. This is because you have assigned the same memory space of lista to listb. So how do we fix this?

If you recall, in slicing we had seen that parentlist[a:b] returns a list from parent list with start index a and end index b and if a and b is not mentioned then by default it considers the first and last element. We use the same concept here. By doing so, we are assigning the data of lista to listb as a variable.",fb9b9562,0.6216216216216216
28783,ccabe7a86825ce,d703b681,**Working on binary Features**,d766cbf9,0.6216216216216216
28802,d905cde3391d2b,9409eb4c,"## Variance and Standard deviation

Standard deviation and variance measures the spread of a dataset. If the data is spread out largely, standard deviation (and variance) is greater. 

In other terms, 
* if more data points are closer to the mean, standard deviation is less
* if the data points are further from the mean, standard deviation is more

Formula for variance for **population** is given as,

$$
\begin{align}
Variance\,=\sigma^2 = {\sum_{i=i}^{n}{(x_i - \mu)}^2 \over n}
\end{align}
$$

where, $\mu$ is the **mean** of the dataset

Standard deviation is just the square root of variance

$$
\begin{align}
Standard\,deviation\,=\sigma = \sqrt{\sum_{i=i}^{n}{(x_i - \mu)}^2 \over n}
\end{align}
$$

> **Note**:
>
> For **Sample**, we use `n - 1` instead of `n`,  $\bar{x}$ - mean of sample

$$
\begin{align}
Standard\,deviation\,=S_{sample} = \sqrt{\sum_{i=i}^{n}{(x_i - \bar{x})}^2 \over n - > 1}
\end{align}
$$

Let's take `win_by_wickets` dataset.",067dba39,0.6222222222222222
28806,d96e03a9e7c030,e006078e,"The code below lists the coefficients of both final models, along with performance metrics r-squared and median absolute error. Notice that the coefficients of both models have the same sign (positive or negative). This is important when determining whether the coefficients of your regression model are stable.

Also notice the median absolute error and r-squared of the models applied to their respective test sets. The two takeaways from these metrics are:
* 58.7% of the variation in percentage of SHSAT takers at a school can be explained by the variables included in the model (from the `r2` output below)
* 50% of observations in the test set have a prediction error of <= 8.386% (from the `median_absolute_error` output below)",d2b72ced,0.6222222222222222
28817,eda49464dd6d1b,5b71c2c9,"## ROC Curve & AUC of Random forest classifier
* The ROC curve sorts all of the probabilities for each customer, then compares them to the customer's actual choices.
* Ideally the AUC would be 1.00",8421f81f,0.6223776223776224
28819,0ad8d416b89b78,1d109bb5,# Logistic Regression:,0b0562f0,0.6226415094339622
28820,43e60eb1362f5c,1541def6,"It shows that maximum of the Arrival Delays are due to the Departure Delays but some flights has still arrived on time even after departed late from the Origin Airport. Now we need to check why departure Delay is happening in the origin Airport Which may be due to security Delays, Air System Delays etc.",87934234,0.6226415094339622
28822,23df07a474aaae,bc5b4059,**Random Forest Regression**,0ea40276,0.6226415094339622
28826,f015d0147e8fbf,50652e85,### Displaying Feature Importances,518954fb,0.6226415094339622
28827,510b8303776bb6,f8c40ecc,## 2. Testing set,18080db8,0.6226415094339622
28830,fe7360cddc13e5,59f48e3c,## Manipülasyon,8979e423,0.6228070175438597
28833,81712ee7510ac5,9b758282,"**I will  use tuple when I want a user cant change the value of the list.**
**Tuple is immutable and list is mutable**",c4685e79,0.6228571428571429
28835,918040fad252ec,89385231,Menampilkan *Classification Report*,966fcd8f,0.6229508196721312
28838,0858e1bb3cbaca,f62f7836,"and

**.idxmax()**
to access the product(index) associated with that value",78548374,0.6229508196721312
28840,d07915a6e6992e,ba49c350,**SibSp**,2b912140,0.6230769230769231
28842,09751c520b0616,2b75eae0,- Final dataset - <b> df_final </b>,a4d0c7e9,0.6230769230769231
28844,548f961125248d,5c762534,"* Some differnt sets of parameters returned as 'best' by hyperopt. 
* {'border_count': 209.0, 'depth': 8.0, 'l2_leaf_reg': 7.476976878626717, 'loss_function': 0, 'rsm': 0.7556557996868841}
* {'border_count': 248.0, 'depth': 4.0, 'l2_leaf_reg': 4.830204209625978, 'scale_pos_weight': 0.4107081177319144}
* {'border_count': 129.0, 'depth': 10.0, 'l2_leaf_reg': 4.450385969436819, 'scale_pos_weight': 0.1034646048953394}",d8c5e8b8,0.6231884057971014
28847,7e275c8d5ff2a0,8c510ef6,"<div style=""color:white;
           padding:8px 10px 0 10px;
           display:inline-block;
           border-radius:5px;
           background-color:#5E7B81;
           font-size:90%;
           font-family:Verdana"">
    <h1 style='color:#ffffff;'>6.1.1. Creating Model</h1>
</div>",b3afcc98,0.6231884057971014
28849,9d9da6c439b96b,66de7322,"North America was leading on global competition rather than other country, however the flow of sales was fluctuative. Japan has same partern as the other, in 1992-1995 japan could lead sales but it was still under line of graph in rest.",361cc7d9,0.6231884057971014
28850,e3fb4c6300cb56,1bf858bf,"<a id=""9""></a> 
## Box Plot",8ebbdf89,0.6231884057971014
28851,b01ee6cb674fa3,0791dbd8,"# Korean Committee of Space Technology - KCST

",a8ffd35e,0.6231884057971014
28854,598b6228760590,29d9c8b7,- XGBoost,be30ab66,0.6231884057971014
28855,17a24d566ffa59,b1920d24,1. ![](https://s3.amazonaws.com/nlp.practicum/svd_vs_pca.png),89049e56,0.6231884057971014
28861,4ae464582bac51,ea29205d,## CREATING DUMMYS VARIABLES,ca6a52ce,0.6233766233766234
28862,90691864eb68c7,bf0bcea8,Dummy variable creation,3555ef9b,0.6233766233766234
28871,fc8e0042411c46,e199adcc,- Most entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.6238244514106583
28876,ba4b3bd184acbb,dd521d07,Many of these rows seem to have all null values except for the app name so we will remove all rows with null values.,0f5de724,0.6240601503759399
28877,b61ab8f81dc03d,3fe8df5a,"<a id=""checking_correlation""></a>
## Checking the correlation
 The correlation is a statistical measure of the strength of a relationship between two quantitative variables.",64d05394,0.624113475177305
28881,726833f92fb87a,3815b26e,"Then, we can analysze the remaing categorical features.",7dc5e1b6,0.6241610738255033
28888,2a724fb7835cdc,40eca2d1,**Transfer Learning Using GLOVE Embeddings**,c38ac61d,0.625
28892,2c8119a4061997,f3471ab3,# MixUp,1836a79c,0.625
28894,f13534449a3750,726885e0,"## Data Sampling
<a id=""subsection-three-2""></a>

It is possible to notive the unblancing of the dataset regarding the presence or not pf ships in the images. In particular in the trainig set we have arounf 22% of the images with ships while the others 78% without. As we will se later we want to select only the images with ships to manage the detection task, so it is a good idea to classify before the images according to the presence of ships and then proceeed with the detection. 

To have a better performance in the classification it could be better to manipulate a little bit the images in such a way to have a better differentiation among the images and a better balancing between the high and low presence classes. In this section we will explore this kind of arguments.",8b7f3332,0.625
28895,fae5023faa435f,5d4e70bd,"LSTMs are very powerful in sequence prediction problems because they’re able to store past information. This is important in our case because the previous price of a stock is crucial in predicting its future price.

Long-Short-Term Memory Recurrent Neural Network belongs to the family of deep learning algorithms. It is a recurrent network because of the feedback connections in its architecture. It has an advantage over traditional neural networks due to its capability to process the entire sequence of data. Its architecture comprises the cell, input gate, output gate and forget gate.

The input gate: The input gate adds information to the cell state,
The forget gate: It removes the information that is no longer required by the model,
The output gate: Output Gate at LSTM selects the information to be shown as output.

While Implementing any LSTM, we should always reshape our X train in 3-D, add 1 the reason behind is the time step and the 1 is given to the LSTM.",b37c893b,0.625
28902,96c4c0e36b8ec0,c86397a2,**Passengers number of siblings / spouses aboard the Titanic sperated by Survival **,4dd6de8c,0.625
28906,95656e8d666b16,7ffb1324,"Re-splitting X_train, X_test, y_train, y_test",65e88599,0.625
28907,bd0e173abb7b52,f96640b8,"**8. Among whom is the proportion of those who earn a lot (>50K) greater: married or single men (*marital-status* feature)? Consider as married those who have a *marital-status* starting with *Married* (Married-civ-spouse, Married-spouse-absent or Married-AF-spouse), the rest are considered bachelors.**",9bce3b0d,0.625
28912,9a040a4f21091e,f59eb9e2,"We can see that now the classifier is more ""eager"" to classify comments as toxic. Again, we have some comments I personally would classify as toxic that the dataset disagrees with. Very curious",f591b57d,0.625
28913,93f5423667b9d5,1c64e436,### ↑ 最終進化を使ってないチームも強かったりする模様,55bdf071,0.625
28916,9daf8b4a46725e,1d22686f,"### VIF is less than 10 for all the attributes, hence, we can keep them all.",7d9cc411,0.625
28921,1011899b959f44,44bd3154,8. List the highest number of battles that took place in a year using two different methods. (Hint: Use .max() and .sort_values()),0b112382,0.625
28924,c65a65d4041018,39afaf8a,### cloud computing services,824fb229,0.625
28930,49f2274c1dd516,e1ff6784,"# ECDC
The European Centre for Disease Prevention and Control collects the number of COVID-19 cases and deaths, based on reports from health authorities worldwide. This comprehensive and systematic process is carried out on a daily basis.",06b0ffee,0.625
28931,64a336ac34d95c,c33adf29,# Chi square implementation,be73a990,0.625
28942,923e97b05be00b,33e3ea1a,"For fast.ai, we want to use this call to find the best learning rate for our model.

This will likely take **5-10 minutes**. Underneath this section we'll explain what it's doing.",3a4a22dd,0.625
28946,5e02999ca74e7e,61bf4ca5,### **Evaluate Train Data**,b69da28e,0.625
28948,5ffe6aa38958a1,20396df6,# 3. Prepare data for training! ,11f5412e,0.625
28950,5ba4207c371899,3f5059cb,"Wow! Look at how many services are in the attack set! Whereas a huge amount of normal traffic is http, our attack traffic is all over the place.",187b1451,0.625
28955,c85c94076e9c3a,428e43ec,## Marital_Status & Total_Spent,3ea0c443,0.625
28959,d1ff7e10ee0102,ac0191fd,"We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. We can live with that, so we'll not do anything.",2cc71c3c,0.625
28965,9ec2fb131cf677,b6e4ef27,# Word Clouds of Authors with Occupation and Comments,211ea6bd,0.625
28967,62487bcd70b199,9964e102,"# <a id='7.'>7. Model Performances </a>
## <a id='7.1'>7.1. Model Performance Metrics</a>",f6ae50af,0.625
28968,84e0e568316ba1,383b4927,"# Transformation

By using yeo-johnson power transformation, the data around zero can be converted, however box-cox transformation can not be used in this situation.  
Perhaps this method was applied for the data of this competition.

![fig](http://matsuken92.github.io/imgs/yeo-johnson_fig.png)

![fieqg](http://matsuken92.github.io/imgs/yeo-johnson_eq.png)

Source : http://www.stat.wisc.edu/sites/default/files/tr1002.pdf",9ebad019,0.625
28978,117fc0956643d0,2411c7c6,"<div id=""step4""></div>",68cef9fd,0.625
28982,a69d41047fdd3e,ba5c7a88,"### 3) Create a crime map

If you wanted to create a map with a dot at the location of each crime, what are the names of the two fields you likely need to pull out of the `crime` table to plot the crimes on a map?",b1f28647,0.625
28984,e7ab5703594800,64fb4059,"## 1. Can a good question depend on number of words asked?

### The variables are negatively correlated to each other. As the number of increases, the score decreases. 

### We can also see from the graph that the score is high for the questions that have 5 to 25 words. ",78546b1b,0.625
28986,e82462cdc998a7,f1993343,"## TABNET

    class TabNet(torch.nn.Module):
        def __init__(self, input_dim, output_dim, n_d=8, n_a=8,
                     n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,
                     n_independent=2, n_shared=2, epsilon=1e-15,
                     virtual_batch_size=128, momentum=0.02, device_name='auto',
                     mask_type=""sparsemax""):",b39bf244,0.625
28987,fdbbd573ba31c2,195047bb,Label encoding for cloud_level,f7c28d74,0.625
28988,7ba63a2d9abb58,218285e0,"To clearly view other countries in the scatter plot, we need to remove the three outliers identified above.",821a261f,0.625
28995,68cceffe5bb8ec,135e98fc,# Set the optimizer and hyperparameters,dcbfcd6e,0.625
29001,3b5903412fe741,73effd2f,Performing a `set_index` is useful if you can come up with an index for the dataset which is better than the current one.,ad231969,0.625
29002,1d5daeca89f48d,d4f16169,## Calculation using Python,48d478bc,0.625
29004,2ada0305b68956,c6307bc9,### 106. Palette = 'gist_ncar_r',133e26f4,0.6257142857142857
29005,30fdc4a6e3c1db,3df9f315,"What we see:
* FOOD sales have increased quite much till 2012 and then is somewhat stagnant frim 2012 to 2016. And show a bit of spike in March and a huge spike in Auguts
* HOBBIES sales have increased in each alternate years from 2012 Aug to 2013 Aug and then 2014 Aug to 2015 Aug. In has remained pretty stagnant in the rest two years. Here, the spikes in March is much more evident.
* HOUSEHOLD sales have a better increasing trend and clean seasonality in March and August",6111ddee,0.6257309941520468
29006,5ce12be6e7b90e,fd2fb3d6,There are shortcuts for taking the first and last characters:,c0ab62dd,0.6257309941520468
29011,c84925c8171900,e2661fb5,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.4.1  Platform wise Video Game Count
            </span>   
        </font>    
</h4>",e21ff7ec,0.6261682242990654
29012,dbd96dd275dc60,16d2ca06,## Testing our model on a subset (to tune hyperparams),1ed493a8,0.6262626262626263
29019,cb570c7b7f0501,49e5e384,saturdays have a little bit higher percent of no show than other days but it's not a significant difference  ,a200a0ec,0.6266666666666667
29027,ba655a261cc09e,794b5ef7,## Preparing a test dataset,48cc549a,0.6268656716417911
29028,a4aa36df07fd53,b6ff8582,"### Should you get formal education?
Seberapa penting sih sekolah tinggi tinggi dalam dunia data science in terms of seberapa tinggi gaji pada umumnya",d2f42b6d,0.6268656716417911
29029,1a222fee3089d2,7a7e9d35,## **Selecting features**,59ab8894,0.6268656716417911
29032,fc8e0042411c46,9dfa7c0a,## Receive More Updates About Our Courses,af476c2a,0.6269592476489029
29037,a077820f7ab459,c0898236,## Interpretation with Grad Cam,05a43104,0.6271186440677966
29038,a44368590e878a,643a8870,#### Infection network for all samples,77743ba8,0.6271186440677966
29039,b660910fcc2954,0260f269,"## Comparison with January competition data

### Features",80b74f88,0.6271186440677966
29040,f2e5e9fb9eaaf7,1d66716f,"[back to top](#table-of-contents)
<a id=""6.1""></a>
## 6.1 Base model
Models that will be evaluated are `XGBoost Classifier`, `LGBM Classifier` and `Catboost Classifier`.

**Observations:**
- All 3 models have quite a same AUC result at around `0.8`. The differences are very small among the models.
- `Catboost Classifier` has the best result with `0.803`.
- `XGBoost Classifier` is the worst performing model with `0.799`.
- The second place is hold by `LGBM Classifier` with `0.801`.",048e0d08,0.6271186440677966
29041,c4bca5d86a38c3,3a0380a3,Rellenando valores null de columna Age con media de la data para esa columna,e23d297c,0.6271186440677966
29043,a81661cc35d8d2,527e5b54,Tranformation 2: Applying RobustScaler,3331f113,0.6271186440677966
29046,dac3c8204a2d1b,a46b0f68,# Top 10 Author with the best reviews,b0d2d0dc,0.6271186440677966
29049,7cfd96218dd933,f44c6ea7,#### JULY 30,7c34d96c,0.6274509803921569
29054,fa02c409161192,64d03b14,Below we will plot the relationship between the size of the traing set against performance. From the plot we can see that the NN does extremely badly for small traing sets but plateaus at around a $10 000$ where the performance levels out. Note the convergence of the performance to $100\%$ and that the data sets need to be exponetially larger to improve the preformance at the high end.,e97077f7,0.6274509803921569
29056,629f2918807a9b,c71dc6d4,"> We have df2 for Order cancelled: 
> 
> - df2 = df.loc[(df['Order Status'] == 0)]
> Changing its name to df_Order_Cancel
> 
> 
> 
> We have df3 for Order Returned: 
> 
> - df3 = df.loc[(df['Order Status'] == 2)]
> Changing its name to df_Order_Returned
> 
> 
> Now We going to make df_Order_Success For Successful orders**",be56dc84,0.6274509803921569
29057,71b75664517244,ae41899d,"Arsene Wenger is the longest manager on premier league, he is there for 22 season. Follow by Alex Ferguson with 21 season.",fc905af5,0.6274509803921569
29060,917957c6c4065f,1e6bfb35,"상위 10개까지 살펴보니, 1위를 제외하고 2~10위는 모두 FeelSoGood 채널에서 게시한 동영상입니다.  
해당 채널은 태그의 개수를 많이 사용하는 경향이 있는 것 같습니다.",55b8ed68,0.6274509803921569
29063,f6648e47713411,f4376aa6,## 2.4 Create custom Dataset,f4af4d1c,0.6276595744680851
29066,396bc36edb95d3,e94bc90d,#### Feature Importance,965e4f8f,0.6277777777777778
29068,72d393488311b6,7f4ebfe7,# Scaling,80663df0,0.627906976744186
29072,806ce45c8fa303,8dd7a881,## Using Autoencode to encode data,3e5c34dc,0.627906976744186
29073,22bd95f4807a23,d3778b03,## Variation of ratings across various types of product,c05d356f,0.627906976744186
29075,743ae010f5e875,3bf610c5,### Transform,02c54445,0.627906976744186
29079,2f47abddfd1928,dabc48b5,"## 3.4. SibSp

Seems that SibSp has a good correlation with age, fare and Parch.

Lets inspect these relations.",ae33cc0b,0.628099173553719
29082,80ad12f326ab70,985750b7,* ### Insights on Products data,da404a16,0.6282051282051282
29085,ac1abfe1dfe815,aa342435,## Positive Words,6529dbcb,0.6283185840707964
29087,c9b4e282e4e2c1,9a39d72b,2-Visualization of the data,f44d339f,0.6283185840707964
29089,2730840089c8eb,50684051,"So much cleaner! We call `.format()` on a ""format string"", where the Python values we want to insert are represented with `{}` placeholders.

Notice how we didn't even have to call `str()` to convert `position` from an int. `format()` takes care of that for us.

If that was all that `format()` did, it would still be incredibly useful. But as it turns out, it can do a *lot* more. Here's just a taste:",34d27dac,0.6285714285714286
29091,81712ee7510ac5,db23928f,**Set is a collection of unique elements. Uses the curly brackets like  dictionaries.**,c4685e79,0.6285714285714286
29092,5d5c9480b5a0a3,b123d5f1,"We can clearly see that most people died from the tragedy. Moreover, number of people who died is significant within the age range 18-32 years.",04d82e2d,0.6285714285714286
29095,38b79494ac749e,cc7c799f,### L1 - Lasso regression,39162a40,0.6285714285714286
29099,fe6750354fb64f,487a6c15,# 2. Polynomial Regression,271741f0,0.6285714285714286
29100,55a5e31d03df9f,e4ec7020,"There seem to still overfit, however it's so much better than previous model. Let's see Keras Application model compare them and fix overfit from there. ",06dce00f,0.6285714285714286
29105,2b36742b49c7bc,9748e3bb,"Оролтын датагаараа бүх тектийг сонгохын оронд `#`-ыг оролцуулан таслах замаар богино өгөгдөл бий болгоно
Жишээ нь доор өгүүлбэр байхад:
```
Эртний хүний сэтгэхүй, орчин тойрныхоо юмыг ,байгалийн түмэн үзэгдлийг хэрхэн үзэж юу гэж тайлбарлаж байсан зэргийг хүн төрөлхтний хамгийн эрт балрын шашин болох бөөгийн шашны ёс заншил, шүтлэг тахилга зэргийн зүйл нь надад их сонин олон юмны түлхүүр нь болж, ардын аман зохиол, билигзүй зэргийн олон баримт байдлын учры нь тодорхой болгож, аман#0000006740 зохиол угсаатны судлал, ардын билигзүй, хэлний шинжлэл зэргийг тусгайлан хэлний тухай түүн дотроо монгол хэлийг гол болгон , монголтой төрөл төс түрэг хэлтэн, хамниган хэлтэн жич, эл, монгол, түрэг, хамниган гурван хэл буюу алтай овог хэл гэдгийнхний хэл угсаатны судлал, ардын билигзүйг, уралын овог хэлтэн гэдэг мажар, фин, эсти, болон слав, герман угсаатны мөн эл зүйлтэй хэлзүн шинжлэлийн үүднээс голлож судалснаараа жич монголын аман зохиол, туульс, бөөгийн дуудлага, зан үйл тэргүүтнийг 1927 оноос <<Заан Залуудайг >> бичиж эхэлсэн 1962 он хүртэл оролдсоор монголын хэл соёлын тухай судалгаа хийж, цуглуулсан хэрэглэгдэхүүн минь <<Заан Залуудай >> гэдэг балар эртний түүхэн романыг бичихэд нэн их тус болсон билээ
```
доорх байдлаар богино болгоно
```
тодорхой болгож, аман#0000006740 зохиол угсаатны
```",c8f8a96d,0.6285714285714286
29110,171494b45650a2,ad308ccc,### ExtraSauce@Company vs Price,9c8cc578,0.6285714285714286
29118,0fa9979b5690e9,5528c2e5,"A biblioteca Pandas tem uma forma fácil de visualizar algumas informações estatísticas sobre um conjunto de dados. No bloco acima, é possível perceber que alguns atributos estão em escalas completamente diferente de outros. Por exemplo, compare o atributo 0 e o atributo 12, ou 0 e 7. Escalas diferentes confundem os métodos baseados em distância. Sabendo disso, uma estratégia é colocar os dados para respeitar um padrão ou distribuição. Isso pode ser feito com o StandardScaler do Scikit-Learn. Vamos verificar como fica a distribuição dos dados após padronizá-los e a visualização em 2D.",c26eea94,0.6285714285714286
29119,3cea0f929a2035,9453f28d,"As it contains years with zero records of suicides, we'd better remove zero recordings from the data, and see if the distribution will change significantly.",04cfbade,0.6285714285714286
29123,8ec771f5600a61,48431d31,# HERE WE ARE USING THE DIFFRENT METHOD TO CHECK WHICH FEATURES ARE USEFULL FOR US.,48364c1f,0.6288659793814433
29128,c0ddb77bf32e2b,c79d8d9d,Let's  take Banqiao station for model building example.,a0cb45f7,0.6290322580645161
29130,57070ad5e0f94f,4281d0c0,# **Preparing Some Data to Use**,d97edc41,0.6290322580645161
29134,5f27526aa6c113,4b483f14,"we see a little bit hike in may, otherwise its mostly same",a5c26ab6,0.6292134831460674
29135,f3c6048d1058e3,ecaa3b69,## Machine Learning Models,1d9056b0,0.6293103448275862
29137,135122550b6483,ac150e57,"<h3> Reference for Feature Engineering and Hybrid Model </h3>
<h4> https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series
",6592d6d8,0.6296296296296297
29138,7baeb0ffc6659e,6b185321,**Title**,8cbebba9,0.6296296296296297
29141,8106640e2f9c7e,50371148,"Создадим датафрейм, содержащий количество пересечений для каждой из пар<br>
**Товар из Data** - **Товар из Target**",faea9b8e,0.6296296296296297
29143,efbcfe95cd7fde,a3ef1fed,Age target 0,54be281a,0.6296296296296297
29148,faa8e6c8ab9246,92c821e9,Fare is high in Pclass 1 compared to Pclass 2 and Pclass 3,2bea1419,0.6296296296296297
29151,b9328fe3b0cefc,3c4241ee,"we see(可以看到)：
- Max gap in 2018 season.(锦标赛平均分差最大的是2018年)
- Min gap in 2015 season.(锦标赛平均分差最小的是2015年)
- Mean gap is almost 17 in last 5 years.(近5年平均分差在17分左右)
- There is little difference between seasons.(各个赛季之间差异不大)

This gap is much bigger than men's data.(对比男子组的数据，分差要更大)",3a35eb23,0.6296296296296297
29158,f4b9042e693b6c,a5f1fcd0,## Training code,676cacc9,0.6296296296296297
29159,2b39f4ff896f97,cc29cda6,Model Summary,3ddfe182,0.6296296296296297
29161,ddcdecdd6a3b6d,1f1348e3,"#  ⽹络中的⽹络（NiN） 
LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。  
NiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。  
⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  
![1576145663%281%29.png](attachment:1576145663%281%29.png)
1×1卷积核作用   
1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。  
2.增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  
3.计算参数少   ",90831448,0.6296296296296297
29163,0dfa3e758551c8,75142fe3,dead women were mostly from class 3,7adbb44c,0.6296296296296297
29166,2500c5fe8497ee,2ef3277f,It is clear that PIG consumption is high Worldwide ,855355f0,0.6296296296296297
29167,1883198d6d8c3c,4901590f,"This method will provide various summary statistics, excluding <code>NaN</code> (Not a Number) values.",69a1d458,0.6296296296296297
29169,e6576e985ccc71,fa8b4725,#mpwolke coping other Kagglers Notebooks. Yes that's Me!,63d2c3a5,0.6296296296296297
29173,4883314a96dc34,e62ee963,Performance metrics on ***test set***:,50d36836,0.6296296296296297
29174,d128317750d689,42afe7b4,"To enable the network to learn, we also need to define the optimizer and the loss function. I've tried many optimizers, but SGD with momentum seems to work best for me. But I suggest you experimenting with other optimizers, like Adam or Adadelta. 

Also, keep in mind that different optimizers work differently with learning rates. While one learning rate might be perfect for Adam, it migth stop network form learning with SGD. You can read more about learning rates in [this article](https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2), it helped me quite a lot with this project.",d87f7428,0.6296296296296297
29180,ce9ed5e2d601d7,120b306a,"But instead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and ""whiskers"" showing the extent of the scores. Note that the `boxplot()` function detects outliers (called ""fliers"") and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box's height), and any score lower than $Q_1 - 1.5 \times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \times IQR$.",f58a2f43,0.6299212598425197
29186,91473a39b85068,648fff43,"### Machine learning models
#### Multilabel problem - Handling tags",6e3d91c2,0.6301369863013698
29188,1eb62c5782f2d7,cc2f5cbe,### Contoh 1,bb69f147,0.6301369863013698
29190,596389bed473be,489bff86,# Summary Functions,5f8af156,0.6301369863013698
29191,4ae6a182abac64,1754e1de,* **Extraction the passengers titles**,418676c5,0.6302521008403361
29194,73ca9abcc2034e,7f045f1a,"# Let's print the most popular words, used over 700 times",cec3446c,0.6304347826086957
29195,7e2644d6b415bc,63bd3394,"**data is too little. It is clear that salary column is fully depended on status column. If placed then have salary, either notplaced have salary 0. So, if we take salary column it will fool us giving 100% accuracy.**

**You Can Try**",52de7ef0,0.6304347826086957
29199,0e2a23fbe41ca9,212154c5,"Observations:
- both ```avg_purchases_lag3``` and ```avg_sales_lag3``` have a few outliers in the extremes ",64e4762c,0.6304347826086957
29200,72d528df923403,d2702c54,"- More units are sold on the weekend, especially on weekends in August and September. Although in 2016 the weekends of February and March have exceeded the sales of past years during the same period.",d51c8e8e,0.6304347826086957
29201,9b5de3823ad5ab,8fe36b1e,### Top layers' training,33e48774,0.6304347826086957
29204,a8c042af6b7245,884be8ce,"There are a strong correlations between the variables:

* ps_reg_02 and ps_reg_03 (0.7)
* ps_car_12 and ps_car13 (0.67)
* ps_car_12 and ps_car14 (0.58)
* ps_car_13 and ps_car15 (0.67)

Seaborn has some handy plots to visualize the (linear) relationship between variables. We could use a pairplot to visualize the relationship between the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.

**NOTE:** I take a sample of the train data to speed up the process.",2487ac62,0.6307692307692307
29205,3cb96bd8eb364b,4dd01cc0,## Model data input prepare,3157af7e,0.6307692307692307
29207,f2f2db16a2f86c,e6d6f4d6,**Splitting into Train set and Test set**,ffc6a115,0.6307692307692307
29208,03048e86a6d806,385d006f,### Highest Paid Countries,1285c231,0.6307692307692307
29212,2d75fd881827b8,c58d364f,**RandomForest**,107b5299,0.6307692307692307
29214,1645979263c148,6d5c5e2f,"## Feature Engineering

1. get_dummies()",fa11663e,0.6307692307692307
29228,2ada0305b68956,978aa6b3,### 107. Palette = 'gist_rainbow',133e26f4,0.6314285714285715
29230,99afe9f3af6dbc,9d213916,Contoh data yang sudah ter-cluster.,cdec9b3a,0.631578947368421
29234,52ee792e228d54,76e167f8,### We'll use the VIF to remove the attributes which are having multicollinearity. Let's remove the attributes for which VIF > 10,5096094e,0.631578947368421
29235,bef2347846e476,36ca8a69,In the following graph we see the rating values of the applications with the plot line.We see they have at most 5.0 as a value.,cb93bf51,0.631578947368421
29237,30fdc4a6e3c1db,c4c66b60,### Plotting monthly sales accross departments,6111ddee,0.631578947368421
29238,26b93b6f4dc148,871e12e4,### Creating Two-Way Interaction between all Features,6f667d22,0.631578947368421
29244,d81d3830152f88,0f2fda2a,"We will use bootstrapping to simulate 10,000 samples. And for each of them, we will measure the difference of `P_higher_3pt_win` and `P_not_higher_win`",9551eac9,0.631578947368421
29245,d93a87fdbdb3d2,a81241c3,#### Split train and test dataset,30d079c3,0.631578947368421
29246,29437539745aa5,fbf29dcf,"<a style=""text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"" id=""inference"">4&nbsp;&nbsp;INFERENCE LOOP</a>",c17b490a,0.631578947368421
29247,f35ee6e9fab592,b49044fd,A distribution plot of review scores; demonstrating a normal distribution skewed left,b15f7073,0.631578947368421
29258,757fa8de4edc4c,41e374a3,Add scaler coupling contribution to training dataset,87211008,0.631578947368421
29259,c950cff74e51ac,85cb05f0,"From the graph above, manhattan is the place where rent price is the most expensive",d59bf323,0.631578947368421
29260,c2a9f2fb3e1594,8d5a33be,"<a id=""ch8""></a>
## 5.1 Evaluate Model Performance
Let's recap, with some basic data cleaning, analysis, and machine learning algorithms (MLA), we are able to predict passenger survival with ~82% accuracy. Not bad for a few lines of code. But the question we always ask is, can we do better and more importantly get an ROI (return on investment) for our time invested? For example, if we're only going to increase our accuracy by 1/10th of a percent, is it really worth 3-months of development. If you work in research maybe the answer is yes, but if you work in business mostly the answer is no. So, keep that in mind when improving your model.

### Data Science 101: Determine a Baseline Accuracy ###
Before we decide how-to make our model better, let's determine if our model is even worth keeping. To do that, we have to go back to the basics of data science 101. We know this is a binary problem, because there are only two possible outcomes; passengers survived or died. So, think of it like a coin flip problem. If you have a fair coin and you guessed heads or tail, then you have a 50-50 chance of guessing correct. So, let's set 50% as the worst model performance; because anything lower than that, then why do I need you when I can just flip a coin?

Okay, so with no information about the dataset, we can always get 50% with a binary problem. But we have information about the dataset, so we should be able to do better. We know that 1,502/2,224 or 67.5% of people died. Therefore, if we just predict the most frequent occurrence, that 100% of people died, then we would be right 67.5% of the time. So, let's set 68% as bad model performance, because again, anything lower than that, then why do I need you, when I can just predict using the most frequent occurrence.

### Data Science 101: How-to Create Your Own Model ###
Our accuracy is increasing, but can we do better? Are there any signals in our data? To illustrate this, we're going to build our own decision tree model, because it is the easiest to conceptualize and requires simple addition and multiplication calculations. When creating a decision tree, you want to ask questions that segment your target response, placing the survived/1 and dead/0 into homogeneous subgroups. This is part science and part art, so let's just play the 21-question game to show you how it works. If you want to follow along on your own, download the train dataset and import into Excel. Create a pivot table with survival in the columns, count and % of row count in the values, and the features described below in the rows.

Remember, the name of the game is to create subgroups using a decision tree model to get survived/1 in one bucket and dead/0 in another bucket. Our rule of thumb will be the majority rules. Meaning, if the majority or 50% or more survived, then everybody in our subgroup survived/1, but if 50% or less survived then if everybody in our subgroup died/0. Also, we will stop if the subgroup is less than 10 and/or our model accuracy plateaus or decreases. Got it? Let's go!

***Question 1: Were you on the Titanic?*** If Yes, then majority (62%) died. Note our sample survival is different than our population of 68%. Nonetheless, if we assumed everybody died, our sample accuracy is 62%.

***Question 2: Are you male or female?*** Male, majority (81%) died. Female, majority (74%) survived. Giving us an accuracy of 79%.

***Question 3A (going down the female branch with count = 314): Are you in class 1, 2, or 3?*** Class 1, majority (97%) survived and Class 2, majority (92%) survived. Since the dead subgroup is less than 10, we will stop going down this branch. Class 3, is even at a 50-50 split. No new information to improve our model is gained.

***Question 4A (going down the female class 3 branch with count = 144): Did you embark from port C, Q, or S?*** We gain a little information. C and Q, the majority still survived, so no change. Also, the dead subgroup is less than 10, so we will stop. S, the majority (63%) died. So, we will change females, class 3, embarked S from assuming they survived, to assuming they died. Our model accuracy increases to 81%. 

***Question 5A (going down the female class 3 embarked S branch with count = 88):*** So far, it looks like we made good decisions. Adding another level does not seem to gain much more information. This subgroup 55 died and 33 survived, since majority died we need to find a signal to identify the 33 or a subgroup to change them from dead to survived and improve our model accuracy. We can play with our features. One I found was fare 0-8, majority survived. It's a small sample size 11-9, but one often used in statistics. We slightly improve our accuracy, but not much to move us past 82%. So, we'll stop here.

***Question 3B (going down the male branch with count = 577):*** Going back to question 2, we know the majority of males died. So, we are looking for a feature that identifies a subgroup that majority survived. Surprisingly, class or even embarked didn't matter like it did for females, but title does and gets us to 82%. Guess and checking other features, none seem to push us past 82%. So, we'll stop here for now.

You did it, with very little information, we get to 82% accuracy. On a worst, bad, good, better, and best scale, we'll set 82% to good, since it's a simple model that yields us decent results. But the question still remains, can we do better than our handmade model? 

Before we do, let's code what we just wrote above. Please note, this is a manual process created by ""hand."" You won't have to do this, but it's important to understand it before you start working with MLA. Think of MLA like a TI-89 calculator on a Calculus Exam. It's very powerful and helps you with a lot of the grunt work. But if you don't know what you're doing on the exam, a calculator, even a TI-89, is not going to help you pass. So, study the next section wisely.

Reference: [Cross-Validation and Decision Tree Tutorial](http://www.cs.utoronto.ca/~fidler/teaching/2015/slides/CSC411/tutorial3_CrossVal-DTs.pdf)",53411c04,0.631578947368421
29265,3fb15e6e48aec2,502541fd,# Age Binning,9d1f4358,0.631578947368421
29268,a1dcd92986bc84,1d8105de,Plotting the training loss:,730acaaa,0.631578947368421
29269,9f3710be6aea65,ad1bdefe,## Find Correlations on data:,ae9bda88,0.631578947368421
29273,9e27af2600925c,fd978507,"**Expected Output**: 

<table style=""width:30%"">
    <tr>
         <td>
             **predictions**
         </td>
          <td>
            [[ 1.  1.  0.]]
         </td>  
   </tr>

</table>
",9b556435,0.631578947368421
29283,e4c6dd957eb5ce,2eef770c,"## Ploting Historical Kernels Creation
- Let's see the patterns of Kernel Creations by Dates",2e383665,0.6323529411764706
29286,9cec5ddf8b6f49,09780531,"### For hyperparameter sampling, Optuna provides the following features:
* optuna.trial.suggest_categorical() for categorical parameters
* optuna.trial.suggest_int() for integer parameters
* optuna.trial.suggest_float() for floating point parameters

With optional arguments of step and log, we can discretize or take the logarithm of integer and floating point parameters.

* suggest_loguniform - another way to ask suggestions for the log of the value

Other terminology in Optuna is as follows:

* Trial: A single call of the objective function
* Study: An optimization session, which is a set of trials
* Parameter: A variable whose value is to be optimized",d39fc8e7,0.6323529411764706
29287,eb0ecd6bebeb15,6ced9ad7,"
scatterplot ile petal.length ve petal.width değişkenlerinin dağılımlarını çizdirelim.",d7b93a60,0.6323529411764706
29290,02b7e38902069e,24450ec9,![](https://i.ytimg.com/vi/Mtkktl0kHV0/mqdefault.jpg)youtube.com,726a03a0,0.6323529411764706
29296,2343dc02ffb96a,eee35ade,"# Feature selection using statsmodels.OLS (Ordinary Least Squares) -- we only have one feature in this data, but I wanted to see the results of the ols on this contrived data.
# Again... ols will not run because of the previously mentioned deprecation error.",29aa95a4,0.6326530612244898
29302,e69a496109e7d8,11f386b0,"Here in survived, it shows 75% of people had node count < 5(approx). And you can see lots of outliers in the survived data",1c640591,0.6326530612244898
29304,f1e162ddd14f11,f1ea7c72,"240 out of 301 of our data is for training, and 61 records are for testing",cdb2e771,0.6326530612244898
29308,08f845750d026a,30f049fd,"Jeypore,Odish has the highest loan amount from kiava",1c54de30,0.6329113924050633
29309,4c47839b067546,8369a80b,### owners,1f517b02,0.6329787234042553
29312,a566b5b7c374e7,c8a3acbd,### End Time,b3dc5545,0.6330935251798561
29317,c91c137284976f,f2dc9892,# 2. Building first model,c6888c0a,0.6333333333333333
29322,b547f0f38f7744,ef094553,"### Create Model

Create a Faster R-CNN model pre-trained on COCO:",b6ba66b3,0.6333333333333333
29324,892be0a523578c,e4a24551,"Among these 3 clusters, althoug they have clear differences in exercise intensity, especially for the **VeryActiveDistance** attribute, the **SedentaryMinutes** attribute among the groups doesn't vary too much. This means our customers may have the occupation that need them sitting a lot during work. Based on this observation, the differences seen in exercise intensity can be explained by customers' attidude towards sports. In summary, these 3 clusters represents: 
* Cluster 0: seldom exercise customers
* Cluster 1: High-intensity exercise customers
* Cluster 2: Moderate-intensity exercise customer",b0e8d7c0,0.6333333333333333
29325,c18267b203f28a,9c8eab7c,"## Building our model
In order to ensure that our model is trained on the TPU, we build it using `with strategy.scope()`.    ",09ca8efb,0.6333333333333333
29331,2bd6c370695ea7,e5759af6,## Train,cbe6aec8,0.6333333333333333
29334,b10bd75889dad9,33c649ca,#### The test and train accuracy score is quite high,ee00ceee,0.6333333333333333
29335,be616f0785c32d,b9664b18,"[Go Top](#top)

Next we pulled all rounds of predictions together. ",b78e18aa,0.6333333333333333
29345,63d0d9b9a8c7d2,bc8eef66,***Random Forest Classifier***,e32e5933,0.6333333333333333
29348,631cd434fc3aa2,616660cf,* _Exterior2nd_ and _Exterior1nd_: we'll impute with the most frequent value.,2b74febb,0.6338028169014085
29353,d8ff894670d506,6d9ce8ec,**Combining the plots**,eb0fb7de,0.6338028169014085
29354,917957c6c4065f,bc4688ed,"### 2.6. Ratio (likes/views, dislikes/views, comment_count/views, dislikes/likes)",55b8ed68,0.6339869281045751
29355,b01ee6cb674fa3,8b2413e9,"# Sandia National Laboratories

A private USA company",a8ffd35e,0.6340579710144928
29357,0b01138ad120fc,8ab1454e,"**Remembering**  
**dayofweek = 
    [""Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday""]  
dayofweek = 
    [----1----,-----2----,-------3-----,------4-----,----5----,----6-----,----7----]**

**dinner = 
    [""Pizza"",""Burguer"",""Pancake"",""Spaghetti"",""Meat"",""Chicken"",""Sausage""]  
dinner =
    [----1---,-----2-----,------3-----,------4------,----5----,----6-----,-----7-----]**",0b4b72e6,0.6341463414634146
29358,0e09587faffa8f,8d90ed23,The maximum number of violations occured in the county of **New York**,0d563d61,0.6341463414634146
29359,fd4017c1514157,482a7d1d,---,fd8f0896,0.6341463414634146
29362,47b2c9be5e31cb,1d2532a8,Correlation matrix:,7d4afe56,0.6341463414634146
29368,9c26c5dcd46a25,824f73f1,"Les métriques de ce premier modèle sont bien meilleurs que la baseline. Le R² (coefficient de détermination) qui est le carré du coefficient de corrélation linéaire, est de 0.64 ce qui en fait une base correcte de prédiction. Nous pouvons d'ailleurs projeter les erreurs de prédictions :",1bbbb677,0.6341463414634146
29369,e19e307b3fd188,5e372464,### Remove outliers,2173955b,0.6341463414634146
29378,6f1481148352e9,9f9fe170,"**The graph shows one pronounced outburst - 08.2006. Also noted 3 more pronounced peaks. Let's check the data for these months, we will study in which states there were fires.**",7cfbdb8f,0.6346153846153846
29389,06ecf7a304c309,2083f61e,이번에도 조기 학습 종료로 학습시켜봅시다. 더 좋은 결과를 원한다면 epochs 수를 늘리면 됩니다.,714de627,0.6349206349206349
29393,3c2033cc99c12c,698a06c7,#### Evaluate the performance of the Logistic Regression Model ,dfa22a54,0.635036496350365
29396,62037c5832129c,13a11fb3,### Additional Note,61474350,0.6351351351351351
29399,e93a41c03638fe,f91aabbc,# 3.4 Text Classification with BERT (Transformer Model):,7363527b,0.6352941176470588
29400,869a39a3d4dea2,ff2783b9,"## Splitting and Merging Channels <a id=""splitandmerge""></a>",9020daf8,0.6352941176470588
29404,1294fb4c86f993,c0f249be,#### There is a peak coincides with  `9/11` terror attack,4471e513,0.635593220338983
29413,32e04b08ff52eb,0f25e1e5,MAP Calculation,8d5b86e0,0.6363636363636364
29422,dd02a9b545f742,aa5efb4f,# Phase 1 | Object Building - Object 3: Returns optimizer,7116cd2d,0.6363636363636364
29424,4ae464582bac51,0ea69c57,"The use of the dummy variable will allow the capture of the difference of the expected value between categories, that is, the coefficient (Beta) of the model will be the average value that a given category represents.",ca6a52ce,0.6363636363636364
29426,b82610a9364f75,6b1cd69d,# Apply tip 5,22c70e69,0.6363636363636364
29435,b241b847319d13,8c5781d0,# **Exporting the TFRecords**,0fb698f0,0.6363636363636364
29436,3a15bac33f2a74,d3c10b1f,**XGBoost**,25204b71,0.6363636363636364
29438,a6eb631926a4d7,d64da2a6,Random Forest,0d502aab,0.6363636363636364
29439,5083d7a61f2426,322e390d,Ploting the whole data,541a0fec,0.6363636363636364
29445,e9b9663777db82,0f5ca95c,#### Categorical Features,648e8507,0.6363636363636364
29447,da199f8fb59439,b4f1b9d6,"Most genre released on Netflix
* *Documentaries*
* *Standup Comedy*
* *Drama*",baaa665d,0.6363636363636364
29448,f269d2fbd5f1be,c7b10fb6,"**Commentary:**
* There seems to be some linear relationship between rating and salary (i.e. higher rating generally translate to higher salary)
* The median rating of players over the age of 30 is higher than the median rating of players under the age of 30
* There are more outliers in the players' rating for players under the age of 30
* Distribution of salary for players over 30 years old is wider as compared to the same for the players under 30 years old",1264c440,0.6363636363636364
29449,f0fab078f8533b,3eb3e60d,## b. We can see that US and India are the countries where most of the content is made. Also further below we can see the top genres made in the last 10 years,bdb5ea32,0.6363636363636364
29452,a0b321057e7402,c570a4fd,"# **SARIMAX**

SARIMAX is short for the Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model. It is a widely used forecasting method for univariate time-series forecasting SARIMAX can handle both trends and seasonality in data. This makes it an excellent choice in forecasting data that has both of these elements.",5f73fb91,0.6363636363636364
29456,c13f73168789c2,31646dcc,"### 1.3 Slice row and columns by label<a id='24'></a>
Syntax : `df.loc[starting_row_label : ending_row_label, 'starting_column_name' : 'ending_column_name']`",16175052,0.6363636363636364
29458,73d8e56bc709b1,82864b2f,> ,78ec3cce,0.6363636363636364
29460,132fa9714f2046,1eaeeec7,"## Exercise 4

** Use plt.subplots(nrows=1, ncols=2) to create the plot below.**",3bb1775f,0.6363636363636364
29461,0475899eec1ffe,af728661,We first cycle through the data coming from the columns of our data set and look at the different data.,d825dc37,0.6363636363636364
29464,90691864eb68c7,97935996,We are going to create dummy variable for categorical variables,3555ef9b,0.6363636363636364
29465,45cf7099ebd023,787f3d3e,# Rankings,0d292462,0.6363636363636364
29474,b066ab2167199c,b35661d3,### 2. Find Missing Values,18a1753d,0.6363636363636364
29475,1691c9f2c2f656,16f5db73,# Make a word cloud,70433e76,0.6363636363636364
29476,be2f4d8a6b73ca,bb3f7403,"<div style=""color:white;
           display:fill;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana;
           letter-spacing:0.5px"">

<p style=""padding: 25px; color:white; text-align:center""><b>Data Preprocessing</b></p>
</div>",5d8ce40a,0.6363636363636364
29477,5a8c553e21c70f,1bb4d815,"## Permutation Feature Importance

Permutation feature importance method is used to evaluate the effect of each feature on the classification task. It is defined as the decrease in model performance when a specific feature is shuffled. Process is repeated a number of times for each feature and mean decrease is taken into account.",9ebd9d8f,0.6363636363636364
29480,016abae0483764,e06098ac,## Model,bc9f289b,0.6363636363636364
29482,01bcce56d30079,d17b8d6b,Let's build a dataset that has a shuffled mix of pe positive and negative images,4436e984,0.6363636363636364
29484,d1ff7e10ee0102,65ecf7cb,# 5. Getting hard core,2cc71c3c,0.6363636363636364
29487,b10bd75889dad9,98ff24c6,## Explanatory Model,ee00ceee,0.6366666666666667
29492,2ada0305b68956,ef1b4e4c,### 108. Palette = 'gist_rainbow_r',133e26f4,0.6371428571428571
29493,ac1abfe1dfe815,8a1c80bb,"The words here are positive like great, thank, love, good, etc.",6529dbcb,0.6371681415929203
29495,c9b4e282e4e2c1,1156a934,A-Which kind of injuries do defensive linemans usually get? And the offensive? The safety players?,f44d339f,0.6371681415929203
29499,52cfd66e9ec908,86c0b147,"Things to note from this correlation analysis:
1. The rotation coordinates with `y` and `z` seem to be uncorrelated most of the time
2. The coordinates which have `x` are correlated strongly with the z-dimensional rotation (could this be indicative of something? I very much think so)",c74adcdf,0.6372549019607843
29500,71b75664517244,7d966c04,"## Performance Chart Best Manager

Let's take a look on how few best managers perform and comparison with their team performance.",fc905af5,0.6372549019607843
29501,5f4ae633cfd090,2fb745cb,"Considering the number of variables, it would make sense to combine similar variables to increase their correlation and reduce complexity by limiting the input features",a30a16e2,0.6373626373626373
29502,5ce12be6e7b90e,8cfcafbd,### Exercise: String access,c0ab62dd,0.6374269005847953
29505,5ffe6aa38958a1,c34bc529,"## 3.1 Concatenate all features and normalize
We select followign features as important: 
1. Class 
2. Sex
3. Age
4. SibSp
5. Parch
6. Cabin
7. Embarked

Let's drop the unnecessary columns and append the cleaned-up features (Cabin, Embarked)

normalization can be done for both training and test sets here. Alternatively, it is possible to use the same normalizaton settings for training and test sets separately",11f5412e,0.6375
29506,fdbbd573ba31c2,9eec59ae,Get_dummies for turbine_status,f7c28d74,0.6375
29507,3dd4294f903768,54efcaf4,"We can see correlation between the number of votes and the aggregate rating, but we can see that it isn't strong.",0d89d098,0.6375
29508,5f32117bcd5255,a3e13ac1,#### HST OPTICAL TWO,85882abf,0.6375838926174496
29511,ea4e559a86d613,f617260b,**Benign vs Malignant**,eff47843,0.6376811594202898
29512,7e275c8d5ff2a0,0f7905d7,Defining our Prophet() model.,b3afcc98,0.6376811594202898
29518,7e89d387feb9f5,2af46116,### Добавленная группа бинарарных признаков №№14 (126 признаков в отдельном датасете). Наличие или отсутствие соответствующей кухни в меню ресторана.,989e3a1b,0.6376811594202898
29520,a1a31459abf078,2107d199,"In the cells above we created flag variables for total timestamp at which questions are answered as well as the total number of questions answered by users. We then plotted user's activity across these flag variables. 

We see also see that majority of users have a learning journey of less than 100 days, a smaller portion of users are active after that period on the app. 

We can also see that a **user's correct answer rate improves massively as they tend to answer more questions, however this is not reflected to a great extent in user's correct answer rate with total time**. As we can see that there is a slight improvement in correct answer rate with time but its not a substantial difference. This maybe due to the fact that users learning journey is not often proportional to time, there may be long gaps between periods when users are active. Therefore simple increase in timestamp does not justify increase in learning rate for users. 


## EDA Summary - Answers to Questions
We've been able to answer some of the questions above through the EDA analysis performed by us. Here are some of the key findings from the analysis -
* There are more than 100M records available in the training dataset. 
* The mean of correct answer rate across questions is quite high, the distribution of percentage correct across questions has a peak between 0.7-0.8. The distribution for correct answer rate is quite spread out - indicating a big difference in correct answer rate across questions
* Instances where students read the explanation for previous question bundle improves the correct answer rate of current bundle. 
* High percentage of questions belong to part 5
* Students get better at answering questions through more and more practice. 
* High share of students have learning journey of less than 100 days
* Students who watch lectures are more likely to answer questions correctly through their learning journey
* Questions with higher number of tags have a higher likelihood of being answered correctly

We will now proceed to feature engineering and model development where we will be using this information obtained above.
",66fc0f54,0.6376811594202898
29521,8336d84cf3ff6b,a82ab39d,"## Random Forest with Optimized with GridSearchCV 

#### uncomment the below cell to run Grid Search CV ",b96b58a0,0.6376811594202898
29524,f3c6048d1058e3,a7387cc8,## 1) Logistic Regression,1d9056b0,0.6379310344827587
29525,00001756c60be8,36002912,За выброс посчитаем значения менее 3 кв.м. и больше 30 кв.м.,945aea18,0.6379310344827587
29527,84127ade6fde87,fecdef80,## Text embeddings,f55d05b6,0.6379310344827587
29532,434f930cb58aee,237266b4,"Now it is time to define to **ModelTrainer** class. In this class, we will create the top layer of our model architecture. Then we will compile our model with loss and optimzer fallowed by fit method. We will also callback such as *EarlyStopping* and *ReduceLROnPlateau* to make our learning process more efficient.",0e1d3554,0.6379310344827587
29537,04bac111ffbe9c,45e38717,"##### OHE features Sex, Embarked, Title, Pclass_new",82576b17,0.638095238095238
29538,bbaa07ad21cf4e,cc65a46c,"## 8. Modelling <a class=""anchor"" id=""8""></a>
",3ab6b254,0.638095238095238
29539,7454fdc444df16,4fe22fc0,"### Shuffling Our Data
Below we shuffle our data such that the patient order is shuffled, however the individual patient order is maintained.",a7818ef5,0.638095238095238
29540,55a5e31d03df9f,41738392,"## <a name='kerasmodel'> Creating models using Keras Applications </a>

Creating models from Keras Application is my preferred way to transfer learn because you can start unfreezing some of the layers as your data increases and you can check more details like the top layers and you're working with a model object type, not a layer. This adds some functionality like checking if a specific layer is trainable, the name of that layer, and more we will see now.",06dce00f,0.638095238095238
29544,957e035ba5b9d5,69277ea7,"## Process data

Read images and resize into memory",778ab3d3,0.6382978723404256
29545,04e6b0d3c70f46,40fcbe70,### For each class get features,56344f77,0.6382978723404256
29546,b61ab8f81dc03d,fb48759d,"<a id=""heatmap""></a>
## Heatmap",64d05394,0.6382978723404256
29548,5f674175839b32,19d1a31c,*Action and Sports are the best selling genres.*,53a2e343,0.6382978723404256
29551,3f25b363afec54,10bb8390,Number of common patients and Health camps between train and test set,bbdaae25,0.6382978723404256
29553,d07915a6e6992e,b469308e,**Parch**,2b912140,0.6384615384615384
29556,835a7b4e660d23,36b3d8b6,#### Histogram,53bc7a6e,0.6385542168674698
29557,4daf6153275cbf,3b9e29ad,#### 5. 2nd and 3rd Popular Cuisines,51db1961,0.6385542168674698
29559,4ae6a182abac64,aba91cef,"* If we have a quick look in the names of the passengers we will notice that each name has a title in it, so it can be a useful information 

  for our analyze. Therefore we can extract this title from the name of each passenger and then encode it like we did for Sex and Embarked.",418676c5,0.6386554621848739
29562,593d1d3d1df05a,5f88e4c7,# Model for Training,bc682ffe,0.6388888888888888
29563,cf39cde80e66b7,00ecc60b,"![](https://miro.medium.com/max/1063/0*N8USfmlDmXq7YuNy.png)
> Measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation, also used as a loss function for regression problems in machine learning. It usually expresses accuracy as a percentage.",aed4bc9b,0.6388888888888888
29566,ab6da5994949a3,e166fc9e,## K-NN Training  Results,fae6b91d,0.6388888888888888
29570,10b5af05d804ff,c7d16c63,"## So, we have mirror. But where magic is? Let's see a score!",4a9b1705,0.6388888888888888
29571,396bc36edb95d3,9a54c36f,### Model Evaluation - Random Forest,965e4f8f,0.6388888888888888
29576,b3e0b7e9ff6849,def95d03,"# 4. Train BG-NBD Model (Expected number of transaction)

In this step, we will train the BG/NBD model to estimate the expected sales in a given period of time.
Also, the trained BG/NBD model will be used in predicting customer lifetime value in the following steps. ",f6e4bb0d,0.6388888888888888
29577,c01049afb6d307,db9397f5,"
* Socialdrinkers' Bodymassindex is higher.",d37d3b5d,0.6388888888888888
29580,0f5085b162bd9f,94934c42,# Spectral Clustering,a3d989ee,0.6388888888888888
29581,69ac33d79f5130,30f30cfe,### Accidents are high on weekdays because low number of people use vehicle on weekends.,9d760d2a,0.6388888888888888
29584,268a610bbc64b4,bb99f46f,Defining function to create number of months advance column,8a16f301,0.6388888888888888
29587,6dcfe6a610d86b,4351e58d,"## Random Sampling
https://docs.python.org/3/library/random.html",d05c59da,0.6388888888888888
29588,ba4b3bd184acbb,04a4d936,***,0f5de724,0.6390977443609023
29590,2a123b4e8f9433,76eb900c,Train the model,0a082218,0.6391752577319587
29599,fc8e0042411c46,9c948738,- All entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.6394984326018809
29601,d96642860ab3dd,c4fe491c,### 2.4 Feature Engineering Fare and Age feature,98419d48,0.6395348837209303
29604,5cb7f999fd1ecb,66d1f0e1,# Plotly - Donut Chart ( Interactive ),88b54f70,0.64
29610,21c1e34efd71b8,ca5cf88a,"Lets run the SMOTE cross validation.
NOTE - It takes ~ 10 mins for the code to run.",23b2cdd6,0.64
29615,67b7354e96113a,3db12075,**Distribution with respect Survived**,dca94250,0.64
29616,0687cd5c8597db,02b1fb12,### **Plot between Training Loss and Training Accuracy**,4edec76a,0.64
29620,83df814455f06c,9d3cb8c6,"Here, **y_test** are the true class labels and **y_pred_gini** are the predicted class labels in the test-set.",c9cff71a,0.64
29622,e5dd725b8fa422,8310839d,# Predicting using AR model,14675d8b,0.64
29624,9395559895004f,0be0871b,## Plotting Accuracy,b5a0494b,0.64
29628,b10bd75889dad9,f2a59d5a,#### Lets build another Explanatory Model using RFE then Random Forest Classifier and finally get variable significance using Logistic Regression,ee00ceee,0.64
29632,19aae4a6ede288,260b7be0,## Making Predictions,56934674,0.64
29637,7e1da639035ac5,afbad0f1,# <a id='12'>12. Supportive Environment analysis</a>,120b6c23,0.64
29639,cb570c7b7f0501,66168d21,"### Research Question 6  ( is it about the date of appionment !? )
Do we have a certian date with a high propability of no_show !?",a200a0ec,0.64
29642,6cade0b6a41ba2,e3279aab,"##### From the box plot, we can see that there are many outliers prsent in the higher region. Technically we can neglect the top oultier to predict output.
##### But from application perspective, we can have pateints with that BMI level and if we neglect these values, our model wont exterpolate higher values. Therefore, we will continue with this missing values.",e6110293,0.6403508771929824
29647,04ff2af52f147b,39ccb367,"However, it is important to take *Surname* feature with a grain of salt as different families travelling onboard can have the same *Surname*.  We can see that with the 'Davies' *Surname*.  There appears to be some strange data for these individuals that is incompatible.  Namely the *Parch*, *SibSp*, and *Ticket* values.  ",d5f37be9,0.6404494382022472
29649,ff3a8ce61fab6a,ba1cff48,"<hr>

### Exampel 2 ",9afe1654,0.640625
29662,9eed0fae1c7958,a1e27cc0,# Train the model,3fb1438e,0.6410256410256411
29667,4d91e84c564cbe,e142dd71,"The `-> None` part is telling us that `list.append` doesn't return anything. But if we check the value of `planets`, we can see that the method call modified the value of `planets`:",355a43e3,0.6410256410256411
29669,897ca904b74a98,ad68b6c9,# Modeling,c5844ad4,0.6410256410256411
29671,d4c5aaa4b36810,7557df0c,"Lots of these features are strongly correllated with one another, so it will likely be worth while to try and ridge and lasso regression. ",65441f28,0.6410256410256411
29677,b01ee6cb674fa3,57542f32,"# International Space Company Kosmotras 

ЗАО Международная космическая компания “Космотрас”

Joint project between Russia, Ukraine and Kazakhstan, founded by 1997. The project was put on hold due to Russia invasion of Crimea


Uses Dnepr rocket, a converted ICBM into a conventional rocket.

I could not find much info on it, though I believe this is a state owned project, a public company",a8ffd35e,0.6413043478260869
29679,0ad8d416b89b78,e8849df0,'Baseline' Logistic Regression,0b0562f0,0.6415094339622641
29682,614ba9f0c62677,c81018e2,"<a id=""13""></a>
### Compile Model
* categorical crossentropy
* We make binary cross entropy at previous parts and in machine learning tutorial
* At this time we use categorical crossentropy. That means that we have multi class.
* <a href=""https://ibb.co/jm1bpp""><img src=""https://preview.ibb.co/nN3ZaU/cce.jpg"" alt=""cce"" border=""0""></a>
",b8551335,0.6415094339622641
29683,a070fd03ae8ed2,c798f830,"# 6. Анализ третьей модели (3)
---
## 6.1 Расчет прогноза дефолта ",c0ec4138,0.6415094339622641
29685,510b8303776bb6,e42b2e7b,# Machine Learning model,18080db8,0.6415094339622641
29687,43e60eb1362f5c,5d97fbc9,# Tableau Visualization to give more clear Insight and Inferences,87934234,0.6415094339622641
29689,62487bcd70b199,235458ca,## <a id='7.2'>7.2. Compare Model Metrics</a>,f6ae50af,0.6416666666666667
29691,21413205980558,f3f41f11,"# It can be seen that the marketing results for the elderly (over 60 years old) and young people (under 20 years old) marketing success rate is higher, even in young people will not fail. But with the growth of age, until the middle-aged (30-40 years old) stage, the marketing failure rate will rise, after that, with the further growth of age, the marketing failure rate will start to decrease.
# 可以看出，营销的结果对于老年人（60岁以上）和年轻人（20岁以下）的营销成功率更高，在年轻人中甚至不会失败。但是随着年龄的增长，直到中年人（30~40岁）阶段，营销的失败率会攀升，在此之后，随着年龄的进一步增长，营销失败率又会开始降低。<p>
#     To sum up, the key marketing targets of banks should be those under 20 years old and over 60 years old.But the 20-to-60-year-old group is the main customer group of the bank, so their feelings also need to be considered.
# 综上可知，银行的重点营销对象应当是20岁以下和60岁以上的群体。但是20到60岁的群体是银行的主要顾客群体，也需要考虑到他们的感受。",84197de0,0.6417910447761194
29693,e58e68e4eeefe5,e79c0c30,"Considering all the features, the accuracy with **Logistic Regression is 78% and Random Forest is 85%**",a87662ce,0.6417910447761194
29700,169177b6e9edea,8e6c9390,<h1><b>Modelling,ca42152f,0.6421052631578947
29701,f91f58d488d4af,40b7029f,"Now, let's do this with an vector argutment.",5df1bbf3,0.6421052631578947
29702,bd380b97b5c894,8725d61c,## XGBClassifier,66f2562a,0.6422018348623854
29704,3c2033cc99c12c,1943c3bb,"+ A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.",dfa22a54,0.6423357664233577
29706,fc8e0042411c46,ef966fd9,## Tags,af476c2a,0.6426332288401254
29707,0d59a3e0130db0,127151b3,"Now, using the keras framework tokenizer, turn the text into a sequence of integers. After that I select the maximum comment length, all comments longer will be truncated, and those that are shorter will be padded",285f04b2,0.6428571428571429
29708,1fac5edd4063ba,9c96b944,#Have a peek at Brazil. ,04bc01e0,0.6428571428571429
29712,675b60eaf415a6,1bfa9b8e,"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**
* **Validation accuracy has been on the higher side than training accuracy for many epochs**
* **This could be for several reasons:**
  * We used a pretrained model trained on ImageNet which contains data from a variety of classes
  * Using dropout can lead to a higher validation accuracy

 
",68c0b725,0.6428571428571429
29719,8dd655515e7d18,01fbe35b,### Category Analysis,895f41cf,0.6428571428571429
29728,20c9a2456e494a,516a4708,# Compiling the model,3e487f55,0.6428571428571429
29730,898d18d501f68d,35d3b08b,"for find the correlation, heatmap is a good plotting So excluing Soil_type , plotting the heatmap",d8bdea2d,0.6428571428571429
29732,953ab4b6631ba8,0040ff15,"We see that in each fold, the distribution of targets is the same. This is what we need. It can also be similar and doesn’t have to be the same all the time. Now, when we build our models, we will have the same distribution of targets across every fold.",8bf6bfa1,0.6428571428571429
29734,2d40f383473fa4,b0cdfddc,"About the categorical values left, I will not make a encode, this part I will let the job for the AutoML.<br>
The Deck `NaN` will be handled by H2O AutoML.",1da1eff0,0.6428571428571429
29735,9c33d1955302bf,b99519f9,# **Prediction and Evaluation**,0d9cfc89,0.6428571428571429
29747,066c5ee1ef39e6,806beed7,### Ruddit TD-IDF + Ridge,0f394e1b,0.6428571428571429
29749,f13534449a3750,8d3dd9a7,"**Create different datasets**

One way to proceed can be to make the dataset used for the trainig in the classification task more balanced, in this case we can use all the images with ships and balance the dataset with part of the non-ship images. We will create two different dataset as follow:

* 50% Ship + 50% No Ship ---> Total of 85.112 Images
* 100% Ship + 0% No Ship ---> Total of 42.556  Images

We will train the classifier and test the performace for different trainig sets. ",8b7f3332,0.6428571428571429
29750,df51d4c54fbb91,8b521455,## Hard model,4226dd72,0.6428571428571429
29752,e78e7edae89049,87724e28,# ARIMA model,9cef1d94,0.6428571428571429
29753,b8849a04581d32,992904a0,"#### Based on the violin graphs, the operationTime characteristic is practically the same for all Punt results, so it makes no sense to take it into account.  
#### The kickContactType characteristic can cause data leakage, since it contains typing of the Punt's outcomes, and the goal of the model to predict the outcomes themselves. Looking ahead, when training the model, adding kickContactType increased the accuracy by 15% at once, which proved to the creation of a data leak.",b8a568cd,0.6428571428571429
29756,1c381451c17150,a3c965dc,"
# Train Model
Even with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way possible. However, it should take an unrealistically long time to get to that point (or just impossible).
",e79b530f,0.6428571428571429
29758,3cd78d8d6d56e4,8167a7ca,"### Model Training with Full Dataset 
In this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from the randomized search from the previous part.

Based on the hyperparameter search the following parameters were found:
- n_neurons = 150
- n_hidden = 10
- learning_rate = 0.03",9f632e94,0.6428571428571429
29759,f4514ec092a771,1d2aaefa,## Postprocess for submission,3739ab1e,0.6428571428571429
29764,b4ecd6e4277e3c,73bd01b1,### Use Stratified K Fold to improve results,94d79d5f,0.6428571428571429
29766,6e9b4020644836,932412ed,#### lets make a function to remove Numbers from the reviews,5ad41fc6,0.6428571428571429
29768,67efe818cb2372,8e8dc12d,"So, what we're not seeing is overfitting. Normally, we'd like to overfit and then generalize the net using dropout, layer removal or a bit of general tweaking. Additionally; if you look at the leader board for the MNIST competition you'll see that 0.994 won't get you into the top 50%. We need at least 3 9s in the validation set to be remotely competitive. If we wanted to save the net though, we'd do the folowing. ",f28a2a34,0.6428571428571429
29771,a76e0e8770b7a0,e744774d,Looks like all countries have terror problems. ,02863d3b,0.6428571428571429
29772,2ada0305b68956,87da9d56,### 109. Palette = 'gist_stern',133e26f4,0.6428571428571429
29776,916ccf243827f1,96994166,## 10. Backward Propagation with Dropout,5147f4d2,0.6428571428571429
29785,9c044fa3072552,aa768273,"There's a slight peek at 7am. But the number of accidents is largest during the afternoon, i.e., 1pm to 6pm.",1362842e,0.6428571428571429
29787,5ce12be6e7b90e,b0666053,"The sequence below (named _seq_) consists of 20 characters. 

1. Print the 2nd and 7th characters.
2. Print the 2nd character from the end.
3. Slice the first half of the sequence.  
4. Slice the second half of the sequence.  
5. Slice the middle 10 characters",c0ab62dd,0.6432748538011696
29797,d5f78aa381f58d,3c757b9a,## XGBoost,d60f358f,0.6436781609195402
29800,1eb62c5782f2d7,3c9d0435,"### Finding a $z$-Score Given an Area [example 1]

- Q1: Cari $z$-score yang sesuai dengan cumulative area $0.3632$.
- Q2: Cari $z$-score yang memiliki $10.75\%$ dari luas distribusi disebelah kanannya.",bb69f147,0.6438356164383562
29803,3d08ca7656dec0,b5f2994f,1. Split data for training and testing,bd3f87e3,0.6438356164383562
29805,ed8009f482b380,603ed63d,Now we do train test split with test size = 30%,e99941fa,0.6440677966101694
29806,a077820f7ab459,3ce9577b,### Create imgs and labels,05a43104,0.6440677966101694
29812,f2e5e9fb9eaaf7,7bc4f6b4,"<a id=""6.1.1""></a>
### 6.1.1 XGBoost Classifier",048e0d08,0.6440677966101694
29813,1294fb4c86f993,5f70bc43,"<a id='Q6'></a>
### Research Question 6: What census data is most associated with high gun per capita?  ",4471e513,0.6440677966101694
29823,726833f92fb87a,e26b4dee,"Depending on the categorical feature, we will choose if encoding it by one hot encoding or leabel encoding.",7dc5e1b6,0.6442953020134228
29824,188731d7fa0604,46f05160,## 모델 및 평가,7cc543d3,0.6444444444444445
29825,e25c0f830df3f4,d011fea4,# Distribution of Subjectivity,fdcf7189,0.6444444444444445
29826,3597174a998d4d,46b465ce,#### 2.2.2.2 the need for meal,276892ed,0.6444444444444445
29829,c8bf959b9608cf,ef98e18a,"## Total Loss

Now we will combine style loss and content loss to find total loss. Let's define variable to store the the loss.

### Calculate Content Loss
Calculate content loss by multiplying content loss with its weight",155e3672,0.6444444444444445
29836,b0c2805cd5c087,f25029a0,Image ilovephd.com,0446f327,0.6444444444444445
29839,d77e6d61ad2e8b,103343b8,# Predict on test / hold-out Sample,03fd0e96,0.6444444444444445
29840,d6cbd7160961dc,a7242f52,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 95550. Note that the chi-squared value of 170.05 is not really meaningful, since the test is inadequate for bigger samples.

* The center graph shows the result for city Torrinha, which had the worst chi-squared value. 

* The rightmost graph shows the result for city Braúna, which had the best chi-squared value. 

Note that although the chi-squared value for the aggregated analysis is not meaningful, the distribution is very similar to what Benford's Law predicted.

Also, we see the importance of a disaggregated analysis, since we can separate results that are adherent to Benford from those who are not. Thus, we focus our attention on the cities where the probability of data manipulation is greater.",36d74664,0.6444444444444445
29841,396bc36edb95d3,5719da18,#### AUC and ROC for Training data - Random Forest,965e4f8f,0.6444444444444445
29842,d58491f2896fc1,f3d2bbbb,**mutation random bir şekilde seçilen bit ya da gen üzerinde mutasyon sağlamakta ve onları rastgele çevirmektedir.**,514bfdff,0.6444444444444445
29843,4fd4b6a80d40e3,90909df3,![image.png](attachment:image.png),f6913cc3,0.6444444444444445
29844,e9b9663777db82,951ae40c,At first we have to encode all the categorical columns to dig deep into the data,648e8507,0.6446280991735537
29845,2f47abddfd1928,6acf3570,"There is not much insight in this feature correlation, basically shows a high value because of the massive amount of people traveling alone.

Also that the families tend to be smal, a part from the passengers traveling alone, most of the passengers travel with a family size of 1-2.",ae33cc0b,0.6446280991735537
29848,0e2a23fbe41ca9,1f615c8c,"Observations:
- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10
- it seems to be between 0 and 10
- Need to handle the outliers
- there is not apparent pattern between the columns though",64e4762c,0.644927536231884
29853,ad26c020235dfc,6c467c9f,Plot the timeseries before and after prediction. The image with the title before shows the origin data with nan values. The after image shows the origin data and the prediction for nan values.,bf766e48,0.6451612903225806
29854,16862cb02d73d5,4f7edc2b,Now we have figured the **anomalous behaviour** at a use case level.But to be actionable on the **anomaly** its important to identify and provide information on **which metrics are anomalous in it individually.**,d7ffa1a6,0.6451612903225806
29856,b59b5aaeedb1fb,789248b5,> #### Plotting the weight attribtes ,1ad63faf,0.6451612903225806
29869,0d9a2067267ba1,6107ab71,"### Missing values imputation
* Categorical or Discrete variables => **most_frequent** imputation
* Linear features => **mean imputation**",abc194fb,0.6451612903225806
29871,f15eac23fbcc9d,82b46c42,"Not entiredly necessary to use split_vals here since it is written to work on time sensitive data. 
I put these functions below here just because this is right off fast.ai's ML lesson1's Jutyper Notebook. Forgive my laziness.",ea46d8af,0.6451612903225806
29872,921fff7d3040db,ebbd8a86,# 6. KNN model,5f36ced9,0.6451612903225806
29876,0cb456a5456cf9,537f53ae,# **Genterate confusion matrix to assess the performance of regression model** <br> 生成混淆矩阵，考察模型的效果,5701729c,0.6451612903225806
29877,56785caebaa256,39fa7914,"## 5.1.2. Results visualization<a class=""anchor"" id=""5.1.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.6453900709219859
29882,ce9ed5e2d601d7,0a290885,"## Confusion matrix

$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$

$Precision = \frac{TP}{TP+FP}$

$Recall = \frac{TP}{TP+FN}$

$F1 = \frac{2*Precision*Recall}{Precision+Recall} = \frac{2*TP}{2*TP+FP+FN}$

",f58a2f43,0.6456692913385826
29884,81712ee7510ac5,446bec0c,"We can also call the set function.   set()
whereas we can select the unique elements from the list",c4685e79,0.6457142857142857
29887,8c7e00ca3dc5a7,c77d0a56,## Handle the Skewness in the Data,c83346e4,0.6458333333333334
29891,3b5903412fe741,e1d80a56,"## Conditional selection

So far we've been indexing various strides of data, using structural properties of the `DataFrame` itself. To do *interesting* things with the data, however, we often need to ask questions based on conditions. 

For example, suppose that we're interested specifically in better-than-average wines produced in Italy.

We can start by asking each wine if it's Italian or not:",ad231969,0.6458333333333334
29892,a3ae04b78e45b5,caa8295b,**KDE PLOT**,4195da8b,0.6458333333333334
29893,2a377ced98d67a,ffaee4e7,#### Note: All the features are highly correleated,262231a8,0.6458333333333334
29894,eb0854a6601407,29580b5e,"The new scatterplot reveals that the less the observations, imply a much more uncertainty in the mean target. ",6d107747,0.6458333333333334
29900,a5a419dc7245b0,743fc26f,"**X will contain all the Independent variables such as no_of_trainings, age, previous_year_rating,  length_of_service, KPIs_met >80%, awards_won?, G_m,  R_sourcing <br>
Y has the is_promoted i.e. the Dependent variable**",4279726e,0.6460176991150443
29906,c115e287523aab,e6c2fce1,"# Loss Function
Loss Function for this competition is **RMSE: Root Mean Squared Error**
$$\textrm{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$

where $\hat{y}_i$ is the **predicted** value and $y_i$ is the **original** value for each instance $i$. Check out this blog, [What does RMSE really mean?](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e) for more details",feb1288b,0.6461538461538462
29911,03048e86a6d806,19415e1c,This part is also using median as the measurement.,1285c231,0.6461538461538462
29915,fd4017c1514157,c1ab3c89,## **Train & Test Data**,fd8f0896,0.6463414634146342
29916,dbd96dd275dc60,6c597096,# Change max samples value,1ed493a8,0.6464646464646465
29918,ba4b3bd184acbb,9eea859f,"# Writing Data

Since our data is more usable now, we should create files to save it for future access.

### To CSV

Our first option is to write to a csv file with the added option of compressing it to reduce storage.",0f5de724,0.6466165413533834
29919,91eaec994e0c6f,cd8720bc,- FOODS_3 and HOBBIES_2 have respectively the highest and lowest number of sales among all departments.,376aef10,0.6466666666666666
29923,52cfd66e9ec908,40cde752,# Augmentation ideas,c74adcdf,0.6470588235294118
29925,6aaee7fdbc7945,808160bb,# **Question 3**,dae653ae,0.6470588235294118
29930,523123dad03177,b4943187,Again Group E are better at Reading too. ,48a5e4e6,0.6470588235294118
29931,21bce4ec54b3fa,f5c0e6be,# Importance based on correlation to target,35546e30,0.6470588235294118
29934,a871419285588a,78a24cbb,## Crop the images on the tumors,5e08e15f,0.6470588235294118
29937,907f08f9a2c6cf,1efadd13,### Final Training with Best Hyperparameters,aa84c325,0.6470588235294118
29938,871901bb4aae21,edad8d8a,# Model,79ab34a9,0.6470588235294118
29939,c4386b8a01d66e,d8d1461b,# Trihalomethanes_random,dc732bf5,0.6470588235294118
29941,917957c6c4065f,21b0bba6,"조회수 대비 좋아요는 평균은 2%이고 최대 36%입니다.  
조회수 대비 싫어요는 대부분 0%로 측정되고 최대 4%입니다.  
조회수 대비 댓글수는 대부분 0%인데 최대가 52%네요. 특이값이 있는 것 같습니다.    
좋아요 대비 싫어요는 평균 11%인데 최대가 976%입니다. 이 항목도 특이값이 있는 것 같습니다.",55b8ed68,0.6470588235294118
29943,d0080e3a39bc5c,3c2852ed,**THAT EXTRA PUSH TO THE SCORE...**,2fcde4cf,0.6470588235294118
29944,36d0d4cb9c7993,c9b44bec,### ploting the line in the scatter plot,34b93e27,0.6470588235294118
29946,7f74a04ae75792,6257b3a3,"# <font color=green>Insights through data visualization<font>

### Create a chart to visualize potential customer breakdown",d01e91da,0.6470588235294118
29951,4fa553c2b837d4,0596eaf0,"In some cases this approach will meaningfully improve results. In other cases, it doesn't help at all.
__________________________________________________________________________________________________________________________________________________________",c65a23e9,0.6470588235294118
29953,0e662a463309e7,f830d5e1,KMeans CLustering,84c57e51,0.6470588235294118
29954,a0a5baa6c7e12a,c39b6932,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Training Set Overview</div>",551d41de,0.6470588235294118
29957,02b7e38902069e,c0258514,#Installing Stanza,726a03a0,0.6470588235294118
29960,e170d33ee1da8c,6c685622,The best performing model is stored and loaded at the end of the training by `SaveModelCallback`:,253cee3c,0.6470588235294118
29964,842547b2def18c,19d5f19a,"### Converting categorical feature to numeric

We can now convert the EmbarkedFill feature by creating a new numeric Port feature.",b8efde6d,0.6470588235294118
29965,7cfd96218dd933,03fb51f7,"#### **ATTENTION**
* SHARP INCREASES ARE OBSERVED ONLY IN TURKEY",7c34d96c,0.6470588235294118
29966,64169805aacf17,447b04c9,# Fire the optimization ,1f12ded0,0.6470588235294118
29969,71b75664517244,c3f6bdc1,### Alex Ferguson,fc905af5,0.6470588235294118
29972,b779c3ce7b671a,ac737f72,![movie](./movies/movie.gif),ca778770,0.6470588235294118
29976,869a39a3d4dea2,8f8c5ce2,"As we know image is composed of different color channel like RGB (Red, Green and Blue) ,we are going to explore the how the different channels are represented and we do some little image processing to exactly the show the channel the image belongs",9020daf8,0.6470588235294118
29978,979f1e99f1b309,fa91a53c,## Tuning Linear Regression Parameter,d1bfebbf,0.6475409836065574
29985,d1ff7e10ee0102,e80a3a74,"In Ayn Rand's novel, 'Atlas Shrugged', there is an often-repeated question: who is John Galt? A big part of the book is about the quest to discover the answer to this question.

I feel Randian now. Who is 'SalePrice'?

The answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.

According to [Hair et al. (2013)](https://amzn.to/2uC3j9p), four assumptions should be tested:

* <b>Normality</b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely  on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.

* <b>Homoscedasticity</b> - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' [(Hair et al., 2013)](https://amzn.to/2uC3j9p). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.

* <b>Linearity</b>- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.

* <b>Absence of correlated errors</b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.

What do you think Elvis would say about this long explanation? 'A little less conversation, a little more action please'? Probably... By the way, do you know what was Elvis's last great hit?

(...)

The bathroom floor.",2cc71c3c,0.6477272727272727
29988,9bcfa825c8b2e6,23d77865,Encoding işlemleri yapılır.,220f36e4,0.647887323943662
29997,c6f8ff61a5fa87,13d27ff6,"# <span style=""color:blue;""><strong>8.Stacking using lightgbm</strong></span>",3eea586b,0.6481481481481481
29998,fdc3afd309b850,1bca73a9,"<a id=""p""></a>
## 7.4 Population",966bde38,0.6481481481481481
30001,e0f03003a69819,3a954012,# Normalization  of Data,609ad1f4,0.6481481481481481
30002,5f4ae633cfd090,09950810,"Some variables are already engineered. They have combined features, so I'll just be doing those that haven't yet been touched.
These are the already engineered features:
1. lose_streak_dif: (Blue lose streak) - (Red lose streak) 
2. winstreakdif: (Blue win streak) - (Red win streak)
3. longest_win_streak_dif: (Blue longest win streak) - (Red longest win streak)
4. win_dif: (Blue wins) - (Red wins)
5. loss_dif: (Blue losses) - (Red losses)
6. total_round_dif: (Blue total rounds fought) - (Red total rounds fought)
7. total_title_bout_dif: (Blue number of title fights) - (Red number of title fights)
8. ko_dif: (Blue wins by KO/TKO) - (Red wins by KO/TKO)
9. sub_dif: (Blue wins by submission) - (Red wins by submission)
10. height_dif: (Blue height) - (Red height) in cms
11. reach_dif: (Blue reach) - (Red reach) in cms
12. age_dif: (Blue age) - (Red age)
13. sig_str_dif: (Blue sig strikes per minute) - (Red sig strikes per minute)
14. avg_sub_att_dif: (Blue submission attempts) - (Red submission attempts)
15. avg_td_dif: (Blue TD attempts) - (Red TD attempts)",a30a16e2,0.6483516483516484
30003,0932046e1f485d,17a350e7,Mandatory look at the game category.,218cc7a3,0.6484375
30005,2ada0305b68956,909f8188,### 110. Palette = 'gist_stern_r',133e26f4,0.6485714285714286
30006,b7b1057764fa02,ab3483b4,"After having defined a neural network in `keras`, the next step is to compile it.",5053a192,0.6486486486486487
30008,d76896b30cebd3,8b53e5fe,"Mean duration of Failed, Successful and Cancelled Campaigns

",1b4e8f34,0.6486486486486487
30010,e4525eb0c96f28,e0ad2f0f,"As you can see, we have come up with a simple y=mx+b model. According to our newly generated model, the sales for a game generally decrease by about two hundred USD after each year. This somewhat makes sense given the general decreasing trend we have seen from earlier explorations. Usually, the time a game gets released is when there is the most hype for a game. Games may update and introduce new content during its lifetime, but eventually the hype will fade at some point, resulting in an overall decline in sales. This might explain the overall decreasing trend we see. We can plot this model on top of our data so we can see how well it fits.",2093a1f1,0.6486486486486487
30011,a6c34cd514e30e,ec999b2c,Scatter and density plots:,bf603ddd,0.6486486486486487
30012,62037c5832129c,5349c8eb,"Remember that we previously encoded the class labels so that *malignant* samples are the ""positive"" class (1), and *benign* samples are the ""negative"" class (0):",61474350,0.6486486486486487
30013,ac9b48d531bad9,e6fac757,# Hyperparameter Tuning Using Grid Search,95965e35,0.6486486486486487
30017,993966a1cb5eb1,5d3d4752,## Metrics,5cd181e2,0.6486486486486487
30018,27d5291d6365ba,5b8bcdab,# Taking out the map coordinates,96b30229,0.6486486486486487
30025,ee23a565163388,faad0bb4,# **Model Building**,88aacbc4,0.648854961832061
30027,4c47839b067546,34ba8adb,"### птс, владельцы",1f517b02,0.648936170212766
30030,9e27af2600925c,5a66d6b9,"<font color='blue'>
**What to remember:**
You've implemented several functions that:
- Initialize (w,b)
- Optimize the loss iteratively to learn parameters (w,b):
    - computing the cost and its gradient 
    - updating the parameters using gradient descent
- Use the learned (w,b) to predict the labels for a given set of examples",9b556435,0.6491228070175439
30037,30fdc4a6e3c1db,3647573e,"What we see:
* In foods, FOODS_3 have the highest sales but also have a high seasonality ranging from 500k to 600k back and forth in the same year. FOODS_2 has also increased over the year.FOODS_1 has pretty much remained stagnant over the years
* HOUSEHOLD_1 has shown the best increase in sales over the years as compared to other departmenrs",6111ddee,0.6491228070175439
30040,6cade0b6a41ba2,016cdab7,## 3.11. Smoking Status,e6110293,0.6491228070175439
30041,54004b32784b68,cf415d59,# 4- Data Visualization,27213ca9,0.6491228070175439
30042,c3498779cda661,171b258c,# Valores Atípicos,0f531b65,0.6491228070175439
30046,722cd844dfbe8f,76df733b,"Thanks to this function, we obtain **4 ordered sequences *(one for each Scan type)* of 24 images of dimension 120 x 120** and the preprocessing has been applied.

To couple our four types of scans, we will use a **multi-modal approach** to create our model. We will integrate 4 different inputs for a single final classifier.",0cedb385,0.6493506493506493
30049,2cb457b60dd246,cd254dbf,### Train the Model,339367df,0.6493506493506493
30050,241cf32abb22d8,8ecce2fa,"* The optimal KNN model based on the full features has a mean AUC score of 0.725 with 14 nearest neighbors and with Manhattan distance.
* The optimal KNN model based on the top 10 features has a mean AUC score of 0.934 with 14 nearest neighbors and with Manhattan distance.

In general, the performance of KNN models seems to have improved after feature selection. ",47157066,0.6493506493506493
30052,663bbc9eaf267b,4f01016c,## engineSize,32445529,0.6493506493506493
30062,3c2033cc99c12c,3b83f39c,"*In this part, i will use the ROC curve and the Precision-Recall curve to evaluate the performance of the model.*",dfa22a54,0.6496350364963503
30065,8cd6656a65e6e7,5c01f990,## Model,c8e1697a,0.65
30072,bfe6c7096b1ad0,33b565ef,## Создаем и обучаем модель,fffd95e0,0.65
30077,b95c657bd26c57,35c5ed88,We get bad precision and recall as feature scaling is not done,eb13c3f8,0.65
30080,edc19e349fe80a,88e1fd2c,### 3. Convert data to numpy array and extract features column only,7882221a,0.65
30082,37b09262279764,17da5259,#### XGBClassifier,37c4c417,0.65
30085,83df814455f06c,a624209a,"### Compare the train-set and test-set accuracy


Now, I will compare the train-set and test-set accuracy to check for overfitting.",c9cff71a,0.65
30086,f8bcb6d96fd560,89f37123,Lets initialize parameters,390bb0f0,0.65
30087,712198370d5521,49c9ec48,"The above cell indicates that four will be an optimal number of clusters for this data. 
Next, we will be fitting the Agglomerative Clustering Model to get the final clusters. ",5882e04c,0.65
30088,ff83da40bcdb19,aa4ecf60,"**TRAINING THE MODEL:**

Epoch: It's just repeat again and again. It's make your accuract better but be careful you may make a overfitting.

",36b5ec8c,0.65
30089,09bac0c221388e,73529496,"Here we utilize the SpaCy NLP pipeline for English, which is very handy because it returns a Doc object that contains the already tokenized and preprocessed text, split into words and sentences.",bea4aa2e,0.65
30096,5ba4207c371899,6211024e,"Feature engineering - Label encoding. pd.get_dummies is a method that allows us to do a quick one hot encoding on our columns. This takes every value it finds in a single column and makes an individual column for each value, with a 0 or 1 indicating whether that column is 'hot'.
We're also going to throw in some basic numeric data: duration, src_bytes, dst_bytes.",187b1451,0.65
30106,5626e84c4e6bf8,62d129f7,"## Bayesian Optimization
Bayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives. Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures our beliefs about the behaviour of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines what the next query point should be.

Source: [Wikipedia](https://en.wikipedia.org/wiki/Bayesian_optimization)",e2ecb669,0.65
30110,e3c0b55ed519e2,6a87fb93,# Locations with *Highest* Hospital beds per projected infected individuals who need to be hospitalized.,9f51352e,0.65
30112,eda49464dd6d1b,c9e3728d,"# Tensorflow Neural Network
* The test and train dataframes are reworked from the beginning to yield the proper format.  
* There are over 200 dummy variables, mostly for locations and sales channels.",8421f81f,0.6503496503496503
30113,e19e307b3fd188,3eb7a6f7,### Comparation,2173955b,0.6504065040650406
30115,a2176d4653ef60,3f8125fd,# Data Evaluation - SVC,ac908675,0.6504854368932039
30126,510b8303776bb6,54211a84,## Splitting data into train test sets,18080db8,0.6509433962264151
30127,43e60eb1362f5c,d4c7e6a2,# Prediction,87934234,0.6509433962264151
30128,726833f92fb87a,154ccbe4,## Job,7dc5e1b6,0.6510067114093959
30135,1660daf8867980,6357a22d,"# 2.3 TD-lambda
**Theory**  
In Monte Carlo we do a full-depth backup while in Temporal Difference Learning we de a 1-step backup. You could also choose a depth in-between: backup by n steps. But what value to choose for n?
* TD lambda uses all n-steps and discounts them with factor lambda
* This is called lambda-returns
* TD-lambda uses an eligibility-trace to keep track of the previously encountered states
* This way action-values can be updated in retrospect",42d7cffc,0.6511627906976745
30140,b2e2c792b886ac,6d796888,Построим кривые обучения,2a184b39,0.6511627906976745
30144,a2444ab5d5f147,ff41acd0,### At this point we can visualize some most common words using the wordcloud,10617755,0.6515151515151515
30145,f166950fa915f8,2e210602,### Callbacks,a7f6ca5e,0.6515151515151515
30148,312135b445bd23,711e0fe6,"At the end of this stage, we had seed sentences for each sub-task.",8ced381f,0.651685393258427
30149,5f27526aa6c113,7106715f,there is no such impact of year on claims,a5c26ab6,0.651685393258427
30152,0e2a23fbe41ca9,b4df1875,"### 7. lag_6 columns

- ```avg_sales_lag6```: Monthly average of revenue in last 6 months divided by revenue in last active month
- ```avg_purchases_lag6```: Monthly average of transactions in last 6 months divided by transactions in last active month
- ```active_months_lag6```: Quantity of active months within last 6 months
",64e4762c,0.6521739130434783
30154,f6c1eb62cceb70,7f021ae7,"Now that we have our Prediction target as well as our features, time to build the model",90a1b790,0.6521739130434783
30161,b01ee6cb674fa3,a5015fc8,"# Khrunichev State Research and Production Space Center

A public company from the Soviet era and now Russian federation

The Khrunichev State Research and Production Space Center (ГКНПЦ им. М. В. Хру́ничева in Russian) is a Moscow-based manufacturer of spacecraft and space-launch systems, including the Proton and Rokot rockets, and the Russian modules of Mir and the International Space Station.

",a8ffd35e,0.6521739130434783
30165,57bad3860b0fa4,68d7fb45,# Model 1,05138a5e,0.6521739130434783
30168,ed5c03987493eb,29b410b0,"Bravo!  
Since we now know how tensorflow probability works, let's jump into a toy dataset and see it predicting 95% confidence interval in a regression problem.",bea97744,0.6521739130434783
30170,33d736abb432d0,75e7e318,## 测试分词效果,d64052a2,0.6521739130434783
30171,4913b61a68d355,fb13c205,### Evaluation,6e269c6a,0.6521739130434783
30182,17a24d566ffa59,3258c7e7,1. ![](https://s3.amazonaws.com/nlp.practicum/svd_vs_pca2.png),89049e56,0.6521739130434783
30190,b61ab8f81dc03d,bc5cfe9c,"<a id=""select_model""></a>
# Select a model
The training set is a subset of the data set used to train a model.
We can name the variables as presented below:
* **X_train** is the training data set.
* **y_train** is the set of labels to all the data in X_train.

The test set is a subset of the data set that you use to test your model.
* **X_test** is the testing data set.
* **y_test** is the set of labels to all the data in X_test (not used in this case).",64d05394,0.6524822695035462
30191,1294fb4c86f993,5bf58b41,#### To analysis the guns registeration based on the census data we will filter data to 2016 which has the most parameters in census data file,4471e513,0.652542372881356
30194,840534f2908a9c,de58b71a,"*In the plot above, we can see two clusters with linear realtionship between taxi fare and distance. But for trip distances >80 km, though a linear relationship exists, the fare amount is very low. Let us check where these trips originate and end.*",8081c3cc,0.6526315789473685
30197,166a62ebb4fc3a,a30d78b3,"Let's take a look at the correlation matrix once again, this time created with our trimmed-down set of variables.",db48a079,0.6527777777777778
30198,c01049afb6d307,aef331f9,## Absenteeismtimeinhours -- Reasonforabsence,d37d3b5d,0.6527777777777778
30202,fdc3afd309b850,d167f356,Web Scraping the Wikipedia getting the AR and Population,966bde38,0.6527777777777778
30204,2f47abddfd1928,4a7a3517,"## 3.5. Parch

Seems that Parch has a good correlation with Sex, age, fare and SibSp.

Lets inspect these relations.",ae33cc0b,0.6528925619834711
30206,12f4d16fc21645,f58aa94d,<h1 style='color:blue'>Split dataset into training and test set</h1>,c7752038,0.6530612244897959
30212,f35bf4df70d310,d2dd0f10,"#### Control Result 

Control result is the accuracy and train time that is taken by the model using the original data without dimentional reduction

For experimentation purpose, I set some control variables which are 

* **Solver** which set to the type of 'lbfgs' which can converge faster [reference here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
* **Max iteration** to 100",10bb859a,0.6530612244897959
30214,0e7ac281a19feb,b3fc9f78,## Augmentation Visualization,5b84d10f,0.6530612244897959
30216,37e461081e47c5,a451fae7,"# Model 3: January Average (2013, 2014 & 2015)
This is because the overall data for 2015 is very spotty. So, using past January averages may be a better indication of future levels. ",b3e6549e,0.6533333333333333
30220,d6ddbe57f59cf7,8d2d6035,"# Swarm plot
To draw datasets on box plot, 
",504a3cda,0.6533333333333333
30224,7e1da639035ac5,d30e6024,### <a id='12.1'>12.1 Supportive Environment % distribution</a>,120b6c23,0.6533333333333333
30233,a4f8ad33c823c5,04f1635e,"### Detecting features with less than 2 unique values
The readmission_status can be dropped from the dataset since it is not very intuitive.",fcd48307,0.6538461538461539
30234,aae204e78a48d1,5cda6d95,"There isn't much change between the the attrition base and exitsing customers in the change in their transactions so customers are probably using their cards less to start with.

**Hypothesis 3: proven**",53ab6133,0.6538461538461539
30237,af6556ced704f6,7af4b6ac,"***Look missing value***
    * data.info gives data type and non null values
    * also we find number of missing values using isnull() function   ",881577c0,0.6538461538461539
30242,d4c5aaa4b36810,854507f5,"# **Model Creation**
I will use cross validation as there is not a huge amount of data. Hyper parameters will be tuned on the same data set that is used to score the data, in reality this isn't good practice but (data leakage) but there isn't much data. 
### Linear Regression
We begin with a simple linear regression model. ",65441f28,0.6538461538461539
30246,e1a69c71c2c282,3147037b,# Test data,b2ca7d3a,0.6538461538461539
30249,09751c520b0616,7905adca,- Separate Train and test dataset,a4d0c7e9,0.6538461538461539
30251,d07915a6e6992e,f4f300d6,**Ticket**,2b912140,0.6538461538461539
30262,8ac70416723897,0c479d4f,Finding the cross validation scores for each model and using .describe() on the distributions,d32fd8f6,0.6538461538461539
30264,c84925c8171900,438e57b8,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.4.2  Platform wise Video Game Sales
            </span>   
        </font>    
</h4>",e21ff7ec,0.6542056074766355
30266,2ada0305b68956,ca7d6e4b,### 111. Palette = 'gist_yarg',133e26f4,0.6542857142857142
30268,4883314a96dc34,ca24c173,#### *EXTRA: deep error check on individual model*,50d36836,0.654320987654321
30275,a566b5b7c374e7,0b4bf0c3,## Quintiles and Best and Worst Days,b3dc5545,0.6546762589928058
30277,6a80f915608fc2,91d56b5d,"## <a id=""tSNEfeatures"">t-SNE on Features shows some Target clusters</a>
Back to <a href=""#Index"">Index</a>

This was motivated by Nelson Ewert's [Discussion post](https://www.kaggle.com/c/lish-moa/discussion/186919) and the code
in his [Notebook](https://www.kaggle.com/nelsonewert/using-t-sne-to-identify-clusters-in-the-data)",636938eb,0.6547619047619048
30279,a5a419dc7245b0,33fe30a7,##### Data Balancing using SMOTE,4279726e,0.6548672566371682
30281,ac1abfe1dfe815,5b5f9146,----,6529dbcb,0.6548672566371682
30282,5ce12be6e7b90e,49b44c86,"## Lists

Lists are similar to strings in being sequential, only they can contain **any type of data**, not just characters. They are also mutable (we'll get back to that distinction).

Lists could even include mixed variable types.

We define a list just like any other variable, but use `[ ]` to surround the list elements and `,` to separate the elements.",c0ab62dd,0.6549707602339181
30283,30fdc4a6e3c1db,d4cc1873,### Plotting sales for each category in each of the state,6111ddee,0.6549707602339181
30284,a4f0a3e1316ff9,e116bb60,"Interesting relationship with the Islander Krona, Hungarian Forint and the Rand. 

This is most likely because the world is trying to adapt to the higher inflation expected and this is leading to different approaches to different currencies. Obviously this is an assumption based on a small time period but it would be my expectation.

Not much surprise that the economies that are close geographically and economically linked to Australia are positively correlated.",53bf0160,0.6551724137931034
30285,14defffcd250f3,48eeefe3,Dropping off the columns we don't need. ,3a683b94,0.6551724137931034
30287,eb800c50fcfbb2,91d6862b,# Inference,e7173f4d,0.6551724137931034
30289,6b54e39f86bdb5,af5ae7d7,## Load the test set to prepare the submission,198084bc,0.6551724137931034
30293,858da4bb312f67,7731ef0a,Colors and patterns of leaves look slightly changing. They are somewhat unhealthy.,9cca4391,0.6551724137931034
30299,6903d3f38c6a66,5f34c391,"# 2. ColorMap

Color is a major factor in creating effective charts. A good set of colors will highlight the story you want the data to tell, while a poor one will hide or distract from a visualization’s purpose.

The matplotlib team has already created a good color palette, and we just need to use it.

Now, This is a very interesting article that you all can read for same matter : 
[How to Choose Colors for Your Data Visualizations](http://https://medium.com/nightingale/how-to-choose-the-colors-for-your-data-visualizations-50b2557fa335)",6067ce5e,0.6551724137931034
30301,84127ade6fde87,01d3f86d,"One-hot encoding is a very useful technique for representing categorical data in tensors. However, as we have anticipated, one-hot encoding starts to break down when the number of items to encode is effectively unbound, as with words in a corpus. In just one book, we had over 7,000 items!",f55d05b6,0.6551724137931034
30304,1750367e54f407,775afe05,# Prediction on test images,a8e655b2,0.6551724137931034
30305,1cd8be6e679620,bc9dfe1d,## Visualize RNA-2D Structure,3ce15a43,0.6551724137931034
30309,00d295edcd117e,7a3fdb19,## 训练网络,f5810f4b,0.6551724137931034
30315,5fc2f23dfbeeb1,b0a9c3b4,### Split train and test data as X1 & X2,f37b4110,0.6551724137931034
30317,6a1d04e8153df3,23dcea21,"**Observation(s)**
- The count of axillary nodes are less then 5 then the survival chances is less.
- Three line shows that ;
last line tells 25 percentile of data
bottom line tells 50 percentile of data
first line shows 75 percentile data.
the lines called whiskers tells (Min and max).

- Orange box shows the death ratio.According to the plot 70 percentile of women who died had less then 10 axillary nodes.

",38572b05,0.6551724137931034
30318,00001756c60be8,bfbde93e,**График распределения целевой переменной - цены**,945aea18,0.6551724137931034
30319,a1ba5ffd30dbde,82638196,- 89% accurate in predicting Disease,48e57546,0.6551724137931034
30320,ef6d1e959a873e,92e665e8,Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided. ,f11a1f43,0.6551724137931034
30321,bb8f5d7807718b,f5f5cfc6,"# 8. [adjustText](https://github.com/Phlya/adjustText) — automatic label placement for matplotlib

A lot of times, we struggle to adjust the text positions in a graph. This happens when there are multiple labels, and these labels start overlapping. adjustText is a pretty useful library for such situations as it automates the placement of labels.",181ec286,0.6551724137931034
30327,63b44c85e32c1f,5219c05f,"---
## Tuples",fb9b9562,0.6554054054054054
30331,892be0a523578c,f4030def,Most of our customers are moderate-intensity exercise customers,b0e8d7c0,0.6555555555555556
30333,3597174a998d4d,cb805777,"According to the defination, the value of meal variable as SC and Undefined should be combined into the same class. And it can be concluded that:
* FB has the highest ratio of cancelation, but it also has the low number of booking. **So meal variable has little relation to the ratio of cancelation.** ",276892ed,0.6555555555555556
30335,396bc36edb95d3,5211ce87,#### AUC and ROC for Testing data - Random Forest,965e4f8f,0.6555555555555556
30337,918040fad252ec,ea4de21d,Menampilkan *Normalized Confusion matrix* dalam bentuk tabel,966fcd8f,0.6557377049180327
30338,0858e1bb3cbaca,184c5f12,# Split-apply-combine,78548374,0.6557377049180327
30346,93f5423667b9d5,b44d0e81,# チーム別の勝率の算出,55bdf071,0.65625
30350,2c3a6969252dc0,e2eeb2b6,Students with better LOR tend to have Higher GRE Scores.,d30f10ce,0.65625
30352,3cc097a5859dc1,d6b8026d,"# **Splitting Data to X,y**",14380d73,0.65625
30353,49f2274c1dd516,63907472,"# Geotab
Geotab is a telematics company specializing in GPS fleet management and vehicle tracking. This data set provides hourly average duration of border crossings between Canada and the US",06b0ffee,0.65625
30356,c85c94076e9c3a,e30f50c3,## Income,3ea0c443,0.65625
30360,9daf8b4a46725e,d3891753,### Splitting Data for Training and Testing,7d9cc411,0.65625
30364,ee23a565163388,bda8168e,Let's build a baseline model with all the features in the dataset. The dataset has 12 independant features. We'll then reduce the model complexity by reducing the number of independant features.,88aacbc4,0.6564885496183206
30367,21413205980558,d04a9610,# 4.2  Deposit business user job（存款业务用户职业）,84197de0,0.6567164179104478
30370,a4aa36df07fd53,8f892a8f,"### Which industry should you target?
Industri apa saja dalam penerapan data science yang berani membayar mahal?",d2f42b6d,0.6567164179104478
30371,e58e68e4eeefe5,df5ca14c,#### Trying with different sets of features based on the observations.,a87662ce,0.6567164179104478
30372,20b372b6e4e276,2a355bc4,"## 5.3. Subtext analysis <a class=""anchor"" id=""5.3""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.6567164179104478
30375,7cfd96218dd933,862fe025,# INVESTIGATION FOR ALL COUNTRIES,7c34d96c,0.6568627450980392
30376,52cfd66e9ec908,cd8646fc,So the idea here is to augment the datas as discussed on the forum by @ryches (https://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles/discussion/188368) since albumentations can use the data with keypoint format.,c74adcdf,0.6568627450980392
30380,9c044fa3072552,a69f8063,### On Mondays,1362842e,0.6571428571428571
30381,bbaa07ad21cf4e,73d273c8,### Naive Bayes,3ab6b254,0.6571428571428571
30382,6b65d81a5743dd,ece88ece,Ok so on everage higher efficiency score correlate with more experienced players,4080a2d2,0.6571428571428571
30384,38b79494ac749e,c6b86591,### L2 - Ridge regression,39162a40,0.6571428571428571
30386,f50dc95483c98f,16458383,## Predicting a new result,cd9e9621,0.6571428571428571
30388,47a1b1fe51b4ad,f01723c8,"### Model Building

Neral Network with two 32-node hidden layers.",331ded2f,0.6571428571428571
30393,72f72525e8a7fb,daa4431c,Differencing,e6380e0b,0.6571428571428571
30395,b3e48999ed0d00,95343643,## Model XGB Classifier,fe9ada0f,0.6571428571428571
30396,04bac111ffbe9c,81ffa697,##### COMPARING TRAIN AND TEST DATA,82576b17,0.6571428571428571
30398,f4b603905215b7,378eb5d7,# 3. Change the Threshold,efe1d587,0.6571428571428571
30403,675b60eaf415a6,23c0fb19,### **Predicting classes for new images from internet using the best trained model**,68c0b725,0.6571428571428571
30404,55a5e31d03df9f,5e831c3d,"As you can see there is no need to call an extra package, you just need to call the model of interes within keras application module.",06dce00f,0.6571428571428571
30405,b4e238fbc6464c,634656d2,## Train & Validate Split,fd1a6cba,0.6571428571428571
30406,ca73f3d2e25b47,a9663a0d,"Split the lablled dataset into training set and test set.

* x_train = features of training data
* y_train = label of training data

* x_test - features of testing data
* y_test = label of testing data",4cd11efe,0.6571428571428571
30408,3a6274ed72cc00,93d21103,We have no outliers.,51369a2a,0.6571428571428571
30412,81712ee7510ac5,0f040b79,How to add elements to set???,c4685e79,0.6571428571428571
30415,ab6da5994949a3,1101f97c,## K-NN Test Results,fae6b91d,0.6574074074074074
30416,2f0f808765fc67,c5d4ad9b,# **E1. Regression Tree**,fd1f6494,0.6574074074074074
30422,1eb62c5782f2d7,88a872ae,### Solusi Q1,bb69f147,0.6575342465753424
30423,fdc9f4863744b1,94ae1d4f,"We have a lot more data on ""Brooklyn"" than any other borough within the data set. This might indicate there are more real estate properties in Brooklyn than other boroughs. Interesting that Queens is the largest borough in NYC but has less real estate properties. This might also indicate that our data set might not be accurate. ",b4529365,0.6575342465753424
30425,5f32117bcd5255,711c98ce,#### HST OPTICAL THREE,85882abf,0.6577181208053692
30426,31b564f11ef638,2c2253ed,### Ridge Model (Apply Gridsearch),424f9692,0.6578947368421053
30427,c950cff74e51ac,7f9b04d8,**Availability of each room types in NYC**,d59bf323,0.6578947368421053
30437,3fb15e6e48aec2,b8ce5276,"# Additional option:
* bin ages by specific groups (i.e. child = age <16)",9d1f4358,0.6578947368421053
30442,49ee86d074de69,2f69748c,"<a id = ""15""></a><br>
## Model",71ccc6d3,0.6581196581196581
30443,b86bda7afe3ac3,7d5e0cc8,Fake dataset,16197934,0.6581196581196581
30448,62487bcd70b199,7d3f9061,"## Inference:

Based on the above table, as we can chose the best model based on business scenario.

as a bank, if I have to chose between these models I would go with the one with best f1 score as data is imbalanced. 
### Decision Tree would be good choice followed by KNN",f6ae50af,0.6583333333333333
30450,8d0aebab1e5914,662ad531,**`ploting the 2d data points with seaborn`**,084e671f,0.6585365853658537
30453,4246295d91a6c1,613c8f32,test on train set,9138104a,0.6585365853658537
30456,514d8de15cb7ef,e3f18576,### B. Suppot Vector Machines,cfe111b2,0.6585365853658537
30460,0b01138ad120fc,182ff4b0,"### It works!
****
****
****
**OK! Now I'll try to predict BTC prices, based on both these previous works  
But this time I'll use LSTM**
<br><br>
**For sure the results won't be too close to the real value, since I'm using simple inputs and an simple Neural Network.**",0b4b72e6,0.6585365853658537
30461,8cefb86a675e5d,846548fa,**What is the mean of the expected target value in test set ?**,79f9e69b,0.6585365853658537
30462,8d70dcae7f40a3,6ca9093a,"| **<h3>Giá trị trên confusion maxtrix</h3>**                        | **<h3>Tỉ lệ trên confusion matrix</h3>**
|--------------------------------------------------------------------|----------------------------------
|**TP** = 11, có 11 người được dự đoán bị bệnh là chính xác.         |**TPR** = TP/(TP + FN)  , Tỉ lệ người được dự đoán có bệnh trên tổng số người có bệnh thực tế. (lớn tốt)
|**FP** = 7, có 7 người được dự đoán bị bệnh là sai.                 |**FPR** = FP/(FP + TN)  , Tỉ lệ người được dự đoán có bệnh nhưng thực tế không có bệnh trên tổng số người không có bệnh. (nhỏ tốt)
|**TN** = 773, có 773 người được dự đoán không bị bệnh là chính xác. |**TNR** = TN/(TN + FP)  , Tỉ lệ người được dự đoán không có bệnh trên tổng số người thực tế ko có bệnh. (lớn tốt)
|**FN** = 123, có 123 người được dự đoán không bị bệnh là sai.       |**FNR** = FN/(FN + TP)  , Tỉ lệ người được dự đoán không có bệnh nhưng sai thực tế trên tổng số ngừoi có bệnh thực tế. (nhỏ tốt)",472c71ce,0.6585365853658537
30466,513ce405d7f6a3,bcf3a7e1,# SVM,8461e086,0.6588235294117647
30469,a26032553265a6,df8361d7,PREDICTIONS,f489744e,0.6590909090909091
30471,da199f8fb59439,8ee41f8d,#### **Genre Vs Rating**,baaa665d,0.6590909090909091
30473,d1ff7e10ee0102,a5f9f96e,### In the search for normality,2cc71c3c,0.6590909090909091
30479,f269d2fbd5f1be,4da96ad0,# Relationship between Rating & Salary by Years in NBA,1264c440,0.6590909090909091
30480,450fda47b03baa,dc4320bf,"Hedef değişkenimizdeki Null değerler yerine ortalama değerler yazdıralım.

Bunun için daha önce görüntülemiş olduğumuz temel istatistik değerleri tekrar görüntüleyelim.
Çoğunluğun sahip olduğu Rating değerini Null alanlara yazdıralım.

Daha sonra Rating değerinde Null değer kalmış mı kontrol edelim.",62c04adb,0.6590909090909091
30483,0a918602a04693,e6df7bb2,"As, the null values are less than 5%. So, we can delete the null values safely as per rule of thumb.",c1ef0e95,0.6590909090909091
30484,73d8e56bc709b1,29bab244,"It seems top strikers really get high scores in these 6 features.  
And we can see C.Ronaldo's jumping feature is the highest, G.Bale's acceleration is the fastest.",78ec3cce,0.6590909090909091
30490,957e035ba5b9d5,9a26fcbf,## Build Sequential Model and fit,778ab3d3,0.6595744680851063
30491,f6648e47713411,ea58d6bf,## 2.5 Create Dataloader,f4af4d1c,0.6595744680851063
30493,c7e5f658090347,dd2029b0,<a id='Final_ML'></a>,43c78e7d,0.6595744680851063
30500,2a123b4e8f9433,d9f7a94e,Run prediction,0a082218,0.6597938144329897
30506,10c5a39a87c47e,938f210c,### Confusion Matrix,09c7337a,0.66
30511,2ada0305b68956,907c43b6,### 112. Palette = 'gist_yarg_r',133e26f4,0.66
30512,7dd46c750653eb,a56bb668,"**Inference**

* 2014 had the highest number of marriages (100483) whereas 2019 had the lowest number of marriages (83010)",c2644713,0.66
30514,917957c6c4065f,aeb11ce0,"조회수 대비 좋아요를 기준으로 하는 상위 5개의 항목입니다.  
1,2위는 로이조TV 채널의 동영상, 3,4위는 방탄소년단 채널의 동영상, 5위는 빅스 채널의 동영상입니다.  


1,2위 영상의 제목,  
1,2위 조회수 대비 댓글수(comment_count/views),  
구독자가 BANGTANTV 2580만명, RealVIXX 123만명, 로이조TV가 68만명인것(2020년02월25일 기준 구독자 수지만 2년 전에도 그 비율은 비슷했을걸로 추정),  
총 3가지를 고려했을 때 로이조TV에서 컴퓨터를 상품으로 좋아요와 댓글 이벤트를 진행한 것 같습니다.   
1,2위에 해당하는 영상이 비공개 처리되어 정확하게 확인할 수는 없네요.

좋아요와 댓글수를 늘리는 것에는 상품이 큰 영향을 주는 것 같습니다.
",55b8ed68,0.6601307189542484
30516,98a6794067932a,f1a95313,La cellule de code suivante de cette section sert donc à représenter le nombre total d'expéditions effectuées par état. Le code permet donc d'effectuer la somme des expéditions pour chacun des états et affiche ensuite le tableau complet. Cette analyse nous sera utile afin de déterminer les états où se trouvent nos meilleurs clients au niveau des expéditions.,08600fe2,0.6601941747572816
30518,23df07a474aaae,c9d9bed7,**Ridge Regression**,0ea40276,0.660377358490566
30525,f015d0147e8fbf,c28bad5e,### Target Encoding Helper Functions for Categorical Features,518954fb,0.660377358490566
30535,f13534449a3750,c4368207,Recall that each image corresponds to a number of masks equal to the number of ships contained in the image.,8b7f3332,0.6607142857142857
30541,bb0905d33ae417,09c1d3a0,## 96x96 images,25fd1965,0.6610169491525424
30542,9169c4e9c33c90,2ac3d6d7,"*Where the Crawdads Sing*, by Delia Owens, received the most amount of reviews.",725bf880,0.6610169491525424
30546,a44368590e878a,23021386,#### Infection network in Korea,77743ba8,0.6610169491525424
30557,9ceb7278784462,4b4e4df5," ## <a id='20'> 16.Decision Tree Regressor</a>
",3768a567,0.6612903225806451
30558,57070ad5e0f94f,a2ad92fe,# **Searching For Optimal(at least close to optimal) Parameters**,d97edc41,0.6612903225806451
30562,fc8e0042411c46,12f9ea86,"- 'Will revert after reading the email' is a mixed emotion, it may be Interested or Not Interested. Depend upon the mood of customer, their requirement & content of the email, Lead can be conveted into a customer.
- 'Closed by Horizon', 'Lost to EINS' are positive tag for Lead
- 'Invalid number', 'wrong number given','Not doing further education'& 'Interested  in full time MBA' are negative tag ",af476c2a,0.6614420062695925
30563,3cb96bd8eb364b,e807becb,### Load Saved data,3157af7e,0.6615384615384615
30567,a8c042af6b7245,b0f94f68,"#### ps_reg_02 and ps_reg_03
As the regression line shows, there is a linear relationship between these variables. Thanks to the hue parameter we can see that the regression lines for target=0 and target=1 are the same.",2487ac62,0.6615384615384615
30571,d07915a6e6992e,44c903ad,"Tickets are of 2 types here.

Type 1 has only number and Type 2 is a combination of some code followed by the number. Let's extract the first digit and compare it with survival probability.",2b912140,0.6615384615384615
30573,f2f2db16a2f86c,1b3e78b3,**Scaling the Features**,ffc6a115,0.6615384615384615
30575,ba4b3bd184acbb,8fd87611,"### To HDF

Another storage format is an HDF file where we can write multiple DataFrames to the same file using a key.",0f5de724,0.6616541353383458
30576,02b7e38902069e,8f8141dd,"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.

Citation: Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton and Christopher D. Manning. 2020. Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. In Association for Computational Linguistics (ACL) System Demonstrations. 2020.

https://stanfordnlp.github.io/stanza/",726a03a0,0.6617647058823529
30577,9cec5ddf8b6f49,5709becb,### * To get the details of the best trial,d39fc8e7,0.6617647058823529
30578,e4c6dd957eb5ce,95f85102,"Hey, we have a very important thing happening on the one Million day;  <br>

In my research to better understand this data I discovered that Kaggle only started the kernels in January, 2016; 

We can see some interesting peaks:
- Feb 26, 2016 was the first Peak. At this day was created 368 kernels;
- On May 3, 2017 the second Peak, It had 646 kernels created;
- The third and fourth peaks are very similar. These ocurred on Feb 13, 2018 and Mar 26, 2018 with 718 and 690 respectively;

It would be interesting if we can get informations about the One Million day. What happened this day?",2e383665,0.6617647058823529
30579,156bbcff05dcea,3bcfca7e,# Classification Report,66ad1fe9,0.6617647058823529
30581,7f74a04ae75792,37ee5068,### How is the Age of the customers distributed? Plot your Answer with the appropriate chart,d01e91da,0.6617647058823529
30582,c65a65d4041018,6067e508,"Quite interesting that while in Russia and India most don't use cloud services, in USA situation is the opposite.
Additionaly Digital Ocean is used by some people.",824fb229,0.6617647058823529
30583,dc0b0e1cb46c6f,bb8bff6d,"<a id='3'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;"">3. Data visualization 📊 by Continent</p>",47b17a7b,0.6617647058823529
30585,eb0ecd6bebeb15,3f3b1d16,"Aynı görselleştirmeye hue = ""variety"" parametresini ekleyerek 3. bir boyut verelim.",d7b93a60,0.6617647058823529
30589,631cd434fc3aa2,ed2f85f3,"* _SaleType_, _Electrical_ and _KitchenQual_: same as before (since the missing values are only two and one value respectively)",2b74febb,0.6619718309859155
30594,63b44c85e32c1f,1b33282f,"Tuples are similar to lists but only big difference is the elements inside a list can be changed but in tuple it cannot be changed. Think of tuples as something which has to be True for a particular something and cannot be True for no other values. For better understanding, Recall **divmod()** function.",fb9b9562,0.6621621621621622
30601,722cd844dfbe8f,038d8041,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_3_3"">Define folds</span>",0cedb385,0.6623376623376623
30603,241cf32abb22d8,6cfb792c,"## 2. Decision Tree

To find the optimal Decision Tree model, I include different criterion (gini index and entropy), maximum depth and minimum sample split in the grid search.",47157066,0.6623376623376623
30604,c13f73168789c2,446a882d,"## 2. Slicing rows and columns by position<a id='25'></a>
To slice a Pandas dataframe by position use the iloc attribute. Remember index starts from 0 to (number of rows/columns - 1).",16175052,0.6623376623376623
30608,5ffe6aa38958a1,f9b37114,"## 3.2 Split train, validation 

**scikit-learn**   
scikit is an API that implements methods for various machine learning utilities. Primarily including: 
1. data pre-processing
2. statistical methods (random forest, logistic regression, svm etc)

We will first use this for pre-processing steps such as dividing the data for training and validation, and normalization. ",11f5412e,0.6625
30610,fdbbd573ba31c2,67bf5219,# Modeling,f7c28d74,0.6625
30617,5f27526aa6c113,019af786,Let's check the claims that were reported before the accident happened!!,a5c26ab6,0.6629213483146067
30618,04ff2af52f147b,7fbca0ad,"**Create FamilySurvival Feature:**

Due to the issue with the *Surname* feature mentioned above, we will need to minimize the issue of grouping together passengers who are not travelling together.  This will be done by grouping passengers together based on *Surname*, *FamilySize*, and *Ticket*.  *Ticket* is included due to the observation that the *Ticket* is shared across all passengers travelling together.  We can see below from the largest family travelling on the Titanic.",d5f37be9,0.6629213483146067
30621,b01ee6cb674fa3,9a213823,"# Korea Aerospace Research Institute - KARI

Korean Space Agency

Founded by october 1989 in Korea
",a8ffd35e,0.6630434782608695
30624,f91f58d488d4af,0d5cc3a1,"The gradients only tell us the slope of our function, they don't actually tell us exactly how far to adjust the parameters. 

But it gives us some idea of how far; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value.",5df1bbf3,0.6631578947368421
30626,b10bd75889dad9,b3da0e7f,#### Assessing the model with StatsModels,ee00ceee,0.6633333333333333
30631,ac1abfe1dfe815,1ff53555,## Columns to dummies,6529dbcb,0.6637168141592921
30634,f3c6048d1058e3,f9258d3d,## 2) Naive Bayes,1d9056b0,0.6637931034482759
30635,4ae6a182abac64,c8c4e772,### 2.4 Removing irrelevant variables,418676c5,0.6638655462184874
30642,3c2033cc99c12c,dc47414a,"**Note:**  *In this part, I will use the two PCA features to train the model in order to visualize the result*",dfa22a54,0.6642335766423357
30644,fdc9f4863744b1,6b4ace96,"We have Sale Date data as a variable in our dataset. However using sale data, considering the amount of possible variations might make it difficult to use as a predictor variable. So I will categorize them and create a season variable.",b4529365,0.6643835616438356
30645,738bfced935b69,8101dea3,## Data with Seven Columns,2d3c592d,0.6643835616438356
30646,726833f92fb87a,1293cc51,"There is no hierarchy, so this variable should be encoded by one hot encoding. Moreover, there are 63 samples with unknown jobs: we will label them as nan values and let XGBoost encode them.",7dc5e1b6,0.6644295302013423
30648,fc8e0042411c46,369a57e4,## Lead Quality,af476c2a,0.664576802507837
30651,2ada0305b68956,d08858fa,### 113. Palette = 'gnuplot',133e26f4,0.6657142857142857
30652,36efe086c3f23c,53a460ba,"'''
But before moving ahead we must trim the values before 2010. That's the secret of magic
Recipie and revisit box plot.
'''",d0676cd0,0.6666666666666666
30655,30fdc4a6e3c1db,fb235d3b,"What we see:
* Sales of Food items in CA have shown very less of an increase over the years although it has a high monthly seasonality peaking at August each year and dipping towards Year end and start (Nov, Dec, Jand and Feb)
* Sales of Food items in TX also shows a similar trend although at a lower scale (High monthly seasonality peaking at August and dipping at Jan, Feb and overall trend remaining somewhat same)
* Sales of Food items in WI has quite surpassed that of WI in 2015 and 2016. It's the only state showing an increase in Food items over the years. Also we dont see an obvious monthly seasonality in Food items at WI where peaks are at the month of March or July
* Sales of Hobbies items doesnt show any monthly seasonalities as such
* Rather in case of CA, sales of Hobbies items tends to increase every alternate year and dip at alternate year (aka cyclicity) with the dip in Nov,2012 as the worst but increase back quite well. Sales of Hobbies items at CA also shows an increasing trend
* The sales of Hobbies items in TX and WI both show an increasing trend and have quite similar sales with a bit better sales increase in TX
* Household sales shows the best increasing trend in all the states.
* CA shows the best increase in Household items over the years and also has a monthly seasonality again peaking at August and dipping at December and January
* TX shows a bit better sales than WI in Household throughout the years but both show similar increasing trend. They show an increase in sales of Household in August but it's not that evident enough to show a monthly seasonality. Infact, both also show a peak in Mar, 2013 ",6111ddee,0.6666666666666666
30656,62487bcd70b199,ba93f72e,"# <a id='8.'>8. Ensemble Methods </a>
## <a id='8.1'>8.1. Bagging Classifier</a>",f6ae50af,0.6666666666666666
30665,548f961125248d,b2b07857,"### Shap

* Refer to https://www.kaggle.com/dansbecker/shap-values for explanation of SHAP. 
* ""SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.""",d8c5e8b8,0.6666666666666666
30667,ddbe4806ed061e,ed9b708b,"<span style=""color: #2130b8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 28px"">Fast Image Resizing</span>",a0d5fd77,0.6666666666666666
30673,06faae14dfe21d,329f4562,## Data cleaning just doing imputations and filling null with mean and categorical values with most occuring on test data,2be83140,0.6666666666666666
30675,3793c438a71b52,6a1fe7bd,**EDA**,13eb76df,0.6666666666666666
30685,df2a7968c08ee4,911f2c04,Double checking wether we are training on GPU's or on CPU's.,a2ba0a72,0.6666666666666666
30687,de577c910a687d,7ac8eb05,"# Step3: Train Model #

Let's try out a simple XGBoost model. This algorithm can handle missing values, but you could try imputing them instead.  We use `XGBClassifier` (instead of `XGBRegressor`, for instance), since this is a classification problem.",c3ebadbc,0.6666666666666666
30691,31268b33de97b5,d1b2d080,# Checking Correlation,1e6f7d14,0.6666666666666666
30693,d46508f983e086,274cff21,**Augmenting the Image Dataset**,454138b8,0.6666666666666666
30697,b0c2805cd5c087,aba6244e,Image elsevier.com,0446f327,0.6666666666666666
30700,e16860fce156b0,bc123497,#Correlation of specified element with all other attributes. (ie. `d1_glucose_max` with everything else).,2054f1ce,0.6666666666666666
30712,999258a81ba32a,58c1c2d7,"# Look at the distribution of the sum of each winning drawing
The most common ball number is shown in the mode table which is shown next",48cd3d21,0.6666666666666666
30722,842547b2def18c,1ea861f1,"### Quick completing and converting a numeric feature

We can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.

Note that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.

We may also want round off the fare to two decimals as it represents currency.",b8efde6d,0.6666666666666666
30725,1d73d04c3aaae8,dd1906e2,"For 2019, Elo performs slighly lower with 64.0% correct. That would still place Elo in the top 6 of professional football predictors in the sports media!

## How Does Elo Do Against Point Spread?

The Elo predictions are design such that the prediction difference divided by 25 is an effective point spread. With this known, we now examine Elo's prediction against its own point spreads.",cd43d0aa,0.6666666666666666
30728,f7436bc492474c,01a74705,This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below).,328fd235,0.6666666666666666
30733,60d500d196eb42,f6f77d6a,Distribution graphs (histogram/bar graph) of sampled columns:,2ad55f3f,0.6666666666666666
30741,523123dad03177,7adaf62a,# 8. Writing Scores,48a5e4e6,0.6666666666666666
30742,c73e07ad6d25c5,e27e714f,## Missing Value ,3ab391fb,0.6666666666666666
30743,4fa553c2b837d4,1158713f,# Categorize the categorical data,c65a23e9,0.6666666666666666
30748,52cfd66e9ec908,703b9b70,Plot with the cutout transformation:,c74adcdf,0.6666666666666666
30749,70c1c70437ce86,c1d72fc7,## Random Forest Classifier,e94552a4,0.6666666666666666
30751,b42180a6a5b42f,d41d74f7,"Aqui vamos criar uma nova série (""coluna"") com a diferença de óbitos atualizados nas últimas 24 horas, a fim de plotar em conjunto a ***distribuição dos óbitos totais mais recente*** com a quantidade de óbitos apuradas nas ***últimas 24 horas***. Com isto poderemos mensurar se ainda há muitos laudos em atraso após o dia 14 de maio. 
Desta feita, não havendo, e os dados das atualizados nas últimas 24 horas se concentrarem mais nos dias recentes, confirmaremos o pico no dia 14 de maio.",987cea5f,0.6666666666666666
30752,17a24d566ffa59,7a8901a1,### SVD Example,89049e56,0.6666666666666666
30756,2c8119a4061997,8b11f82d,The code below modifies fast.ai MixUp calback to make it compatible with the current data.,1836a79c,0.6666666666666666
30757,98ea617d18c9cc,f9b721f9,# Creating Callbacks,e6316d11,0.6666666666666666
30760,d96e03a9e7c030,d755c41a,"## Step 5: Visualize model results
Last but not least, let's look at some visualizations from the model to figure out how useful it could be. First things first, let's look at how the **predicted** percentage of testtakers matched with the **actual** percentage of testtakers.

*Note:* Apparently there is a bug within `pandas.DataFrame.plot` that cuts off the x-axis label when using a colormap in a scatter plot. [Link to pandas github issue here](http://github.com/pandas-dev/pandas/issues/10611). The x-axis is comprised of Percent of *Actual* testtakers, with the ticks representing [0, 0.2, 0.4, 0.6, 0.8, 1.0]",d2b72ced,0.6666666666666666
30764,7e89d387feb9f5,b15cb64f,"Пока оставим новые признаки в отдельном датафрейме encoded,
так как после конкатенации 126 бинарных признаков в основной датафрейм
анализировать его через df.info() становится затруднительным.
Конкатенацию в дальнейшем будем производить при помощи кода:
``` python
pd.concat([df, encoded], axis=1)
```",989e3a1b,0.6666666666666666
30767,4d91e84c564cbe,96c009f8,`list.pop` removes and returns the last element of a list:,355a43e3,0.6666666666666666
30768,2d40f383473fa4,4887badc,# 5. AutoML Model,1da1eff0,0.6666666666666666
30769,adf419444a59df,7bd9d64e,## Dataset,3a275e7f,0.6666666666666666
30779,80ad12f326ab70,cc21d738,"Google LLC is the top provider with a count more than 25, other companies/providers are below 10.",da404a16,0.6666666666666666
30786,b547f0f38f7744,a618ff74,### Finetune Model,b6ba66b3,0.6666666666666666
30789,70193f0c034b98,4710443d,# Lr scheduler,f8cacd26,0.6666666666666666
30793,d905cde3391d2b,b8097ffd,Let's calculate the standard deviation by formula,067dba39,0.6666666666666666
30794,50b03ce5b1a286,246f5e6e,"#Reproducibility

The qlattice will be reset when we get it, but to ensure that we get exactly the same result every time we run the notebook we need to seed the QLattice. This is done with the reset method",d49896a5,0.6666666666666666
30802,2bd6c370695ea7,808b4638,## Image,cbe6aec8,0.6666666666666666
30803,6fad63bfd45ef9,8f882b2c,# Data augmentation,b3c6f1d6,0.6666666666666666
30810,f4b9042e693b6c,ddb616f1,"Let's start training! To do so, we start by initializing the model. Let's make sure we initialize with the correct number of classes and input channels. We use the `xmp.MpModelWrapper` provided by PyTorch XLA to save memory when initializing the model.",676cacc9,0.6666666666666666
30817,b39684e6670dd7,ca9545d6,# transform data using RobustScaler,83de9873,0.6666666666666666
30818,28a1ff0f223da9,e2797515,**Which university hosted majority of our teachers?**,c945b27d,0.6666666666666666
30823,7705fc44251194,72ce1ba2,# EfficientNetB7 Model,bd2cebe8,0.6666666666666666
30827,6471597c5d2f66,1f4983a8,Distribution graphs (histogram/bar graph) of sampled columns:,a41b4abe,0.6666666666666666
30828,9456df44ab9308,e24fb826,## Output Body-Pose to .csv ,a8e94298,0.6666666666666666
30829,2f964d08c25d93,841b4aa3,Correlation matrix:,1f2e4468,0.6666666666666666
30830,4b64dc653fb7eb,9baf3e67,"<a id=""7""> Step 7 - Convert Word to Base Form or Lematize </a> 

Converting each word to its base form e.g. trying to try, or tried to try for simplification; using **WordNetLemmatizer** function from **NLTK** library.",57675cc2,0.6666666666666666
30834,06ecf7a304c309,dec05c36,모델의 예측 결과를 검증 데이터에서 살펴봅시다.,714de627,0.6666666666666666
30837,dd3721cb49c1fd,8f9167e8,"<div style=""margin: 0px; padding: 10px; background-color: #ef9a9a ;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h2 style=""color:white;text-align:center"">In the above cell, the metrics for both train and test data are listed.</h2>
  </div>
</div>",1a53fdd9,0.6666666666666666
30840,2a377ced98d67a,aaaf7e61,## 4. Predict and Evaluation Function,262231a8,0.6666666666666666
30842,54004b32784b68,fb111750,**Distribution of SalePrice**,27213ca9,0.6666666666666666
30843,6dcfe6a610d86b,ac6775d5,"## List Comprehension
https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions",d05c59da,0.6666666666666666
30845,9e27af2600925c,778634a8,"## 5 - Merge all functions into a model ##

You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.

**Exercise:** Implement the model function. Use the following notation:
    - Y_prediction_test for your predictions on the test set
    - Y_prediction_train for your predictions on the train set
    - w, costs, grads for the outputs of optimize()",9b556435,0.6666666666666666
30850,d6cbd7160961dc,ddaf77a8,## 5.3.2. Results: Second Digit,36d74664,0.6666666666666666
30851,adb8441ad28019,4232f85f,"<div class=""alert alert-success"">
    <h1 align='center'>K Neighbors Classifier</h1>
</div>",d89de993,0.6666666666666666
30852,864302b10e7730,6e77fca6,"# To check the relationships, we plot the *bargraphs*, *scatterplots*, *regplot*",e9dd1d2d,0.6666666666666666
30853,b57862be79c695,9ee852e5,## Create an ANN Model,989222cb,0.6666666666666666
30865,cb570c7b7f0501,7f8fe24a,it's almost with the same ratio 0.2 ,a200a0ec,0.6666666666666666
30868,efd44ce2c08541,124e2cb6,# Train LGB model,ebc2d00c,0.6666666666666666
30872,63d0d9b9a8c7d2,102a24fd,***Bagging Classifier***,e32e5933,0.6666666666666666
30874,659f5f3ef8aa0e,4660ab62,Distribution graphs (histogram/bar graph) of sampled columns:,3654c2d0,0.6666666666666666
30875,6f4795cfdc96c7,5bf662f1,Distribution graphs (histogram/bar graph) of sampled columns:,1f3ab82f,0.6666666666666666
30877,56cc8fb47bef6a,49791bed,"<p>&nbsp; <span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Split the data into training and test sets</span></span></p>",652d6670,0.6666666666666666
30884,0caaec057f7184,7ae6de90,"- item_id: 4240  shop_numbers_intest: 2  shop_numbers_intrain: 57

The item 4240 (Kinect Dance Central 3 (only for MS Kinect) [Xbox 360]), in category 23 (Games - Xbox360) has the sales peak around the 23rd date_block_num. Cate 23 is also having the similar trend with the item 4240.

- item_id: 13818  shop_numbers_intest: 18  shop_numbers_intrain: 25

The item 13818 (LEGENDS OF NIGHT GUARDS WB (BD)), in category 37 (Games - Xbox360) start having better sales on the 30th date_block_num. The trend of cate 37 isn't similar to the item's.

- item_id: 11286  shop_numbers_intest: 41  shop_numbers_intrain: 1  

The item 11286 (Truckers 3: The Conquest of America + Great Race [PC, Digital Version]), in category 37 (PC Games - Digital) have sales ranging from 2 to 16 through months, starting from month 20. Cate 37 is also having more sales from month 20 to month 30.

- similar trends

For each shop, they follow similar trend of total sales, while there are also some with 0 saling records or missing points.
In addition, the average sales of each shop having sales are also following some of the trends of total sales.
To find out the previous records of not-having-sales-record-in-shop items, we can use the average of the item sales among the shops having saling records. ",b875533e,0.6666666666666666
30890,c968dbd8d49ae6,47d7f728,# **Prediction**,dfb2684d,0.6666666666666666
30902,7cfd96218dd933,6813c604,### READING,7c34d96c,0.6666666666666666
30904,6116b13d4464d0,eed86017,"# Calling ffmpeg using subprocess.run()

Here I create a string first and then pass that string to subprocess.run() which sends the ffmpeg command to the system. When *shell=True*, subprocess.run() will use the first argument supplied as a string to pass directly to the OS. *check=True* will raise an exception if the process raises an exception on exit. Fortunately, subprocess.run() will wait for the process to end before creating a new process, so you can run it in the background without fear of memory problems.

The command line string is of the form:

    ffmpeg -f concat -safe 0 -i path_to/input_file.txt -c copy path_to/concat_file.wav

The run-time on my local machine (8 cores) was around 30 minutes using the CPU.

Note: Not all audio file formats work properly with ffmpeg. More information on using ffmpeg to concat audio files can be found here: https://trac.ffmpeg.org/wiki/Concatenate",e2826468,0.6666666666666666
30905,b74076b2f8ba1d,482edb2a,Distribution graphs (histogram/bar graph) of sampled columns:,9ace22d4,0.6666666666666666
30911,163ceeb80d6923,56f20135,## Retrieve Preds for 1 piece of text,4adfbb90,0.6666666666666666
30916,f998cece696659,7db6dd6a,"<a id=""4""></a><br>
# Polynomial Linear Regression",7964297e,0.6666666666666666
30917,e2a94f078e1161,fca2b7eb,"Hypothesis :- Students with attendence above 7 tend to participate less in Class Activities

Testing:- I will be following the same procedure as the one above",5fc53059,0.6666666666666666
30925,10b5af05d804ff,73b2816a,### Plain dataset,4a9b1705,0.6666666666666666
30929,8336d84cf3ff6b,8995ce98,"## Result of Grid Search CV 

> Best Parameters{'criterion': 'entropy', 'max_depth': 25, 'min_samples_split': 2, 'n_estimators': 250}",b96b58a0,0.6666666666666666
30931,e2a907e1c7d7f9,c42bef9a,## Split Data Feature & Target,f09fb692,0.6666666666666666
30932,67b7354e96113a,cfb4da42,## Modelling,dca94250,0.6666666666666666
30949,faa8e6c8ab9246,56a8be96,Check outliers in the data,2bea1419,0.6666666666666666
30950,7c89a32e3562ca,82c8ea1f,# Random Forest,32dd8913,0.6666666666666666
30953,7341f069d9b2ee,6cd0fdd7,Handling Text and Categorical Attributes,e0a49e62,0.6666666666666666
30954,fa02c409161192,e4b3f289,### 1.4.2 How the number of hidden node effects the performance,e97077f7,0.6666666666666666
30955,6b2776f151ed9c,6581d80e,# QuantileTransformer,40406d5c,0.6666666666666666
30959,e19e307b3fd188,bdfe2278,"Much better!
P.S. Thanks to [Samuel Natividade](http://www.kaggle.com/juxwzera) for the very useful notebook.",2173955b,0.6666666666666666
30964,a758983a68c014,2cde19b5,"Next, weight initialization. Look above on the architecture of NN. W1 matrix of size **vocab_size $\times $ embedding_dims**, W2 of shape **embedding_dims $\times $ vocab_size**. Pay attention we put requires_grad as True, because we want NN to compute gradients for those weights matices for their optimization. Function torch.randn randomly initialize weights. But very important to initialize weights correctly. What does it mean? Weights should be initialized to small random numbers. If you are not careful enough with this step, model can generate unexpected and not useful results. For this `uniform` function is used here, it limits bounds of weights to $(-0.5/$embedding_dims, $0.5/$embedding_dims).",ab89f181,0.6666666666666666
30970,d0080e3a39bc5c,e09e032e,"These are the basic model training steps.

These should already give us pretty good results as we can see from our F-1 scores on our validation set.

Here comes a few more additional steps which helped me push the score further and squeeze out those last decimal points - 

1. FastAI by default uses **Adam Optimizer** in order to train the models. 
   Also, Adam Optimizer usually **converges faster than SGD**.
   
   So, I used Adam First till training stagnates and then I switched out the optimizer with SGD because SGD    being the slower one, converges better, squeezing out a bit more from the model.
   
   Thus effectively, I used Adam to get the training parameters near the optimal values and then used SGD to get to the optimal values.
   
   
2.  **Discriminative Learning rates** were used so that the inner layers of the pretrained model do not get changed much, and the outer layers get updated at a greater rate than that.

    *( For a brief idea about Discriminative Learning Rate, please refer to Edit 1 at the end of the kernel )*
    

3. **Cyclical Learning rate scheduler** was used, following the **1-cycle policy** so that the we do not get stuck at an instable minima and get a stabler minima which performs over a wider range of loss functions and not just the train set specifically.

   More about the 1-cycle learning policy - https://arxiv.org/pdf/1803.09820.pdf
   
   
4. Finally, **ensembling** the different models trained gave a great push to the score of about 0.0147 to the F-1 Score.

    ",2fcde4cf,0.6666666666666666
30972,eb0854a6601407,431758e8,"Strategy: in training you need to control this effect by expliciting the number of observations because this is predictive of the uncertainty of the predictions. In the test phase, instead, when you are working with an asset that you don't know about, you need to impute an average number of observations, thus expecting an average dispersion of predictions for that asset.",6d107747,0.6666666666666666
30973,0a1fcda859252c,fa20d01d,"I have commented out the training step as of now as it will train the network again while rendering the notebook and I would have to wait for an hour or so which I don't want to. I have uploaded the weights of the best model I achieved so far. Feel free to use it for further fine-tuning of the network. We will load those weights and will run the inference on the test set using those weights only. But...but for your reference, I will attach the screenshot of the training steps here.",13a38774,0.6666666666666666
30980,aa7db7b023d0a2,efa1d75d,"#val_accuracy: 0.0
Best val_accuracy So Far: 0.15789473056793213",ec912af3,0.6666666666666666
30981,b6c0ad74f95b8c,e8e29445,"Nearest Neighbour function. The output are the 10 nearest neighbours (the 10 most similar QBs based on their first 3 seasons in the league) and their scaled statistics (comp%, YDS, TD, int). The output is the QBs most similar to Deshaun Watson.",5de5b241,0.6666666666666666
30982,9ab7ebae0bfedb,b72cc694,"加速度センサの軸方向の確認を行う．

おそらく，9.8m/s付近で推移している軸があるはず
",ad87076d,0.6666666666666666
30984,a1a31459abf078,c0550b24,"# Feature Engineering<a name=""feature""></a>

After getting a good understanding of the data, our next step is to generate some features on the training data and create a model using them to generate predictions. Lot of our flag variables created above can directly be translated into features. In additon to that, here are some of the features i've come up with - 

**User Based Features** - 
* Percentage Answered Correctly
* Total Questions Answered
* Std Deviation Answered Correctly
* Variance Answered Correctly
* User min value answered_correctly
* User max value answered_correctly

**Question Based Features** - 
* Percentage Correct
* Total count
* Question answered_correctly Variance
* Question answered_correctly Standard Deviation

**Other Features** - 
* Bundle Flag
* Tag Flag 
* Part Flag
* Tag Count
* Prior Question Had Explanation
* Prior Question Elapsed Time
* Lecture Flag (Based on whether a student has seen a lecture)
* Number of lectures seen by a student

The feature generation process is generally iterative, we go with intuition and come up with an initial set of features and then remove the ones which don't improve model performance and then come up with newer features that might improve prediction accuracy. ",66fc0f54,0.6666666666666666
30985,c1984e64b35234,e30bda1d,"Sample appears to be heterozygous - having one A allele and one G allele.
![image.png](attachment:image.png)
",1811225b,0.6666666666666666
30990,c8c4705cca1ebb,46dcd85e,"Test veri seti ile df DataFrame ' i , test kesişimine göre birleştir birleştir",6d9d7107,0.6666666666666666
30993,9ad9a97e628bfa,98ea2f13,"단, Cabin Initial 이 확보된 전체 데이터의 양이 크지 않다는점을 고려해야 함. (특히 G, T의 경우)",0a7e1136,0.6666666666666666
30994,b9328fe3b0cefc,6967d7b5,### Seed number and region with champion(冠军对应的种子号统计、区域统计),3a35eb23,0.6666666666666666
30997,454672c0f11328,c014c935,"### Lv2
**Input**: Predictions from previous level along with lv1 predictions from this [notebook](https://www.kaggle.com/manabendrarout/custom-stacking-of-classifiers-gpu-tps-sep2021/).

**Models**: [XGB, CATB, LGBM]",bfa2868b,0.6666666666666666
31006,a76e0e8770b7a0,9368aef1,İs millenium Affected terrorism?,02863d3b,0.6666666666666666
31007,2473d004f92592,9d205402,"Multinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in.

It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.",18d3b6ee,0.6666666666666666
31010,e5dd725b8fa422,bd869a4e,# Validating the AR model,14675d8b,0.6666666666666666
31016,396bc36edb95d3,28135ea1,#### Confusion Matrix and Classification Report for Training Data - Random Forest,965e4f8f,0.6666666666666666
31023,7f74a04ae75792,2578213e,Customer last purchased NaN value was replaced with mean,d01e91da,0.6691176470588235
31024,c65a65d4041018,96161ae1,### programming languages,824fb229,0.6691176470588235
31026,a4f8ad33c823c5,59adc1e3,"### Detecting outliers

2 points of outliers were observed. There is a data point where the patient's length-of-stay is around 174 and 160 respectively. These points could be further investigated to see if these outliers need to be removed from the dataset.",fcd48307,0.6692307692307692
31032,2f47abddfd1928,58f28174,"As we have been commenting so far, most of the people travels alone. But it is interesting to see how as the amount of parents/children increases, so does the fare.

And not only in amount but also in spread. This might be because children's tickets are paid by their parents or because the people that travel with family can usually afford better tickets. I expect the workers to travel alone.",ae33cc0b,0.6694214876033058
31035,9169c4e9c33c90,6f8f08be,"<a id=""Price""></a>",725bf880,0.6694915254237288
31038,510b8303776bb6,55210bda,## Using polynomial on the dataset,18080db8,0.6698113207547169
31042,78fcac7baa760f,fc88ca68,Outlier treatment,c96cafcc,0.6699029126213593
31052,4c47839b067546,f237771c,### transmission,1f517b02,0.6702127659574468
31053,b01ee6cb674fa3,8557936c,"# European Space Agency - ESA

Founded by 1975 in Paris, France ",a8ffd35e,0.6702898550724637
31056,d1ff7e10ee0102,d856afd9,"The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:

* <b>Histogram</b> - Kurtosis and skewness.
* <b>Normal probability plot</b> - Data distribution should closely follow the diagonal that represents the normal distribution.",2cc71c3c,0.6704545454545454
31057,73d8e56bc709b1,d6302eb0,# **C.Ronaldo vs Messi**,78ec3cce,0.6704545454545454
31058,869a39a3d4dea2,77031263,"Split method in open cv provides in BGR format, so we read as B,G,R Tuple",9020daf8,0.6705882352941176
31061,9c26c5dcd46a25,91419369,"On remarque une dispersion encore importante des prédictions aux valeurs test. Ce premier modèle peut être amélioré, par exemple en ajoutant la catégorie du produit.  ",1bbbb677,0.6707317073170732
31063,0b01138ad120fc,eeb0dbe7,## Long Short-Term Memory,0b4b72e6,0.6707317073170732
31072,3d08ca7656dec0,397d7d95,# Decission Tree,bd3f87e3,0.6712328767123288
31074,596389bed473be,50306d92,# Specific Metric Functions,5f8af156,0.6712328767123288
31080,2730840089c8eb,dab41da8,"You could probably write a short book just on `str.format`, so I'll stop here, and point you to [pyformat.info](https://pyformat.info/) and [the official docs](https://docs.python.org/3/library/string.html#formatstrings) for further reading.",34d27dac,0.6714285714285714
31083,2ada0305b68956,f85ec3a6,### 114. Palette = 'gnuplot_r',133e26f4,0.6714285714285714
31087,c80939c7c626cf,8f21e826,# 7 Cabin,b9ac31e2,0.6715328467153284
31094,1a222fee3089d2,848ee079,## **Preprocessing**,59ab8894,0.6716417910447762
31095,ee23a565163388,1cd22e20,The independant and dependant features need to be split into two different dataframes.,88aacbc4,0.6717557251908397
31096,ff3a8ce61fab6a,16a5e4d6,"<hr>

<div>
    🔺 <b>Alert</b><br><br>
    <b>datatype equavilant</b> which we talk about it at previous             <b>alert</b> ☝ has same importance in <b>multiplication</b>.
</div>


<p>
    Let's talk about <b>subtract</b> function We can use it to <b>Number subtraction.</b>
</p>

## Number subtraction<hr>

### Exampel 1 ",9afe1654,0.671875
31099,0932046e1f485d,08fabc8a,Looks like Action and Arcade really dominate the mobile game market.,218cc7a3,0.671875
31103,0858e1bb3cbaca,4e1f3672,"When we look at a whole book of sales data, categorizing them can be very helpful. With pandas, we can *split* our data with specific criteria, and *apply* or *combine* different calculations on these data. We can split the dataset by

**.groupby(' ')**

in which the string should be the name of category you want to group by.",78548374,0.6721311475409836
31107,4ae6a182abac64,9d63249e,"* The next step is dropping the less relevant features because, The problem with less important features is that they create more noise
 
  and actually take over the importance of real features like Sex and Pclass.",418676c5,0.6722689075630253
31110,e3f3f108cd3869,8fd24553,**Scaling The Data**,2b78de2d,0.6724137931034483
31113,84127ade6fde87,781cf349,"We certainly could do some work to deduplicate words, condense alternate spellings, collapse past and future tenses into a single token, and that kind of thing. Still, a general-purpose English-language encoding would be huge. Even worse, every time we encountered a new word, we would have to add a new column to the vector, which would mean adding a new set of weights to the model to account for that new vocabulary entry—which would be painful from a training perspective.",f55d05b6,0.6724137931034483
31116,1dd9c6aa74d289,7e25758f,"## Summary Q2:
1. Cumulative distribution function (CDF) helps to quickly identify which height range you belong to among climbers, but it is not relevant to which height will be beneficial to climbing. It is more to help distinguishing a style or beta version that might be usful to you from others.
2. The medium height of male climbers is 178 cm and that of female is 165 cm. If the distribution fits in a simple Gaussian, the results are 177.8 ±8.0 cm and 164.8 ±7.9 cm respectively. It is also interesting to compare it with [contemporary national average](https://en.wikipedia.org/wiki/Average_human_height_by_country). For example, the height ratio of male to female is about 1.08 in the climbing community, consistent with many countries.",5ef9a1be,0.6724137931034483
31124,30fdc4a6e3c1db,6a1cd340,### 4.2 Across Week,6111ddee,0.672514619883041
31130,5a8c553e21c70f,e3e5415a,The least important 2 features are dropped. Note that these features have very low correlation with Class (refer to correlation matrix).,9ebd9d8f,0.6727272727272727
31135,582cb872d19026,5a36ae24,"
******************************************************************************************************************************
#### This figure shows that the installation values and 5 stars rating values of the 20 most downloaded games in the same graphic.
******************************************************************************************************************************",8d966d69,0.6730769230769231
31137,71d3e4aee86e3e,7b3f5c74,> # Worst-Hit State Analysis,69706f0b,0.6730769230769231
31138,99bf357eaf61f1,c2f50bbe,#### Deleting different columns from test in train,9d92fafe,0.6730769230769231
31140,95efc1ad1d3e26,ae519ac9,# 2. Variational Autoencoder,79de1120,0.6730769230769231
31142,6f1481148352e9,e64b39e5,"**The highest number of fires was in Bahia - 995, Minas Gerais - 959 и Piau - 911.**",7cfbdb8f,0.6730769230769231
31143,917957c6c4065f,53e8face,"조회수 대비 싫어요를 기준으로 하는 상위 5개의 항목입니다.  
갓건배 채널의 동영상이 3개가 있습니다.  
찾아보니 욕설 등의 논란으로 유튜브에서 여러 번 영구정지 처분을 받았고 그때마다 새로운 계정을 만들었다고 하네요.   ",55b8ed68,0.673202614379085
31149,f1e162ddd14f11,1a2bbabc,let's work with the hyperparamters and some other parameters of RandomForestRegressor,cdb2e771,0.673469387755102
31152,e69a496109e7d8,9dc89165,"In the above plot, we come to know that more people in survived had nodecount zero(most) until 5. In non-survived people had nodecount from 3 to 20",1c640591,0.673469387755102
31154,2343dc02ffb96a,e6e66315,"# The following results were obtained in my jupyter notebook with this same code.

If the data is good for modeling, then our residuals will have certain characteristics. These characteristics are:

1. The data is “linear”. That is, the dependent variable is a linear function of independent variables and an error term e, and is largely dependent on characteristics 2-4. Think of the equation of a line in two dimensions: y = mx + b + e. yis the dependent or “response” variable, xis the input, mis the dimensional coefficient and bis the intercept (when x = 0). We can easily extend this “line” to higher dimensions by adding more inputs and coefficients, creating a hyperplane with the following form: y = a1\*x1+ a2\*x2+ … + an\*xn
2. Errors are normally distributed across the data. In other words, if you plotted the errors on a graph, they should take on the traditional bell-curve or Gaussian shape.
3. There is “homoscedasticity”. This means that the variance of the errors is consistent across the entire dataset. We want to avoid situations where the error rate grows in a particular direction. 
4. The independent variables are actually independent and not collinear. We want to ensure independence between all of our inputs, otherwise our inputs will affect each other, instead of our response.



Omnibus:	0.191	
Omnibus/Prob(Omnibus) – a test of the skewness and kurtosis of the residual. (Errors are normally distributed across the data. In other words, if you plotted the errors on a graph, they should take on the traditional bell-curve or Gaussian shape.) We hope to see a value close to *zero* which would indicate normalcy. 

Prob(Omnibus):	0.909
The Prob (Omnibus) performs a statistical test indicating the probability that the residuals are normally distributed. We hope to see something *close to 1* here. 

In this case Omnibus is close to zero and the Prob (Omnibus) is 
close to one. A linear regression approach will very likely yield good results.

Skew:	0.037
Skew – a measure of data symmetry. We want to see something close to *zero*, indicating the residual distribution is normal. Note that this value also drives the Omnibus. This result has close to zero, and therefore a good, skew.

Kurtosis:	2.945	
Kurtosis – a measure of “peakiness”, or curvature of the data. Higher peaks lead to greater Kurtosis. Greater Kurtosis can be interpreted as a tighter clustering of residuals around zero, implying a better model with few outliers. 

Durbin-Watson:	1.967
Durbin-Watson – tests for homoscedasticity. (There is “homoscedasticity”. This means that the variance of the errors is consistent across the entire dataset. We want to avoid situations where the error rate grows in a particular direction.). We hope to have a value *between 1 and 2*. In this case, the data yields a value of under 2.

Jarque-Bera (JB):	0.246
Jarque-Bera (JB)/Prob(JB) – like the Omnibus test in that it tests both skew and kurtosis. We hope to see in this test a confirmation of the Omnibus test. In this case, we do since the Omnibus test yielded 0.191 

Cond. No.	1.00
Condition Number – This test measures the sensitivity of a function’s output as compared to its input. (The independent variables are actually independent and not collinear. We want to ensure independence between all of our inputs, otherwise our inputs will affect each other, instead of our response.) When we have multicollinearity, we can expect much higher fluctuations to small changes in the data, hence, we hope to see a relatively small number, something *below 30*. We only have one feature in this data, so we are not surprised to see a 1.00 here.",29aa95a4,0.673469387755102
31157,ffd1df95ca5289,5b737d87,"#### 'default_yes' , 'job_unknown', 'month_nov', 'month_aug', , 'job_entrepreneur' , 'job_self-employed',  'month_jul' , 'day' as been removed as the p-value of the variables are greater than 0.05 which means they are not significant",db00c338,0.673469387755102
31159,f91f58d488d4af,74684fe9,"#### Stepping your parameters using an Optimization step.

w -= w.grad * learning_rate

* learning rate is often a number between 0.001 and 0.1

If you pick a learning rate that's too low, it can mean having to do a lot of steps.

But picking a learning rate that's too high is even worse—it can actually result in the loss getting worse.

If the learning rate is too high, it may also ""bounce"" around, rather than actually diverging;

",5df1bbf3,0.6736842105263158
31160,169177b6e9edea,cdd76b4c,<h2><b>Hyperparameter tuning,ca42152f,0.6736842105263158
31164,0e2a23fbe41ca9,432946e4,"Observations:
- Quantity of active months within last 6 months, is 3 in 97.8% of data meaning, they were active during all the 6 months",64e4762c,0.6739130434782609
31168,7e89d387feb9f5,a85ce424,## 4. Отзывы,989e3a1b,0.6739130434782609
31173,72d528df923403,d88c9a0e,Let's review the influence of different types of events on the quantity of items sold.,d51c8e8e,0.6739130434782609
31177,e67925694c07d3,32a1eb3c,## credit card,83af4c4a,0.6741573033707865
31182,1660daf8867980,d8f0fe6a,**Implementation**,42d7cffc,0.6744186046511628
31191,743ae010f5e875,ccac1bdd,### Predict,02c54445,0.6744186046511628
31192,4daf6153275cbf,4c9b1a43,"Almost every country has their local cuisine as their most popular cuisine. Except for Germans, Belgians and Norwegians. Belgians apparently does not like their food, as their local food are the 3rd popular (and the rating above)",51db1961,0.6746987951807228
31193,835a7b4e660d23,9c7af46b,### Indexing Pandas Time Series,53bc7a6e,0.6746987951807228
31194,e19e307b3fd188,bae42b55,### Train new data without outliers,2173955b,0.6747967479674797
31195,3dd4294f903768,65b7e098,"We can see that there is a bigger correlation between the delivery feature than the table booking feature. But, we can see that both of them can help the model, so we will use them both.",0d89d098,0.675
31201,fdbbd573ba31c2,b5dbba68,## Feature Selection,f7c28d74,0.675
31205,5626e84c4e6bf8,9402fae2,"## Hyperopt: A great library for Bayesian Optimization
Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions. According to GitHub, it is ""Distributed Asynchronous Hyper-parameter Optimization"".
Source: [GitHub](https://github.com/hyperopt/hyperopt)

**The sigma parameter can have a lot of impact on the clustering in the map as it is essentially the spread of the neighborhood function. So, we will optimize the sigma function to minimize the quantization error(```quantization_error``` - A MiniSom method that returns the quantization error computed as the average distance between each input sample and its best matching unit.).**

```fmin``` - For minimizing the parameter

```Trials``` - To really see the purpose of returning a dictionary, let's modify the objective function to return some more things, and pass an explicit trials argument to fmin.

```hp``` - Objective function for defining the search space

```tpe``` - Algorithm used for optimization

**We will have the training and test set contain only '4s' and '8s' as optimization for all labels will take a lot of time. At the end of the day, this is just a example to get you all to know about great libraries like 'MiniSom' and'Hyperopt'.**",e2ecb669,0.675
31208,9a040a4f21091e,93a8a247,We also see that the most and least toxic words make much more sense in this case. ,f591b57d,0.675
31213,49ee86d074de69,817d2eae,### Training The Model,71ccc6d3,0.6752136752136753
31216,663bbc9eaf267b,4005f913,## mileage,32445529,0.6753246753246753
31217,c13f73168789c2,aa14bcea,"### 2.1 To slice rows by index position<a id='26'></a>
Syntax : `df.iloc[starting_row_index : ending_row_index, :]`",16175052,0.6753246753246753
31220,75adb7945ef9bd,4fd12f10,"## 9. Simple Model: Logistic Regression

We try the simplest model with logistic regression, based on all features we created above. Before we fit a model, we first transform the features into the same scale with minimum 0 and maximum 1. We do this in the form of pipeline.",785c5095,0.6753246753246753
31224,6cade0b6a41ba2,db20ecb1,##### These values dont have any definate order. Hence we will asume them to be Categorical Nominal Variable,e6110293,0.6754385964912281
31226,3fb15e6e48aec2,5a9aa6e7,# Standard Scaler,9d1f4358,0.6754385964912281
31229,e4525eb0c96f28,7df70fff,"As you can see, this plot looks almost identical to our first plot involving year and sales. Note that this plot has a different fit line than that earlier plot since we included all of the data, even the the outliers we excluded before. The bad news is, our model looks like it is going to have a lot of residuals, especially above the line. We can still plot out the residuals just to be sure. Since each year has its own distribution of sales, looking at a violin plot of residuals would also be useful in this case.",2093a1f1,0.6756756756756757
31238,8276973853faa1,5b539f63,# Top 5 Veg consumption States in India,88da542b,0.6756756756756757
31242,63b44c85e32c1f,c9f681c2,Here the quotient has to be 3 and the remainder has to be 1. These values cannot be changed whatsoever when 10 is divided by 3. Hence divmod returns these values in a tuple.,fb9b9562,0.6756756756756757
31245,ac9b48d531bad9,1084a886,**FINDING BEST ESTIMATOR FOR RANDOM FOREST CLASSIFIER USING GRID SEARCH CV**,95965e35,0.6756756756756757
31246,ab6da5994949a3,56e17ffe,## Visualising the K-NN Training set results,fae6b91d,0.6759259259259259
31251,bddd799cdbbae8,9a853b40,**Multinomial Naive Bayes Classifier**,b44e3c08,0.676056338028169
31255,55a5e31d03df9f,d19a3a0d,"Here we can see the name of some layers, if that layer is trainable or not, and the dtype. 

If you go through the documentation of EfficientNet model you can read that the RESCALING is in the model included as a layer, so far all the models that we have explored need that rescaling before passing it through the model, for proper use of EffNet we need to reload the data but without rescaling this time.",06dce00f,0.6761904761904762
31256,7454fdc444df16,28ec881b,"## Storing the Images
We know that each RGB image has a dimension of (50,50,3), in order to store them in numpy arrays, we will flatten the image into a (1,7500) shape. This means that each row of our numpy array will store an image, and that our final numpy array dimensions will be about (280000,7500). We have about 280,000 rows because we have about 280,000 crops in total.

We know that each RGB image has a dimension of (50,50,3). One way of storing them is by flattening the image array into an array with a shape of (1,7500). This means that in order to store all the images we will need a numpy array with a total dimension of (280000,7500), this is because we have a total of 280,000 images. The problem with following this method comes to the limited amount of memory that we have when running a kaggle notebook. So what can we do? 

We can try and reduce the number of dimensions that we are dealing with. One way of doing that is by converting our image from an RGB image into a grayscale image. An image is converted from an RGB image into grayscale by taking the weighted sum of the R,G, & B values and getting a single dimension in return. While this works, we can perform a better approximation, by using dimensionality reduction techniques such as Principle Compononent Analysis (PCA), Single Value Decomposition (SVD),& Latent Dirichlet Allocation (LDA).

For this example we will be applying PCA. 

## Applying PCA

Let's briefly talk about PCA before delving into the application. PCA is a dimentionality reduction technique. What it is essentially doing is reducing the number of dimensions that we have by determining the covariance between the different variables that we have. 

The covariance is how likely a variable is to vary relative to another variable. This means that variables with very high covariance can be redundant. Let's say I know that A will vary when B varies, then I can formulate a formula to explain the variation of B given A. 

Let's take another more concrete example: we know that the velocity is equal to the displacement over time, here the third variable is always explained given the other 2. PCA does the same thing, it tries to determine the relations between variables and uses them to explain each other. 

PCA linearly maps the features, along the axis of maximum variation, this axis is also called the eigen vector. The distance between a point and the eigen vector and a point is also known as the eigen value. To learn more about PCA 2 great articles are linked below. 

* [Article explaining the theoretical aspect of dimensionality reduction techniques such as PCA and SVD.](https://towardsdatascience.com/pca-and-svd-explained-with-numpy-5d13b0d2a4d8)
* [Article applying PCA on satellite images.](https://towardsdatascience.com/principal-component-analysis-in-depth-understanding-through-image-visualization-892922f77d9f)
* [Very useful answer on stackexchange that talks about the considerations that need to be taken before applying PCA](https://stats.stackexchange.com/a/450089/267920)
",a7818ef5,0.6761904761904762
31263,ab657da5329e3f,edbbeb89,# Training,021526f8,0.6764705882352942
31264,e4c6dd957eb5ce,065c0c22,"# Competitions 
Now, we will explore the competitions:
- How many competitions we have? 
- Which are the distribution of paid and not paid competitions by the time?
- What is the distribution of Prizes in paid competitions? 

Let's investigate it further. ",2e383665,0.6764705882352942
31269,8f50c9c16db95f,b024e0e4,"Apart from that, it is also found that a longer approach run can help you accelerate to the high speed before contacting the ball - the correlation between the two is highly positive, **~55%**.",26cc763a,0.6764705882352942
31272,395ed8e0b4fd17,678a2446,### Bitcoin(BTC) OHLC Chart for first 200 rows,7573ea31,0.6764705882352942
31274,99821bc6a45be6,b1184af4,## VGG16 model: ,b9d59346,0.6764705882352942
31276,71b75664517244,c945b04a,"Alex ferguson has his best performance since early season up until 2012, and retired after season 2014.",fc905af5,0.6764705882352942
31277,1d1598b6fa2aa7,11290343,"### 3D Plots

Ofcourse you can create a 3D plots - https://plotly.com/python/#3d-charts",e066accf,0.6764705882352942
31278,410285582f4f7e,06d1b596,"**Countries visited by Air BnB Users**

It looks for bulk of data there is no country desitnation specified. Second most visited place by users of Air BnB users is United States. ",d026266b,0.6764705882352942
31281,21bce4ec54b3fa,0827b25e,"Another way to perform variable selection that is more traditional and related to linear models is by picking features that are most correlated to target variable.
Since most of the relationships in real world are not linear, we'll use Spearman's rank correlation instead of Pearson's correlation, because it's more robust and better captures non-linearities.
Also, even though pandas has in-built correlation method, let's use scipy's implementation, since it's more efficient.

Some practitioners suggest investigating relationships between features first, because multicollinearity has an effect on importances, i.e. correlated variables will both seem important but contain nearly identical information. But let's leave it for now, because it would unnecessarily complicate implementation if we wanted to automate this process.",35546e30,0.6764705882352942
31282,55c34673c1f760,8a495f08,## Training,2663c47f,0.6764705882352942
31285,dc0b0e1cb46c6f,ac52b4f7,"In this case, I think that percent of people vaccinated per hundred is the most representative value so I will only plot this values.",47b17a7b,0.6764705882352942
31286,b10bd75889dad9,1740ca1e,#### looks like there is lot of multicollinearity,ee00ceee,0.6766666666666666
31287,ba4b3bd184acbb,b48d4f29,There are many other options for storage formats and optional parameters that can be explored [here](https://pandas.pydata.org/docs/reference/frame.html#serialization-io-conversion).,0f5de724,0.6766917293233082
31289,09751c520b0616,12f1beff,## 3. Exploratory Data Analysis,a4d0c7e9,0.676923076923077
31290,c115e287523aab,24043ef7,"# Build Model
",feb1288b,0.676923076923077
31292,03048e86a6d806,f117e0d8,# Care to Prepare,1285c231,0.676923076923077
31293,485de87c50af82,94a55cac,## **Model Construction**,a5bd438e,0.676923076923077
31296,1645979263c148,fc2922a1,"## Model Buliding
here we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.

1. Logistic Regression (f1 score: 0.9827586206896551 )
2. k-nearest neighbors (f1 score: 0.8571428571428571 )
3. **naive bayes (f1 score: 1.0)**
4. support vector classification (f1 score: 0.7730061349693252 )
5. **DecisionTreeClassifier (f1 score: 1.0)**
6. **RandomForestClassifier (f1 score: 1.0)**",fa11663e,0.676923076923077
31297,be9597c72542a2,03aefb24,# DAY 9,6f29c6d8,0.676923076923077
31301,fc8e0042411c46,7e3a2899,"- 'Not Sure' is a mixed emotion, it may be Interested or Not Interested. Depend upon the mood of customer,their requirement & content of the communication, Lead can be conveted into a customer.
- 'Worst' Lead Quality brings less business ",af476c2a,0.677115987460815
31302,2ada0305b68956,da8f5012,### 115) palette = 'gnuplot2',133e26f4,0.6771428571428572
31306,1c2948c70624cf,7bb55de4,# **Data Modeling**,eeb97474,0.6774193548387096
31307,16862cb02d73d5,21d4ccbc,"The anomalies identified by the algorithm should make sense when viewed **visually**(sudden dip/peaks) by the business user to action upon it. So creating a** good visualization is equally important in this process**.

This function creates **actuals plot on a time series with anomaly points highlighted on it**. Also a table which provides actual data, the change and conditional formatting based on anomalies.",d7ffa1a6,0.6774193548387096
31308,c0ddb77bf32e2b,08cf8fe0,"Is it possible to use the data from last hour to predict the pm2.5 air quality now?

e.g. 

use data from 2015/1/1/0:00

to predict PM2.5 of 2015/1/1/1:00

Since this is a time-series problem, instead of using  sklearn.model_selection.train_test_split for making test data, I will choose December for testing data.(737 rows)
From 2015/12/1 0:00  to  2015/12/31 22:00",a0cb45f7,0.6774193548387096
31309,0925f172b5eb74,3ddb9ace,# Training the model,ec34cd72,0.6774193548387096
31315,0caaec057f7184,9d319207,"## New items new launched in shop items

There are around 7% of test data in the category.

For new items new launched in shop items, the real conditions can be 
- new launched 
- launched with 0 selling records
- launched with missing selling records

Question to ask for this kind of condition:
- How's the category saling in the shop? in other shops? 

After some observation, we'll find out how to deal with the kind of condition.",b875533e,0.6774193548387096
31316,b05ee1ea1c8269,0c55a403,"# Create Bar Chart Race
",19e4d303,0.6774193548387096
31318,7dec6bdea6d779,c5141cfd,"pred is of shape  7443 x 1103: 7443 test examples, and 1103 different labels.

Next step: apply threshold, if over a certain threshold, then consider it as a label. Otherwise, not.",18be5949,0.6774193548387096
31323,56e58d53ac9c57,9e539c19,We see that positive feedback column has some outliers. Later we will need to remove them,90e2ab8e,0.6774193548387096
31327,a3e8d6ef4c5188,50179f0d,![](http://)### Feature Scaling,7c8212dd,0.6774193548387096
31328,ad26c020235dfc,c46cf12d,"## Aquifer_Luco
There is only one target with the name Depth_to_Groundwater_Podere_Casetta.",bf766e48,0.6774193548387096
31329,78998e078eaaa1,793604cc,"plt.imshow(test_img['truth labels'].to_numpy().reshape((584,565),order='C'),cmap='gray')",2b29364c,0.6774193548387096
31330,b01ee6cb674fa3,ff5f163d,"# National Aeronautics and Space Administration - NASA

The USA Space Agency

Founded in July of 1958 ",a8ffd35e,0.677536231884058
31332,2f47abddfd1928,f0dde482,"## 3.6. Fare

Seems that Fare has a good correlation with survived, Pclass, Sex, age, SibSp, Parch and Cabin_Letter.

Lets inspect these relations.",ae33cc0b,0.6776859504132231
31333,892be0a523578c,901baab8,"Next, I want to take a further step to identify customer groups for customers in cluster 1 and 2 above based on the time (morning, afternoon, evening, night) when they do exercise and exercise intentisy",b0e8d7c0,0.6777777777777778
31334,3597174a998d4d,3f2698c8,#### 2.2.2.3 deposit type,276892ed,0.6777777777777778
31339,726833f92fb87a,ff4e31eb,## Age cate,7dc5e1b6,0.6778523489932886
31340,5f32117bcd5255,d69f03e6,#### HST OPTICAL FOUR,85882abf,0.6778523489932886
31342,c4bca5d86a38c3,df91991a,Separando la data en X y Y,e23d297c,0.6779661016949152
31344,ed8009f482b380,40e97aa8,We can see that the y dataframe is now evenly distributed. Now we build the model.,e99941fa,0.6779661016949152
31345,9169c4e9c33c90,644ebaaf,"# Price

[Back to top](#Top)",725bf880,0.6779661016949152
31347,f2e5e9fb9eaaf7,c0cb1257,"<a id=""6.1.2""></a>
### 6.1.2 LGBM Classifier",048e0d08,0.6779661016949152
31349,dac3c8204a2d1b,d80cabf4,We extract top  10 authors names based on the ratings,b0d2d0dc,0.6779661016949152
31352,149cb8d3489224,58c78bb8,### Extract date and time features,116858e7,0.6779661016949152
31353,a81661cc35d8d2,22a6a57b,***,3331f113,0.6779661016949152
31354,b9bc7dc9f582e5,90afe9ef,# XGBoost,15cc4d28,0.6779661016949152
31362,eda49464dd6d1b,02917e30,"### The train and test datasets have a different number of columns, and this will not work for the model.  Thisis the result of a few of the less common regions or policy sales channels that exist in one and not the other.  We will simply delete these columns, since the number of customers in them is very low.",8421f81f,0.6783216783216783
31363,30fdc4a6e3c1db,68a83afd,### Plotting sales over the week,6111ddee,0.6783625730994152
31364,5ce12be6e7b90e,d967b190,"### Access

You can access list elements just like strings, using indexes (starting from 0):",c0ab62dd,0.6783625730994152
31367,c54ea4523bd49c,9fc2c93d,I found I could do slightly better and have fewer trainable parameters if I use max pooling layers and a dense layer at the end.,097ccba2,0.6785714285714286
31369,f4514ec092a771,c2d42a6b,"After training, you can file the output process at log folder from Kaldi. Something like this: *$kaldi_path/egs/command/exp/tri3b/decode*/log/*  
In this log folder, there are many files decode.*.log (the number of files depends on how much number of job you use for decoding in Kaldi script, it should be lower than your CPU-core and lower than the number of speakers). And each file of decode.*.log will look like:  

>000044442_clip_000044442 no   
LOG (gmm-latgen-faster[5.5.382~1-c2163]:DecodeUtteranceLatticeFaster():decoder-wrappers.cc:289) Log-like per frame for utterance 000044442_clip_000044442 is -3.39615 over 98 frames.  
0000adecb_clip_0000adecb happy   
LOG (gmm-latgen-faster[5.5.382~1-c2163]:DecodeUtteranceLatticeFaster():decoder-wrappers.cc:289) Log-like per frame for utterance 0000adecb_clip_0000adecb is -4.63466 over 98 frames.",3739ab1e,0.6785714285714286
31371,912bb73358069c,69f12f4a,"### The truth has become known to all. The keras loss fuction is uncorrect. It calculats every sample's mcrmse, then returns the average. ",0cf9db82,0.6785714285714286
31373,0dd3ac2d55efd7,ad0718ab,# Model Building,e9aa2cc2,0.6785714285714286
31381,92e9fc3a0ff5c0,6fbb9f00,# **Dropping all rows in Trump's and Biden's dataset whose statements are neutral with polarity 'zero'**,d53da425,0.6785714285714286
31382,7686f42e1f28d2,a8810cf0,Training time for me was around 80 minutes on this kaggle kernel,6c128859,0.6785714285714286
31384,8dd655515e7d18,b6edca4d,"**Analysis:**

Category-2 has the highest Users, while Category-5 has the least Users",895f41cf,0.6785714285714286
31389,b8849a04581d32,dadc718c,## Model training,b8a568cd,0.6785714285714286
31396,b290039151fb39,461d58b1,## MixUp,1836a79c,0.6785714285714286
31404,07bfec3562f9b3,b14ae37a,## Text #3,327e7d5b,0.6792452830188679
31406,0ad8d416b89b78,4f4c40bf,Hyperparameter Tuning utilising 'GridSearchCV',0b0562f0,0.6792452830188679
31408,614ba9f0c62677,f486e555,"<a id=""14""></a>
### Epochs and Batch Size
* Say you have a dataset of 10 examples (or samples). You have a **batch size** of 2, and you've specified you want the algorithm to run for 3 **epochs**. Therefore, in each epoch, you have 5 **batches** (10/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations **per epoch**.
* reference: https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks",b8551335,0.6792452830188679
31410,4dd47072617594,1b7f0c56,# TF-IDF,44ff1d11,0.6792452830188679
31411,a070fd03ae8ed2,1375586e,## 6.2 Расчет бинарного вектора прогноза (targets_pred) из вероятностного прогноза (pred_proba),c0ec4138,0.6792452830188679
31420,98a6794067932a,784a2946,"La cellule ci-dessous ne nous permet pas de tirer d'analyses supplémentaires, mais elle sera utile lors des analyses suivantes. En fait, ce code permet d'associer la colonne ""State"" à l'index de ce tableau, ce qui nous sera utile par la suite afin de modifier le nom des états.",08600fe2,0.6796116504854369
31425,81712ee7510ac5,8c5655a3,**Comparison Operators**,c4685e79,0.68
31426,91eaec994e0c6f,93c4b63b,After the analyzing and visualizing part comes the forecast part  !,376aef10,0.68
31429,519e936017c30a,803f981f,"La plataforma que hemos escogido para nuestro análisis es ""XBOX 360"", que se compone de 1.234 videojuegos, que encontramos repartidos entre los siguientes géneros. Tal y como se puede apreciar en la gráfica, vemos que los videojuegos de acción son los que mayor presencia tienen en esta plataforma, pero ¿Es esta categoría la que mayor ventas presenta? Para responder a esta pregunta, vamos a examinar las ventas generadas por cada género.  ",dc34915d,0.68
31430,cb570c7b7f0501,19b86bf3,"# Research Question 6
# ( is it about the difference between the scheduled date and appiontment date !? )
Maybe when the patient get an appiontment in the same day he will have a better chance to attend !? ",a200a0ec,0.68
31432,1bf50feb2b65e0,1d2b7787,~84% accuract,0206642c,0.68
31434,4cd25e50c7e007,60bc663a,"### Building model using statsmodel, for the detailed statistics",ceb0c525,0.68
31440,cee088a6840708,e435de0b,"# Step 9 -> describe the GCN (Graph convolution network) model

Similar to any linear model that we write as a class

you will define the layers in the init method and use them in the call method.",55463e1c,0.68
31441,50d4ddf1953997,4d08cf37,How is the duration of movies changing in time?,90bdddd6,0.68
31443,0687cd5c8597db,5e90f7e5,### **Plot between Testing Loss and Testing Accuracy**,4edec76a,0.68
31444,7e1da639035ac5,ac56d8e5,### <a id='12.2'>12.2 Supportive Environment ratings statistical analysis</a>,120b6c23,0.68
31445,bbad077c274022,5bd58d74,"*It seems I walk more in home on weekends. This is for sure as ""Who goes to the office on Sundays? Right?"" But the other thing I notice is that Thursdays are my lazy days - least outdoor walking plus least basic walking.* 

*I guess I prepare on Thurdays by saving fuel for Friday parties :)*",3c2e3dea,0.68
31446,83df814455f06c,248b4fb6,### Check for overfitting and underfitting,c9cff71a,0.68
31451,caaa6793391520,f5db5832,"We see that Age is numeric, and Cabin is an object feature. Let's update them using our imputers",1e79f342,0.68
31455,b10bd75889dad9,56fba1fa,#### Lets apply Random forest Clasifier to get the top 10 columns,ee00ceee,0.68
31457,6338f6b0178d13,830dec78,# Calculating the probability score for each classification,ae81b18b,0.68
31460,fc8e0042411c46,eb656230,## Update me on Supply Chain Content,af476c2a,0.6802507836990596
31464,2a123b4e8f9433,d047193c,Distribution of validation scores,0a082218,0.6804123711340206
31466,166a62ebb4fc3a,bc2f1df6,"Looks great! Now let's move on to our model.

Let's describe data once more",db48a079,0.6805555555555556
31468,1014e6be391084,cf4f1edc,None of the variables shows high correlation with each other,46f9168f,0.6805555555555556
31469,593d1d3d1df05a,857e8cd1,# Making a Condition for Ending the Training Quickly once the required accuracy is achieved,bc682ffe,0.6805555555555556
31470,c01049afb6d307,c941d076,"* **(2)** Neoplasms
* **(6)** Diseases of the nervous system
* **(9)** Diseases of the circulatory system
* **(12)** Diseases of the skin and subcutaneous tissue
* **(19)** Injury, poisoning and certain other consequences of external causes

**These absence reasons are the ones that cause the most time loss.**

**Now let's examine the workers using excuses 2, 6, 9 ,12 and 19.** ",d37d3b5d,0.6805555555555556
31472,69ac33d79f5130,b87dffe7,### On weekends accident are in afternoon.,9d760d2a,0.6805555555555556
31475,04e6b0d3c70f46,41ca3d58,### Get feature and label from data frame,56344f77,0.6808510638297872
31478,73893f0467d5e3,a1056fd8,## Feature Importance ,279787c6,0.6808510638297872
31481,3f25b363afec54,fc7638d8,8460 patients are common between train and test,bbdaae25,0.6808510638297872
31482,c7e5f658090347,57e18193,"## Reflections on the Above Results And Moving on to Final Model Training 

#### Reflections:
1. Outlier removal seems to be useful, of course there is a danger here that one could overfit if the outlier removal is too aggressive (especially considering that I am only removing outliers from the not fraud class). So I will not use a too harsh outlier removal value in the upcoming model building.
2. Reducing the number of features seems to have a beneficial impact on model quality, (this is consistent with the expectations from the exploratory data analysis performed earlier). Given that this reduces model complexity, we will take this approach forward, but use a narrower range of options than what was attempted above.
3. No clear winner between MinMax and StandardScaler, so I will keep them both in the pipeline where relevant (not important for Random Forest or Gradient Boosting) 
4. Oversampling did not have any notable impact. At first I thought this could be due to the nature of ML model I chose to evaluate (logistic regression does not tend to require too much data to fit relative to other models), so I performed some tests with a random forest model too. But the results were repeated there, so I will just undersample. 

#### Model Training Time:
Now it's time to build the final classification model. 

**I will try 4 classification models:**
1. Logistic Regression
2. Random Forest
3. Support Vector Machine
4. Gradient Boosting ",43c78e7d,0.6808510638297872
31484,5f674175839b32,cf5f75a5,*In action Genre the most games had released.* ,53a2e343,0.6808510638297872
31487,598b6228760590,52d81997,- SVM,be30ab66,0.6811594202898551
31499,5f4ae633cfd090,a5235b7f,"After extracting the necessary information from these variables, there's no more need for them. So, I'll just drop them",a30a16e2,0.6813186813186813
31500,c9b4e282e4e2c1,2a915773,"* It seems like Defensive Linemans get more injuries playing as DE(Defensive End), specially knee and foot injuries.
* Offensive Linemans get more ankle injuries playing as C (center), but more knee injuries playing as T (offensive tackle).
* Safety players get more injuries playing as FS (free safety), specially ankle injuries.",f44d339f,0.6814159292035398
31502,a5a419dc7245b0,420d0f0f,##### Splitting the dataset into the Training set and Test set,4279726e,0.6814159292035398
31504,ae058c3f1439c3,27c8ed82,"> After looking at the above cloud of words which states the most available locations for jobs in Google, We can Conclude that Some of the most available Locations Include United States, Switchzerland, Germany, Netherlands, and Cities of US include San Francisco, Mountain View, Los Angeles etc.",965da99d,0.6818181818181818
31506,be2f4d8a6b73ca,f7dc9639,**seperating input features and output features**,5d8ce40a,0.6818181818181818
31507,73d8e56bc709b1,77cc8ccb,"In the same way, we could compare attritubes between Ronaldo and Messi.  
Ronaldo's position is ST, Messi's position is RF, for the sake of fairness, we choose one ST's features and one RF's features.(For there are four same features of ST and RF.)",78ec3cce,0.6818181818181818
31511,8578b9a8d00730,767638e3,## ***Efficientnet Model Building***,7648721b,0.6818181818181818
31512,b066ab2167199c,523d648b,"#### Mising Value Checking
- I use five function for mising value checking
   - insull()
      - If any value is null return True
      - Otherewise return False
      
   - isnull().any()
      - If any columns have null value return True
      - Otherewise return False
      
   - isnull().sum()
      - If any columns have null value return how many null values have
      - If no null value present return 0
      
   - missingno()
      - Showing values by bar graph
      
   - Heatmap()
      - Showing values by graph",18a1753d,0.6818181818181818
31513,7b59fc0ff0d10b,251d33fa,We see that the NO2 measured is almost inversely proportional to the cloud fraction when we zoom into a small area of ~5 km x ~10 km.,c192b94c,0.6818181818181818
31515,930cd79ca51204,7d88bad9,Wouldn't it be nice if the size of the dots corresponds to the population?,5506779a,0.6818181818181818
31516,f06fd8f5916431,b3d5bf01,"I leverage the RLE encoder provided by @xhlulu here:
[https://www.kaggle.com/xhlulu/efficient-mask2rle](https://www.kaggle.com/xhlulu/efficient-mask2rle)",854fa433,0.6818181818181818
31520,6d66ced0028dea,6a8812bb,# 2. Очистка данных,f50aae52,0.6818181818181818
31527,0cb9adc158b705,bdd767f9,lets train it some more,3abf056e,0.6818181818181818
31529,9c19668d6b7295,27fb396e,## Feature Extraction,35be7001,0.6818181818181818
31531,bb3d1b4b9f1248,379a115f,"**Observations:**
1. Expected to see the 2020 consumption was higher for each state, but looking at the above the plot with color ""RdPu"" some states has less average PerCapita consumption as below.

       a. More consumption of Beer in 2019 than 2020 in many of the 16 states
       b. But these states have less consumption in other beverages spirits and wine

 2. Overall the consumption is high,** as expected**",bf7de324,0.6818181818181818
31533,5083d7a61f2426,f861d99b,Ploting the train and the predicted test data ,541a0fec,0.6818181818181818
31534,c2be02442e8cfd,6daabdcd,"# Observations: 
    
   1. people less than 5 axillary nodes will survive more, as per the CDF we can see the survival chance is 80%",2d364acc,0.6818181818181818
31537,f166950fa915f8,ad3dbba8,### Train,a7f6ca5e,0.6818181818181818
31541,e323e594ef918f,f000e0c4,Now we use einsum notation to calculate the dot product between our activation grid and the channel weights. The output should be a grid showing activation strength for each of classes that we predict. ,6e829ab6,0.6818181818181818
31544,c84925c8171900,8892eda8,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.4.3  Platform WordCloud
            </span>   
        </font>    
</h4>",e21ff7ec,0.6822429906542056
31547,513ce405d7f6a3,ad5c498a,Model traing Acc,8461e086,0.6823529411764706
31548,63b44c85e32c1f,5a3ec3e0,"To define a tuple, A variable is assigned to paranthesis ( ) or tuple( ).",fb9b9562,0.6824324324324325
31553,06ecf7a304c309,27427676,- 노이즈 적용된 검증 데이터,714de627,0.6825396825396826
31557,2ada0305b68956,efe6c642,### 116. Palette = 'gnuplot2_r',133e26f4,0.6828571428571428
31559,0e09587faffa8f,f49a8898,**SUBN** vehicle body type accounted for the maximum number of violations ,0d563d61,0.6829268292682927
31562,e19e307b3fd188,81904f0c,#### Categorical columns handler,2173955b,0.6829268292682927
31564,47b2c9be5e31cb,563110d1,Scatter and density plots:,7d4afe56,0.6829268292682927
31567,9c26c5dcd46a25,c722963d,"#### <font color=""#114b98"" id=""section_2_3"">2.3. Régression linéaire avec catégorie produits</font>

Ajoutons à présent dans le modèle les 2 principales variables de catégorisation de produits : `pnns_groups_1`, `pnns_groups_2`. Ces variables catégorielles vont devoir être encodées pour être incorporées au modèle. Pour cette modélisattion, nous allons réaliser un OneHotEncoder.",1bbbb677,0.6829268292682927
31570,0b01138ad120fc,2f4c9251,"**LSTMs are an special kind of RNNs.  
LSTMs can memorize past terms better than RNN**",0b4b72e6,0.6829268292682927
31572,786475feda0190,a76ea961,"## Audio Analysis.

1. **FFT**: A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis    converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. 

5. **MFCCS** : Mel Frequency cepstral coefficients",e4663d97,0.6829268292682927
31573,8d70dcae7f40a3,110fb510,"### **==> Kết luận:** *Chỉ số false càng nhỏ càng tốt, chỉ số true càng lớn càng tốt.*",472c71ce,0.6829268292682927
31577,62487bcd70b199,ef1e8e0b,## <a id='8.1.1.'>8.1.1. KNeighborsClassifier</a>,f6ae50af,0.6833333333333333
31579,712198370d5521,01aa9e4d,To examine the clusters formed let's have a look at the 3-D distribution of the clusters. ,5882e04c,0.6833333333333333
31584,cf4d1c1ad1476c,d9f08214,# Splitting Train and Valid Dataset,768c1a59,0.6833333333333333
31586,b547f0f38f7744,2bb895a3,Print the last layer:,b6ba66b3,0.6833333333333333
31587,c18267b203f28a,5ff7635b,"# Train the model
As our model is training you'll see a printout for each epoch, and can also monitor TPU usage by clicking on the TPU metrics in the toolbar at the top right of your notebook.",09ca8efb,0.6833333333333333
31603,826ccb616bd2a8,4fd2a1ec,"# Rule Generation
Effective rules for recommending a drug to a patient",4d7df2ec,0.6842105263157895
31606,038abade89e59f,a5ebbaec,# Inference,cb32a3fe,0.6842105263157895
31608,169177b6e9edea,a9023da5,<h3><b>RandomForest,ca42152f,0.6842105263157895
31614,a1dcd92986bc84,defbaa1a,"## Search for images using natural language queries

We can then retrieve images corresponding to natural language queries via
the following steps:

1. Generate embeddings for the images by feeding them into the `vision_encoder`.
2. Feed the natural language query to the `text_encoder` to generate a query embedding.
3. Compute the similarity between the query embedding and the image embeddings
in the index to retrieve the indices of the top matches.
4. Look up the paths of the top matching images to display them.

Note that, after training the `dual encoder`, only the fine-tuned `vision_encoder`
and `text_encoder` models will be used, while the `dual_encoder` model will be discarded.",730acaaa,0.6842105263157895
31616,ba4b3bd184acbb,d99c8bc2,***,0f5de724,0.6842105263157895
31621,f35ee6e9fab592,9917b735,The most prolific video game reviewers at IGN (top 5 reviewers with the most reviews),b15f7073,0.6842105263157895
31624,bef2347846e476,d145ef57,"An in the following graph again we've used line plot, but this time we have two column type of float.They are called Sentiment Polarity and Sentiment Subjectivity.We observe that Sentiment Subjectivity values are intense at the above of the zero and we observe that the Sentiment Polarity values are under the zero.",cb93bf51,0.6842105263157895
31630,f14f6708035916,291b4d28,"## Days of Week Spending
What days of the week do I spend the most money? Can this be predicted and avoid it?",ca22d04b,0.6842105263157895
31632,0d8df2c2983694,225857ec,### Declare the dependent and independent variables,9bf7fa4e,0.6842105263157895
31634,29437539745aa5,ae85fb59,"<h3 style=""text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;"">4.1 LOAD THE MODEL</h3>

---

* Load the models
* Define the parameters
* Make subset dataframes
* Initialize",c17b490a,0.6842105263157895
31637,caa0ce2715bf34,cf61de84,"### Observations:
From the above plot, we can see, there is a sharp decrease in the inertia from cluster = 1 till cluster= 4, Hence we can either choose 3 or 4 clusters. But we will verify the silhouette_score for clusters upto 10.

## Predicting KMean and silhouette_score with n clusters",78a5dc51,0.6842105263157895
31639,f91f58d488d4af,ef47e6a4,"Now, let's do it for our MNIST_SAMPLE dataset.",5df1bbf3,0.6842105263157895
31640,1f3295ed0d4e4a,0004c6df,# Tensor Slices,b0aeb172,0.6842105263157895
31641,840534f2908a9c,bc235200,*Most of the long trips dropoffs and pickups are in lower Manhattan. There are a lot of dropoffs in Brooklyn*,8081c3cc,0.6842105263157895
31645,e03eb63c1f725d,f2c381f2,"<a id=""11""></a>
<font size=""+2"" color=""blue""><b>Hyperparameterization (with MultinomialNB) for TfidfVectorizer</b> </font><br>",e204b7e3,0.6842105263157895
31649,d07915a6e6992e,3e301219,"Most of these tickets belong to category 1, 2, 3, S, P, C. Based on value counts and average survival, we can put all other ticket categories into a new category '4'.
",2b912140,0.6846153846153846
31652,09751c520b0616,d1be969d,<b>we analyse the dataset through some plots,a4d0c7e9,0.6846153846153846
31653,b01ee6cb674fa3,6044cd48,"# Boeing

A USA private company",a8ffd35e,0.6847826086956522
31657,fdc9f4863744b1,9e80de48,"There is no correlation between sale price and seaons. For example; Sale prices go up and down during the summer for all boroughs. There is definately a correlation between boroughs and sale price though. Keep in mind, correlation does not mean causation. Cause of something will require more investigation. In this case, I can't use seasons as a predictor variable for my model.",b4529365,0.684931506849315
31658,738bfced935b69,9a031e93,drop duplicated rows and reset index of data,2d3c592d,0.684931506849315
31671,04ff2af52f147b,49caafc2,"Given this fact, we will use it to group together families aboard the Titanic.  Note that the value for *DEFAULT_SURVIVAL* of 0.5 is just a placeholder and will be replaced during encoding.  The value is just to differentiate between families with known survivors and known deaths.  The idea here is that passengers in families with known survivors are more likely to survive.  Contrarily, passengers in families with known deaths are less likely to survive.",d5f37be9,0.6853932584269663
31672,312135b445bd23,a469833a,"# **Semantic Search for Relevant Sentences**
After finding seed sentences, we wanted to expand our evidences by finding sentences with similar semantic meaning in the whole corpus (that haven't came up in the search engine from previous phase) in order to collect more information and evidence to support the research question.

There are many methods and techniques for sentence embedding in the NLP litretature and from our vast experience in the field we chose to use to implement 2 and to ty both
1. The techniques from the paper [""A Simple but Tough-to-Beat Baseline for Sentence Embeddings""](https://openreview.net/forum?id=SyK00v5xx) (Sanjeev Arora, Yingyu Liang, Tengyu Ma). In their work, they use a pretrained word embedding model on unsupervised large corpus and in order to create embedding for the whole sentence, they use a smooth weighted average on the word embeddings of the the words in the sentence, and remove the 1st principal component from the vector after performing a dimension reduction technique (e.g. SVD or PCA). The latter improves to reduce noise from the sentence embeddings. We found that using phrases (bi-grams and tri-grams) in the semantic search engine, using this technique, performed worser than with them so we decided to not using them for the semantic search engine part.
2. Using fine-tuned BERT on Stanford Natural Language Inference (SNLI) task that predicts if two sentences are semantically related or not. For the implementation of BERT encoder, we used UKP's sentence-transformers library. We chose to use the model called `bert-base-nli-stsb-mean-tokens`.

We developed a method that gets list of sentences and retrieves similar semantic sentences to them. Because the input is not just one sentence, we had to aggerage them in different manngers. We did research one some aggregation techniques:
1. Union
2. Mean
3. 1st Principal Component (pc_1)
4. 2nd Principal Component (pc_2)

The method that worked best was fine-tuned BERT (#2) with Union aggregation.

We iterate through all the sentences in our filtered corpus and encoded the sentences using the techniques mentioned above. When giving a new sentence, we encoded it and compared it to all existing pre-encoded sentences we have in our index (using Cosine Similarity function between the vectors) in order to get the most similar sentences for each input sentence. We used `nmslib` for storing the pre-encoded sentences and to perform the similarity measures. All code is available in our repo in [corpus_indx](https://github.com/Hazoom/covid19/tree/master/src/corpus_index) and [encoders](https://github.com/Hazoom/covid19/tree/master/src/encoders). 

Credit to [Tal Almagor](https://github.com/talmago) for helping to develop this module.",8ced381f,0.6853932584269663
31674,e67925694c07d3,059a856e,Number of loans per customer,83af4c4a,0.6853932584269663
31681,1823d096209b96,54e6cb9b,# Train an Long Short-term Memory (LSTM) time series model,cb2a79e0,0.6857142857142857
31686,171494b45650a2,6aa751f2,### ExtraCheese@Company vs Price,9c8cc578,0.6857142857142857
31688,3cea0f929a2035,7819d04d,Now let's check the relationship of all numerical features in the file.,04cfbade,0.6857142857142857
31689,bbaa07ad21cf4e,706611ed,### Random forest,3ab6b254,0.6857142857142857
31691,5d5c9480b5a0a3,843a86b4,"We can somewhat make out that more Class 3 passengers died as opposed to Class 1 and Class 2. Infact, Class 1 has the highest 
survival rate.",04d82e2d,0.6857142857142857
31692,04bac111ffbe9c,1b12361e,"##### LOOKS GOOD.
##### NOW WE START BUILDING THE MODELS
We'll focus on these models :
1. Logistic Regression (as it is classification based)
2. KNN
3. Random Forest
4. Adaboost

##### LOGISTIC REGRESSION",82576b17,0.6857142857142857
31693,38b79494ac749e,2d0986b5,### Summary,39162a40,0.6857142857142857
31699,9c044fa3072552,eaba07ef,> The Distribution on Mondays is very different from that on Sundays,1362842e,0.6857142857142857
31700,7454fdc444df16,1656e588,"We first apply PCA on the RGB color dimension. As mentioned before, it is possible to skip this step entirely and just convert the RGB image to a grayscale, however applying PCA ensures that we retain maximum amount of information per image. After applying PCA to the color dimensions we would have reduced our total number of dimensions per flattened image from (1,7500) to (1,2500). This will help us reduce memory consumption with later operations.",a7818ef5,0.6857142857142857
31703,2b36742b49c7bc,318e84be,Одоо текст датагаа tokenize хийе. Энд `CustomDataset` class нь энэхүү үйлдийг wrap хийсэн болно.,c8f8a96d,0.6857142857142857
31704,72e098fe5b2a04,0cfe82a6,# inference roberta large 462,5399eebd,0.6857142857142857
31706,2730840089c8eb,28f3813d,"# Dictionaries

Dictionaries are a built-in Python data structure for mapping keys to values.",34d27dac,0.6857142857142857
31711,c09fac3c943d51,51acfbc6,# Preparing validation,678d076d,0.686046511627907
31712,d96642860ab3dd,d5283737,### 2.5 Correcting Data Types,98419d48,0.686046511627907
31717,917957c6c4065f,6634ad56,"좋아요 대비 싫어요를 기준으로 하는 상위 5개의 항목입니다.  
대부분 당시 사회 이슈와 관련되어 있고, 논란이 있었던 소재들인 것으로 보입니다.
",55b8ed68,0.6862745098039216
31720,fa02c409161192,d05b43e6,"In this section, we will see how the number of hidden nodes effects the performance of the NN. The NN will be trained of the same data set for each iteration and we will vary the number of hidden layers and see how this effects the performance. The NN will ne train on the entire MNIST data set.

We will plot the results, I am not sure what the outcome will be. I predict that for a small number of hidden nodes the NN will perform badly and will increase until the time complexity is just to large, there may be a decrease due to **over fitting**. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that more hidden nodes takes longer to train, I'm guessing here.",e97077f7,0.6862745098039216
31722,842547b2def18c,2cef1abb,We can not create FareBand.,b8efde6d,0.6862745098039216
31723,4fa553c2b837d4,9aaac86c,"Since string values cant be processed by any model, we convert the categorical data to numerical data using One Hot Encoding.

One Hot Encoding creates new columns and store the binary data on that particular columns to represent the presence of original data.

For Example : A gender column has two categorical value 'Male' and 'Female'. So using One Hot Encoding it create two columns named 'Male' and 'Female' and store 0 and 1 based on the presence of original data.",c65a23e9,0.6862745098039216
31724,629f2918807a9b,8d683739,"### 2020 was the good year for our client, as they sold 23,784 copies despite pandemic. These all are successfull orders (Returned or cancelled are not included)

### In 2021, till last reports they have sold around 6272 copies within less than a month, So lets check it out succes rate of january 2020 and january 2021",be56dc84,0.6862745098039216
31725,71b75664517244,1b2fd163,"#### Comparison With Team

His team is Manchester United, we are going to compare their performance",fc905af5,0.6862745098039216
31726,64169805aacf17,e4210733,# Create some videos from our paint,1f12ded0,0.6862745098039216
31727,52cfd66e9ec908,13ea3046,"Albumentations basically uses the keypoints provided in order to facilitate augmentation. (using cutout is dangerous because of loss of information possibilities btw, this is just for an example).",c74adcdf,0.6862745098039216
31733,21413205980558,14d4d362,"* # students and retirees are more likely to have deposits and succeed in marketing;
* # 学生及退休人员更加可能有定期存款，同时在营销方面取得成功；
* # Blue collar workers, entrepreneurs, service providers and technicians are not easy to sell successfully
* # 蓝领、企业家、服务者、技术员不容易推销成功",84197de0,0.6865671641791045
31737,a4aa36df07fd53,434794a4,"### Should You Aim at the C-level?
CFO, CEO, dan C C lainnya seberapa besar gaji mereka di compare dengan pekerja bawahan seperti saya?",d2f42b6d,0.6865671641791045
31741,91eaec994e0c6f,4773a1d0,# 3. Forecast,376aef10,0.6866666666666666
31743,4daf6153275cbf,064d8df1,#### 6. Hypotesis Testing,51db1961,0.6867469879518072
31749,9d561aa4a298f3,ca5d0924,# Data exploration,f56bdd1c,0.6875
31752,7325ce9461a814,e2ac41b8,# 1.2.Dealing with Outliers,c73a7825,0.6875
31754,8c7e00ca3dc5a7,9d08b832,Based on above data there're some skewed data present in the data. So it will be log transformed,c83346e4,0.6875
31755,117fc0956643d0,a8776467,### Step 4.1. Print the Top Relevant Articles with an Highlighted Excerpt,68cef9fd,0.6875
31756,3b5903412fe741,5bb39782,This operation produced a `Series` of `True`/`False` booleans based on the `country` of each record.  This result can then be used inside of `loc` to select the relevant data:,ad231969,0.6875
31759,2ca509e51a6e4b,1d24aea3,"This gives us a mapping of the values in the country feature, the index of the dataframe, to our encoded vectors. Next, we need to replace the values in our data with these vectors. We can do this using the `.reindex` method. This method takes the values in the country column and creates a new dataframe from from `svd_encoding` using those values as the index. Then we need to set the index back to the original index. Note that I learned the encodings from the training data, but I'm applying them to the whole dataset.",e63b0ba6,0.6875
31762,2a724fb7835cdc,f725990c,"**Prepare the sequences for LSTM**

- Tokenizer: word to index
- GloVe Embedding: word to vector",c38ac61d,0.6875
31767,e8e4447e99a463,aad027f5,# Preparing submission,7256de70,0.6875
31771,9ec2fb131cf677,7bc9efc2,"<table style=""border-collapse: collapse; width: 454.4500pt; margin-left: 4.6500pt; border: none;"">
<tbody>
<tr style=""height: 21.0000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; background: #ffc000; border: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">Main Speaker</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: 1.0000pt solid #000000; border-bottom: 1.0000pt solid #000000; background: #ffc000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">Speaker Occupation</span></strong></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: 1.0000pt solid #000000; border-bottom: 1.0000pt solid #000000; background: #ffc000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 16.0000pt;"">Comments</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Richard Dawkins</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Evolutionary biologist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">7,044</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Ken Robinson</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Author/educator</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">6,894</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Sam Harris</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Neuroscientist, philosopher</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">3,846</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Hans Rosling</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Global health expert; data visionary</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">3,085</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Jill Bolte Taylor</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Neuroanatomist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,877</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Lesley Hazleton</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Writer, psychologist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,815</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">David Chalmers</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Philosopher</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,673</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Bren&Atilde;&copy; Brown</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Vulnerability researcher</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,634</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Janet Echelman</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Artist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,492</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Amy Cuddy</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Social psychologist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,290</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Michael Specter</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Writer</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,272</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Simon Sinek</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Leadership expert</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,224</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Bill Gates</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Philanthropist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,223</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Al Gore</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Climate advocate</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,137</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Dan Dennett</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Philosopher, cognitive scientist</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,012</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 133.5000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: 1.0000pt solid #000000; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Jane McGonigal</span></strong></p>
</td>
<td style=""width: 197.9500pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">Game Designer</span></p>
</td>
<td style=""width: 123.0000pt; padding: 0.7500pt 0.7500pt 0.7500pt 0.7500pt; border-left: none; border-right: 1.0000pt solid #000000; border-top: none; border-bottom: 1.0000pt solid #000000;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">2,010</span></strong></p>
</td>
</tr>
</tbody>
</table>",211ea6bd,0.6875
31775,0932046e1f485d,af40638c,## <a id=review_dataset>Reviews Dataset</a>,218cc7a3,0.6875
31783,ffc9490c4f6c38,0d3430db,"### Dimension reduction by data range

First, I drop columns whose data range is upper 15.
(The number 15 is from looking plots shown earlier)",ae7bbbb3,0.6875
31785,5e02999ca74e7e,83769cb1,### **Visualization for Actual and Predicted Sales in Training Data**,b69da28e,0.6875
31787,69130a37583a06,5c0430a9,"### addr2 distribution :
",65a4de1c,0.6875
31788,5ffe6aa38958a1,987e7f13,"# 4. Model 

## 4.1 Model Selection
### 4.1.1 Logistic Regression",11f5412e,0.6875
31790,49f2274c1dd516,321aff3e,"# Github
These data sets provide case counts for several countries, and are updated and maintained by citizens from government sources.",06b0ffee,0.6875
31792,f5ca8fb6a465f3,e40bed6b,# Process Submission,56c45a1b,0.6875
31802,13c7672da1b571,8c4ab460,Find best XGBRegressor parameters.,002d3ec0,0.6875
31804,3cc097a5859dc1,84271c47,"# **Correlation Heatmap**
",14380d73,0.6875
31806,957e035ba5b9d5,637e0add,## Visualize Loss/Accuracy,778ab3d3,0.6879432624113475
31821,bcd7e398c4d0ec,af7c4949,Reduce the dimension of feature by PCA.,77a143f6,0.6885245901639344
31824,601e18072783b4,fa0a4e12,# Analysis and visualization,36b2b1fa,0.6885245901639344
31827,2ada0305b68956,8a3d8744,### 117. Palette = 'gray',133e26f4,0.6885714285714286
31830,d77e6d61ad2e8b,89291c41,# Finalize Model for Deployment,03fd0e96,0.6888888888888889
31831,d58491f2896fc1,6f0a861f,**generations belirtilen nesil sayısı için yukarıdaki tüm  fonksiyonları çalıştırmaktadır**,514bfdff,0.6888888888888889
31835,4fd4b6a80d40e3,8d7cf4d3,"## Signaling from the 1 Layer to the 2 Layer

![image.png](attachment:image.png)",f6913cc3,0.6888888888888889
31837,c8bf959b9608cf,f5745b38,"### Calculate Style Loss and add to Content Loss
Here, we will calculate style loss for a particular set of layers and add the style loss calculated at each of these layers to the content loss to find the total loss. 
![lLoss.PNG](attachment:lLoss.PNG)",155e3672,0.6888888888888889
31839,e0a041e5e2372f,2fc99a8b,"## Water Quality Index Calculation

I will be using ""Weighted Arithmetic Water Quality Index Method"" to calculate WQI of each water sample. The formula to calculate WQI is - 

**WQI = ∑ qi × wi / ∑ wi**

Here wi - Unit weight of ith parameter

     qi - Quality estimate scale of each parameter, it is calculated with the formula - 

**qi = 100 × ( Vi − VIdeal / Si − VIdeal )**

Here Vi - Measured value of ith parameter

     Videal - Ideal value of ith parameter in pure water
     
     Si - Standard value recommended for ith parameter

wi is calculated by the formula - 

**wi = K /  Si**

Here K is proportionality constant which is - 

 **K = 1 /  ∑ Si**",7c4357b2,0.6888888888888889
31844,3597174a998d4d,b64a871b,It can be found that Non Refund has the highest ratio of cancelation.,276892ed,0.6888888888888889
31845,5be39e4e35cec7,72911dbf,"<a id = ""8""></a><br>
# Missing Value
* Find Missing Value
* Fill Missing Value",14d617c9,0.6888888888888889
31849,e25c0f830df3f4,acfbd0fc,# Converting the polarity values from continuous to categorical,fdcf7189,0.6888888888888889
31850,c4386b8a01d66e,cd36c7b7,# Potability,dc732bf5,0.6890756302521008
31851,4ae6a182abac64,bcddad82,### 2.5 Creating dummy variables,418676c5,0.6890756302521008
31854,62037c5832129c,8e045913,"Next, we printed the confusion matrix like so:",61474350,0.6891891891891891
31860,9ceb7278784462,0bcad534,## Model Tuning,3768a567,0.6895161290322581
31862,858da4bb312f67,ac38623a,## Other Transforms,9cca4391,0.6896551724137931
31866,656185a18260be,b65b4eeb,"# Further Postprocessing

MURIL doesn't require additional postprocessing because its tokenizer is good at splitting punctuation from subwords. Let's still keep it here as it might be helpful if we switch the backbone model e.g. to XLM-Roberta or Rembert. ",0318cab5,0.6896551724137931
31868,6903d3f38c6a66,8d0f75f6,"### Diverging Colormap

This colormap is usually used in visualizations where the median is obvious.

It is usually visualized on a white background, white in the center, and darker in color toward both ends. In other words, the lighter the value, the closer to the center, the darker, the closer to the end.

Currently it is a continuous colormap, but you can also use discrete colorpalette depending on the interval.Matplotlib loads the library's palette with that element in the cmap parameter. You can, of course, make it custom.",6067ce5e,0.6896551724137931
31871,ee9ddc756b2d4a,1f59dc82,"> Per la parte di model selection ho fatto alcuni tentativi, servendomi di gridsearch, il quale metodo però riporta risultati ottimali per la metrica accuracy, dunque li ho poi modificati

> Ho regolarizzato meno nei due modelli e di piu nel modello finale di logistic regression",e367eab3,0.6896551724137931
31876,4b7039cb44a54c,fbcdf0bf,# CNN Prediction,24e806af,0.6896551724137931
31879,84127ade6fde87,c26779c9,"How can we compress our encoding down to a more manageable size and put a cap on the size growth? Well, instead of vectors of many zeros and a single one, we can use vectors of floating-point numbers. A vector of, say, 100 floating-point numbers can indeed represent a large number of words. The trick is to find an effective way to map individual words into this 100-dimensional space in a way that facilitates downstream learning. This is called an embedding.",f55d05b6,0.6896551724137931
31880,00001756c60be8,1496beaf,Корреляция,945aea18,0.6896551724137931
31883,cd10f3afd970b3,2b2cdb9e,Distribution graphs (histogram/bar graph) of sampled columns:,2db3c8e4,0.6896551724137931
31884,401338428b2d1c,5442b463,## Predicting the Test set results,e4b768be,0.6896551724137931
31886,fb5c6021d127ef,9f033e56,"## 6) Exercise: Looking at predictions

A good way to figure out where your model is going wrong is to look closer at a small set of predictions. Use your model to predict the number of rides for the 22nd & Pearl station in 2018. Compare the mean values of predicted vs actual riders.",dd05cbd3,0.6896551724137931
31889,5ea840754577e3,9f01f89d,"Similar with Parch, SibSp has almost the same distribution. With SibSp = 0, the percent of people who has survived is less than the people who did not. While for passengers with SipSp = 1, the people who survived are lesser than that of not survived.",9cf9b73f,0.6896551724137931
31892,a4f0a3e1316ff9,c3d986af,# Look at Trends,53bf0160,0.6896551724137931
31893,18a96bb5711ed9,f578d38e,"# Migration flow chart <br>

From the migration flow chart, we can see places that people from a certain region tended to migrate, but they had unfortunate incidents while going their destiniation. <br>
",e79768db,0.6896551724137931
31897,1cd8be6e679620,2571187b,## Method for Graph Representation of RNA structure,3ce15a43,0.6896551724137931
31898,a1ba5ffd30dbde,06279694,<h2 align='center'> Random Forest </h2>,48e57546,0.6896551724137931
31899,14defffcd250f3,e00e936e,Check if there are any nan or infinite values,3a683b94,0.6896551724137931
31900,f3c6048d1058e3,06001a6f,### 3) XGBoost,1d9056b0,0.6896551724137931
31902,1dd9c6aa74d289,51ec2f18,# [Q3: Where are the popular cimbing places?],5ef9a1be,0.6896551724137931
31903,87e96e14f8f5ce,4ca2f636,# Model,f9f7a3a2,0.6896551724137931
31904,fdc3afd309b850,601f991e," Arniqueiras population was 45091 in 2015 and that was part of Aguas Claras so we have to subtract this value from Aguas claras
",966bde38,0.6898148148148148
31908,5ce12be6e7b90e,50512f5b,"Lists are dynamic and mutable - you can append, remove and insert into them. This is done using _list methods_.

We can access and change list elements:",c0ab62dd,0.6900584795321637
31909,30fdc4a6e3c1db,eb285ef2,Saturday sees the highest overall sales probably because of the first day of weekend and people rushing to buy groceries followed by Sunday also being a weekend,6111ddee,0.6900584795321637
31910,631cd434fc3aa2,fb462133,"Ok, it should be done by now. Let's check if we have still some missing value.",2b74febb,0.6901408450704225
31913,3d77c1560bd16e,25d0251d,"<a id='7'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Vaccine types and providers</div>",87c141ca,0.6901408450704225
31917,c9b4e282e4e2c1,e5e4af5a,B-Which weather seems to be related with more injuries?,f44d339f,0.6902654867256637
31918,ac1abfe1dfe815,20179a88,### Split to training and testing,6529dbcb,0.6902654867256637
31922,1084376bc4897c,a9e7af87,## 4.3 Logistic regression ,1b598487,0.6904761904761905
31924,2d40f383473fa4,55f4f580,"The model is built using the great [H2O AutoML](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html), read about, it is a nice addition to your skills.<br>
It will train so many Machine Learning algorithms to search for a best fit and report it, even using ensemble in the models trained. There are so many parameters to configure and you need to be cautios and how many models you will train, because it will get so much time to train and get a lot of resources! Known your data and decide if you'll use H2O AutoML or any other AutoML module.",1da1eff0,0.6904761904761905
31928,b4ecd6e4277e3c,b6e505a1,"### Cyclic CLR
Code taken from https://www.kaggle.com/dannykliu/lstm-with-attention-clr-in-pytorch",94d79d5f,0.6904761904761905
31930,916ccf243827f1,44d774df,## 11. Update Parameters,5147f4d2,0.6904761904761905
31932,1fac5edd4063ba,9a519f7a,"#Surprising New Study: Brazil Now A Global Leader In Gender Equality In Science, by Shannon Sims

Most encouraging of all, Brazil's strong showing in this report signals a significant improvement in Brazilian women's participation in the sciences. During the period of 1996 to 2000, only 38% of the scholarly scientific articles published by Brazilians were authored by women. That is, just since 2000, Brazilian women have reached near-parity with men when it comes to scientific authorship.

Brazil also fares well in other indicators featured in the study. The proportion of female inventors in Brazil rose from 11% to 17% between 1996 and 2015. Today, the proportion of female inventors in Brazil is higher than in the United States (14%), United Kingdom (12%) or the European Union (12%).https://www.forbes.com/sites/shannonsims/2017/03/08/surprising-new-study-brazil-now-a-global-leader-in-gender-equality-in-science/#68b4f3c76f44",04bc01e0,0.6904761904761905
31934,27778055896e17,4f9ccdd3,# Ada Boosting,1dbe0165,0.6904761904761905
31936,87e94f864d74be,d9fc5664,> There's less movies for the kids while more for teens and adults.,294bfe9f,0.6904761904761905
31941,8ec771f5600a61,bfc83e83,# BY THIS METHOD WE CAN CHANGE THE INPUT OF TRAIN AND TEST FOR IMPROVING OUR RESULTS,48364c1f,0.6907216494845361
31944,225b4fe5d3894a,392881c2,This is underfitting model,4b4197b3,0.6907216494845361
31950,9cec5ddf8b6f49,5ce6a92a,### * To get the dictionary of parameter name and parameter values of the best trial,d39fc8e7,0.6911764705882353
31953,eb0ecd6bebeb15,63a6fd6e,sns.lmplot() görselleştirmesini petal.length ve petal.width değişkenleriyle implemente edelim. Petal length ile petal width arasında ne tür bir ilişki var ve bu ilişki güçlü müdür? sorusunu yanıtlayalım.,d7b93a60,0.6911764705882353
31957,7f74a04ae75792,5acc74f5,"### Explore the data by visualizing other features (such as customer annual income, distribution of gender)
- Start by asking questions that can add value to the business (example: how did my customers behaving lately?)
- Then continue by identifying which data are needed for the questions to be answered",d01e91da,0.6911764705882353
31958,99821bc6a45be6,51f155ef,"The specific architecture used here is inspired by the research paper [**Deep convolutional neural network based medical image classification for disease diagnosis**](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0276-2#Sec6), which is concerned with detecting pneumonia in chest Xrays. This model seemed like a good starting point as it tackles a problem similar to ours.

The base for this model is VGG16, which is pretrained on the ImageNet dataset.. To make the pretrained model adapt to our dataset, the last convolutional layer's weights are unlocked to be updated during model fitting. 

The VGG16 architecture is shown below:
![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)



The paper proposes the general structure of the network and discusses possible augmentations to make to the images to better generalize to data outside of the training set. 

The augmentations that worked the best according to the above paper were:

* Random Rotation within the range of .05
* Random Shearing within the range of .05
* Random Zooming within the range of .05
* Random Vertical Flipping
* Random Horizontal Flipping

These augmentations have been incorporated into the data generator defined above.",b9d59346,0.6911764705882353
31960,726833f92fb87a,881a9490,We can encode this feature by ordinal encoding,7dc5e1b6,0.6912751677852349
31962,4883314a96dc34,18f606eb,## Iteration (2),50d36836,0.691358024691358
31963,faa8e6c8ab9246,87cefa87,"There are outliers in Age, SibSp, Parch and Fare variables. I am using clip() funtion to remove the outliers. Parch variable is removed because more than 75% of the values are 0.",2bea1419,0.691358024691358
31970,37b09262279764,38321cec,#### ExtraTreeClassifier,37c4c417,0.6916666666666667
31972,ba4b3bd184acbb,807d7594,"# Manipulation Methods

Another important feature of Pandas DataFrames are the manipulation methods.

### Concatinate

The `concat` method is for combining DataFrame either by column or by row.

Multiple DataFrames can be concatinated at a time and there are many join type and indexing options.",0f5de724,0.6917293233082706
31974,fdc9f4863744b1,83c1b68c,"Lets see if there is a linear correlation between ""Land Square Feet"" and ""Sales Price"" . Simple Linear Regression is a very straight forward data model; it can help us understand the relationship between Land Square Feet and Sales Price.",b4529365,0.6917808219178082
31975,b01ee6cb674fa3,d5ad7c67,"# Institute of Space and Astronautical Science - ISAS

(宇宙科学研究所, Uchū kagaku kenkyūjo) (ISAS) is a Japanese national research organization of astrophysics using rockets, astronomical satellites and interplanetary probes which played a major role in Japan's space development. Since 2003, it is a division of Japan Aerospace Exploration Agency (JAXA).[1]",a8ffd35e,0.6920289855072463
31977,80ad12f326ab70,f53bb7ec,Perk-12 Sector of education is where the products are most used with a percentage of 48.3,da404a16,0.6923076923076923
31980,34fff8ce731b03,39c59e97,"Para **avaliar** a acurácia do modelo, o resultado da predição *y_pred* é comparado com o resultado esperado *y_valid* para gerar as métricas ROC, Acurácia e F1.",6f9e5b2e,0.6923076923076923
31981,eda49464dd6d1b,017efcd5,"### Above we can see the list of columns present in training but not in test.  They are in a handful of policy sales channels and 2 extremely high annual premium bins.  They will be deleted.  It's also important to note that ""Response"" is included, but we definitely don't want to delete it because we will need those numbers.",8421f81f,0.6923076923076923
31984,33398ae40da63d,2b3d0f13,"We want to select several reputable publications for our dataset as well as several other 'less-reputable' publications in order to have a more balanced representation of news published by a real news publication company. Those we included are listed below.

We used a media bias/fact check website (source: https://mediabiasfactcheck.com/) to assess each news publication company. The results are below (each company is rated on their factual reporting).

Factual reporting:
- Very High: reuters, npr
- High: new york times, atlantic, guardian, washington post, vox
- Mixed: fox news, cnn, national review
",7bd02b88,0.6923076923076923
31986,3536195ad632ee,9dc8ab1b,"And here comes the training.
Before the actual training there is a further split of the MNIST training dataset into the actual training set and a validation set. This split is randomly done for each network. So very probably we use up all images for training.",3c26aafc,0.6923076923076923
31990,a915263bc207da,91fd7279,"### An intuition of the filtering strength
##### Lets check the movies similar to Star Wars (1977)",b17ebcda,0.6923076923076923
31991,3e325daf577158,2ab36ce3,## Training,c873dfec,0.6923076923076923
31995,aa46e9376825a5,11a847da,### Clean the dataset and replace the fatal flag NaN with “No”,57792d96,0.6923076923076923
31996,3f451680b1857b,22d99b93,"# [YOLOv5](https://github.com/ultralytics/yolov5)
![](https://user-images.githubusercontent.com/26833433/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg)
![](https://user-images.githubusercontent.com/26833433/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png)",56c45a1b,0.6923076923076923
32000,e424c111c44669,a88dda6a,**Pretreat**,d9fccfba,0.6923076923076923
32009,aae204e78a48d1,e5ed5f87,"# Hypothesis 4: customers who attrite will use less of their credit line (i.e. have lower utilisation)
Given what we saw in the transactional data, our next hypothesis is that customers who attrite have lower utilisation.  This means they use less of their credit line with us.   The first thing we need to check is whether we are giving these customers a similar credit line.",53ab6133,0.6923076923076923
32011,2facf256353117,f300d587,"# issue happen when working with ISBI Dataset
## some files header contain corrupted data 
* pixdim[0] (qfac) should be 1 (default) or -1; setting qfac to 1
* when read with nifti it updated automatically
* when read it with simpleitk can find actual value of the pixdim values",18f579be,0.6923076923076923
32013,d78988cb5a1b02,62613a78,**Now we are going to calculate the mean of all predictions of the  x test data to make a final prediction data**,233f3a92,0.6923076923076923
32015,866b157128a09c,38e18dab,Few Utility Functions,0909f459,0.6923076923076923
32016,5af9bf52e5f17c,1d96f5de,"Some of the countries are further sub-divided by province/state: US, Canada, China, Netherlands, Australia, Denmark, UK, France, Cruise Ship. The x-axis for these graphs is wrong.

Also Diamond/Grand Princess (listed as Province/State) has two parts.

Try to subdivide by province/state.",f98ab90d,0.6923076923076923
32017,cf08b03b002c13,a50fa7c7,Since residuals are not random and tend to cluster there is no correlation between number of comments and score of NSFW post,104d416f,0.6923076923076923
32019,09751c520b0616,49f4c29c,- Skewness of SalePrice,a4d0c7e9,0.6923076923076923
32020,f2f2db16a2f86c,6b1774f4,### **Fitting Random Forest Regression Model**,ffc6a115,0.6923076923076923
32022,d4c5aaa4b36810,d967832b,### Lasso Regression,65441f28,0.6923076923076923
32031,80d4e83ed91035,1bdfcc7e,# Test,e8e1db41,0.6923076923076923
32033,71d3e4aee86e3e,aac4cdae,> ## 1. Confirmed Cases,69706f0b,0.6923076923076923
32041,03048e86a6d806,d25eee29,"So, you now know at a gist about the data folks and current state of your potential employers. What's next? Here are several things that might help you become more ready to roll.",1285c231,0.6923076923076923
32043,dbe40fdf51456d,ce68be8a,"### Example 2.
The ground truth relationship between the treatment feature and the outcome is given by 
$$ \theta(x) = \sin(\pi x)$$",f3e8a1e4,0.6923076923076923
32051,4f69b7bb1ca287,40dd6da5,## Making Submission,34abc6a1,0.6923076923076923
32058,fc8e0042411c46,953fab12,- All entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.6927899686520376
32059,917957c6c4065f,86164842,### 2.7. title_length,55b8ed68,0.6928104575163399
32060,ce9ed5e2d601d7,057fe640,## Bootstrap blending,f58a2f43,0.6929133858267716
32062,6cade0b6a41ba2,ef95216b,## 3.12. Stroke  Status,e6110293,0.6929824561403509
32065,73d8e56bc709b1,4d4addd5,"Position RF: **Agility**, **Acceleration**, **Balance**, **BallControl**, Dribbling, LongShots  
Position ST: **Acceleration**, Jumping, Finishing, **Agility**, **Balance**, **BallControl** ",78ec3cce,0.6931818181818182
32066,d1ff7e10ee0102,83f4a80b,"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.

But everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well. When I discovered this, I felt like an Hogwarts' student discovering a new cool spell.

*Avada kedavra!*",2cc71c3c,0.6931818181818182
32067,37e461081e47c5,6e70d75a,# Model 4: Lasso,b3e6549e,0.6933333333333334
32068,d6ddbe57f59cf7,839b9226,# Boxenplot (Enhanced box plot),504a3cda,0.6933333333333334
32069,91eaec994e0c6f,d3eed99e,"- In this part, we'll choose 6 Time Series from the 30490 ones we have and use them to test different forecasting approaches and evaluate them.",376aef10,0.6933333333333334
32072,67b7354e96113a,4e660bc5,**Logistic regression**,dca94250,0.6933333333333334
32077,3c2033cc99c12c,c7e8fd22,"**Findings:** *From the above visualization, we could find that under the situation of logistic regression, if we only consider the PC1 & PC2 columns, the algorithm is still efective but may not have better performance.*",dfa22a54,0.6934306569343066
32084,12f4d16fc21645,d3a7fb07,<h1 style='color:blue'>LightGBM Model Building and Training</h1>,c7752038,0.6938775510204082
32085,2343dc02ffb96a,a2ae7623,# Linear Regression Model:,29aa95a4,0.6938775510204082
32091,e69a496109e7d8,cbec9e0d,There isn't a proper conclusion that we can come with he use of any one variable,1c640591,0.6938775510204082
32094,20b372b6e4e276,4b9d6c37,"There are problems in processing: ""!"", ""!!"", ""!!!"", "":/"", ""..."", ""http"" etc.",ec8b0860,0.6940298507462687
32097,869a39a3d4dea2,9ecf834c,from the result blue chennal is more darker than red and green because the original image has minimal blue color spread where red is more compared to green and blue,9020daf8,0.6941176470588235
32098,2f47abddfd1928,142bff00,"This relationship confirms what we were expecting, the cabins 5 and 6 that are composed of 3rd class mainly cost a fare very close to 0 in a highly skewed distribution.

The rest of Cabins of 1st and 2nd class show more variable fare cost not following an initial tendency. I suppose it is related with some extra items of each cabin, even though all are 1st class there are always addons to purchase.",ae33cc0b,0.6942148760330579
32099,e9b9663777db82,b7727edf,#### Find Missing Data,648e8507,0.6942148760330579
32100,2ada0305b68956,dda8df62,### 118. Palette = 'gray_r',133e26f4,0.6942857142857143
32104,1014e6be391084,1f3c1f10,# Feature Engineering,46f9168f,0.6944444444444444
32111,c01049afb6d307,c673ad8a,### (2) Neoplasms,d37d3b5d,0.6944444444444444
32112,f18e737fcc4b06,affdf492,## Outlier Detection,087b8637,0.6944444444444444
32113,cf39cde80e66b7,06593b45,"# <div class=""h3"">R² and R-Squared: Coefficient of determination</div>

<a id=""m5""></a>
[Back to Table of Contents](#top)

[The End](#theend)

",aed4bc9b,0.6944444444444444
32124,ab6da5994949a3,43389066,## Visualising the K-NN Test set results,fae6b91d,0.6944444444444444
32125,396bc36edb95d3,86a65193,#### Confusion Matrix and Classification Report for Testing data - Random Forest,965e4f8f,0.6944444444444444
32128,c349ee5a821411,822cbeac,"We can see very interesting points in the specific correlation matrix per continents.
The correlation with the Happiness is not always the same in differens continents",572b269d,0.6944444444444444
32129,ee23a565163388,e47eafb8,We've split in such a way that the testing set has 30% of the data. The parameter 'stratify' ensures that the equal distribution of the dependant feature is split across different sets.,88aacbc4,0.6946564885496184
32131,840534f2908a9c,d5e5123f,New York city is divided into 7 Boroughs. Let us calculate which borough pickup and dropoff points are. And whether that effects the fare,8081c3cc,0.6947368421052632
32133,a81661cc35d8d2,5b7aebf6,# Modeling,3331f113,0.6949152542372882
32134,1294fb4c86f993,05f98ca6,<b> Transpose `census_2016` to get the states into rows like guns_2016,4471e513,0.6949152542372882
32135,9169c4e9c33c90,18f3abd7,"Of all the titles that made it over the years, which was the most expensive?",725bf880,0.6949152542372882
32143,a44368590e878a,eebffa34,# Route,77743ba8,0.6949152542372882
32144,ed8009f482b380,52ddac2c,## Building model,e99941fa,0.6949152542372882
32155,7454fdc444df16,98d66afc,"The function below, takes a list of paths to the desired images. Then reads and reshapes the arrays from (50,50,3) to (2500,3). After reshaping it adds the reshaped array to the total array. The final array will have a total dimension of (2500\*120000,3) = (300000000,3). PCA is then applied to this array and the principle components and fitted to this object. ",a7818ef5,0.6952380952380952
32158,0932046e1f485d,4d817bb2,Merging the two datasets based on the Apps.,218cc7a3,0.6953125
32160,548f961125248d,d36b8e8c,"* We will look at SHAP values for a single row of the dataset (we arbitrarily chose row 2). For context, we'll look at the raw predictions before looking at the SHAP values. ",d8c5e8b8,0.6956521739130435
32162,73ca9abcc2034e,906ff863,# Descriptive statistics - body,cec3446c,0.6956521739130435
32165,69d50f5e1373f1,ca007951,# Using the correct prediction format,ec7545ee,0.6956521739130435
32169,57bad3860b0fa4,0f73546a,"This is the model found in Chapter 14 of the textbook I recreated, usually tests around 9.15% - 92% accuracy.",05138a5e,0.6956521739130435
32173,7e275c8d5ff2a0,598a365d,"<div style=""color:white;
           padding:8px 10px 0 10px;
           display:inline-block;
           border-radius:5px;
           background-color:#5E7B81;
           font-size:90%;
           font-family:Verdana"">
    <h1 style='color:#ffffff;'>6.1.2. Making Predictions</h1>
</div>",b3afcc98,0.6956521739130435
32174,8336d84cf3ff6b,ca48718c,# XGB Boost optimized with RandomSearch CV ,b96b58a0,0.6956521739130435
32176,90ead00a8ee283,561de8d7,**Exercise 6**: Suppose we have the following `DataFrame`:,612efa48,0.6956521739130435
32179,a6b9837940ee38,f3e04a7e,* Draw the learning curve,52d2acc7,0.6956521739130435
32182,e3fb4c6300cb56,a45cc3e2,"<a id=""10""></a> 
## Swarm Plot",8ebbdf89,0.6956521739130435
32184,3319c5c562f607,f260f518,# counting the most common words,f298250a,0.6956521739130435
32185,0e2a23fbe41ca9,4c8a6f06,"Observations:
- both ```avg_purchases_lag6``` and ```avg_sales_lag6``` have a few outliers in the extremes ",64e4762c,0.6956521739130435
32187,b49bb7b41806a7,ff6598ec,The bin which has the highest count is the bin 3985-7074. It has count of 11193.,d034d34d,0.6956521739130435
32188,59236ba162ab7b,83604d86,3. Prepare  data,ace5b0ef,0.6956521739130435
32189,ea4e559a86d613,806252fc,> # Augmentations,eff47843,0.6956521739130435
32191,db7890856ec28b,3a429923,"# Scikit-learn's Multinomial Naive Bayes Classifier
### with CountVectorizer and TfidfVectorizer comparison",106f6dca,0.6956521739130435
32194,fe118026267a88,ea982ef7,"## 5.

Run the cell below to create and display a DataFrame called `animals`:",612efa48,0.6956521739130435
32196,30fdc4a6e3c1db,5003826d,### Plotting sales over the week for the 3 categories,6111ddee,0.695906432748538
32198,fc8e0042411c46,e390d34f,## Get updates on DM Content,af476c2a,0.6959247648902821
32199,63b44c85e32c1f,43845f84,If you want to directly declare a tuple it can be done by using a comma at the end of the data.,fb9b9562,0.6959459459459459
32201,52cfd66e9ec908,a784a726,Let's see the pixel distributions for the above plot:,c74adcdf,0.696078431372549
32203,7cfd96218dd933,ba1c8133,### ANALYSIS,7c34d96c,0.696078431372549
32208,585c280865b46e,0c15a70c,# Correlation analysis ,4d6056f1,0.6964285714285714
32211,8dd655515e7d18,c2b18e5b,"## Correlation

Finding correlation between the 5 personality traits and word-count",895f41cf,0.6964285714285714
32213,f13534449a3750,3a0c9384,To obtain the first dataset (50% Ship + 50% No Ship) we need to undersample the empty images. The number of images with ships are 42.556 so we need to remove 107443 (=149999 - 42.556) images from the no ship class.,8b7f3332,0.6964285714285714
32214,312135b445bd23,d0b2fd72,"# **Answer Summarization**
The final module in our project is the abstractive answer summarization. The goal is to build informative and consice answer for each sub-task using the relevant sentences we found from previous task. We chose to use Facebook's [BART](https://arxiv.org/abs/1910.13461) model. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. BART is really useful for text generation tasks, hence we chose it for this task. We use HuggingFace's great library [transformers](https://github.com/huggingface/transformers) for that end. A future improvement here can be to fine-tune BART model with COVID-19 articles. Due to lack of resources, we took the pretrained BART model.",8ced381f,0.6966292134831461
32227,efd44ce2c08541,ebaebc18,"### F1 Optimization for stopping criteria

https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/37221",ebc2d00c,0.696969696969697
32233,b241b847319d13,c9a55d9d,Download it from the output folder,0fb698f0,0.696969696969697
32236,bd380b97b5c894,1012e613,"Due to the imbalance of our data towards objects of class 0, we have learned very well (perhaps even too much) to determine class 0 when we really have class 0 (TN)

At the same moment, we almost did not find objects with class 1 (lower right corner - TP), that is, we did not find any such object correctly, and at the same moment we made quite a lot of mistakes (lower left corner) when we attribute the object to the class 0, but in fact it is 1c, that is, we admit a large FN (error of the second kind)

The problem is that the data is not balanced, we use the parameter scale_pos_weight - Balancing of positive and negative weights.
which adds a penalty for misclassifying the minority class

And also reg_lambda which will add regularization",66f2562a,0.6972477064220184
32251,22bd95f4807a23,e768ae8b,* Trendy attire have a large variance compared to other types of attire,c05d356f,0.6976744186046512
32252,a566b5b7c374e7,85e493fd,### Average Heart Rate Variability,b3dc5545,0.697841726618705
32253,5f32117bcd5255,c0354701,### OBSERVING,85882abf,0.697986577181208
32258,510b8303776bb6,e777f37c,## Trying different models to see which one works best for the given data. ,18080db8,0.6981132075471698
32259,f015d0147e8fbf,c3d3b3d5,"### Train LightGBM Model

My method for training a LightGBM model. It's called below when I run serial CV and when I generate test predictions.",518954fb,0.6981132075471698
32260,23df07a474aaae,46ff866b,**Lasso Regression**,0ea40276,0.6981132075471698
32268,8985a124d4b657,0b9725db,"Above, accuracy increase a lot in the last few epochs. Below, the loss gradually decrease. These are positive signs that our model is doing very good.",586d1846,0.6984126984126984
32273,1eb62c5782f2d7,b4a8029b,### Solusi Q2,bb69f147,0.6986301369863014
32279,4daf6153275cbf,fa81dab4,"Because I am investigating localization, my three hypoteses are on this subject. I am going to define them and show the results with numerical tests and visually.",51db1961,0.6987951807228916
32281,0caaec057f7184,1712b255,"We look at the kind of data by using the history data of category sales in the shop.
There are around 15246 kinds of inputs, 16K groups when considering shops and category, distributed in 39 category, 42 shops, 363 items. (For each item, we're predicting the sales in the 42 shops.)
Since we don't have the item sales record, for each input, we can take a look at the category sales in the shop and over all sales in all shop. Take cate 72 for example.

",b875533e,0.6989247311827957
32283,a2176d4653ef60,8132aa78,# Data Modeling - KNN,ac908675,0.6990291262135923
32284,98a6794067932a,013561ee,"La cellule ci-dessous permet d'effectuer un changement au sein de notre dataframe afin de pouvoir utiliser ces données dans nos analyses par la suite. Ce code permet de prendre le dictionnaire créé précédemment dans cette analyse et d'associer l'abréviation à la colonne ""State"" selon l'état représenté dans l'index.",08600fe2,0.6990291262135923
32287,a5a419dc7245b0,4f2957a7,#### Phase 2: Making the Neural Network (NN),4279726e,0.6991150442477876
32288,c9b4e282e4e2c1,087e26a9,Let's see the possible values of Weather.,f44d339f,0.6991150442477876
32290,e19e307b3fd188,2297a9f9,#### Numerical columns handler,2173955b,0.6991869918699187
32292,b01ee6cb674fa3,76b32e67,"# SRC

Could Find nothing, so lets look at the data",a8ffd35e,0.6992753623188406
32297,ad121e0531afa4,802cd710,<h1 align='center' style='color: #8532a8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px'>4. Pytorch Lightning Model Class</h1>,a3492905,0.7
32307,36c35f0a9f70f7,bf126d06,## kernal SVM,67358bc7,0.7
32309,7dd46c750653eb,6bc2a141,"**Inference**

* Over the years the months from June to September has more number of marriages.

* In 2013 and 2014 , August had the most number of marriages.",c2644713,0.7
32312,62487bcd70b199,b6e0a2c6,## <a id='8.1.2.'>8.1.2. LogisticRegression</a>,f6ae50af,0.7
32317,38b79494ac749e,26526040,"1. We can control the regularization strength by changing the hyperparameter `alpha`.
2. Regularized version of the model performs pretty well. Even in case the original original (unregularized) model is heavily overfitting due to excessive complexity.",39162a40,0.7
32319,6998861ff6ff01,70da8d66,"# Plot the day of the month to check the date parsing
___

One of the biggest dangers in parsing dates is mixing up the months and days. The to_datetime() function does have very helpful error messages, but it doesn't hurt to double-check that the days of the month we've extracted make sense. 

To do this, let's plot a histogram of the days of the month. We expect it to have values between 1 and 31 and, since there's no reason to suppose the landslides are more common on some days of the month than others, a relatively even distribution. (With a dip on 31 because not all months have 31 days.) Let's see if that's the case:",ea9e72cf,0.7
32321,10c5a39a87c47e,0fb3bef5,### Plotting History of Model's Accuracy ,09c7337a,0.7
32325,b0c2805cd5c087,5f74bdb2,Image websiteplanet.com,0446f327,0.7
32330,3dd4294f903768,60e3b43d,"Now, after we deecided which columns we will use, we have to create subset from our dataset.",0d89d098,0.7
32331,83df814455f06c,92435ca5,"Here, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting. 
",c9cff71a,0.7
32335,d6cbd7160961dc,9af803de,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 95400. Note that the chi-squared value of 3807.58 is not meaningful, since the test is inadequate for bigger samples.

* The center graph shows the result for city Roseira, which had the worst chi-squared value. 

* The rightmost graph shows the result for city Bálsamo, which had the best chi-squared value. 

Note that both the aggregated analysis and the worst adherence city have a high presence of number 0 in the second digit. This behavior is very usual when the data informed was rounded. For example, instead of an expense of 19.93, the city may have rounded to 20. In this case the number shifted from 9 to 0.",36d74664,0.7
32337,864302b10e7730,24488e88,# 1. Bar Graph,e9dd1d2d,0.7
32339,b95c657bd26c57,ec7c6367,# Model Improvement by Scaling ,eb13c3f8,0.7
32342,49ac6594c8f5cf,b436e7cd,SVC Model,6f19f28a,0.7
32343,a78d363403fce2,9b6460c1,"# Fitting
Just run a few epochs of fitting",0f824cb6,0.7
32344,312d2a3c7547f1,43f810df,Logistic Regression Model,8fd1efcd,0.7
32351,2bace980aeb34c,7d68afd1,"# Step 2: Generate test predictions

Now, you'll use your trained model to generate predictions with the test data.",dc05ef6c,0.7
32354,2ada0305b68956,7da419e4,### 119. Palette = 'hot',133e26f4,0.7
32357,6e28c4f557f736,56f755cd,### Make Prediction,021fdf75,0.7
32358,254cccd5145725,bccdb5b8,Splitting the data,a49b4037,0.7
32359,70b7a24d522250,f5a5b9d8,#Instead of  tf.`reset_default_graph`() Method use tf.compat.v1.`reset_default_graph`(),83f3c002,0.7
32361,9c044fa3072552,3be8aa66,"The 2 Gaussian curves in the above graph could represent 
1. People going to work
2. People returning home.

Amazing how the human systems that we've built mess with the general statistics ;)",1362842e,0.7
32362,be616f0785c32d,ce4f25a6,"[Go Top](#top)


#### C.2 Results

###### C.2.1 Top candidates in FDA approved drugs

Among the FDA approved drugs, we identified some top candidates that do not exist in the training gold standard. We hand-searched in literature for each of the top candidates with a probability >0.05 (55 in total). Most of them come from contaminations, i.e., overlapping with an example in the training set even though the drug appears with a different name. 

Cleaned-up list:

| Drug name | Original usage | Potential issues in the candidate|
| --- | --- | --- |
| OLUMIANT(Baricitinib) | Janus kinase (JAK) inhibitor ||
| MEKTOVI | Targeted therapy to treat BRAF V600E or V600K cancers | May come from bias in cancer targeted therapy/screening|
| BRIMONIDINE | Treating glaucoma ||
| CAPRELSA | kinase inhibitor, medullary thyroid cancer (MTC) | May come from bias in cancer targeted therapy/screening|
| EDURANT(rilpivirine)| Treating Human Immunodeficiency Virus-1 (HIV-1) ||
| MARPLAN | Treating depression | Some schizophrenia drugs are used in the protein interaction training set, and might result in an implicit contamination here|
| Corlanor (ivabradine) | reduces the spontaneous pacemaker activity of the cardiac sinus node||
| LORBRENA | kinase inhibitor, ALK mutant cancer| May come from bias in cancer targeted therapy/screening|
| BRAFTOVI | kinase inhibitor, Metastatic Melanoma| May come from bias in cancer targeted therapy/screening|
| TAVALISSE | kinase inhibitor indicated for the treatment of thrombocytopenia| May come from bias in cancer targeted therapy/screening|

<div id='PartClim'></div>

#### C.3 Limitations and biases in the finding

Drugs proposed by in vitro or computational protein targets/gene-gene network approaches are definitely biased towards targeted therapies in cancers, because these drugs were intensively screened in cell line experiments. This is true for both the above list and probably the original list proposed through the binding experiments, and certainly other studies.

Second, low scores only mean the drugs are not similar to others that are being investigated in the study, rather than they are not useful. Remdesivir had a high score of 0.09 (we are not sure if this is an implicit contamination from the training set), the others had low scores, including Vitamin C, hydroxychloroquine and favipiravir.


[Go Top](#top)


<div id='PartD'></div>
## Part D. Epitope study for vaccines

#### D.1 Methods

We identied all paragraphs that contain the word vaccine and COVID-19/SARS-COV-2. Then, we looked through each of the abstract. If deemed relevant, we go to the original paper and record down their methods and proposed epitopes


",b78e18aa,0.7
32373,bfe6c7096b1ad0,80951af7,"### https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor
![image.png](attachment:image.png)
#### Это, как минимум, +0.01",fffd95e0,0.7
32376,a4f8ad33c823c5,5f3b981c,"# EDA

Studying Gender Distribution and its relationship with the diabetics factor",fcd48307,0.7
32377,c91c137284976f,22c62ad7,# 3. Tuning hyperparameters,c6888c0a,0.7
32378,edc19e349fe80a,1a055ce5,We then check its shape and data,7882221a,0.7
32380,5b92c712910a11,89ff1362,# Remove HTML tag from the text ,e1d17100,0.7
32381,8696921d9adc93,b103ac65,**6. Split training and Validation set**,b8908b23,0.7
32384,1011899b959f44,367b24c7,"# Split-Apply-Combine
Using this we can easily group data and display a summary of a specific statistic we select in the parenthesis () of our groupby. 
First, it *splits* the data according to column, then *applies* the summary statistic, and lastly it *combines* the dataset into a new one.

Split-apply-combine methods include:
* .groupby()",0b112382,0.7
32385,4bada947d597ac,78372fe0,# Visualise accuracy and loss,eab5094a,0.7
32386,9276fa5cc2fef6,065751bf,Checking the bad_word features...,24aa6a52,0.7
32388,63d0d9b9a8c7d2,9f5b915b,***Naive Bayes***,e32e5933,0.7
32391,3c2033cc99c12c,3ed6370f,#### Using the Cross Validation to test the accuracy,dfa22a54,0.7007299270072993
32396,c84925c8171900,fc0dc18f,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.4.4  Region Wise Sales per Platform
            </span>   
        </font>    
</h4>",e21ff7ec,0.7009345794392523
32397,2a123b4e8f9433,bcb1da59,# Build and train a K Nearest Neighbors model,0a082218,0.7010309278350515
32398,8ec771f5600a61,4d561d65,## AFTER THIS WE USE DIFFRENT CLASSIFIRESS TO TRAIN OURMATCHINE,48364c1f,0.7010309278350515
32399,063a35f644e3c5,df09b84d,## Feature Selection,1c30fb0a,0.7010309278350515
32405,663bbc9eaf267b,de659302,# Categorical Feature Encoding,32445529,0.7012987012987013
32406,c13f73168789c2,2b567319,"### 2.2 To slice columns by index position<a id='27'></a>
Syntax : `df.iloc[:, starting_column_index : ending_column_index]`",16175052,0.7012987012987013
32409,722cd844dfbe8f,eba8eaeb,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_3_4"">Keras custom data generator</span>
Thanks to the Sequence module of the Keras library, we are going to create a personalized image generator. This will prevent us from creating Numpy arrays or Tensors containing all the sequences which would quickly overload the memory.",0cedb385,0.7012987012987013
32413,21413205980558,a3709ab1,# 4.3 Marketing month analysis（营销月份分析）,84197de0,0.7014925373134329
32424,5ce12be6e7b90e,0905ce02,This is because strings are **immutable** whereas lists are **mutable**. We'll get back to this notion soon.,c0ab62dd,0.7017543859649122
32425,9f3710be6aea65,5bddb56c,"## Normalizing data
https://medium.com/@rrfd/standardize-or-normalize-examples-in-python-e3f174b65dfc
",ae9bda88,0.7017543859649122
32426,9e27af2600925c,f930b352,Run the following cell to train your model.,9b556435,0.7017543859649122
32429,c3498779cda661,55fc224d,Para columnas **mileage** y **price** se van a tomar los datos hasta percentil 99. Para columna **hp** se utilizarán los datos mayores a percentil 1.,0f531b65,0.7017543859649122
32433,d8fb26c4197325,92b2e74b,# Gradient Boosting,b190ac50,0.7017543859649122
32437,f6648e47713411,3bef2962,# 3. Define model,f4af4d1c,0.7021276595744681
32440,3f25b363afec54,cd99d487,## Data preparation,bbdaae25,0.7021276595744681
32442,4c47839b067546,104a7c7f,### Linear Regression,1f517b02,0.7021276595744681
32445,b61ab8f81dc03d,f5afd17a,"<a id=""basic_predicting""></a>
## Basic predicting in different models",64d05394,0.7021276595744681
32452,2f47abddfd1928,3848058c,"## 3.7. Cabin

Seems that Cabin_Letter has a good correlation with survived, Pclass, age and fare.

Lets inspect these relations.",ae33cc0b,0.7024793388429752
32455,a6c34cd514e30e,5c8698fa,### Let's check 3rd file: ../input/listings.csv,bf603ddd,0.7027027027027027
32457,c3336e5345f6c5,f449132e,"## 4. Modelo preditivo
![texto alternativo](https://scikit-learn.org/stable/_static/ml_map.png)

",a5ec5530,0.7027027027027027
32461,bbb3f4b76a4559,46e1e83b,"### KMEANS
As you can see, train/test dataset distribution is not so far and distributed not evenly. So let's try clustering methods to split train/eval data keep distribution.  
The bellow cell show k_means clustering. You can modify ""n_clusters"" to decide cluster counts.",75185823,0.7027027027027027
32462,ccabe7a86825ce,f2dc4595,**Nominal cols with low cardinality modify with One-hot encoding**,d766cbf9,0.7027027027027027
32465,b7b1057764fa02,8d9b1dc0,"We're ready to fit it to the training data. The values `acc` will give us the accuracy of the model after each epoch. I will run the model for a total of 5 epochs. Despite our high number of parameters and large dataset, the model runs rather quickly as `keras` is able to engage the GPU in its functioning.",5053a192,0.7027027027027027
32466,e4525eb0c96f28,db0ba442,"Just as previously predicted, our residual plot looks terrible. We have too many residuals above the linear model, resulting in a general pattern of residual distributions being skewed right for each year. At the very least, we seem to have uni-modal distributions across all years. We also have certain years with more symmetric looking distributions, such as the years 1989 through 1993. But, those years look rather off since the distributions have means greater than zero. So, this is still unsatisfactory as there seems to be no constant variance of residuals. Therefore, the next step we can take is to add features and interactions to our linear regression model. We can try adding interactions for the country the game originated from, the ESRB rating of the game, and the genre of the game. Interactions and features for linear models allow for multiple independent variables to affect the predicted outcome, resulting in the introduction of more terms into our model.",2093a1f1,0.7027027027027027
32467,993966a1cb5eb1,eb7fa4e4,## Training Loop,5cd181e2,0.7027027027027027
32480,ff3a8ce61fab6a,c45d9e4f,"<hr>

### Exampel 2 ",9afe1654,0.703125
32481,5f4ae633cfd090,0350a674,"Next step will be dropping the variables that are common to both the fighters(like date, country, etc.) and thus provide no advantage to anyone",a30a16e2,0.7032967032967034
32485,efbcfe95cd7fde,2e51f9b6,Age target 1¶,54be281a,0.7037037037037037
32486,0c452d3a0b9339,960541d2,"1. From the above plots Observed Significant Difference between Reactivity, Degradataion Rates of BasePairs and nonBasePairs.
2. Few Sequences have outliers in the Non Basepairs.
3. We can See that BasePaired means are more narrow distributions than NonBasePaired Mean distributions.
4. Do the Pairs in the Sequence will have the Similar reactivity & degradation ",5d857385,0.7037037037037037
32491,24e550b8226932,a092f3ab,# Model:,0caee953,0.7037037037037037
32495,135122550b6483,90be7997,"We have all the dependencies in place and Now we can start with Data Cleaning
and Feature Engineering",6592d6d8,0.7037037037037037
32496,2500c5fe8497ee,c707547b,"# Q2) What country had the most meat consumption?
",855355f0,0.7037037037037037
32500,cebaf20167fb49,93b2bb10,# Plotting Graphs,702e4057,0.7037037037037037
32503,aa7db7b023d0a2,5a5022dd,#H2OAutoML,ec912af3,0.7037037037037037
32507,fce6f1b02867e3,5f3b7873,## Add new column to dataset,3fb572c2,0.7037037037037037
32508,e6576e985ccc71,5e6aedea,#Individual channels visualization,63d2c3a5,0.7037037037037037
32510,1883198d6d8c3c,57a42b24,"To describe all the columns in dataframe include the type of object -
<b>df.describe(include = ""all"")</b>",69a1d458,0.7037037037037037
32511,4883314a96dc34,6cebbf4b,"### Preprocess Data (2)
#### Normalization (Min-Max)",50d36836,0.7037037037037037
32512,233cb23d9e01b9,4d765bef,## gender error,ffa56c19,0.7037037037037037
32514,6d29650083cbde,df8ecd03,"To check how good my prediction actually is, I randomly select the same number of players and compare the groups' average rating:",e65fd993,0.7037037037037037
32517,d128317750d689,e4f368e1,"And now we can finally train the network! There are quite a few of steps that are PyTorch specific, so if it seems confusing, check out that [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py), where every step is explained in detail. 

I've also created two lists for loss and accuracy just for logging. They will come in handy after the training to see what happened and how the performance has been changing over the training process. ",d87f7428,0.7037037037037037
32518,ffc3bb768dcf97,0a4d9ae2,"Time to evaluate Decision Tree, Random Forest, Linear Regression and KNeighbors. I'm gonna use MAE for calculating accuracy",15ce84be,0.7037037037037037
32522,613bf7bfdcb9e3,29623f9c,# axis = None => most common value of all data ,32beb65d,0.7037037037037037
32524,1667a100fc8b42,d3b93f66,"## Data Model Selection ##

Simple test to run multiple models against our data. First, with raw features. No PCA.",6c8cd6b6,0.7037037037037037
32525,7baeb0ffc6659e,881e6943,**Age**,8cbebba9,0.7037037037037037
32526,c1984e64b35234,5f001461,"rs1333049 has been reported in a large study to be **associated with heart disease, in particular, coronary artery disease**.

The risk allele (oriented to the dbSNP entry) is most likely (C); the odds ratio associated with heterozygotes is 1.47 (CI 1.27-1.70), and for homozygotes, 1.9 (CI 1.61-2.24). [PMID 17554300OA-icon.png]",1811225b,0.7037037037037037
32527,9ad9a97e628bfa,ae490422,"**Cabin Initial** 과 **Fare**의 관계가 심상치 않아보인다. Cabin number는 호실 번호이지만 Cabin Initial 은 선박 내 선실의 위치, 등급 등과 관련이 있을 가능성이 크므로 Null valu를 꼭 채워 Attribute으로 사용하면 좋을 듯 하다. ",0a7e1136,0.7037037037037037
32531,9bcfa825c8b2e6,9d9465b6,Nadir olan durumlar 0.01 oranında görmezden gelinir.,220f36e4,0.704225352112676
32537,a0b321057e7402,34cba27c,"The evaluation metric for the model selection was AIC (Akaike Information Criterion -> AIC=ln (sm2) + 2m/T). As a model selection tool, AIC has some limitations as it only provides a relative evaluation of the model. However, it is an excellent metric for checking the general quality of a model such as SARIMAX.",5f73fb91,0.7045454545454546
32538,da199f8fb59439,536d3c00,***Drama type movies are mostly rated as TV-14 & Most of the Stand Up Comedy are rated as**** TV-MA***,baaa665d,0.7045454545454546
32539,a35cdce61f4059,d1cb5294,"* **Improving the ratio between classes**
1. Undersampling",acc8eab6,0.7045454545454546
32545,0a918602a04693,91c4205c,"So, no more null values",c1ef0e95,0.7045454545454546
32546,73d8e56bc709b1,dbb0f163,"Finally, we choose these 6 features: Acceleration, Agility, Balance, BallControl, Dribbling,Jumping.",78ec3cce,0.7045454545454546
32547,f269d2fbd5f1be,03dbbb15,"**Commentary:**
* While higher rating generally translate to higher salary, the effect of each additional unit of rating on salary is different between players of different experience level
* For players with more than 10 years experience, each additional unit of rating translate to higher salary increment as compared to the same for players with less than 5 years experience ",1264c440,0.7045454545454546
32553,726833f92fb87a,3ec1c5b3,## Marital status,7dc5e1b6,0.7046979865771812
32554,5f32117bcd5255,01bf3661,"### ATTENTION
* RUN FOR CHECKING
* THEY ARE RAW",85882abf,0.7046979865771812
32558,04bac111ffbe9c,b1a60522,## Confusion matrix and accuracy score,82576b17,0.7047619047619048
32559,55a5e31d03df9f,c0b96193,"Here we used the same Functional API in the past BUT! we added a new layer that we hadn't seen before, `GlobalAveragePooling` is a pooling operation designed to replace Dense layers and they help to minimize overfitting by reducing the total number of parameters in the model. Why? It's because instead of taking all the pixels to flatten in a dense layer we're taking the Avg of the tensor. Let me show you about:",06dce00f,0.7047619047619048
32561,601e18072783b4,454db40a,## Time series - Average IMDb rating on Netflix overall,36b2b1fa,0.7049180327868853
32563,979f1e99f1b309,d4b49b77,## Try Lasso Model,d1bfebbf,0.7049180327868853
32565,918040fad252ec,cabafe6b,Membuat variabel alias untuk model format .h5,966fcd8f,0.7049180327868853
32569,897ca904b74a98,54e87871,## Evaluation,c5844ad4,0.7051282051282052
32570,4d91e84c564cbe,a4d1126b,"### Searching lists

Where does Earth fall in the order of planets? We can get its index using the `list.index` method.",355a43e3,0.7051282051282052
32572,f91f58d488d4af,d6e411fb,"A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality",5df1bbf3,0.7052631578947368
32580,2ada0305b68956,16189bae,### 120. Palette = 'hot_r',133e26f4,0.7057142857142857
32586,842547b2def18c,4e434214,Convert the Fare feature to ordinal values based on the FareBand.,b8efde6d,0.7058823529411765
32593,16ca1123840e9f,04815a57,"#### This ranking is based on the number of launches happended.
#### Top 10 countries in Space Technology as of 2018 is as follows (Source : [ESRI](https://www.arcgis.com/apps/MapTour/index.html?appid=09bad7b2f1c3418b9c381749a006092e))
1. USA
2. China
3. Russia
4. Japan
5. United Kingdom
6. India
7. Canada
8. Germany
9. France
10. Luxembourg

Kazakhstan having the 3rd maximum number of launches is because most of the rockets of Russia are launched from Kazakhstan hence that also comes under the count of Russion space missions. The Baikonur Cosmodrome is a space launch facility located in southern Kazakhstan. The spaceport is currently leased by the Kazakh Government to Russia until 2050, and is managed jointly by the Roscosmos State Corporation and the Russian Aerospace Forces.",e8b8f086,0.7058823529411765
32602,917957c6c4065f,ce5b5709,"title_length는 평균 42, 범위는 2 ~ 100입니다.  
항목들 중 그나마 극단적으로 치우치지 않은 분포를 보입니다.  
길이가 10~60자 이내인 동영상이 많은 비중을 차지하고 있습니다.",55b8ed68,0.7058823529411765
32606,d0080e3a39bc5c,4b104138,Tuning the model a bit more to squeeze out the last drop of scores...,2fcde4cf,0.7058823529411765
32608,4ae6a182abac64,c8ace6b4,## 3 Pre-Modeling Tasks,418676c5,0.7058823529411765
32615,1a0bd2f72bbe36,3120ce53,## 02. Bivariate Analysis:,2fa311dc,0.7058823529411765
32618,3d905ce4828057,b410f97b,"#  Interpret and evaluate the results you have obtained.
Looking at the estimation results, we can see that avg monetary and expected avg_profit are close to values, thus making a consistent estimation.
When we look at the scaled_clv value, we see that the average is 1/100. The reason for this is that the majority of the customers make small-scale purchases and accordingly, the clv is low. This can be explained by Pareto analysis; So the company's top 20% customers account for 80% of sales. Since there is no homogeneous structure between customers, clv is quite low.",5b006cc3,0.7058823529411765
32620,a0a5baa6c7e12a,5068bf9f,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Detecting Cardinality of the Variables in Training Set</div>",551d41de,0.7058823529411765
32622,e4c6dd957eb5ce,2e25fc7f,## Reward Type Distribution,2e383665,0.7058823529411765
32624,c65a65d4041018,134530b7,Python and SQL are widely used. In Russia R is quite rare. Russia and India use C/C++ a lot and Java is popular in India.,824fb229,0.7058823529411765
32630,869a39a3d4dea2,0d76c599,Merge - show the channel the image channel belongs to using the image processing,9020daf8,0.7058823529411765
32632,99821bc6a45be6,a056f2d8,### Model Definition:,b9d59346,0.7058823529411765
32635,523123dad03177,c374981d,"Again, the girls win!",48a5e4e6,0.7058823529411765
32639,b01ee6cb674fa3,c5ae8447,"We see that there were 3 launches, all conducted by submarine @ Barents Sea Launch Area... And the rockets' names are russian, so, my guess is that this is of russian responsability",a8ffd35e,0.7065217391304348
32640,7e1da639035ac5,ba4c5165,# <a id='13'>13. Effective School Leadership analysis</a>,120b6c23,0.7066666666666667
32643,e5dd725b8fa422,f7dd6f4e,# Predicting using MA model,14675d8b,0.7066666666666667
32650,1cd8be6e679620,86133822,"* In 2D-Visualization it is difficult to picture which section is which part of RNA. To make it easier, we will generate a Graph Structure. The neato method can take that as input and create a nice visualization of the graph:",3ce15a43,0.7068965517241379
32653,84127ade6fde87,63707548,"In principle, we could simply iterate over our vocabulary and generate a set of 100 random floating-point numbers for each word. This would work, in that we could cram a very large vocabulary into just 100 numbers, but it would forgo any concept of distance between words based on meaning or context. A model using this word embedding would have to deal with very little structure in its input vectors. An ideal solution would be to generate the embedding in such a way that words used in similar contexts mapped to nearby regions of the embedding.",f55d05b6,0.7068965517241379
32654,1750367e54f407,5bbb737c,"I apply some very basic test time augmentation to every local image extracted from the original 600x800 image. We know we can do some fancy augmentation with `imgaug` or `albumentations` but I wanted to do that exclusively with Keras' preprocessing layers to keep the ""cleanest"" pipeline possible.",a8e655b2,0.7068965517241379
32662,20e1ba19eb9b5e,57372da9,**Transforming some numerical variables that are really categorical**,4569bfc1,0.7068965517241379
32666,fd4017c1514157,8eeebc51,Both Sites have equal distribution.,fd8f0896,0.7073170731707317
32667,0b01138ad120fc,1b7e51be,"**Forgetting: The Irrelevant Informations are forgetted by the Forget Gate.  
Possibilities: All output possibilities, given the input.  
Ignoring: Words to be ignored in actual LSTM cell  
Selection: Selected words, given the input**
<br></br><br></br>
**First, the LSTM select the possibilities giving P possible words (Possibilities).  
Them some of them (let's say the I ignored words) will be ignored by the Ignoring.   
Non-ignored words will pass (Filtered possibilities).  
Selected words from the forgetting will be removed(forgetted) from the Filtered Possibilities, and new words (Non-forgetted) will be: 1- collected by the memory 2- moved forward (both called Collected Possibilities).  
Finally, LSTM will """"merge"""" Selected Words with Collected Possibilities giving prediction. Also this prediction comes to the next LSTM cell as part of the input**",0b4b72e6,0.7073170731707317
32668,8d0aebab1e5914,4c075d96,# PCA using Scikit-Learn,084e671f,0.7073170731707317
32669,74a03887600114,25cb1f40,So from above we can see that we don't have any movie with rating 5,c0ffb2f0,0.7073170731707317
32671,8cefb86a675e5d,8e1defd4,# Evaluate Linear Regression Accuracy using Root Mean Square Error,79f9e69b,0.7073170731707317
32674,786475feda0190,44fdac1c,"### [Mel-frequency cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)

- In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.

- **Mel-frequency cepstral coefficients (MFCCs)** are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear ""spectrum-of-a-spectrum""). 

- The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, **the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum.**

#### [What is Mel Scale ?](https://en.wikipedia.org/wiki/Mel_scale)

![mel-scale.gif](attachment:mel-scale.gif)

- The mel scale, named by Stevens, Volkmann, and Newman in 1937 is a perceptual scale of pitches judged by listeners to be equal in distance from one another. 

- The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. 

- The name mel comes from the word melody to indicate that the scale is based on pitch comparisons.

- See the comparison of mel scale with frequency in heartz in the image above to get a sense of the scale parameters. 

#### MFCCs are commonly derived as follows:

1. Take the Fourier transform of (a windowed excerpt of) a signal.
2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.
3. Take the logs of the powers at each of the mel frequencies.
4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.
5. The MFCCs are the amplitudes of the resulting spectrum.
6.  There can be variations on this process, for example: differences in the shape or spacing of the windows used to map the scale,or addition of dynamics features such as ""delta"" and ""delta-delta"" (first- and second-order frame-to-frame difference) coefficients.

![Mel-filter-banks.png](attachment:Mel-filter-banks.png)",e4663d97,0.7073170731707317
32677,8d70dcae7f40a3,c46d9d75,### **Precision Recall**,472c71ce,0.7073170731707317
32684,5ce12be6e7b90e,90541721,"### List methods

Lists also have many methods. 
The most useful ones we'll see here make use of the fact that lists are **mutable**.

`append` adds an element to the end of the list:",c0ab62dd,0.7076023391812866
32685,30fdc4a6e3c1db,5caf8d31,"What we see:
* Sunday (~28.3K units) followed by Saturday (~28K units) are the dominant days for sales of Food Items 
* Saturday has more sales than Sunday in the case of Household and Hobbies although the differences are quite small
* We do see a sense of weekly seasonality with highest sales in Saturday and Sunday and lowest sales in Wednesday and Thursday",6111ddee,0.7076023391812866
32690,c115e287523aab,e49c3c5c,## Model Check,feb1288b,0.7076923076923077
32692,d07915a6e6992e,ec3dc192,**Cabin**,2b912140,0.7076923076923077
32693,03048e86a6d806,1929deed,### The Most Important Part of The Job,1285c231,0.7076923076923077
32696,09751c520b0616,4114ef12,"- Behavior of <b>SalePrice</b> according to Area related feature<br>
<br>
Using scatterplot<br>
SalePrice v/s LotArea,TotalBsmtSF,1stFlrSF,2ndFlrSF,TotalSF, GarageArea and GrLivArea",a4d0c7e9,0.7076923076923077
32698,04ff2af52f147b,3c7339ad,"**Create GroupSurvival Feature:**

Similar to the reasoning above for the *FamilySurvival* feature, we will create a new feature *GroupSurvival* simply based on having the same *Ticket*.  There will be overlap between the two features, but there is still some usefulness to be extracted.  Passengers who are family would be likely to stick together and as a result, likely survive or perish together.  The sentiment can be extended to passengers who are travelling together but are not family (ie. friends, nannies, etc.).  These individuals may not be in the same cabin as would be true for families, so sticking together would not be as easy during the chaos of the sinking ship.",d5f37be9,0.7078651685393258
32703,a5a419dc7245b0,723d1516,##### Importing the Keras libraries and packages,4279726e,0.7079646017699115
32705,c80939c7c626cf,26376afe,"# I have given diff values above you can give your values
This is called feature Scaling
these above values are convinient because
ML Classifiers use Euclidian distances",b9ac31e2,0.708029197080292
32709,a3ae04b78e45b5,75c549cd,**VIOLIN PLOT** (VAGINA PLOT),4195da8b,0.7083333333333334
32710,02773bdc5d3c7a,a6907766,"*5. Split the data into training set and testing set*

*6. Deal with problem of class imbalance with the following methods: *",86245f35,0.7083333333333334
32712,593d1d3d1df05a,31a789fd,# Training,bc682ffe,0.7083333333333334
32716,923e97b05be00b,616a016c,"In order to get the best results, we want to make sure our model isn't learning too quickly (jumping to conclusions) or learning too slowly (not making any conclusions at all). Fast.ai contains a method for us to assess that. When we begin to actually train our model, we want to plug in something called <code>max_lr</code>, which stands for maximum learning rate. We want to type in a range of values in which the loss is increasing the most quickly \[*where the derivative is the highest*\]. 

As an example, in the graph below, we'd want our learning rate to be around that red dot. We'll type two numbers in exponential format below where it says <code>max_lr=slice(number,number)</code>. Continuing with the example, we'd want to type in two numbers slightly less than and greater than 1e-01, so we'd go with <code>max_lr=slice(5e-2,5e-1)</code>

![image](https://i.imgur.com/nDdgl6R.png)

Now, we will train our model! Every time it looks through the data is called an epoch. Let's run 25 epochs -- this might take a few minutes.
>My test set of 500 positive and 500 negative cases takes ~30s per epoch, for 5 minutes total. Yours may take more or less time depending on (1) how many images it's looking through and (2) the resolution of the images.",3a4a22dd,0.7083333333333334
32721,64a336ac34d95c,3d8f04b9,# Corelation matrix after data refactoring,be73a990,0.7083333333333334
32728,386c42a7fb27a4,89fe484e,## Drinks,9e9f6974,0.7083333333333334
32737,7ba63a2d9abb58,239d3acb,<h2>Heatmap - To quantify and visualize correlation</h2>,821a261f,0.7083333333333334
32740,69ac33d79f5130,53d76974,"### Graph show 2 Gausian curve . 
##### 1) People going to work
##### 2) People returning from work",9d760d2a,0.7083333333333334
32742,56a583a039b57c,3780260b,"## There is an obvious peak at 11 August in terms of stake, boost and native balance",c0526ea5,0.7083333333333334
32744,166a62ebb4fc3a,8556f5b8,"Let's assign x and y, and accordingly split the data.",db48a079,0.7083333333333334
32746,2a377ced98d67a,d49ec5b7,## 5. Preprocessing ,262231a8,0.7083333333333334
32750,fc8e0042411c46,519d912c,- All entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.7084639498432602
32760,f0fab078f8533b,3a0ad641,## c. A function 'top_actor_or_director' with i/p parameter country can take any of the countries and attribute ('director' or 'cast) gives us pie chart of top (parameter 'top' take any int n) n 'director' or 'actor' in that specific country,bdb5ea32,0.7090909090909091
32761,5a8c553e21c70f,06986f18,"## Bootstrap Samples

**Scikit-learn** **resample** function can be used to get **bootstrap** samples but it does not give the **out of bag (OOB)** samples. **OOB** samples are the ones that are not chosen during resampling. **OOB** samples are used for validation. 6 bootstrap samples for 6 ensemble members are created with **custom bootstrapping function** below. Each sample has the same size with training set. Two empty lists, one for features and one for labels, are created to store **bootstrap** samples and two empty lists are created to store **OOB** samples.",9ebd9d8f,0.7090909090909091
32765,957e035ba5b9d5,5ccc014e,## Display confusion matrix,778ab3d3,0.7092198581560284
32768,c09fac3c943d51,196a336e,Function to tell us the score using the metric we actually care about,678d076d,0.7093023255813954
32770,49ee86d074de69,c66eba2f,### Manually Check The Accuracy,71ccc6d3,0.7094017094017094
32771,b86bda7afe3ac3,324d84fe,Another score,16197934,0.7094017094017094
32772,63b44c85e32c1f,868292be,"27 when multiplied by 2 yields 54, But when multiplied with a tuple the data is repeated twice.",fb9b9562,0.7094594594594594
32773,ad26c020235dfc,b5c0b145,Plot the timeseries with missing values.,bf766e48,0.7096774193548387
32778,0d9a2067267ba1,8a7c8a95,"### Label encoding
we will apply the label encoding both in the **categorical** features and the **discrete** features (features with finite values number)",abc194fb,0.7096774193548387
32781,b90ef792fd07c2,0629325f,# Adding Custom Metric in PyCaret,5e2762eb,0.7096774193548387
32784,b59b5aaeedb1fb,6579c863,### multivariate,1ad63faf,0.7096774193548387
32789,57070ad5e0f94f,db1f7d12,# **Checking the Accuracy on Some Data**,d97edc41,0.7096774193548387
32798,0cb456a5456cf9,c1c53e3e,"# **The performance of logistic regression is not satisfying because the accuray is low: 10196 clients who canceled the reservation are misclassified as normal clients. For this model, we only take numerical features into consideration but ignore categorical one, it's necessary for us to choose another model and make some improvements** <br> 用logistic回归训练出的效果不好，有大量的退订用户被划分为正常用户，因为在这个模型中，我们只考虑了数值类特征但是没有考虑分类型特征，所以我们需要建立另一个模型并做一些改进。",5701729c,0.7096774193548387
32799,ee23a565163388,39ab4bd0,## **Logistic Regression**,88aacbc4,0.7099236641221374
32801,83df814455f06c,2d26f51f,### Visualize decision-trees,c9cff71a,0.71
32806,0e2a23fbe41ca9,31b00b1e,"Observations:
- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10
- it seems to be between 0 and 10
- Need to handle the outliers
- there is not apparent pattern between the columns though",64e4762c,0.7101449275362319
32807,ea4e559a86d613,8aa13783,**B& W view**,eff47843,0.7101449275362319
32808,9d9da6c439b96b,1b7affc6,## Preprocessing,361cc7d9,0.7101449275362319
32811,598b6228760590,e261160e,"- **As the data set increases, the model has three stages: over-fitting-under-fitting-over-fitting.** 
- **We need to increase the intensity of regularization for overfitting. Let's start tuning parameters.**",be30ab66,0.7101449275362319
32816,31b564f11ef638,ee235c98,### Lasso Model (Apply Gridsearch),424f9692,0.7105263157894737
32818,a1dcd92986bc84,253c62e2,"### Generate embeddings for the images

We load the images and feed them into the `vision_encoder` to generate their embeddings.
In large scale systems, this step is performed using a parallel data processing framework,
such as [Apache Spark](https://spark.apache.org) or [Apache Beam](https://beam.apache.org).
Generating the image embeddings may take several minutes.",730acaaa,0.7105263157894737
32834,4daf6153275cbf,d29380fe,"H0: There is no difference between local cuisines and other cuisines in terms of price

HA: There is difference",51db1961,0.7108433734939759
32835,0932046e1f485d,887ba451,Making another dataset with the columns that we will need later.,218cc7a3,0.7109375
32840,d96e03a9e7c030,5c747c5e,"Let's re-create the maps we saw earlier, as well. There are a few straightforward calculations to create the data set that we'll visualize. Somehing important to keep in mind is that the `actual_<>_testtakers` columns broken out by demographics (e.g., `actual_testtakers_black_hispanic` and `actual_testtakers_below_poverty_lvl`) are calculated from two known numbers - the number of actual testtakers at a school * percentage of students that are in the given demographic. I do not know of any public resource to find that data directly.

Similarly, the `predicted_<>_testtakers` columns broken out by demographics are calculated by the number of predicted testtakers at a school * percentage of students that are in the given demographic.",d2b72ced,0.7111111111111111
32841,7341f069d9b2ee,a8c1cab5,Transformation Pipelines,e0a49e62,0.7111111111111111
32848,e0a041e5e2372f,fb4b8bc5,"##### Standard value recommended for parameters (Si) - 

Dissolved oxygen, mg/L               10

pH                                   8.5

Conductivity, µS/cm                  1000

Biological oxygen demand, mg/L       5

Nitrate, mg/L                        45

Fecal coliform/100 mL                100

Total coliform/100 mL                1000",7c4357b2,0.7111111111111111
32851,3597174a998d4d,b59b9c22,#### 2.2.2.4 special requests,276892ed,0.7111111111111111
32854,d905cde3391d2b,0414f0b4,Let's verify the result using `pandas.DataFrame.std` (Note: We're passing `ddof = 0` for population),067dba39,0.7111111111111111
32857,8ec771f5600a61,19ae9f6c,# Using support vactor regressor,48364c1f,0.711340206185567
32861,5f32117bcd5255,0c722a72,#### NEAR INFRARED,85882abf,0.7114093959731543
32862,2ada0305b68956,44fb8d55,### 121. Palette = 'hsv',133e26f4,0.7114285714285714
32870,6f1481148352e9,d13d2ca6,"**The highest number of fires was in Rio - 1009, Rondonia - 969 и Goiaa - 837.**",7cfbdb8f,0.7115384615384616
32871,95efc1ad1d3e26,fd7ad4d8,"## Loss

$$\mathcal{L} = -D_{KL}(q_{\phi}(z|x)||p(z)) + \log p_{\theta}(x|z)$$
$$D_{KL} = -\frac{1}{2}\sum_{i=1}^{dimZ}(1+log(\sigma_i^2)-\mu_i^2-\sigma_i^2)$$",79de1120,0.7115384615384616
32872,fc8e0042411c46,beaa018b,## I agree to pay the amount through cheque,af476c2a,0.7115987460815048
32873,1294fb4c86f993,6b7252a8,Renaming `census_2016 columns` to easy access it,4471e513,0.711864406779661
32874,dac3c8204a2d1b,7b8f4a17,Extract Year wise reviews got to books. we can see here 2019 got highest reviews compare to with others reviews.,b0d2d0dc,0.711864406779661
32875,f2e5e9fb9eaaf7,69d7bed8,"<a id=""6.1.3""></a>
### 6.1.3 Catboost Classifier",048e0d08,0.711864406779661
32880,a81661cc35d8d2,f60cc33c,"<b><font size=""4"">Methodology</font></b>",3331f113,0.711864406779661
32881,a44368590e878a,e3e9bbb1,"**Columns**


1. **patient_id** the ID of the patient
2. **global_num** the number given by KCDC
3. **date** YYYY-MM-DD
4. **province** Special City / Metropolitan City / Province(-do)
5. **city** City(-si) / Country (-gun) / District (-gu)
6. **latitude** the latitude of the visit (WGS84)
7. **longitude** the longitude of the visit (WGS84)",77743ba8,0.711864406779661
32884,c4bca5d86a38c3,35b486f3,"Haciendo normalización de las columnas que lo requieren: Age, Fare y FamilyMembers",e23d297c,0.711864406779661
32886,ed8009f482b380,e4a1d1b7,"For this project we're gonna use Logistic Regression, Random Forest, KNN, and Gaussian Naive Bayes and then we're gonna compare the scores of each model.",e99941fa,0.711864406779661
32888,f166950fa915f8,578d85f1,### Evaluate,a7f6ca5e,0.7121212121212122
32891,596389bed473be,c40c3104,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html,5f8af156,0.7123287671232876
32895,91473a39b85068,bb82a2e4,### Splitting into train and test set with 80:20 ratio,6e3d91c2,0.7123287671232876
32898,5ffe6aa38958a1,1314b0f8,# 4.2 Training the model,11f5412e,0.7125
32901,d5f78aa381f58d,af8ae357,## Random Forest,d60f358f,0.7126436781609196
32905,4c47839b067546,6cc547e7,Линейная регрессия не показала никаких улучшений. Поработаем с выбросами и проверим результат.,1f517b02,0.7127659574468085
32907,f6648e47713411,b2f0fa50,## 3.1 Load model if exists ,f4af4d1c,0.7127659574468085
32908,ab6da5994949a3,11797122,"# Naive Bayes Classification Model
## Fitting Naive Bayes classifier to the Training set",fae6b91d,0.7129629629629629
32909,2f0f808765fc67,a2158794,# **E2. Decision Tree with Pruning**,fd1f6494,0.7129629629629629
32913,c65a65d4041018,38d9b679,### ML Frameworks,824fb229,0.7132352941176471
32914,7f74a04ae75792,cfc87101,Most customers annual incomes are around 50000 $,d01e91da,0.7132352941176471
32920,30fdc4a6e3c1db,603d080e,### 4.3 Across Months,6111ddee,0.7134502923976608
32922,b01ee6cb674fa3,0589fd6f,"# Moscow Institute of Thermal Technology - MITT

Корпорация Московский институт теплотехники

a public russian agency",a8ffd35e,0.7137681159420289
32924,ffdb3fe29f4755,fc65d116,"# Part 2
**Additional calculations and visualizations**",019b2e69,0.7142857142857143
32926,c98a0dbd5eb6d7,901211d5,We now calculate the number of tests per confirmed cases (positives) for each Country/Region,33c15d04,0.7142857142857143
32932,324c699253abc2,b286edbc,It looks like the first component describe pretty well most of the variables.,7e81e44e,0.7142857142857143
32936,9c044fa3072552,b69ed268,"Let's look at the monthly distribution

",1362842e,0.7142857142857143
32940,f35bf4df70d310,c7134537,"#### Testing

Independent variables : Data variance

Dependent variables : Training score, Testing score, Train time",10bb859a,0.7142857142857143
32946,7a058705183598,d34be08a,5. K Neighbors Classifier,b0ead917,0.7142857142857143
32947,31268b33de97b5,65ee2eef,# Comparing two different variables,1e6f7d14,0.7142857142857143
32948,87e94f864d74be,84f154d2,> TV Show are more for the kids and teens.,294bfe9f,0.7142857142857143
32951,066c5ee1ef39e6,78289e06,## Validate the Pipeline,0f394e1b,0.7142857142857143
32954,f1e162ddd14f11,f40ccd1e,the array shows the number of different decision trees that will be used in RandomForestRegressor,cdb2e771,0.7142857142857143
32955,b290039151fb39,a8c936c7,The code below modifies fast.ai MixUp calback to make it compatible with the current data.,1836a79c,0.7142857142857143
32962,ffcfb4bb9a5812,0df92c31,# Lets gooo 🚀.... We got a model with testing accuracy 99.7%,477e0eb5,0.7142857142857143
32964,241cf32abb22d8,b0262361,"The optimal Decision Tree model based on the full features has a mean AUC score of 0.956 using gini index. It has a maximum depth of 3 and minimum split value of 2 samples.
The optimal Decision Tree model based on the top 10 features has a mean AUC score of 0.963 using entropy. It also has a maximum depth of 3 and minimum split value of 2 samples.
It seems the performance of KNN models does not improve much after feature selection, but I will confirm it with a paired t-tests later.",47157066,0.7142857142857143
32965,84d1ef55b89e17,a70f920d,**Test**,2d0b9d51,0.7142857142857143
32966,55339ceb40d5e9,59c16222,"# Multivariate Analysis
* numeric variables: correlation and pairplot
* categorical & categorical: grouped bar chart
* numeric & categorical: box plot and pairplot with hue",d0f687dd,0.7142857142857143
32970,f4514ec092a771,deeb362a,"There will be the utterance_id<whitespace>transcript, and if the transcript is an empty string, we will consider it as silence sound.  
Let make a sample of it! We will create a *decode.1.log* file inside *log/* folder.",3739ab1e,0.7142857142857143
32985,bbaa07ad21cf4e,ad3a1c76,### Stochastic Gradient Descent Classifier,3ab6b254,0.7142857142857143
32986,400bbcc496138f,f2e2da31,Adapting the threshold per category,191b86b8,0.7142857142857143
32988,0c57e3132ae184,d12f758f,#### Use longitude and latitude as geographical features instead of categorical variables neighborhood (which are of high ordinality).,f6bac298,0.7142857142857143
32991,0fa9979b5690e9,5973a39b,"Os dados agora estão todos muito próximos de 0 com desvio padrão 1, e isso melhora a disposição gráfica como pode ser visto na imagem. Não há mais tanto aglomeração, e existe separações lineares mais visíveis. Isso são fortes indícios de que agora um método de aprendizagem terá melhor desempenho. No entanto, enquanto para visualizar o StandardScaler foi aplicado em toda a base, é importante perceber que ao testar o modelo, **o StandardScaler só pode ter dados de treino no fit**. Ou seja, ele não pode conhecer a disposição dos dados de teste.",c26eea94,0.7142857142857143
32998,4b4117cf42ef8d,e51a8720,### 2- Lasso Regression,457cd6f4,0.7142857142857143
32999,4ae6a182abac64,57e9a7f2,### 3.1 Separating the independant and the dependant variable,418676c5,0.7142857142857143
33004,f50dc95483c98f,80be2a58,## Predicting the Test set results,cd9e9621,0.7142857142857143
33008,764c279bb8005c,7cd85601,## Model Building,78f93ad0,0.7142857142857143
33011,e16860fce156b0,5e9a50f9,"#All the correlation values that lie within the given range. (-1, 0.3) for apache_2_diagnosis will appear in the plot.",2054f1ce,0.7142857142857143
33012,f4b603905215b7,1a22cc0e,# 4. Logistic Regression with balanced class weight,efe1d587,0.7142857142857143
33015,98fd05fcc5c3e3,4ff8a3cb,## Selecting correct number of Clusters using Elbow Method,55fe7ece,0.7142857142857143
33024,be357c1e2c975d,8e69890c,### Task 6: Train and Evaluate Model,2486a061,0.7142857142857143
33028,3a6274ed72cc00,303e3bf5,We have no missing values.,51369a2a,0.7142857142857143
33030,a758983a68c014,ae301cf7,Initalization of model hyperparams.,ab89f181,0.7142857142857143
33032,1a285e4c830f3f,97da4bfa,- Interpretieren Sie den obigen Plot. Welche Aussagen lassen sich machen? Was empfehlen Sie für den weiteren Projektverlauf?,360b50e9,0.7142857142857143
33036,38b79494ac749e,7026b1ba,## Part 3: Homework assignment (10 points),39162a40,0.7142857142857143
33039,3879ef16f5eb28,249c98ea,# Area plot,784b8d8e,0.7142857142857143
33040,7454fdc444df16,57188745,"Above, we created a function using incremental PCA, which uses the exact same technique as PCA, however instead of loading the whole array in memory, it loads it in batches. After some experimentation we opted for 10,000, rows per batch, anything more than that and we risk running out of memory. Furthermore, we opted for a maximum of 120,000 rows to be fed into the function, and again this is for memory management purposes.",a7818ef5,0.7142857142857143
33042,099311d5463909,54780fc3,Command to remove moved images,71c095ce,0.7142857142857143
33043,06ecf7a304c309,18540a62,- 노이즈를 없앤 오토인코딩 후 검증 데이터,714de627,0.7142857142857143
33044,6a80f915608fc2,5f7dc768,"### End of looking at Targets and Features
Back to <a href=""#Index"">Index</a>",636938eb,0.7142857142857143
33048,ffd1df95ca5289,2221edcf,## Logistic Regression,db00c338,0.7142857142857143
33055,1c381451c17150,85a41da1,"# Save the Model
Training is done, save it. This is also a great place to load any pretrained models before generating new text.",e79b530f,0.7142857142857143
33056,6b65d81a5743dd,0027b3f4,Not much difference between the two targets when it comes to participation score,4080a2d2,0.7142857142857143
33060,e69a496109e7d8,df48aad7,# Bi-Variate Analysis,1c640591,0.7142857142857143
33066,2730840089c8eb,df5f3acd,"In this case `'one'`, `'two'`, and `'three'` are the **keys**, and 1, 2 and 3 are their corresponding values.

Values are accessed via square bracket syntax similar to indexing into lists and strings.",34d27dac,0.7142857142857143
33068,e78e7edae89049,0d2b600d,# Facebook Prophet,9cef1d94,0.7142857142857143
33070,0dd3ac2d55efd7,64a0ad65,"Do go through the below tensorflow tutorials if you want to get a full idea on how RNNs, GRUs, LSTMs and Bidirectional layers work in the background. You will also get an idea of how to use back to back LSTM layers by calling return_sequence=True and use of return_state and much more.
1. [Text classification with an RNN](https://www.tensorflow.org/tutorials/text/text_classification_rnn)
2. [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)
3. [Masking and padding with Keras](https://www.tensorflow.org/guide/keras/masking_and_padding)
4. You can find a research paper [here](https://arxiv.org/abs/1512.05287) if you want to know difference between dropout and recurrent dropout in RNNs.",e9aa2cc2,0.7142857142857143
33072,b6e698d389d0d3,5e153dae,# train model,f02f68b5,0.7142857142857143
33074,b8849a04581d32,907ed240,"#### The assigned task with the selected data set belongs to the classification task. Since there is a set of characteristics for each outcome, their combinations during the match ensure that the desired outcome of the Punt is played.",b8a568cd,0.7142857142857143
33078,3c2033cc99c12c,e8bb090e,### Support Vector Machine ,dfa22a54,0.7153284671532847
33082,a4f8ad33c823c5,d3c1a35e,"# Dealing with Class Imbalance Issue

Resampling is one of the methods to deal with highly unbalanced datasets. Upon further research, 
resampling would involve removing samples from the majority class (under-sampling) and adding more examples from the minority class. 

Oversampling would duplicate random records from the minority class and that might cause overfitting.

In undersampling, the simplest technique involves removing random records from the majority class, which can loss of information.

Resource : https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets",fcd48307,0.7153846153846154
33083,e19e307b3fd188,b654f063,#### Select FEATURES (X),2173955b,0.7154471544715447
33090,52cfd66e9ec908,7fb9983a,And how much they have changed from the original:,c74adcdf,0.7156862745098039
33094,0a918602a04693,23bfc8b8,# Data ,c1ef0e95,0.7159090909090909
33099,e4525eb0c96f28,4e23421e,"As you may have noticed, some of the plots above show that the general linear trend of the data for each ESRB, genre, and country value share the same pattern as the overall data: right-skewed, non-normal, unsymmetric, and uneven varianced distributions for each year. However, because some of the plots for a few of the categorical values actually seem promising, we would still like to try adding these values as interaction terms just to see if it makes anything better.",2093a1f1,0.7162162162162162
33101,62037c5832129c,3d71bc25,"* Note that the (true) class 0 samples that are correctly predicted as class 0 (true negatives) are now in the upper left corner of the matrix (index 0, 0). 
* In order to change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positves are in the upper left, we can use the `labels` argument like shown below:",61474350,0.7162162162162162
33104,b61ab8f81dc03d,64eac42d,"<a id=""model_comparison""></a>
## Model comparison",64d05394,0.7163120567375887
33105,56785caebaa256,3636a146,"## 5.2. Stage 2 - Tuning seasonality parameters<a class=""anchor"" id=""5.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.7163120567375887
33117,bc058fe14d3d1b,66349cb6,shop_item_id,d0273670,0.7166666666666667
33119,cf4d1c1ad1476c,6c2b1ad8,"# Creating Train , valid and Test Data loader",768c1a59,0.7166666666666667
33121,712198370d5521,882adb68,"<a id=""7""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">EVALUATING MODELS</p>

Since this is an unsupervised clustering. We do not have a tagged feature to evaluate or score our model. The purpose of this section is to study the patterns in the clusters formed and determine the nature of the clusters' patterns. 

For that, we will be having a look at the data in light of clusters via exploratory data analysis and drawing conclusions. 

**Firstly, let us have a look at the group distribution of clustring**",5882e04c,0.7166666666666667
33123,62487bcd70b199,22b13c8f,## <a id='8.1.3.'>8.1.3. GaussianNB</a>,f6ae50af,0.7166666666666667
33127,b547f0f38f7744,9857fb3e,Finetune the last layer:,b6ba66b3,0.7166666666666667
33134,614ba9f0c62677,250b8d0a,"<a id=""15""></a>
### Data Augmentation
* To avoid overfitting problem, we need to expand artificially our handwritten digit dataset
* Alter the training data with small transformations to reproduce the variations of digit.
* For example, the number is not centered The scale is not the same (some who write with big/small numbers) The image is rotated.
* <a href=""https://ibb.co/k24CUp""><img src=""https://preview.ibb.co/nMxXUp/augment.jpg"" alt=""augment"" border=""0""></a>",b8551335,0.7169811320754716
33135,4dd47072617594,308749d1,# 5. Modeling,44ff1d11,0.7169811320754716
33137,a070fd03ae8ed2,d8bdd34d,"## 6.3 Метрики, матрицы ошибок и кривые",c0ec4138,0.7169811320754716
33141,2ada0305b68956,d80059e2,### 122. Palette = 'hsv_r',133e26f4,0.7171428571428572
33144,cfcb3bdee4f1e4,524dcd98,"Embedding layer, project the words to a defined vector space depending on the distance of the surrounding words in a sentence. <br> 
Embedding allows us to reduce model size and most importantly the huge dimensions we have to deal with.",eb4ed2ae,0.717391304347826
33149,9b5de3823ad5ab,32a3b33f,### All layers' training,33e48774,0.717391304347826
33150,0e2a23fbe41ca9,28df86e9,"### 8. lag_12 columns

- ```avg_sales_lag12```: Monthly average of revenue in last 12 months divided by revenue in last active month
- ```avg_purchases_lag12```: Monthly average of transactions in last 12 months divided by transactions in last active month
- ```active_months_lag12```: Quantity of active months within last 12 months

work in progress",64e4762c,0.717391304347826
33153,ee23a565163388,16793743,Logistic Regression calculates the probabilities of an observation falling under a particular class by using a sigmoid curve.,88aacbc4,0.7175572519083969
33154,fdc3afd309b850,4bc1c5cf,"<a id=""DV""></a>
# 8 Data Visualization and Data Preparation",966bde38,0.7175925925925926
33166,d4c5aaa4b36810,1c90ce19,"Lasso regression does not improve results, the best parameter for alpha is the one that makes it most like linear regression. ",65441f28,0.717948717948718
33168,0a1fcda859252c,e3770ceb,![training](https://i.imgur.com/vVZgEKV.png),13a38774,0.717948717948718
33172,9b42412e75d640,7ee228bd,We can see from above theat the highest accuracy is for n=4. ,b616570a,0.717948717948718
33175,50b03ce5b1a286,ec317769,"#Search for the best model

We are now ready to instruct the QLattice to search for the best mathematical model to explain the data. Here we use the high-level convenience function that does everything with sensible defaults: https://docs.abzu.ai/docs/guides/essentials/auto_run.html.

For more detailed control, we could use the primitives: https://docs.abzu.ai/docs/guides/primitives/using_primitives.html

Notice that the stypes dictionary we created earlier gets passed to the QLattice here.

NOTE: This will take several minutes to complete. It invoves work done on the QLattice machine remotely as well as in the local notebook. The part that runs locally is slowing things down because of the limited CPU resources on Kaggle. Running the same on my machine locally only takes 20 seconds!",d49896a5,0.717948717948718
33176,80ad12f326ab70,f3c14a2d,"The above shows the used functionalities of the websites;
LC - Learning and Curiculum, CM - Class Room Management, SDO - School and District Operation.

Most products fall in the Learning and Curriculum category. Let's have a look at the sub-categories.",da404a16,0.717948717948718
33178,9eed0fae1c7958,59035452,# plot error and accuracy (train and validation),3fb1438e,0.717948717948718
33180,726833f92fb87a,6270938f,We can encode this variable by ordinal encoding.,7dc5e1b6,0.7181208053691275
33184,631cd434fc3aa2,1145293d,"### Feature engineering
It seems that some feature listed as numerical are actually categorical.
* _MSSubClass_: Identifies the type of dwelling involved in the sale (e.g. 020, 050, 160).
* _OverallCond_: Rates the overall condition of the house (ordinal).
* _MoSold_: month sold (we transform the month from numbers to names just for easier visualization).",2b74febb,0.7183098591549296
33186,06c7ba9203293f,bed4a16d,# Comparing numerical series,1e1a2b48,0.7183098591549296
33189,98a6794067932a,e13a3751,"La cellule de code ci-dessous permet de représenter sur une carte des États-Unis, l'intensité des expéditions pour chacun des états avec l'aide d'une charte de couleurs. Plus la couleur est foncée vers le rouge, plus le volume des expéditions est élevé dans cet état et plus la couleur est pâle vers le blanc, moins le volume des expéditions est élevé dans cet état. Dans un premier temps, ce code permet d'importer les modules permettant de réaliser cette carte et d'utiliser pandas. Ensuite, dans un deuxième temps, le code permet d'établir les différents paramètres devant être pris en compte afin de produire la carte. Cette carte prend en compte le dataframe créé avec l'aide des cellules précédentes. Bien évidemment, cette carte permet aux gestionnaires de constater rapidement les états où l'entreprise effectue ses plus grands volumes d'expéditions, ce qui pourrait grandement influencer les futurs emplacements de ses centres de distribution étant donné que cela signifie que de nombreux coûts de transports sont générés afin de subvenir aux demandes des clients se trouvant dans ces états. De plus, la carte est interactive, donc elle permet d'indiquer la somme totale des expéditions pour chacun des états.",08600fe2,0.7184466019417476
33192,2c3a6969252dc0,973961d2,"**Research experience and LOR are highly related to rest features:**
Research Experience and LOR usually mean Higher GRE Score / TOEFL Score / SOP / CGPA, eventually result with better chance of admit.",d30f10ce,0.71875
33196,96c4c0e36b8ec0,0d9b0a54,"What the graphs tell us:
* Passengers with a larger number of sibling or spouses (3<) were more likely to perish",4dd6de8c,0.71875
33197,9daf8b4a46725e,922b099b,### Decision Tree Model ,7d9cc411,0.71875
33200,3cc097a5859dc1,5f847f79,# **Feature Selection**,14380d73,0.71875
33203,6f05f4ea9addbf,08dde406,We need to make sure if the date column is either a categorical type or a datetype. In our case date is a categorical datatype so we need to change it to datetime.,dfb04c84,0.71875
33205,68cceffe5bb8ec,e4138b89,# Training time!,dcbfcd6e,0.71875
33207,49f2274c1dd516,5ff5be18,"# Harvard Global Health Institute
The Harvard Global Health institute created a model of hospital capacity and readiness across the US. This model builds on bed capacity data for each of 306 U.S. hospital markets (Hospital Referral Regions, HRR) with localized estimates of available beds, and beds needed to accommodate COVID-19 patients over the coming months. It highlights where hospitals might find additional bed and ICU bed capacity as well as other shortages that need to be addressed—from workforce to ventilators.

",06b0ffee,0.71875
33209,c5fef7cc592736,98dcec27,Saving the model before some extra epochs for fine tuning,d21dc2c1,0.71875
33211,917957c6c4065f,991edd45,"제목의 길이가 2인 인기동영상 4개입니다.  
특이한 점은 likes/views 평균이 0.02인 것을 고려했을 때, 4개의 동영상 모두 likes/views의 비율이 높은 편인 것입니다.  
17806 안재효 채널은 아이돌그룹 블락비의 멤버의 채널이라 likes/views가 높은 것으로 추정됩니다.  
나머지 3개의 동영상은 현재 비공개 처리되어 확인할 수 없었습니다.  ",55b8ed68,0.7189542483660131
33212,2f47abddfd1928,2fd2dbb0,"As we have been seen previously first and second class have much better survival rate and this is translated to the cabins 0 to 4 that are mainly occupaid by those passenger classes.

It is interesting the case of the cabin 0 that shows a different tendecy, maybe even though it is full of 1st class passengers, it was not in the optimum location to survive.# 4. Feature Engineering

## 4.1 Title

The first feature we can create is a title feature.

From name feature we can extract the title and workout different groups that can add socio-economic insights.

It is also interesting to see that the 3rd class cabins 5-6 have a kind of balanced survival rate, it means (and we can confirm) that most of the non-survival passengers from 3rd class had unknown cabin details.",ae33cc0b,0.71900826446281
33215,5f27526aa6c113,f00e31d4,full time workers claim more,a5c26ab6,0.7191011235955056
33223,6cade0b6a41ba2,f9ee7acf,# 4. Data Preparation (For Modelling),e6110293,0.7192982456140351
33224,30fdc4a6e3c1db,4832b0f6,### Plotting sales over the months,6111ddee,0.7192982456140351
33225,54004b32784b68,011a9fe5,**> Prices by Neighborhood**,27213ca9,0.7192982456140351
33226,e03eb63c1f725d,f440629e,"<a id=""12""></a>
<font size=""+2"" color=""blue""><b>PassiveAggressiveClassifier for TfidfVectorizer</b> </font><br>",e204b7e3,0.7192982456140351
33227,c2a9f2fb3e1594,3308be36,"## 5.11 Model Performance with Cross-Validation (CV)
In step 5.0, we used [sklearn cross_validate](http://scikit-learn.org/stable/modules/cross_validation.html#multimetric-cross-validation) function to train, test, and score our model performance.

Remember, it's important we use a different subset for train data to build our model and test data to evaluate our model. Otherwise, our model will be overfitted. Meaning it's great at ""predicting"" data it's already seen, but terrible at predicting data it has not seen; which is not prediction at all. It's like cheating on a school quiz to get 100%, but then when you go to take the exam, you fail because you never truly learned anything. The same is true with machine learning.

CV is basically a shortcut to split and score our model multiple times, so we can get an idea of how well it will perform on unseen data. It’s a little more expensive in computer processing, but it's important so we don't gain false confidence. This is helpful in a Kaggle Competition or any use case where consistency matters and surprises should be avoided.
 
In addition to CV, we used a customized [sklearn train test splitter](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), to allow a little more randomness in our test scoring. Below is an image of the default CV split.

![CV](http://blog-test.goldenhelix.com/wp-content/uploads/2015/04/B-fig-1.jpg)",53411c04,0.7192982456140351
33229,5ce12be6e7b90e,d7c210c3,`insert` adds an element at a given index:,c0ab62dd,0.7192982456140351
33236,9c26c5dcd46a25,75592e4e,"Les métriques pour ce second modèle sont meilleures que le premier modèle simple de régression linéaire. En effet, nous avons vu préalablement que l'hypothèse selon laquelle la catégorie influençait le Nutriscore était vérifiée.

Pour améliorer encore les performances, nous pourrions traiter d'autres types d'algorithmes comme la régression Ridge par exemple ou encore la régression lasso.

Nous allons à présent faire une réduction de dimensions afin de trouver les meilleurs plans factoriels du jeu de données.",1bbbb677,0.7195121951219512
33238,c84925c8171900,c7432db7,"<a id=""genre""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.5 Genre Wise Analysis
            </span>   
        </font>    
</h3>",e21ff7ec,0.719626168224299
33240,7e1da639035ac5,c57631ee,### <a id='13.1'>13.1 Effective School Leadership % distribution</a>,120b6c23,0.72
33241,4cd25e50c7e007,8737f651,##### We have all the variables with p-value less than 0.05,ceb0c525,0.72
33247,7dd46c750653eb,24913f75,**Divorces over the years**,c2644713,0.72
33250,0687cd5c8597db,91e62468,## <center> **Model Evaluation** </center>,4edec76a,0.72
33253,5cb7f999fd1ecb,28fcc1b8,"# Plotly - 2D Histogram ( Interactive )
### Effect of Type on paint color",88b54f70,0.72
33254,81712ee7510ac5,7fe5c2b6,**We can do with the strings also**,c4685e79,0.72
33259,639e8aae4e046e,99557fb1,**Regression: LGBM**,77deb4cb,0.72
33261,274b32da3b19a8,70cd7f3d,## Prediction,408f7268,0.72
33262,9395559895004f,c55f8939,## Prediction,b5a0494b,0.72
33266,2bd6c370695ea7,d89eaa18,## Category Embedding,cbe6aec8,0.72
33267,bbad077c274022,f43d12df,"**Let us find my walking central cendency for both the types of steps. Also, I wanna check my steps' spread.**",3c2e3dea,0.72
33268,91eaec994e0c6f,aa0fcb36,## 3.1 Time Series Selection,376aef10,0.72
33275,6a80f915608fc2,45c66b04,## _ _ _ _ _ _ _ _ _ _,636938eb,0.7202380952380952
33276,eda49464dd6d1b,345bf8c4,### It appears there were 2 policy sales channels present in test but not in training.  These also need to be removed.,8421f81f,0.7202797202797203
33279,0caaec057f7184,7895ab4f,We can apply the history data of monthly item average sales of the category for the kind of data.,b875533e,0.7204301075268817
33283,eb0ecd6bebeb15,8ecc2ef7,Bu sorunun yanıtını pekiştirmek için iki değişken arasında korelasyon katsayısını yazdıralım. ,d7b93a60,0.7205882352941176
33284,3d905ce4828057,54c00f74,"# MISSION 2

**1. Calculate 1-month and 12-month CLTV for 2010-2011 UK customers.**

**2. Analyze the 10 highest individuals at 1 month CLTV and the 10 highest at 12 months. Is there a difference?If there is, why do you think it might be?**",5b006cc3,0.7205882352941176
33285,9cec5ddf8b6f49,e47e24dd,### * To get the best observed value of the objective function:,d39fc8e7,0.7205882352941176
33291,b2e2c792b886ac,1c807858,### Ну и что теперь со всем этим делать?,2a184b39,0.7209302325581395
33292,806ce45c8fa303,f276e7eb,"We can observe that the encoded fraud data points have been moved towards one cluster, whereas there are only few fraud transaction datapoints are there among the normal transaction data points. 

## Split into Train and Test",3e5c34dc,0.7209302325581395
33293,1660daf8867980,a3c15550,**Demonstration**,42d7cffc,0.7209302325581395
33294,2e40928927c0d4,53718b49,"**keras Callbacks:**
Defining callback for EarlyStopping of training if the result is not significantly improving through some mentioned number of epochs. Defining callback for Reducnig learning rate on Platau regions of the underlying cost function.",b6385ef2,0.7209302325581395
33296,743ae010f5e875,1b94b13e,# Baseline Model,02c54445,0.7209302325581395
33298,22ba3a8149c2f1,1c3ea84f,## 2.2 model 2 - densenet,19c82be5,0.7209302325581395
33300,8539260444e6b5,f795fdf9,# TF-IDF model,0369463f,0.7209302325581395
33302,b01ee6cb674fa3,dd106f3e,"# Lockheed

A private US company",a8ffd35e,0.7210144927536232
33303,44f6a002ecd033,cd3711e3,## Modeling the Data,70bbe106,0.7211538461538461
33305,99bf357eaf61f1,5ab1f558,# Data Splitting,9d92fafe,0.7211538461538461
33307,0858e1bb3cbaca,30eb883f,"To count how many items are in the groups, we can use the function

**.size()**",78548374,0.7213114754098361
33314,063a35f644e3c5,0e5663a2,### Feature Selected Columns,1c30fb0a,0.7216494845360825
33317,225b4fe5d3894a,1316a794,"The above model is highly overfitting, it recalls every value from the training set",4b4197b3,0.7216494845360825
33320,ba4b3bd184acbb,2f69462e,"### Append

The `append` method can easily concatinate rows, providing a subset of the `concat` method's functionality.",0f5de724,0.7218045112781954
33323,fdc3afd309b850,d1e92184,"To help us understand and prepare the DataFrame for the modeling we are going to visualize the data.

 * Firt, before we drop latitude and longitude, we are going to plot the apartments on the map.
 * Second, we will plot the DF as Box Plot and look for some inaccuracy.
 * Third, for understanding the normalization, we are going to plot the distribution of our continuous variables. If necessary we will normalize and Scale them.
 * Fourth, to visualize the personal correlation we will use a Heat Map.
At the end of this session the data will be ready for the modeling.
",966bde38,0.7222222222222222
33326,18a864b56ac3b8,bf45ec49,"# Once more with feeling!

Notice that with just this step the error rate of the model is already well under 10%. Now to train the model on the images that are full size to create the final model",f3ca0a7c,0.7222222222222222
33329,3ac432b2cac29c,328c8925,"What do you notice that is different from what you saw with in-sample predictions (which are printed after the top code cell in this page).

Do you remember why validation predictions differ from in-sample (or training) predictions? This is an important idea from the last lesson.

## Step 4: Calculate the Mean Absolute Error in Validation Data
",a358669e,0.7222222222222222
33334,3597174a998d4d,a9438209,"In this dataset, booking_changes, required_car_parking_spaces and total_of_special_requests can be used to describe customers' special requests. It can be found that:
* Most bookings have no sepcial requests and haven't been changed. Once the bookings have sepcial requests or are changed, their ratio of cancelation are low.
* The ratio of cancelation of bookings which require car parking spaces is 0.
",276892ed,0.7222222222222222
33336,d6cbd7160961dc,880f5179,## 5.3.3. Results: Third Digit,36d74664,0.7222222222222222
33341,c01049afb6d307,b76fe391,### (6) Diseases of the nervous system,d37d3b5d,0.7222222222222222
33342,95d896e75f9a50,f1149a12,"Cool - makes sense that these features split the data perfectly. Seems like an opportunity for some feature engineering here as well!

### Pattern 2 - Weak Predictors",2721b6f5,0.7222222222222222
33347,1014e6be391084,644a7614,The above shown scatter plot does not show any kind of proper pattern for the Fraud transactions,46f9168f,0.7222222222222222
33348,49ac6594c8f5cf,a2661075,KNN Model,6f19f28a,0.7222222222222222
33350,df2a7968c08ee4,d9305e13,"### Training Loop

This is where Pytorch really differs from Tensorflow. 

We have to define the training loop in a much more pythonic way. I have provided comments throughout the following cell which explain the components of the training loop.

I found the Pytorch Tutorials really helpful when building the training loop --> [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)",a2ba0a72,0.7222222222222222
33355,dd3721cb49c1fd,b7144fc2,"<a id='9'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center"">9. <b>Analyse the forecast Dataframe</b></h1>
  </div>
</div>",1a53fdd9,0.7222222222222222
33363,e0f03003a69819,1935e96f,We do not have any missing data. #fairytale,609ad1f4,0.7222222222222222
33368,a4a494c667c673,8594da3c,# Training,397d12f8,0.7222222222222222
33370,cf39cde80e66b7,aecebe71,"![](https://miro.medium.com/max/888/0*-lBX506Imc6Hjqpu)

> R² and R-Squared help us to know how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.
",aed4bc9b,0.7222222222222222
33375,5d6d539f8e7121,b45a342e,## 풀이,79340a85,0.7222222222222222
33377,268a610bbc64b4,a8f879c8,It is evident that most of the customers book only 1 month prior.,8a16f301,0.7222222222222222
33380,fbb1f9d3818830,d6310b6b,"# Main function

All the operations are explained in the comments",c7027f86,0.7222222222222222
33382,cb4ad8ed4cb300,ba6eb1e2,# 6. Test Function with Some Questions,7c0f3236,0.7222222222222222
33383,caee5b3bdf65c1,8adbc46e,# K-Means Clustering,a46111dd,0.7222222222222222
33386,c80939c7c626cf,4af84c5c,# 8 Family Size,b9ac31e2,0.7226277372262774
33387,3c2033cc99c12c,537f7c9a,![image.png](attachment:image.png),dfa22a54,0.7226277372262774
33389,c4386b8a01d66e,d179a98f,# Data Preprocessing,dc732bf5,0.7226890756302521
33390,2ada0305b68956,026e0cbe,### 123. Palette = 'icefire',133e26f4,0.7228571428571429
33393,63b44c85e32c1f,552d3525,Values can be assigned while declaring a tuple. It takes a list as input and converts it into a tuple or it takes a string and converts it into a tuple.,fb9b9562,0.722972972972973
33398,03048e86a6d806,d126076f,"As a start, what do the data folks consider as the most important part of their work? ",1285c231,0.7230769230769231
33401,09751c520b0616,77a7b65b,- <b>SalePrice</b> variation through out years according to year features,a4d0c7e9,0.7230769230769231
33402,f2f2db16a2f86c,d976ed7d,### **Predicting Test Set Results**,ffc6a115,0.7230769230769231
33406,b10bd75889dad9,f908aeb9,#### Here also we get the very good test and train accuracy score,ee00ceee,0.7233333333333334
33407,4c47839b067546,eeebc596,### Работа с выбросами,1f517b02,0.723404255319149
33408,56785caebaa256,9ae948f8,"## 5.2.1. Model training, forecasting and evaluation<a class=""anchor"" id=""5.2.1""></a>

[Back to Table of Contents](#0.1)",a792961a,0.723404255319149
33409,3f25b363afec54,5f8bef6f,### Imputing missing values,bbdaae25,0.723404255319149
33412,04e6b0d3c70f46,3af8b067,### Normalize features,56344f77,0.723404255319149
33413,73893f0467d5e3,badcc9d7,## Correation ,279787c6,0.723404255319149
33414,5f674175839b32,85c27eec,*PS2 has done the best sales globally.*,53a2e343,0.723404255319149
33415,957e035ba5b9d5,5606f2d2,**Note**: The first two categories seem to be close together in style and therefore have some overlap.,778ab3d3,0.723404255319149
33418,52ee792e228d54,e0abc5c3,"### Let's try building different classification models to fit our data and test the performance.
### Logistic Regression",5096094e,0.7236842105263158
33419,04bac111ffbe9c,ae2ff482,## Creating output file,82576b17,0.7238095238095238
33423,55a5e31d03df9f,91c75edf,"As you can see with Average Pooling we reduced the number of parameters we're going to train, and since we're taking the avg for the dimensions model is less likely to memorize and would be inclined to generalize more.",06dce00f,0.7238095238095238
33426,84127ade6fde87,3f8c5f8b,"Well, if we were to design a solution to this problem by hand, we might decide to build our embedding space by choosing to map basic nouns and adjectives along the axes. We can generate a 2D space where axes map to nouns—fruit (0.0-0.33), flower (0.33-0.66), and dog (0.66-1.0)—and adjectives—red (0.0-0.2), orange (0.2-0.4), yellow (0.4-0.6), white (0.6-0.8), and brown (0.8-1.0). Our goal is to take actual fruit, flowers, and dogs and lay them out in the embedding.",f55d05b6,0.7241379310344828
33428,5fc2f23dfbeeb1,a6bf6c8c,### Create Model by Using Multinomial Naive Bayes from Sci-Kit Learn,f37b4110,0.7241379310344828
33433,858da4bb312f67,7e4118a5,### 1. CBB <-> HEALTHY,9cca4391,0.7241379310344828
33435,5ea840754577e3,f89b923a,## Feature: Fare,9cf9b73f,0.7241379310344828
33438,00001756c60be8,b69a4f9b,Создания класса подготовки данных,945aea18,0.7241379310344828
33440,ef6d1e959a873e,f5b26b15,Let's convert data back into train and test data sets. Its generally a good idea to export both of these as modified data sets so that they can be re-used for multiple sessions. This can be achieved using following code:,f11a1f43,0.7241379310344828
33444,00d295edcd117e,b7df78c4,## 在测试集上测试网络,f5810f4b,0.7241379310344828
33450,fb5c6021d127ef,c8b666c7,Write your query below:,dd05cbd3,0.7241379310344828
33452,fc8e0042411c46,0b19372e,All entries are 'No'. No Inference can be drawn with this parameter.,af476c2a,0.7241379310344828
33457,eb800c50fcfbb2,6bcd3674,# Summary of prediction results,e7173f4d,0.7241379310344828
33462,6b54e39f86bdb5,321406b5,"## Prediction on a single example

We can compute the inference time on a single example and check if our model work properly. Also, it is time to enjoy our work and see that we have done a good job at classifying handwritten numbers.",198084bc,0.7241379310344828
33463,6a1d04e8153df3,48af9437,"**Observation(s)**
- The survival rate are 25 percentile of thershold age is 43 ,50 percentile of thershold age is 53 and 75 percentile thershold age is 60.

- The death rate are 25 percentile of thershold age is 47 ,50 percentile of thershold age is 55 and 75 percentile of thershold age is 61.",38572b05,0.7241379310344828
33468,ce9ed5e2d601d7,ccefe27e,Merging pseudolabels with prediction,f58a2f43,0.7244094488188977
33471,8336d84cf3ff6b,575be9b5,"# The best paramerts from RandomSearch CV

> {'subsample': 0.8000000000000002, 'reg_lambda': 0.5, 'n_estimators': 100, 'min_child_weight': 11, 'max_depth': 10, 'learning_rate': 1300, 'gamma': 2.0, 'colsample_bytree': 0.6000000000000001}",b96b58a0,0.7246376811594203
33478,7e89d387feb9f5,6af2fef7,"### Добавленный числовой признак №15. Количество дней, прошедших между последними двумя отзывами",989e3a1b,0.7246376811594203
33483,5f32117bcd5255,124a3f04,#### HST OPTICAL ONE,85882abf,0.7248322147651006
33485,3dd4294f903768,6b5b6a0e,We can see that we have some features that have to be encoded in order to fit the machine learning algorithms (the scikit-learn library can't get any text).,0d89d098,0.725
33490,9a040a4f21091e,01ded7fe,"So why did the percentage of words with a toxic coefficient actually decrease? Observe how the tail for the positive end of the distribution has grown fatter; this means that this model has actually found more words that surely must be toxic, as opposed to before where the vast majority of words were somewhat toxic and somewhat non-toxic. ",f591b57d,0.725
33494,5ba4207c371899,94f875d6,Machine Learning model fitting,187b1451,0.725
33496,0d58c434c7db1e,facaa57c,"Polgar, Judit from Hungary has highest Standard, Rapid and Blitz rating.",517e01d3,0.725
33499,1011899b959f44,443e8e9b,9. Use .groupby() to find the average major capture per battle.,0b112382,0.725
33506,5f4ae633cfd090,83961c17,"Before moving on to the rank variables, the _Stance variables need some encoding. I'll encode them as I visualized them above.i.e. Orthodox is superior so it will be 4, Southpaw after that will be 3, Switch, 2, and Open Stance 1",a30a16e2,0.7252747252747253
33508,fa02c409161192,ff9ce027,Below we will plot the relationship between number of hidden nodes against performance. From the plot we can see that the NN does extremely badly for smal numbers of hidden nodes but plateaus at around a $100$ nodes where the performance levels out. Note the convergence of the performance to $100\%$ and that the number of hidden nodes has to be exponetially larger to improve the preformance at the high end.,e97077f7,0.7254901960784313
33512,842547b2def18c,a1010aec,And the test dataset.,b8efde6d,0.7254901960784313
33517,71b75664517244,e6ef7485,"This is a comparison between Alex Ferguson performance and his team Manchester United performance. Their performance chart is exactly same, which os why we only see one kind of line here. This is a proof how bad MU condition without Alex Ferguson as their manager.",fc905af5,0.7254901960784313
33518,917957c6c4065f,c6229cd3,### 2.8. description_length,55b8ed68,0.7254901960784313
33522,a5a419dc7245b0,e86902d7,##### Initialising the ANN,4279726e,0.7256637168141593
33523,ac1abfe1dfe815,685fbdaf,## TF_DIF,6529dbcb,0.7256637168141593
33537,87e94f864d74be,0fbe6d9d,"I learnt the above visualization from @subinium , reference to his <a href=""https://www.kaggle.com/subinium/simple-matplotlib-visualization-tips""> notebook </a>",294bfe9f,0.7261904761904762
33538,565ad413cd802f,dd0ad7af,"## Making predictions & submission

To start with, let's create a helper function to make a prediction on a single image.",397b074e,0.7261904761904762
33539,6a80f915608fc2,db6b5583,"## <a id=""MachineLearning"">Machine Learning</a>
Back to <a href=""#Index"">Index</a>

Setup a classifier to decide MoA or not MoA,
using the general structure from my [""Titanic Confusion-Dots Plot""](https://www.kaggle.com/dan3dewey/titanic-confusion-dots-plot) notebook.

(up through v18) Note that the y=1 case is ""notMoA"" -- we're trying to identify sig_ids that we can be sure have no MoA targets selected, the precision, TP/(TP+FP) = ""what fraction of claimed notMoA=1 are correctly identified"", measures how well we're doing; this can also be seen in the ""confusion dots"" plot. Conversely, setting a lower threshold and looking at the ""negative precision"", TN/(TN+FN), shows how well we're doing at detecting sig_ids that are likely to have one or more MoAs active.

(V19 etc.) The y=1 was changed to mean numMoA > 0; in this case the precision is a measure of ""what fraction of claimed MoA>1 are correctly identified.""",636938eb,0.7261904761904762
33541,f91f58d488d4af,245f7a94,"Randomly Initialize weight for every pixel.

The function *weights x pixels* won't be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its intercept is 0).

You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We'll initialize it to a random number too.",5df1bbf3,0.7263157894736842
33549,a566b5b7c374e7,c0966c8f,## Average Resting Heart Rate,b3dc5545,0.7266187050359713
33551,91eaec994e0c6f,d700acc3,"We will choose:
- 3 Time Series with enough data and few zeros.
- 3 Time Series with many zeros.",376aef10,0.7266666666666667
33553,fdc3afd309b850,2048beb0,"
<a id=""mp""></a>
## 8.1 Map plot",966bde38,0.7268518518518519
33556,fc8e0042411c46,aa34ed32,## A free copy of Mastering The Interview,af476c2a,0.7272727272727273
33560,a0b321057e7402,70b1fff4,Lets start training.,5f73fb91,0.7272727272727273
33562,dbd96dd275dc60,957b20ea,"## Train a model with the best HyperParams
**Note** These were found after a 100 iterations of RandomizedSearchCV",1ed493a8,0.7272727272727273
33563,5083d7a61f2426,d6cff0f9,predicted test data and the original test data,541a0fec,0.7272727272727273
33568,bb3d1b4b9f1248,7c60955c,Average PerCapita analysis 2019 vs 2020 for spirits consumption,bf7de324,0.7272727272727273
33569,f269d2fbd5f1be,9d937bb3,# Relationship between Years in NBA & Salary by Rating Group,1264c440,0.7272727272727273
33572,da199f8fb59439,951b945c,### Duration,baaa665d,0.7272727272727273
33573,4b64dc653fb7eb,3b7416d2,"Data obtained after Lemmatization is in array form, and is converted to Dataframe in the next step.",57675cc2,0.7272727272727273
33574,dd02a9b545f742,7a160107,# Phase 2 | Test mode,7116cd2d,0.7272727272727273
33575,450fda47b03baa,5697ca2f,Category özniteliğine göre bir gruplama işlemi yapalım ve görselleştirelim.,62c04adb,0.7272727272727273
33577,a2573183738753,5f8b552a,# for training evaluation ,f6429599,0.7272727272727273
33578,663bbc9eaf267b,1e854c06,"There are three categorical variables in this dataset:
* model
* transmission
* tax
",32445529,0.7272727272727273
33583,4ae464582bac51,a78a0c00,## PUTTING VALUES ON THE SAME SCALE,ca6a52ce,0.7272727272727273
33585,d1ff7e10ee0102,2b9f88b6,Done! Let's check what's going on with 'GrLivArea'.,2cc71c3c,0.7272727272727273
33587,347c7b0f48c53f,6eee228f,"**NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.**",c58305ba,0.7272727272727273
33588,c13f73168789c2,e3815877,"### 2.3 To slice row and columns by index position<a id='28'></a>
Syntax 1 : `df.iloc[starting_row_index : ending_row_index, starting_column_index : ending_column_index]`

Syntax 2 : `df.iloc[:starting_row_index, :ending_column_index]`",16175052,0.7272727272727273
33589,32e04b08ff52eb,a27f5b1e,k Fold Cross Validation,8d5b86e0,0.7272727272727273
33593,241cf32abb22d8,55f7e642,"## 3. (Gaussian) Naive Bayes

I include different var_smoothing to search for the optimal Gaussian Naive Bayes model, starting with 1 to 10^-9 with 100 different values. Before fitting into the algorithm, I perform a power transformation to ensure that each descriptive feature follows a Gaussian distribution.",47157066,0.7272727272727273
33598,2f964d08c25d93,f6ad305b,Scatter and density plots:,1f2e4468,0.7272727272727273
33600,73d8e56bc709b1,e8d7ff93,"From the Scatterpolar, Messi and Ronaldo got almost same score or got quite close in BallControl, Acceleration, Agility and Dribbling. And Messi got higher score than Ronaldo in Balance, while Ronaldo won in Jumping.  
Therefore, they are neck and neck, beyond all doubt, they are two best players in the football world!",78ec3cce,0.7272727272727273
33601,25c2f1ef13b402,80fbcf55,**TABULAR MODEL TO USE THE OUTPUTS OF THE COLLABORATIVE FILTERING TOGETHER WITH THE OTHER CREAATED DATA**,b028c35a,0.7272727272727273
33603,be2f4d8a6b73ca,4e4a2d8f,**splitting the data for training and testing**,5d8ce40a,0.7272727272727273
33606,70193f0c034b98,741f9d87,# Using Binary and Categorical focal loss,f8cacd26,0.7272727272727273
33612,132fa9714f2046,30b41823,"** Now plot (x,y) and (x,z) on the axes. Play around with the linewidth and style**",3bb1775f,0.7272727272727273
33620,d83e5b44d1b80d,65a1f210,# Programming Lanaguage,62845930,0.7272727272727273
33621,2cb457b60dd246,5bcf1795,### Evaluate the model using the val set,339367df,0.7272727272727273
33625,0475899eec1ffe,5a67d685,All Data ,d825dc37,0.7272727272727273
33626,016abae0483764,5f144192,"Spiltting is a very crucial step if you want to test your model rightaway. Using what we have as in the datset inself, we can compare if the resulted predicted value is close to the value already present in the dataset. 
</br>
Thus we can see if our model works in short...",bc9f289b,0.7272727272727273
33629,b1684dfa49524a,d84406c4,## To show you the change in the maximum temperature of the year,60a2599b,0.7272727272727273
33630,3793c438a71b52,2cb4df22,****Jointplot to compare the Time on Website and Yearly Amount Spent columns****,13eb76df,0.7272727272727273
33633,37360278c19104,713f6516,# Keras model to make prediction,21473a41,0.7272727272727273
33645,b01ee6cb674fa3,41274ef9,"# Agência Espacial Brasileira - AEB

Public owned, founded by 1994 - sad story of this",a8ffd35e,0.7282608695652174
33646,faa8e6c8ab9246,34a14708,There are no outliers in the train dataset,2bea1419,0.7283950617283951
33647,4883314a96dc34,c62d7072,Split *rescaled* dataset into train/test set:,50d36836,0.7283950617283951
33648,fe6750354fb64f,5c37e4b4,# 3. SVM Regression,271741f0,0.7285714285714285
33651,2ada0305b68956,cd419eea,### 124. Palette = 'icefire_r',133e26f4,0.7285714285714285
33652,9c044fa3072552,85bfc360,### By Month,1362842e,0.7285714285714285
33654,38b79494ac749e,3411919f,### Excercise 1 - Magnitude (4 points),39162a40,0.7285714285714285
33661,149cb8d3489224,06cf6966,### Time variation,116858e7,0.7288135593220338
33666,dac3c8204a2d1b,a632ce9f,# Highest Price Bestselling Books 2009 - 2019,b0d2d0dc,0.7288135593220338
33671,a81661cc35d8d2,f23ec37f,"We will run Logistic Regression and Random Forest classification algorithms on all of our datasets, comparing performance across different levels of feature engineering and exploring further parameters we can modify to improve our prediction",3331f113,0.7288135593220338
33672,c84925c8171900,57979a0c,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.1 Genre wise Video Games 
            </span>   
        </font>    
</h4>",e21ff7ec,0.7289719626168224
33677,2a377ced98d67a,5c881456,"Feature scaling helps improve model performance by reducing feature variance. Features with larger values can dominate other features in the dataset and reduce importance of other features in model building. By using feature scaling this feature space can be reduced. This helps reduce training time and helps model perform better. There are couple of feature scaling techniques including normalization and standardization. Here I have used standardization 

MinMax Scaler transforms features by scaling each feature to a given range. Specifically, the estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.

The transformation is given by: 
```math
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min)
```",262231a8,0.7291666666666666
33678,3b5903412fe741,f5df7be2,"This `DataFrame` has ~20,000 rows. The original had ~130,000. That means that around 15% of wines originate from Italy.

We also wanted to know which ones are better than average. Wines are reviewed on a 80-to-100 point scale, so this could mean wines that accrued at least 90 points.

We can use the ampersand (`&`) to bring the two questions together:",ad231969,0.7291666666666666
33682,eb0854a6601407,94761ec9,"As we have reasoned how the investments with less observations seem more risky, we notice how the number of the assets present at each time step is quite different and also highly oscillating. By the end of the avaliable time, the number of assets has grown by one third. We can see that the number of investments given the time id varies especially around the id 400.",6d107747,0.7291666666666666
33683,5cfb546af2b8ce,7551c5de,Lets see some more data,e8730ad1,0.7291666666666666
33684,95656e8d666b16,0e6de682,"85% accuracy using LogisticRegression, decent",65e88599,0.7291666666666666
33687,869a39a3d4dea2,66a9d958,"## Color Spaces <a id=""colorspace""></a>",9020daf8,0.7294117647058823
33688,513ce405d7f6a3,8de596a3,# TensorFlow Model,8461e086,0.7294117647058823
33693,d76896b30cebd3,b1639607,"Distribution of main categories in Sucessful & Failed Campaigns

",1b4e8f34,0.7297297297297297
33695,2dda7facf3c1e0,ed5dd9e2,#Generate and Evaluate/Plot from the resulting model,45552d2b,0.7297297297297297
33708,9ceb7278784462,f0146c8e,## <a id='21'> 17.Bagging Regressor</a>,3768a567,0.7298387096774194
33710,3c2033cc99c12c,0d3d59a0,**Note:** *The algorithm of support vector machine a kind of traditional statistical method that focus on seeking a higher dimensinal flat so that it could maximize the margin between two classes.*,dfa22a54,0.7299270072992701
33711,83df814455f06c,750f4635,### Visualize decision-trees with graphviz,c9cff71a,0.73
33713,b10bd75889dad9,9ee3ecb2,### Lets get the top 10 columns,ee00ceee,0.73
33717,8985a124d4b657,45df1075,"Until now, we just ran our model for prediction of a single pollutant. We have 6 pollutants in our dataset and can make predictions for all of them. So, I have made a function which can be used to predict the other pollutants rather than having to write the code again and again. I have commented the function calls. You can fork this kernel to uncomment and predit the other pollutants (Coz it would take up a lot of space and time).",586d1846,0.7301587301587301
33722,04ff2af52f147b,86d9375d,"**Discretize Continuous Features:**

Now we can discretize our continuous features (*Age*, *Fare*) by placing the data into bins where each bin has an equal ($\pm 1$) number of passengers.  This provides a better signal-to-noise ratio so long as the bins are not too large.  This binning also serves to encode our *Age* and *Fare* features as the *labels* parameter to *qcut* is provided.",d5f37be9,0.7303370786516854
33725,957e035ba5b9d5,1f7d251b,# flow_from_dataframe approach,778ab3d3,0.7304964539007093
33730,669ce946943d60,255c417b,### Submission,0f63c4ce,0.7307692307692307
33731,d4c5aaa4b36810,9a935bdf,### Ridge Regression ,65441f28,0.7307692307692307
33735,3f451680b1857b,9ce281b8,# YOLOv5 Stuff,56c45a1b,0.7307692307692307
33736,cf08b03b002c13,effbb32f,How many NSWF posts get deleted?,104d416f,0.7307692307692307
33737,1bd6cc83c02681,2a91137a,# Making the movie recommendation system model,17ff92f0,0.7307692307692307
33738,44f6a002ecd033,22dd3950,"Now we are into the fun part working with building the best model we can for this data. A couple methods will be done. We are going to be using train_test_split and trying a couple different models like LogisticRegression, DecisionTreeClassifier, RandomForest and XGBoost. We will start with our original dataset and then like with the cleaning do the same methods with our log transformation dataset to see if that improved our model at all.",70bbe106,0.7307692307692307
33739,4d91e84c564cbe,309c3334,"It comes third (i.e. at index 2 - 0 indexing!).

At what index does Pluto occur?",355a43e3,0.7307692307692307
33746,e7237da7cbec10,a0ceb7d2,5. Model Score Check  모델 학습,5fcf5e3d,0.7307692307692307
33753,8ddaa0c6c395ec,b5f7ab79,## Evaluation,9fccabdc,0.7307692307692307
33759,0fc0cbf884acd6,17902a90,# XG by Store and ITEM By Month,064949f1,0.7307692307692307
33764,30fdc4a6e3c1db,1a96bcd3,August sees the highest overall sales in a year (~ 36K units sold in a month),6111ddee,0.7309941520467836
33765,5ce12be6e7b90e,602617a5,`remove` finds and deletes an element from list:,c0ab62dd,0.7309941520467836
33766,4ae6a182abac64,66785d47,"### 3.2 Splitting the training data 
",418676c5,0.7310924369747899
33768,0caaec057f7184,0d9256af,"## Discussion

From the above analysis, we've found that there are mainly three kinds of test data. In addition, depends on the kinds, we can think of the training data set presented to us in different ways. Here are some questions we can ask:

- Whether the training data is really 'the complete' history data ?
- Whether non-record item is really having no sales? or is its history missing? or is it new launched?

Here are some directions that we can think of based on the kind of test data:

- Old items already launched (52%)
    - Believe all the training data are the total selling records in history
    - Training data are missing some selling records
- Old items new launched in shops (42%)
    - 0 selling records in the shop
    - Missing selling records of the shop
    - New launched in the shop at the month to be predicted
- New items new launched (7%)
    - 0 selling records in the shop
    - Missing selling records of the shop
    - New launched in the shop at the month to be predicted

Depending on the way we look at the training data, we will have different methods on dealing with these kinds of data:

- Training data not missing history (0 sales if not having records)
When predicting the sales, directly assign 0 to the time that don't have records and then predict the monthly sales.
- Training data missing history (items launched in the shop)
On the analysis above, we found out that giving the average monthly sales can be a kind of solution, such as using average monthly sales of the item in all shop (Old items new launched) or using average monthly item sales of the category in the shop (New items new launched).
- New launched at the month to be predicted
Need to find out the new launched items in training data, both as new launched in a shop or all shops, and use the kind of data to create new set of training data and then predict. This is different then two types above, since that they're using all the history data in training data. The method here needs to distinguish the start-selling day, and the predicted results will depend on their selling records afterwards, with other features like category, shops, etc included.

Note that since 'Old items new launched' has around 40%, the way we deal with the kinds of data can also play an essential role on having a better predicting results.",b875533e,0.7311827956989247
33772,a4aa36df07fd53,316a9886,"### Which countries pay more?
Negara mana yang paling bersahabat dengan data scientist? (Gajinya gede maksudnya)",d2f42b6d,0.7313432835820896
33774,e58e68e4eeefe5,e98a6c99,"**features = {ejection_fraction, serum_creatinine, time}**

The accuracies over here are slightly corrected as it is tested on sampled data.

* Logistic Regression --> 90%
* Random Forest --> 90%
* Gradient Boosting --> 95%",a87662ce,0.7313432835820896
33777,81712ee7510ac5,4014a918,**Logic Operators**,c4685e79,0.7314285714285714
33778,ab6da5994949a3,87f1cde8,## Naive Bayes Training Results,fae6b91d,0.7314814814814815
33782,726833f92fb87a,7d5dddfe,## Education,7dc5e1b6,0.7315436241610739
33784,0e09587faffa8f,0dd5cf37,The highest number of tickets were issued to **Toyota** vehicles,0d563d61,0.7317073170731707
33791,47b2c9be5e31cb,9e287107,### Let's check 3rd file: /kaggle/input/FRvideos.csv,7d4afe56,0.7317073170731707
33792,fd4017c1514157,786541a7,There is quite large 5 seconds window in recordings where there is no call present.,fd8f0896,0.7317073170731707
33796,9c26c5dcd46a25,08f69ef3,"### <font color=""#ea1c60"" id=""section_3"">3. Réduction dimensionnelle.</font>

Pour cette réduction du nombre de dimensions, nous allons réaliser une **Analyse en Composantes Principale *(PCA)***, l'une des méthodes d'analyse de données multivariées les plus utilisées. Elle permet d'explorer des jeux de données multidimensionnels constitués de variables quantitatives.

Pour cela, nous utiliserons la méthode `PCA` du module `decomposition` Sklearn sur les variables numériques centrées et réduite. Nous prendrons en variable illustrative de cette ACP le grade Nutriscore du produit.",1bbbb677,0.7317073170731707
33802,8ec771f5600a61,08282d9d,# From randomforest,48364c1f,0.7319587628865979
33803,225b4fe5d3894a,908d78d5,"<a id=""7b""></a>
### b. Better Evaluation Using Cross Validation",4b4197b3,0.7319587628865979
33811,f13534449a3750,8abc9d9b,"**Split into training and validation groups**

We stratify by the number of boats appearing so we keep the proportion of occurrences before the split.",8b7f3332,0.7321428571428571
33812,8dd655515e7d18,2a64de10,"**Analysis:**

Correlation between the following:
* Openness & Conscientiousness = Positive
* Openness & Extraversion = Negative
* Openness & Agreeableness = Negative
* Openness & Neuroticism = Negative

* Openness & Wordcount = Positive
* Conscientiousness & Wordcount = Positive
* Extraversion & Wordcount = Negative
* Agreeableness & Wordcount = Negative
* Neuroticism & Wordcount = Positive
",895f41cf,0.7321428571428571
33818,bddd799cdbbae8,6e39ab7d,**Logistic Regression Classifier**,b44e3c08,0.7323943661971831
33821,d96642860ab3dd,ab8d680d,### 2.6 Encode,98419d48,0.7325581395348837
33822,c09fac3c943d51,9d120203,Cute function to validate and prepare stacking,678d076d,0.7325581395348837
33823,f3c6048d1058e3,31c71b47,### 4) SVM,1d9056b0,0.7327586206896551
33824,ee23a565163388,3b5d0d92,The training and test scores both look to be decent. Let's try other models as well to conclude on the best approach.,88aacbc4,0.732824427480916
33825,738bfced935b69,703ed845,## Exploratory Data Analysis,2d3c592d,0.7328767123287672
33827,541d0fa0e26b80,c4d24e51,# Visualization with respect to Results ,a29e0f29,0.7333333333333333
33830,67b7354e96113a,98b35953,**SVM Classifier**,dca94250,0.7333333333333333
33832,62487bcd70b199,80210a8f,## <a id='8.1.4.'>8.1.4. DecisionTreeClassifier</a>,f6ae50af,0.7333333333333333
33833,e5dd725b8fa422,a2c61f0e,# Validating the MA model,14675d8b,0.7333333333333333
33835,63d0d9b9a8c7d2,76ab5023,***KNN***,e32e5933,0.7333333333333333
33837,e0a041e5e2372f,12568c25,"##### Values of unit weights for each parameter (wi) - 

Dissolved Oxygen                     0.2213

pH                                   0.2604

Conductivity                         0.0022

Biological Oxygen Demand             0.4426

Nitrate                              0.0492

Fecal Coliform                       0.0221

Total Coliform                       0.0022",7c4357b2,0.7333333333333333
33842,4fd4b6a80d40e3,7392ef32,"## Signal Transfer from the 2 Layer to the Output Layer

![image.png](attachment:image.png)",f6913cc3,0.7333333333333333
33847,061d6757dfbce0,22c7a265,"We have no missing data for meter readings, but a lot of missing data for:

* building: floor_count (75%)

* building: year_built (53%)

* weather: cloud_coverage (51%)

* weather: precip_depth_1_hr (35%)

* weather: sea_level_pressure (8%)

* weather: wind_direction (5%)",c0c2915a,0.7333333333333333
33864,051b118f751e77,d70deec3,### Performance comparison based on Validation Accuracy,9fad25fd,0.7333333333333333
33866,4c55891bcb068d,e7fe1a5b,# Prediction,01b9cd67,0.7333333333333333
33867,396bc36edb95d3,ad32d6a4,### Conclusion for Random Forest Model,965e4f8f,0.7333333333333333
33868,6fad63bfd45ef9,a674b1f5,# Training,b3c6f1d6,0.7333333333333333
33869,d6ddbe57f59cf7,c79417e7,# Violin plot,504a3cda,0.7333333333333333
33870,f7436bc492474c,2943c58d,Here's the basic naive bayes feature equation:,328fd235,0.7333333333333333
33874,c8bf959b9608cf,85dcab5e,"### Calculate gradient 
#### Calculate gradient of loss with respect to the pixels of the generated image. Here, we do not change the pre-trained weights of the VGG19. We use the pre-trained weights just to capture the different style and content aspect of the images and then we calculate the loss. 

### Output of the model
We will use scipy-based optimization (L-BFGS) over the pixels of the generated image as loss function(instead of SGD or other loss function). The L-BFGS optimiser takes two parameters: the loss and the gradient of the image w.r.t. the loss. To generate the gradient and the loss, we'll use Keras' k.function().

### K.function()
We need to define a function, which takes in the generated image, spits out the loss and the gradient. The output of the function will be used for optimisation. Till now, Keras used to do all this for you in the beacked. But in this problem, you need . In Keras, we define such a custom function (with a custom input and custom output), using K.function().

Additional reading: https://keras.io/getting-started/faq/",155e3672,0.7333333333333333
33879,65245c6e88a2ee,36329459,"## Prediction and Results
### Let's see how accurate is our model!",71d6e90e,0.7333333333333333
33883,37b09262279764,1e7c8674,#### AdaBoostClassifier,37c4c417,0.7333333333333333
33888,6e472c6c591c7d,3806a414,"For a hint or the solution, uncomment the appropriate line below.",65532a3d,0.7333333333333333
33896,e25c0f830df3f4,0e09a784,"# Calculating the count of Positive, Negative & Neutral comments",fdcf7189,0.7333333333333333
33900,07d6ca51d43510,c35feb0a,***,38e74a14,0.7333333333333333
33909,bd380b97b5c894,2fc3da56,now let's build the error matrix again,66f2562a,0.7339449541284404
33912,f6648e47713411,1cd3f6ae,## 3.2 Define new model,f4af4d1c,0.7340425531914894
33914,5d2a3e82679cf3,df44663f,# RIDGE REGRESSION,9e60b1e3,0.7341772151898734
33916,2ada0305b68956,82472225,### 125. Palette = 'inferno',133e26f4,0.7342857142857143
33917,ff3a8ce61fab6a,c78481ec,"<hr>[](http://)

<div>
    🔺 <b>Alert</b><br><br>
    <b>datatype equavilant</b> which we talk about it at first             <b>alert</b> ☝ has same importance in <b>subtraction</b>.
</div>

<p>
    Let's talk about <b>divide</b> function We can use it to  <b>Number dividation.</b>
</p>

## Number dividation<hr>

### Exampel 1 ",9afe1654,0.734375
33919,c85c94076e9c3a,ebac9e7a,## Kids_Teen_at_home,3ea0c443,0.734375
33925,087e21401d7dfc,d4ffc5d0,# Improve Model,42000489,0.7346938775510204
33926,eb33e05704d647,a73c28be,"#### Explore 'data_gen_train'

data_gen_train is an **generator**, so we get the data by calling **next(data_gen_train)**",cd80436d,0.7346938775510204
33927,12f4d16fc21645,a598989d,<h1 style='color:blue'>Model Prediction</h1>,c7752038,0.7346938775510204
33934,4daf6153275cbf,9e5f3bc0,"Visually speaking, people in Europe seems to pay more for their own cuisines than the others. The code piece below is the two-sided 2-sample t-test with %95 significance",51db1961,0.7349397590361446
33939,52cfd66e9ec908,7cb70371,Not much difference here except that we've shaved a lot off the `0.0` peak which is the effect of the cutout augmentatioin.,c74adcdf,0.7352941176470589
33940,e4c6dd957eb5ce,51ac30fe,"Cool! <br>
It's a very interesting chart to understand Kaggle competitions profile. <br>

We can see that:
- Only 13.69% of all competitions has money prizes. 
- The None values (that is the most common value with 65.10%) we can consider as ""Knowledge"" competition. It's very interesting because sometimes we consider Kaggle a plataform of paid competitions, but the data shows us the truth. 

Now, that we have clealy this differentiation of paid or not comps, what about we plot the comps through the time and try understand how it happened;
",2e383665,0.7352941176470589
33942,71b75664517244,d138ca32,### José Mourinho,fc905af5,0.7352941176470589
33945,99821bc6a45be6,0f5f3fa8,### Training:,b9d59346,0.7352941176470589
33946,156bbcff05dcea,6f9ed096,The similarity between the scikit learn classification report and custom PySpark classification report function seem to be very impressive.,66ad1fe9,0.7352941176470589
33948,8f50c9c16db95f,c8467b5a,"# Variable 2 - Body orientation <a id=""orientation""></a>
Another interesting fact is the ball kicked to the right direction usually has a shorter length than the one to the left or to the middle. ",26cc763a,0.7352941176470589
33949,21bce4ec54b3fa,030a0f69,"Most of the variables have higher absolute correlation to target than random column, so this heuristic doesn't provide a good variable selection strategy.
Also all variables have very low correlations to target, e.g. 10% at most, so it's not evident how many top variables to select or what correlation threshold to set.
Let's try taking top 20% most correlated variables and see how the score changes.",35546e30,0.7352941176470589
33951,55c34673c1f760,c1e41491,## Model Training History,2663c47f,0.7352941176470589
33952,395ed8e0b4fd17,31a9cabf,### Ethereum(ETH) OHLC Chart for last 100 rows,7573ea31,0.7352941176470589
33954,410285582f4f7e,ea08b726,"**Number of Users by Language**

It looks like most users are English speaks. ",d026266b,0.7352941176470589
33962,02b7e38902069e,d2cd6142,#Document to Python Object,726a03a0,0.7352941176470589
33966,2f47abddfd1928,f358d83a,"We can see that most of the people are under the common titles (Master, Miss, Mr, Mrs). I will try to pack the uncommon ones in less categories.

- Capt, Col and Majo --> Military Rank
- Don --> Mr
- Dona --> Mrs
- Dr, Jonkheer, Lady, Sir, rev --> High class
- Mlle --> Miss
- Mme --> Mrs
- Ms --> Mrsdf_all_title_Survived_count
- the --> Mr",ae33cc0b,0.7355371900826446
33968,14defffcd250f3,0e3e759d,Since there is only one null value we can replace it with mode,3a683b94,0.735632183908046
33971,0ad8d416b89b78,30087b5d,Adjusted Logistic Regression ,0b0562f0,0.7358490566037735
33972,f3c8651cb08234,09f19f6c,# XGBOOST Model,37f86e36,0.7358490566037735
33974,510b8303776bb6,8d72ba8f,As we can see that the XGBoost model works best in this therefore we use the XGBoost model.,18080db8,0.7358490566037735
33978,23df07a474aaae,9a0a4f09,# Using Deep Learning to Predict Streams,0ea40276,0.7358490566037735
33980,f015d0147e8fbf,5c7d35ed,"### LightGBM Parameters

Commented-out hyperparameter values give an indication of the different values I tried on my journey to ending up at my best-performing combination. 

I first try to pick a combination of 'max_depth'/'num_leaves' that is deep/large enough for the dataset without overfitting. At the same time, I choose the highest possible learning rate (usually 0.2, 0.1, or 0.009) and then observe how the CV score changes as I adjust other hyperparameter values. 

""Highest possible learning rate"" means: a learning rate that gives the model the chance to run for enough boosting rounds so that I can confirm that the model is incrementally learning from round to round. If the learning rate is too high, the model's score won't steadily improve from round to round, and we won't be able to observe how tuning other hyperparameters affects overall performance. If the learning rate is too low, we'll be taking an unnecessarily long time to make these observations.

I like to tune hyperparameters one-by-one, and empirically observe how the CV score changes. Sometimes, but not that often, I will adjust pairs of hyperparameters together if I believe that the two hyperparameters have a unique interrelationship. I'm not a fan of computational tools like GridSearchCV. I find that simple, rote trial and error gives me a far stronger intuition about how the different hyperparameters are responding to my dataset and affecting CV score, in a much shorter amount of time than it takes GridSearchCV to finish churning through all the different hyperparamter combos while running on my laptop's CPU.

Learning the fundamentals of decision trees, LightGBM specifically, and what each of its hyperparameters purports to do helped me to begin to be able to make mental shortcuts of know what hyperparameters to tune, and when and how to tune them. The following two resources helped me immensely:

1. The paper on LightGBM: https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf

2. Laurea's site that explains all hyperparameter values for LightGBM/XGBoost: https://sites.google.com/view/lauraepp/parameters

After tuning the various hyperparameters at the highest possible learning rate, such that I can confirm improvements in performance, I then lower my learning rate to the value that will maximize my model's performance. This will be the learning rate I use when training and making predictions.",518954fb,0.7358490566037735
33983,69ac33d79f5130,e63cde9e,### Accidents are more in winters due to less visibility.,9d760d2a,0.7361111111111112
33987,593d1d3d1df05a,d76fbf6d,"# Storing the Model, we just trained
",bc682ffe,0.7361111111111112
33989,63b44c85e32c1f,6ab8bed7,It follows the same indexing and slicing as Lists.,fb9b9562,0.7364864864864865
33996,f35ee6e9fab592,05b77df1,The following gives us an idea of when video games tend to be released,b15f7073,0.7368421052631579
34005,bef2347846e476,52c46cb5,Here we see Application ratings and some of their names with the horizontal bar graph.,cb93bf51,0.7368421052631579
34007,3fb15e6e48aec2,e942d4c6,"# Quick Survival prediction
* We can use this as a baseline",9d1f4358,0.7368421052631579
34015,d81d3830152f88,dc268785,`p_obs_diff` is the difference of `P_higher_3pt_win` and `P_not_higher_win` of our observation data above,9551eac9,0.7368421052631579
34019,ba4b3bd184acbb,0991c6d6,"### Merge

The `merge` method can be used to combine two DataFrames based on column values, similar to the SQL join function.

The `left` and `right` parameters are the respective DataFrames while the `left_on` and `right_on` parameters dictate which columns will be used for the join.",0f5de724,0.7368421052631579
34021,d93a87fdbdb3d2,2ad31836,## Build model,30d079c3,0.7368421052631579
34022,30fdc4a6e3c1db,6a1bff0c,### Plotting sales over the months across the 3 categories,6111ddee,0.7368421052631579
34024,6cade0b6a41ba2,2009c87f,## 4.1. Defining Input / Output Data,e6110293,0.7368421052631579
34028,b568d0238ff53f,dd021878,-------------------------------,ba30ccf4,0.7368421052631579
34032,99afe9f3af6dbc,23df332e,"**Multidimensional scaling**

Mengubah matrix dist menjadi 2-dimensi array.",cdec9b3a,0.7368421052631579
34034,c2a9f2fb3e1594,deeeb9d9,"<a id=""ch9""></a>
# 5.12 Tune Model with Hyper-Parameters
When we used [sklearn Decision Tree (DT) Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier), we accepted all the function defaults. This leaves opportunity to see how various hyper-parameter settings will change the model accuracy.  [(Click here to learn more about parameters vs hyper-parameters.)](https://www.youtube.com/watch?v=EJtTNboTsm8)

However, in order to tune a model, we need to actually understand it. That's why I took the time in the previous sections to show you how predictions work. Now let's learn a little bit more about our DT algorithm.

Credit: [sklearn](http://scikit-learn.org/stable/modules/tree.html#classification)

>**Some advantages of decision trees are:**
* Simple to understand and to interpret. Trees can be visualized.
* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.
* The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.
* Able to handle both numerical and categorical data. Other techniques are usually specialized in analyzing datasets that have only one type of variable. See algorithms for more information.
* Able to handle multi-output problems.
* Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by Boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.
* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.
* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.

> **The disadvantages of decision trees include:**
* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.
* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.
* The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.
* There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.
* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.



Below are available hyper-parameters and [defintions](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier):
> class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)


We will tune our model using [ParameterGrid](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid), [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), and customized [sklearn scoring](http://scikit-learn.org/stable/modules/model_evaluation.html); [click here to learn more about ROC_AUC scores](http://www.dataschool.io/roc-curves-and-auc-explained/). We will then visualize our tree with [graphviz](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). [Click here to learn more about ROC_AUC scores](http://www.dataschool.io/roc-curves-and-auc-explained/).
",53411c04,0.7368421052631579
34037,9e27af2600925c,c1028b80,"**Expected Output**: 

<table style=""width:40%""> 

    <tr>
        <td> **Cost after iteration 0 **  </td> 
        <td> 0.693147 </td>
    </tr>
      <tr>
        <td> <center> $\vdots$ </center> </td> 
        <td> <center> $\vdots$ </center> </td> 
    </tr>  
    <tr>
        <td> **Train Accuracy**  </td> 
        <td> 99.04306220095694 % </td>
    </tr>

    <tr>
        <td>**Test Accuracy** </td> 
        <td> 70.0 % </td>
    </tr>
</table> 


",9b556435,0.7368421052631579
34038,89afa0f49378c7,874cea5a,FUTURE FORECASTING,32dbe10b,0.7368421052631579
34044,1294fb4c86f993,6cbc7900,<b> Now it seems to be okay to merge the two dataframe utilizing `state` as Key,4471e513,0.7372881355932204
34045,9169c4e9c33c90,5aea70ae,"The *Diagnostic and Statistical Manual of Mental Disorders, 5th Edition: DSM-5* is the most expensive book that made the Top 50.",725bf880,0.7372881355932204
34048,5ffe6aa38958a1,f4c8386a,## 4.3 Check prediction on validation set,11f5412e,0.7375
34049,254cccd5145725,aac89ed7,Assigning the X which are the features and y which is our Target.,a49b4037,0.7375
34053,b61ab8f81dc03d,910118f0,"As showed above the accuracy results pointing to Randon Forest Regressor as the most fit for this data, so let's use it to do a ""hyper parameter tuning"" before submit the results.",64d05394,0.7375886524822695
34057,918040fad252ec,294f7c64,Menampilkan squential tabel,966fcd8f,0.7377049180327869
34061,98a6794067932a,397ea6cb,"Les trois prochaines cellules de codes ci-dessous permettront d'effectuer les mêmes analyses. La seule différence se retrouvera au niveau de l'état étant donné que la même analyse sera effectuée auprès des trois états mentionnés ci-dessus. Ces analyses permettent de démontrer les 10 villes où l'entreprise effectue le plus grand volume d'expéditions au sein des états de la Californie, New York et le Texas. Dans un premier temps, le code permet de regrouper le nombre de fois où une expédition est effectuée vers une ville en particulier. Ensuite, le code effectue un classement en ordre décroissant des villes selon le nombre d'expéditions. Dans un troisième temps, le code permet de sélectionner seulement les données pour lesquelles l'état de référence est la Californie. Finalement, le code permet de représenter graphiquement seulement les 10 villes où le volume d'expéditions est le plus élevé. Cette analyse sera très pertinente afin de nous aider à préciser les recommandations que nous effectuerons aux dirigeants. Effectivement, plus tôt dans ce rapport, nous avions ciblé les états les plus populaires, mais ces analyses supplémentaires nous permettent maintenant de cibler directement une ville au sein de ces états afin de possiblement y installer un centre de distribution.",08600fe2,0.7378640776699029
34066,898d18d501f68d,091180d2,from the diagonal part we can observed that ttehy are highly correlated. darker the color manes highly correaled and lighter the color less correlated,d8bdea2d,0.7380952380952381
34068,6a80f915608fc2,7d70eece,### Select the Features,636938eb,0.7380952380952381
34074,2d40f383473fa4,8f4a2d1e,"Train the model for 10 algorithms and 5-folds.
The sort will be made based on AUC Score, H2O AutoML doesn't have accuracy as sort metric.",1da1eff0,0.7380952380952381
34075,a76e0e8770b7a0,696a7237,All of Afganistans terror is after 1999,02863d3b,0.7380952380952381
34078,b4ecd6e4277e3c,316f4064,"### Model Architecture

Binary LSTM with an attention layer and an additional fully connected layer. Also added extra features taken from a winning kernel of the toxic comments competition. Also using CLR and a capsule Layer. Blended together in concatentation.

Initial idea borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm",94d79d5f,0.7380952380952381
34080,1fac5edd4063ba,fb9a8ed0,![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQJ7805SNyhKnRPlQHeRz4n5Ivlg4cLYEpfBA&usqp=CAU)facebook.com,04bc01e0,0.7380952380952381
34081,916ccf243827f1,95b406d0,## 12. Update parameter with Adam,5147f4d2,0.7380952380952381
34083,0d59a3e0130db0,a3848998,"And here it is - the simplest convolutional neural network :)

Let's see if it can handle it. Among the quality metrics, in this case, a high recall is important, because it is better to erroneously react to a normal comment than to skip a toxic one.",285f04b2,0.7380952380952381
34085,5f32117bcd5255,b13c4ae3,#### HST OPTICAL TWO,85882abf,0.738255033557047
34088,3cb96bd8eb364b,7a8ecef2,## Train Models,3157af7e,0.7384615384615385
34090,09751c520b0616,e6ebf699,"- <b>SalePrice</b> v/s <b>Age</b><br>
SalePrice behavior with age of the house",a4d0c7e9,0.7384615384615385
34091,d07915a6e6992e,6850caaa,**Embarked**,2b912140,0.7384615384615385
34096,c115e287523aab,4353e393,# Learning-Rate Scheduler,feb1288b,0.7384615384615385
34099,a8c042af6b7245,7b87e183,"Allright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Component Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made this kernel to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting.",2487ac62,0.7384615384615385
34100,917957c6c4065f,e58d6c18,"description_length는 평균 298, 범위는 0 ~ 5235  
길이가 500 이하인 데이터가 대부분입니다.  ",55b8ed68,0.738562091503268
34102,73d8e56bc709b1,37087c17,# 6. Linear Regression,78ec3cce,0.7386363636363636
34104,20b372b6e4e276,a2c8205e,"## 5.4. Metric analysis <a class=""anchor"" id=""5.4""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.7388059701492538
34105,396bc36edb95d3,24babe03,"<b>Train Data:</b>  
    AUC: 85%        
    Accuracy: 80%             
    Precision: 69%        
    f1-Score: 64%       
            
<b>Test Data:</b>      
    AUC: 80%      
    Accuracy: 80%         
    Precision: 73%       
    f1-Score: 65%     
  
Training and Test set results are almost similar, and with the overall measures high, the model is a good model.  
  
Agency_Code is the most important variable for predicting claim status. ",965e4f8f,0.7388888888888889
34106,9d9da6c439b96b,ea2eb7b3,"From distribution, we can see that there is outlier, so I decided to clean outler using iqr method",361cc7d9,0.7391304347826086
34107,4913b61a68d355,af02735b,# Predictions ,6e269c6a,0.7391304347826086
34109,0e2a23fbe41ca9,7805009b,"Observations:
- Quantity of active months within last 12 months, is 3 in 91.1% of data meaning, they were active during all the 12 months",64e4762c,0.7391304347826086
34114,7e275c8d5ff2a0,f8e6a6e0,"We can clearly see the prediction of our model that on **'16 August, 2020'** there will be a total of **'~1.2M (12 lakh)'** confirmed cases in India if the number of confirmed cases goes on increasing like this.",b3afcc98,0.7391304347826086
34117,77f958b3f41a70,a2031c60,# Governmental public health,2ad9bb69,0.7391304347826086
34119,b01ee6cb674fa3,d957599e,"# Starsem

a private company between France and Russia, HQ in France",a8ffd35e,0.7391304347826086
34125,e3fb4c6300cb56,b13ad80f,"<a id=""11""></a> 
## Pair Plot",8ebbdf89,0.7391304347826086
34126,f6c1eb62cceb70,0fc337d6,"Inspect your predictions, print a few prediction data",90a1b790,0.7391304347826086
34132,8336d84cf3ff6b,fefafd97,# XGB Boost Tuned after Random Grid CV ,b96b58a0,0.7391304347826086
34133,548f961125248d,f9e5137c,"* There is 97% likelihood for positive label - for this instance.


* From the plot we see that the base value is 2.638. 
* The SHAP values of all features sum up to explain why our prediction is different from the baseline (value of +3.46 - a positive value).
* The contribution of each of the features towards change from base values is shown in the plot. 
* 'RESOURCES' contributes towards a more positive values. All the other features contribute to more negative value for the prediction in row 2. ",d8c5e8b8,0.7391304347826086
34138,7e2644d6b415bc,522383ea,# From my curiosity ------ I do regression analysis,52de7ef0,0.7391304347826086
34142,598b6228760590,d8ce2a91,"- **After the above-commented code tuning, it was found that the effect was worse, so we decided to use the default parameters.**",be30ab66,0.7391304347826086
34147,91473a39b85068,cc5be0af,### Featurization of Training Data,6e3d91c2,0.7397260273972602
34151,1eb62c5782f2d7,ca70e56b,"### Contoh 2
### Cari $z$-Score jika diberikan Percentile berikut:

- Q1: $P_{5}$         
- Q2: $P_{50}$         
- Q3: $P_{90}$",bb69f147,0.7397260273972602
34152,fc8e0042411c46,41a1ee86,- 'A free copy of Mastering The Interview' doesn't play role in decision making. ,af476c2a,0.7398119122257053
34153,e19e307b3fd188,74762d87,#### Select TARGET (y),2173955b,0.7398373983739838
34155,0687cd5c8597db,8153cde8,### **Displaying the Training and Testing Accuracies**,4edec76a,0.74
34162,2ada0305b68956,358b3c3c,### 126. Palette = 'inferno_r',133e26f4,0.74
34166,75adb7945ef9bd,a97857b8,This forms the basis of evaluating and modifying our models in the next section.,785c5095,0.7402597402597403
34167,722cd844dfbe8f,30cb880d,"Once the generators are created, we can project an image to check:",0cedb385,0.7402597402597403
34169,663bbc9eaf267b,89ad9da4,# Categorical Encoding,32445529,0.7402597402597403
34171,90691864eb68c7,6a402903,Correlation Analysis,3555ef9b,0.7402597402597403
34173,4ae464582bac51,52802754,The transformation of your data is a practice to prevent your algorithm from being biased towards variables with a higher order of magnitude.,ca6a52ce,0.7402597402597403
34174,44f6a002ecd033,2920050e,### Base Modeling with train_test_split,70bbe106,0.7403846153846154
34177,ee23a565163388,cf64c32c,## **Decision Tree Classifier**,88aacbc4,0.7404580152671756
34181,07544ba83da480,7ebd2b43,# Question 4: What products are most often sold together?,dc2f52b1,0.7407407407407407
34185,f4b9042e693b6c,36b798db,Let's now define our training and validationfunctions. ,676cacc9,0.7407407407407407
34187,b9328fe3b0cefc,07a0b47e,"we see(可以看到)：
- The No.1 seed of each division is still the hot spot of the champion, No.1 seed had 9 times winning the champion, and that tell us how important seed number is.(各个分区的1号种子依然是冠军的大热门，共9次获得冠军，而2号种子获得1次，其余则没有，这说明在女子组，几乎可以认为没有黑马产生)
- Look at competition area, Bridgeport won the most Championships.(赛区上看，Bridgeport获得冠军最多，这说明在赛区这一个维度上，没有能够明显的差异)
- From the trend chart of the number of seeds of the team that won the championship, it is more and more difficult for the champion to have a complete black horse, which basically appears in the number 1, 2.(从获得冠军的球队种子号的趋势图看，冠军越来越难以出现完全的黑马，基本上以1,2号种子中出现)",3a35eb23,0.7407407407407407
34191,0c452d3a0b9339,57fb49a5,"# Check the Reactivity, Degradations for the BasePairs.",5d857385,0.7407407407407407
34192,ac04ba639d1c93,238c3476,## GridSearch for Best Params,748059d5,0.7407407407407407
34200,9ad9a97e628bfa,c5a82c85,Cabin 의 Initial은 이후 imputation을 통해 예측하여 채워 Attribute으로 사용하도록 하겠다.,0a7e1136,0.7407407407407407
34209,c6f8ff61a5fa87,ad097dd2,"# <span style=""color:blue;""><strong>9.Total Analysis of all model</strong></span>

| Model | RMSE | F1SCORE |
|--|--|--|
|**nuSVR**|**2.647**| **0.48** |
|**SVR**|**2.635**|**0.485**|
|**Kernel Ridge**|**2.653**|**0.478**|
|**Lightgbm**|**2.029**|**0.695**|
|**Catboost**|**2.491**|**0.54**|
|**Stacking**|**1.200**|**0.695**|",3eea586b,0.7407407407407407
34222,869a39a3d4dea2,efcbb020,"Many different color spaces available like Grayscale, HSV (Hue Saturation Value), L* a * b* like RGB, will show an example of each of them",9020daf8,0.7411764705882353
34225,eda49464dd6d1b,744ece3d,### This is exactly as expected.  Test has 1 fewer column because it should lack 'Response'.  We will break out response into its own variable next.,8421f81f,0.7412587412587412
34228,e3f3f108cd3869,42c4f1b9,**Applying Linear Regression**,2b78de2d,0.7413793103448276
34233,20e1ba19eb9b5e,cb187d2c,## 2.4 Label Encoding,4569bfc1,0.7413793103448276
34234,84127ade6fde87,e413f35b,"As we start embedding words, we can map apple to a number in the fruit and red quadrant. Likewise, we can easily map tangerine, lemon, lychee, and kiwi (to round out our list of colorful fruits). Then we can start on flowers, and assign rose, poppy, daffodil, lily, and ... Hmm. Not many brown flowers out there. Well, sunflower can get flower, yellow, and brown, and then daisy can get flower, white, and yellow. Perhaps we should update kiwi to map close to fruit, brown, and green. For dogs and color, we can embed redbone near red; uh, fox perhaps for orange; golden retriever for yellow, poodle for white, and... most kinds of dogs are brown.",f55d05b6,0.7413793103448276
34239,1dd9c6aa74d289,ace0f6b1,### Rope climbing log done in countries of:,5ef9a1be,0.7413793103448276
34242,312135b445bd23,2d841083,"# **Putting it all together**
Now that we have all the components ready, we can visualize the results for the task.
We will iterate the different sub-tasks, for each one we will find relevant sentences and create an abstractive summary.",8ced381f,0.7415730337078652
34248,f15eac23fbcc9d,d6836e50,"Now let's fit the 40 frees, min_samples_leaf = 3, and max_features = 0.5. Pretty good default and train our RF to see how it works on MNIST.",ea46d8af,0.7419354838709677
34251,a3e8d6ef4c5188,f6a3898b,### Logistic Regression,7c8212dd,0.7419354838709677
34252,ad26c020235dfc,0c92ec28,Create new features based on the timestamp index:,bf766e48,0.7419354838709677
34254,0cb456a5456cf9,1e89957c,# **Q3**<br>**Using random forest to make predictions**<br>使用随机森林进行预测,5701729c,0.7419354838709677
34259,57070ad5e0f94f,728b39ca,# **From Now On We Will Prepare Test Set but Since I have Already Explained at Training Set(there will be really no change) I Won't Explain Again and You Can Skip to Bottom of Page If You Want**,d97edc41,0.7419354838709677
34261,098fedfcd07456,89494142,"# Model Compliation . 
1. Optimizer is Adman update network weights iterative based in training data
1. Categorical crossentropy is a loss function that is used in multi-class classification tasks
1. Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right",052ece26,0.7419354838709677
34262,c0ddb77bf32e2b,1c8abc54,Let's have a look in tensorflow and LSTM,a0cb45f7,0.7419354838709677
34265,0925f172b5eb74,d4b59bb9,# Building up the model: Efficient Net,ec34cd72,0.7419354838709677
34266,921fff7d3040db,e67309df,# 7. Naive Bayes model,5f36ced9,0.7419354838709677
34267,16862cb02d73d5,28393083,"A helper function to find percentage change,classify anomaly based on **severity**.

The **predict function** classifies the data as anomalies based on the results from **decision function** on crossing a threshold.
Say if the business needs to find the next level of anomalies which might have an impact,this could be used to identify those points.

The top 12 quantile are identified anomalies(high severity),based on decision function here we identify the 12-24 quantile points and classify them as low severity anomalies.
",d7ffa1a6,0.7419354838709677
34269,0caaec057f7184,21cce99e,# Special Case - negative values of price and sales in train data,b875533e,0.7419354838709677
34272,0932046e1f485d,e9413819,Data preparation for a stacked bar plot.,218cc7a3,0.7421875
34273,063a35f644e3c5,dbfe51c3,### Original Columns,1c30fb0a,0.7422680412371134
34279,7f74a04ae75792,c86d93bd,"### What is the distribution of customers' last purchase
",d01e91da,0.7426470588235294
34281,5ce12be6e7b90e,56dc8557,`pop` deletes an elements from a list by its index:,c0ab62dd,0.7426900584795322
34290,171494b45650a2,51d283ca,### ExtraMushrooms@Company vs Price,9c8cc578,0.7428571428571429
34293,5d5c9480b5a0a3,7081c64d,"Again, it has become evident that people from Class 3 had the highest death rate.",04d82e2d,0.7428571428571429
34296,60da9bbfe39c4b,30937618,"
# So, we can show that in July 2021, Bangladesh was most affected by COVID-19.",b0dd8ad6,0.7428571428571429
34297,55a5e31d03df9f,de0a186c,Let's finally train our model with the GlobalAveragePooling!,06dce00f,0.7428571428571429
34298,7454fdc444df16,85822737,"## Before and After RGB PCA
Let's visualize the image before and after PCA was performed.",a7818ef5,0.7428571428571429
34299,bbaa07ad21cf4e,68705973,### Decision Tree,3ab6b254,0.7428571428571429
34303,2730840089c8eb,6c30a80d,"We can use the same syntax to add another key, value pair",34d27dac,0.7428571428571429
34304,6b65d81a5743dd,e8e8f7a2,"Now let's make some predictions.
",4080a2d2,0.7428571428571429
34307,38b79494ac749e,81a5a7eb,"As discussed earlier, regularization methods are expected to constraint the weights (model coefficients). 

Is it indeed happening? 

Please do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.",39162a40,0.7428571428571429
34310,04bac111ffbe9c,e205d048,## KNN,82576b17,0.7428571428571429
34313,3cea0f929a2035,f1ef9d86,Now let's see the total number of suicides in the listed countries for 1985-2016.,04cfbade,0.7428571428571429
34317,fc8e0042411c46,06ca0a3a,## City,af476c2a,0.7429467084639498
34319,e4525eb0c96f28,b16b3cee,"Alright, after sucessfully fitting our new model, we can now test the residuals of it. Before we do that, let's first take a look at the the model's parameters to try to see if any of these coefficients are significantly far from zero. This way we can tell if any of these interactions we have added really make a difference. To do this, we will conduct a t-test on the set of parameters. Note that we are going to be using a two-tailed significance level of 0.05 since we want most things to be ""close"" to zero.",2093a1f1,0.7432432432432432
34321,62037c5832129c,feca89b5,"We conclude:

* Assuming that class 1 (malignant) is the positive class in this example, our model correctly classified 71 of the samples that belong to class 0 (benign - true negatives) and 40 samples that belong to class 1 (malignant- true positives), respectively. 
* However, our model also incorrectly misclassified 1 sample from class 0 as class 1 (false positive), and it predicted that 2 samples are benign although it is a malignant tumor (false negatives).",61474350,0.7432432432432432
34326,a5a419dc7245b0,17581dc5,##### Adding the input layer and the hidden layer,4279726e,0.7433628318584071
34327,ac1abfe1dfe815,e4687c81,**Add the other columns**,6529dbcb,0.7433628318584071
34330,34fff8ce731b03,28152b1f,"## Predição sobre os dados de testes

Nesta última etapa, o modelo busca predizer se o cliente está ou não inadimplente sobre os dados de teste (coluna defaulting igual a nulo). Os dados de teste ficarão armazenados no DataFrame *df_test*. ",6f9e5b2e,0.7435897435897436
34332,50b03ce5b1a286,ef7106b6,"#If the target variable is continuous rather than boolean, so you should use a Regression model rather than a classification model. Just change the auto_run line to:

models = ql.auto_run(train, output_name=""Rain"", kind=""regression"", stypes = stypes)",d49896a5,0.7435897435897436
34337,80ad12f326ab70,dc1a6fb5,"Sites, resources and refernece category has more products, then Digital learning platforms",da404a16,0.7435897435897436
34347,fdbbd573ba31c2,6db3e413,"Combined with the previous EDA, we can do feature selection. I delete 'gearbox_temperature(°C)', 'windmill_body_temperature(°C)'.",f7c28d74,0.74375
34348,e9b9663777db82,26ef333f,# Train and validation data,648e8507,0.743801652892562
34363,72d393488311b6,e45420e0,# Create Model,80663df0,0.7441860465116279
34366,49ac6594c8f5cf,85edffa0,****Confusion Matrix****,6f19f28a,0.7444444444444445
34367,396bc36edb95d3,81f2a856,### Building Neural Network Model,965e4f8f,0.7444444444444445
34368,3597174a998d4d,3fcdc933,"Thus, I construct two new variables named new_required_car_parking_spaces and booking_changes_class.",276892ed,0.7444444444444445
34369,b0c2805cd5c087,d9622170,Image mobile-industrial-robots.com,0446f327,0.7444444444444445
34376,b61ab8f81dc03d,18cc6f01,"<a id=""hyperparameter_tuning""></a>
# Hyperparameter Tuning
On this part I will just pick the first 4 models and try to bring better results for each one using the hyperparameter tuning technique.",64d05394,0.7446808510638298
34382,957e035ba5b9d5,f174f0dc,## Create dataframes from directories,778ab3d3,0.7446808510638298
34386,726833f92fb87a,390c727e,"We can encode this feature with ordinal encoding. Moreover, there are 417 samples with unknown education: they will be labeled as nan values.",7dc5e1b6,0.7449664429530202
34388,629f2918807a9b,e03e7dfe,"### In 2021 January, they have sold around 6272 copies while in jan'2020; they have sold around 1060 copies only. Their selling rate increased upto 6 times according to stats of january 2021. Remember there are only success orders in the list (Not returned or cancelled)",be56dc84,0.7450980392156863
34390,523123dad03177,21f611b1,# 9. Relationship between the subject scores,48a5e4e6,0.7450980392156863
34393,d0080e3a39bc5c,d8c82cf8,Lets have a look at how our model is performing now...,2fcde4cf,0.7450980392156863
34395,52cfd66e9ec908,917b54c6,# Baseline model (source: [here](https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb)),c74adcdf,0.7450980392156863
34397,4fa553c2b837d4,7fefeffa,"It's most common to one-hot encode these ""object"" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called get_dummies to get one-hot encodings. Call it like this:",c65a23e9,0.7450980392156863
34398,7cfd96218dd933,88f08223,"#### **ATTENTION**
* DATA WAS RECORDED ON AUGUST 1 AT THE MOST AS HISTORICAL RECORD.",7c34d96c,0.7450980392156863
34399,842547b2def18c,6dbce32d,"## Model, predict and solve

Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:

- Logistic Regression
- KNN or k-Nearest Neighbors
- Support Vector Machines
- Naive Bayes classifier
- Decision Tree
- Random Forrest
- Perceptron
- Artificial neural network
- RVM or Relevance Vector Machine",b8efde6d,0.7450980392156863
34402,510b8303776bb6,6782768e,## Retraining the model over the whole dataset. ,18080db8,0.7452830188679245
34403,fdc3afd309b850,2f0828ac,"<a id=""bp""></a>
## 8.2 BoxPlot ",966bde38,0.7453703703703703
34407,016abae0483764,eb4f8985,### K-Nearest Neighbors,bc9f289b,0.7454545454545455
34411,2ada0305b68956,b6dfd29c,### 127. Palette = 'magma',133e26f4,0.7457142857142857
34413,ed8009f482b380,8234bde6,Elbow method for optimal number of n_neighbors in KNN,e99941fa,0.7457627118644068
34415,9169c4e9c33c90,3d9af0a4,"<a id=""Year""></a>",725bf880,0.7457627118644068
34416,2a56d6b0e153f2,0def6778,# MODEL SELECTION,8dc315e6,0.7457627118644068
34417,a81661cc35d8d2,782e05e3,"<b><font size=""4"">Logistic Regression</font></b>",3331f113,0.7457627118644068
34419,b9bc7dc9f582e5,a4aacf95,# Gridsearch CV to improve performance,15cc4d28,0.7457627118644068
34421,f2e5e9fb9eaaf7,26149a8b,"[back to top](#table-of-contents)
<a id=""6.2""></a>
## 6.2 Base model & feature engineering
This section will `blindly` try feature engineering using previous created notebook [TPS Feb 2021 Base Model & Features Engineering](https://www.kaggle.com/dwin183287/tps-feb-2021-base-model-features-engineering), to see if there are any new features that are useful. This section will use `LGBM Classifier` as the base model.

**Observations:**
- Adding up `multiply` feature increased the model performance which can be seen in `6.2.5 Multiplication of features`.
- Calculate minimum of all features in a row and put it in a new column, slightly increased the model performance, which can be seen in `6.2.2 Minimum of features`.
- Others feature engineering attempts decrease the model performance.

<a id=""6.2.1""></a>
### 6.2.1 Log
It seems converting all the features into a log decrease the OOF AUC substantialy from `0.801` to `0.701`.",048e0d08,0.7457627118644068
34426,979f1e99f1b309,92571a15,## Try Elastic Net Model,d1bfebbf,0.7459016393442623
34429,06ecf7a304c309,c13890e3,"여기서는 적은 epoch로 시도했기에 비교적 불충분한 결과가 나올 수 있지만, 500~1000 epoch 정도로 시도하면 더 좋은 결과가 나올 것입니다.",714de627,0.746031746031746
34438,21413205980558,cd2485f1,"# It can be seen from the above two pictures that the number of marketing activities in July is the most. However, since many marketing activities in July are actually of unkown's, it is necessary to exclude unkown from drawing again.
# 从以上两张图可以看到七月份的营销活动最多，但是由于7月份有很多营销活动其实是unkown的，所以要排除unkown重新作图。",84197de0,0.746268656716418
34439,1a222fee3089d2,969f332d,## **Training**,59ab8894,0.746268656716418
34442,e58e68e4eeefe5,f193abd8,# Model Building with SMOTE,a87662ce,0.746268656716418
34445,b01ee6cb674fa3,36a7c3ba,"# Strategic Rocket Forces - RVSN USSR

",a8ffd35e,0.7463768115942029
34447,3d77c1560bd16e,abeed84c,> Other counties received only Moderna vaccine doses. ,87c141ca,0.7464788732394366
34449,631cd434fc3aa2,fa9687c4,"#### Label encoding
Now it's time to transform the categorical feature that have ordinal relationship.

**NOTE**: I'm not so sure about _MSSubClass_. Maybe we should specify the ordering?",2b74febb,0.7464788732394366
34452,9bcfa825c8b2e6,a981b8a1,"Modele katmak için standartlaştırma işlemi yapıLır veri seti küçük olduğundan ve çok uç bir değer olmadığından
aykırı değerlere herhangi bir işlem yapılmadı bu yüzden robust scaler kullanıldı.",220f36e4,0.7464788732394366
34453,738bfced935b69,0518c995,The distribution of year is skewed to left.,2d3c592d,0.7465753424657534
34456,7e1da639035ac5,c748f411,### <a id='13.2'>13.2 Effective School Leadership ratings statistical analysis</a>,120b6c23,0.7466666666666667
34459,cb570c7b7f0501,82a28660,"# This is The Most Satisfaing plot ! Finally we have found a real reasonable relation between the no-show and something hidden in the data
",a200a0ec,0.7466666666666667
34462,b10bd75889dad9,ce2296e3,### Logistic Regression Model,ee00ceee,0.7466666666666667
34468,ee9ddc756b2d4a,7fafa48f,## Modello 2: uso le features che ho creato con nmf e uso svm,e367eab3,0.7471264367816092
34473,f91f58d488d4af,ba19e62a,"Now, let's create a function which defines our model.
",5df1bbf3,0.7473684210526316
34474,840534f2908a9c,97abb531,*There is a significant difference in pickups and dropoffs fare amount for each burough exceept Manhattan. We can see pickups from Queens is expensive compared to pickups from other Buroughs.Very high difference in pickup and dropoff prices for Staten Island.,8081c3cc,0.7473684210526316
34481,71c3c1eab0377d,1f5405cb,"# Part 2
# Implementation of GBM in H2O.ai for predicting ",52b4e360,0.7478260869565218
34482,4ae6a182abac64,250c5e04,## 4- Modeling ,418676c5,0.7478991596638656
34483,c4386b8a01d66e,6ab77547,"Since the data is not in a uniform shape, we scale the data using standard scalar",dc732bf5,0.7478991596638656
34485,ce9ed5e2d601d7,c8170450,"# Super blender 👀👀👀
Credits: All the submissions as linked in the Data tab.

- https://www.kaggle.com/kavehshahhosseini/tps-dec-2021-simple-ensemble-public-notebooks
- https://www.kaggle.com/samuelcortinhas/tps-dec-feat-eng-pseudolab-clean-version
- https://www.kaggle.com/kaaveland/tps202112-reasonable-xgboost-model
- https://www.kaggle.com/mlanhenke/tps-12-g-res-variable-selection-nn-keras
- https://www.kaggle.com/remekkinas/tps-12-super-fast-blending-tool
- https://www.kaggle.com/remekkinas/tps-12-nn-tpu-pseudolabeling-0-95690
- https://www.kaggle.com/gaolang/tps-dec-2021-simple-ensemble-public-notebooks
- https://www.kaggle.com/ambrosm/tpsdec21-12-eliminate-cover-type-4
- https://www.kaggle.com/pourchot/tps-12-simple-nn-with-skip-connection/notebook
- https://www.kaggle.com/slythe/tps-dec-2021-lightgbm-top-200",f58a2f43,0.7480314960629921
34486,ee23a565163388,602ae5fb,Decision Tree algorithm creates a model that can use to predict the class or value of the target variable by learning simple decision rules inferred from training dataset.,88aacbc4,0.7480916030534351
34490,30fdc4a6e3c1db,5027de18,"What we see:
* August sees the highest sales in Foods which might be because of holidays or other climate conditions like high temperature and high precipitation
* August also sees the highest sales in Household items
* April and June sees the highest sales in Hobbies",6111ddee,0.7485380116959064
34496,268a610bbc64b4,3e2cec7a,# 4. What percentage of bookings do we get through apps (Android + Ios)?,8a16f301,0.75
34501,0800c019d227f2,a12f2f38,## Tuning Result,9cd3ffa1,0.75
34502,99f84fa59cb1da,5365041f,"#### Cool, the data seems to be evenly split",41e95f63,0.75
34511,a2286e7c88bb76,9c5ee28f,## Views with Plotly,be48f3fb,0.75
34512,8c7e00ca3dc5a7,491f0dce,"# Normalize the data. 
",c83346e4,0.75
34518,4c47839b067546,27e05125,"### enginePower 
признак имеет выбросы, избавимся от них.",1f517b02,0.75
34519,71d3e4aee86e3e,ae715568,>  ## 2. Active Cases,69706f0b,0.75
34523,8447633e1d256c,70991fd8,## Building Model,60d593ce,0.75
34525,dc0b0e1cb46c6f,899815e9,"<a id='3.1'></a>
## <p style=""background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;"">3.1 World Map: vaccinations distribution</p>",47b17a7b,0.75
34528,5e02999ca74e7e,f4bae4dc,### **Evaluate Valid Data**,b69da28e,0.75
34534,df51d4c54fbb91,344a31ca,## Hardmodel merge dig+train,4226dd72,0.75
34539,b547f0f38f7744,139aaec3,Print the last layer again:,b6ba66b3,0.75
34540,d4cdedc1cd6d7a,7d1d5419,**VISUALIZATIONS**,84c60e71,0.75
34542,63b44c85e32c1f,bbcfb945,### Mapping one tuple to another,fb9b9562,0.75
34544,0635781991a885,f12fee50,"### Trial with gbdt: 0.84461
### Trial With dart: 0.84451",13ab3e33,0.75
34545,edc19e349fe80a,61e18fee,### 4. Our prediction,7882221a,0.75
34553,6b383ec35229a2,368bee08,"We have a loss of  0.1144 which is better than the previous one (0.1152) without optimal parameters

Next, we run the same model with the same parameters, but now, we will use Glove embeddings",0c41b61b,0.75
34557,bd0e173abb7b52,6f373bc4,"<h5> Observation </h5>

Its good to be married, as we can see from the findings.",9bce3b0d,0.75
34559,1d73d04c3aaae8,6f9aeaf8,"Based on the mean of the point spread of -0.15, we can assume team1 is the home team, with a slight advantage in point spread.
",cd43d0aa,0.75
34563,10b5af05d804ff,3a2a82c2,### Double dataset,4a9b1705,0.75
34570,09bac0c221388e,524771f1,"I already mentioned how extractive summarization is essentially based on **sentence scoring**. Therefore, we need to find a way to give an **importance score** to each sentence, so that we can include in the summary the most important ones. To give each sentence a score, we **sum the relative word frequencies** in each sentence and then we create a dictionary that pairs the sentences and their scores.",bea4aa2e,0.75
34572,d46508f983e086,adbd3917,**Building a Model**,454138b8,0.75
34573,e3c0b55ed519e2,7461bc4d,# RATIO OF THE INDIVIDUALS WHO NEED ICU CARE TO THE AVAILABLE ICU BEDS.,9f51352e,0.75
34575,fc3adf9d45953e,50e7a1d4,"#Below the original code was numb = 101. Since I got Error:single positional indexer is out-of-bounds' I changed to 40. There was 44 female/male answering, I reduced to 40 to fit.",e3789b90,0.75
34580,56a583a039b57c,b3c7b29f,### Group the data over address,c0526ea5,0.75
34582,3cd78d8d6d56e4,3f46906f,# Image Prediction of Unknown Data (Test Data),9f632e94,0.75
34586,3cc097a5859dc1,dd5cfc93,# **MinMaxScaler for Data**,14380d73,0.75
34587,0119199286f381,ab9232f3,"**Confusion Matrix, Recall score and Precision score**",0db72675,0.75
34592,69130a37583a06,fde06b7d,### Plotting P_emailDomain :,65a4de1c,0.75
34595,bc058fe14d3d1b,589e2123,month,d0273670,0.75
34600,d1ff7e10ee0102,8e5b2d13,Tastes like skewness... *Avada kedavra!*,2cc71c3c,0.75
34601,a7eab06345d255,7176ea89,Running model and loading results in Output,f87834a4,0.75
34602,9085cba2265204,3f771729,# Training using machine learning models ,de766eb3,0.75
34604,08e3444f9eddcf,24511978,# Evaluation,1d9d4f73,0.75
34605,7686f42e1f28d2,7c9c1c2d,"As one may expect, the most important features are all from the reviews. Surprisingly, however, features regarding the recipe itself do not play a major role in predicting. Let's look at some of the most important words:",6c128859,0.75
34606,565ad413cd802f,c6bb8ab0,"Next, let's create a test dataset using the `submission.csv` file. Note that the file contains dummy labels (always `'0'`). We'll use the same transforms we used for the training set.",397b074e,0.75
34608,c18267b203f28a,97cd46ba,## Model Summary,09ca8efb,0.75
34609,64a336ac34d95c,a02285ac,# PCA,be73a990,0.75
34611,7650e0ac081e94,72838c85,"### Train

Training for 100 iterations here for demonstration.

I trained the model for about 100000 iterations and selected best model.",0081cee5,0.75
34613,156bbcff05dcea,005fcdd7,# Logloss,66ad1fe9,0.75
34614,eb0ecd6bebeb15,44bb9a6a,Petal Length ile Sepal Length değerlerini toplayarak yeni bir total length özniteliği oluşturalım.,d7b93a60,0.75
34618,c65a65d4041018,fef4c3d4,"In Russia catboost library from Yandex is quite popular. It is even more used than lightgbm, which for some reason is almost not used in other countries.

It is interesting that Theano is still used. It was one of the first frameworks, I suppose a lot of people used it (myself included :)).",824fb229,0.75
34620,6a80f915608fc2,6f06da1f,"### Select Target: numMoA or numSub
And create the 3 sets of X,y s, one for each cp_time.",636938eb,0.75
34621,7ba63a2d9abb58,5fbf6663,"We can see that Cases, deaths and recoveries are all moving together with high correlation",821a261f,0.75
34625,9a040a4f21091e,1f978755,"# RNN
Credit to https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/ for most of the meat and the main skeleton of this code
",f591b57d,0.75
34626,71d5f1925c2b4b,4bf982a6,"#### From the above graph, we can see that there is an logarithmic relationship between views and likes ",63ee03ac,0.75
34627,6b2776f151ed9c,74d06f0b,# PCA,40406d5c,0.75
34629,1a285e4c830f3f,0206bda7,"### Grid Search 
Im obigen Ansatz haben Sie zunächst den Parameter C (für gamma=1.0) optimiert, dann für diesen optimalen Wert von C den Parameter gamma optimiert. Vermuten Sie hier auch einen Fehlschluss? Besser wäre es, alle Kombinationen von ($\gamma$,C)-Wertpaaren zu überprüfen. Das ist aufwändig, bringt aber manchmal etwas:",360b50e9,0.75
34635,109169be630edc,9e69c13a,#Since I can change the name of the column above I reduced the number of snippets.,d1b2947c,0.75
34641,b3681fd423741d,d082f5cd,"# 5. Pada batas berapa kilometer total jarak pemakaian bisa dikategorikan sebagai rendah atau tinggi? Sertakan argumen yang mendukung jawaban.
",1aec06ce,0.75
34643,ab6da5994949a3,9b109c02,## Naive Bayes Test Results,fae6b91d,0.75
34646,1c5aaf7bea6414,e5cec9f1,# Scoring RFM Metrics,34d8f42d,0.75
34647,7c7a7db391c517,8fa90eba,"Based on the geograph, we can find that there is an area that is close to river, but it is still with a really high PM2.5 value. Printing the detailed information regarding the geograph",f53450dc,0.75
34649,57740be713cf12,484398cd,## ***6. Evaluation***,ac122df5,0.75
34651,83df814455f06c,ab9509ed,"# **14. Decision Tree Classifier with criterion entropy** <a class=""anchor"" id=""14""></a>

[Table of Contents](#0.1)",c9cff71a,0.75
34653,a69d41047fdd3e,5729e179,"For a hint or the solution, uncomment the appropriate line below.",b1f28647,0.75
34654,8dd655515e7d18,c0952b86,## Finding Users with high & low score in each Personality Traits & Wordcounts,895f41cf,0.75
34658,fae5023faa435f,590c2853,# Prediction,b37c893b,0.75
34667,96c4c0e36b8ec0,09933df4, **Passangers number of parents / children aboard the Titanic spereated by survival**,4dd6de8c,0.75
34668,87e94f864d74be,7ef42eb1,"* The growth in number of movies on netflix is much higher than that of TV shows
* About 1200 new movies were added in both 2018 and 2019
* The growth in content started from 2013",294bfe9f,0.75
34677,62487bcd70b199,695c7d90,"## <a id='8.2'>8.2. Boosting Methods</a>
## <a id='8.2.1.'>8.2.1. AdaBoostClassifier</a>",f6ae50af,0.75
34680,5b92c712910a11,18cfb129,# Removal of URLs,e1d17100,0.75
34684,0d58c434c7db1e,56bb6daa,Q: Which Country has most Grand Master title?,517e01d3,0.75
34686,6d66ced0028dea,fba5f853,"
# 3. Feature engineering",f50aae52,0.75
34690,0504abe8519634,891c5c74,"# 3. without stopwords, with ngrams",46df846a,0.75
34692,2c8119a4061997,9b75864b,# Training,1836a79c,0.75
34694,be53cf61cd596f,0292aecc,"- In image recognition filed, the effect of RNN model is not ideal, far less than convolution neural network",704627de,0.75
34702,02773bdc5d3c7a,b505356a,"    1. By doing nothing and training the algorithm with the data as it is 
    2. By oversampling the minority class in the training dataset 
    3. By undersampling the majority class in the training dataset 
    4. By applying SMOTE(Synthetic Minority Oversampling TEchnique) on the training dataset",86245f35,0.75
34703,a3ae04b78e45b5,38a5afa2,**HEAT MAP**,4195da8b,0.75
34704,cf46cd6f7c55c0,c65c6889,# Submit Predictions,191b86b8,0.75
34709,73d8e56bc709b1,b4d315d6,1. Height vs Weight?,78ec3cce,0.75
34713,2a724fb7835cdc,7b8d2fa3,**Define the Model**,c38ac61d,0.75
34714,52ee792e228d54,cdd54b92,"### We have test accuracy of 94% , Not bad!!
### Or is it? If we take a closer look, we can see that the recall of customers who opted the loan(our target) is very less (54%). In simpler terms, we predicted 46% of the people as Non Personal Loan takers, when they had actually taken the loan. This is something which affects the marketing campaign and should be reduced.
### Let's look at other models now.",5096094e,0.75
34715,98ea617d18c9cc,eefbb035,# Training the model,e6316d11,0.75
34721,5e1d001f8764e0,8c554f92,##### ACCURACY SCORE= 91.43 %,62be464c,0.75
34727,d8d227c158d883,fb048f8e,### Test,3391b4a7,0.75
34729,9d561aa4a298f3,413d348e,"Let's look to a treemap representation of variants, grouped on variant and location.",f56bdd1c,0.75
34732,712198370d5521,4799d483,The clusters seem to be fairly distributed.,5882e04c,0.75
34733,6f1481148352e9,8b028180,"**The highest number of fires was in Tocantins - 976, Matto Grosso - 941 и Parб - 772.**",7cfbdb8f,0.75
34742,49f2274c1dd516,82d669ad,"# HDE
The Humanitarian Data Exchange (HDX) is an open platform for sharing data across crises and organisations. Provided are data sets with global information on testing, government responses, and school closures.",06b0ffee,0.75
34744,999258a81ba32a,5bcb7971,# Most Frequent Numbers by Ball,48cd3d21,0.75
34746,0a918602a04693,3c3e8459,# Modeling,c1ef0e95,0.75
34747,9cec5ddf8b6f49,685968d9,#### Optuna also provides the study data as a dataframe object,d39fc8e7,0.75
34750,2ada0305b68956,bcee76ff,### 128. Palette = 'magma_r',133e26f4,0.7514285714285714
34751,917957c6c4065f,a18a758a,"1~3위는 모두 lamuqe 채널에서 게시한 동영상으로, 해당 채널은 description을 길게 작성하는 경향이 있는 것 같습니다.  
또한 다른 속성들의 상위 동영상들과 달리, description_length 상위에 있는 동영상들은 제품 혹은 서비스를 리뷰하는 동영상이 많은 비중을 차지하고 있습니다.",55b8ed68,0.7516339869281046
34753,5f32117bcd5255,ea42400b,#### HST OPTICAL THREE,85882abf,0.7516778523489933
34755,56785caebaa256,01c052a3,"## 5.2.2. Results visualization<a class=""anchor"" id=""5.2.2""></a>

[Back to Table of Contents](#0.1)",a792961a,0.75177304964539
34760,e9b9663777db82,74a408ba,As we are predicting the price of the Houses we are going to put the prices column in the Y and rest of the data in X,648e8507,0.7520661157024794
34778,8ec771f5600a61,3a52a1be,# same implementation for test data,48364c1f,0.7525773195876289
34779,0caaec057f7184,673ca866,"## Negative price

- We first find about the items with negative price (one case)
- Further analysis on their features (shop, price, sales)",b875533e,0.7526881720430108
34780,04ff2af52f147b,08f85897,"Now we look at our binned groups for *Age*.  Keep in mind that the x-axis labels are the top end of the age range, so the count includes all values from the previous bar label to the current one ($[0, 16)$, $[16, 20)$, etc.).",d5f37be9,0.7528089887640449
34787,4883314a96dc34,dff4a80f,### Optimize Models by Fitting Parameters (2),50d36836,0.7530864197530864
34788,faa8e6c8ab9246,c5b9431b,"There are outliers in Age, SibSp, Parch and Fare variables. clip() funtion is used to remove the outliers. Parch variable is removed because more than 75% of the values are 0.",2bea1419,0.7530864197530864
34795,663bbc9eaf267b,70f9cee6,"#### We will be using Ordered Label Encoding method to encode the categorical variables. This method:
* Replaces categories with integers based on the target mean.
* Establishes a monotonic relationship between features and target.
* Makes the dataset simple and interpretable, as it doesn't expand the feature space.",32445529,0.7532467532467533
34796,75adb7945ef9bd,2e94c69b,"## 10. Evaluate and Improve Our Model

Several things will be done:
- Cross validation with shuffle split
- Feature selections
- Grid search for hyperparameters
- Identify errors

Reference: [scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)",785c5095,0.7532467532467533
34803,596389bed473be,98b3af20,# Maps,5f8af156,0.7534246575342466
34805,3d08ca7656dec0,761dbc23,# Random Forest,bd3f87e3,0.7534246575342466
34806,b01ee6cb674fa3,d1adf21d,"# EER 

US private company",a8ffd35e,0.7536231884057971
34807,598b6228760590,2cd2cbc8,- LightGBM,be30ab66,0.7536231884057971
34812,ea4e559a86d613,72cfcff4,**Importing torch library for augmentations**,eff47843,0.7536231884057971
34815,7e89d387feb9f5,b7586bec,### Добавленный числовой признак №16. Количество дней между текущей датой и датой размещения последнего отзыва о ресторане.,989e3a1b,0.7536231884057971
34825,a4f8ad33c823c5,776a238b,"To bring further value to the vital signs such as the heart rate, we could also calculate the range and mean of these vital measures.",fcd48307,0.7538461538461538
34827,09751c520b0616,40a982aa,- Some more plots to analyse the data,a4d0c7e9,0.7538461538461538
34829,a8c042af6b7245,c1807469,#### Checking the correlations between ordinal vaiables,2487ac62,0.7538461538461538
34830,03048e86a6d806,a2441999,Most of these data-related job titles consider analyzing and understanding data to influence product or business decisions as the most important part of their job.,1285c231,0.7538461538461538
34836,0858e1bb3cbaca,df3f24b6,# Plots,78548374,0.7540983606557377
34838,9169c4e9c33c90,a247b39c,"# Year

[Back to top](#Top)",725bf880,0.7542372881355932
34839,1294fb4c86f993,6edbaa91,<b> Now we got `census_2016` merged with `totals` of `guns_2016` which is the base of our analysis,4471e513,0.7542372881355932
34846,9e27af2600925c,5f86d858,"**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!

Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set.",9b556435,0.7543859649122807
34849,e03eb63c1f725d,ce10be4c,"<a id=""13""></a>
<font size=""+2"" color=""blue""><b>Hashing Vectorizer</b> </font><br>",e204b7e3,0.7543859649122807
34850,54004b32784b68,39e96107,**> Relation of House's quality and House Price**,27213ca9,0.7543859649122807
34852,9f3710be6aea65,dca578b8,"# Machine Learning

-Logistic regression vs SVM",ae9bda88,0.7543859649122807
34855,30fdc4a6e3c1db,68deeb86,### 4.4 Across Days of Month,6111ddee,0.7543859649122807
34862,614ba9f0c62677,2770a129,"<a id=""16""></a>
### Fit the model",b8551335,0.7547169811320755
34863,07bfec3562f9b3,6804d779,"# Method #2. (Does not work)   
Finding contours and combine then into lines.",327e7d5b,0.7547169811320755
34866,23df07a474aaae,ebc04b66,**Building the Model**,0ea40276,0.7547169811320755
34867,52cfd66e9ec908,796f2370,![](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png),c74adcdf,0.7549019607843137
34877,f35bf4df70d310,60edb554,"#### Function to plot training & validation accuracy curves

Source : https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html",10bb859a,0.7551020408163265
34880,e69a496109e7d8,864a769e,"Here people with zero node count are more likely to survive eventhough there are non-survived people having zero count. People of age range from 50-60 havingzero count are more likely to survive than of people who belong to age range of 40-50 of same case.

People who have not survived and had counts<=10 are of age range 40-60. And people who had counts>10 are less likely to survive",1c640591,0.7551020408163265
34886,a566b5b7c374e7,cbcb9f44,## Deep Sleep Time,b3dc5545,0.7553956834532374
34887,fc8e0042411c46,c42c6e9b,- Most leads are from mumbai with around 30% conversion rate.,af476c2a,0.7554858934169278
34891,d905cde3391d2b,a0ab52a3,"i.e. matches are won by an average of 6.28 wickets with standard deviation of 1.83 (spread = 6.28 $\pm$ 1.83)

### Comparison with IQR

IQR is calculated with respect to **median**, Standard deviation is calculated with respect to **mean**.

Let's compare those for `win_by_runs` data",067dba39,0.7555555555555555
34893,5be39e4e35cec7,0d4c49b1,"<a id = ""9""></a><br>
## Find Missing Value",14d617c9,0.7555555555555555
34895,42e0005bed28aa,600e3bdb,# Accuracy Metrics,5616d451,0.7555555555555555
34898,e0a041e5e2372f,ab782b89,"##### Ideal value of parameters (Videal) - 

Dissolved Oxygen                     14.6

pH                                   7.0

Conductivity                         0

Biological Oxygen Demand             0

Nitrate                              0

Fecal Coliform                       0

Total Coliform                       0",7c4357b2,0.7555555555555555
34900,892be0a523578c,6132686a,"It turns out that 4 components are enough to represent the activity features, and **the customers can be divided into 3 groups**",b0e8d7c0,0.7555555555555555
34903,49ac6594c8f5cf,3f51a70a,Logistic Regression Confusion Matrix,6f19f28a,0.7555555555555555
34904,d6cbd7160961dc,aa12e9e0,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results, considering all cities as if they were one and selecting a sample of size 94200. Note that the chi-squared value of 30028.17 is not meaningful, since the test is inadequate for bigger samples.

* The center graph shows the result for city Oriente, which had the worst chi-squared value. 

* The rightmost graph shows the result for city Miracatú, which had the best chi-squared value. 

Note that both the aggregated analysis and the worst adherence city have a high presence of number 0 in the second digit. This behavior is very usual when the data informed was rounded. For example, instead of an expense of 193.93, the city may have rounded to 200. In this case the number shifted from 3 to 0.",36d74664,0.7555555555555555
34905,c73e07ad6d25c5,3143ac6b,## Create Model,3ab391fb,0.7555555555555555
34906,d96e03a9e7c030,dd753474,### Expected Additional Testtakers - All,d2b72ced,0.7555555555555555
34909,d96642860ab3dd,a9617330,### 2.7 Feter Scaling are mostly used where the distance formula is used in algorithm,98419d48,0.7558139534883721
34913,e19e307b3fd188,63703281,#### Select numerical features,2173955b,0.7560975609756098
34915,8d70dcae7f40a3,c760a148,"precision = 11 / (11 + 7) = 61%

recall = 11 / (123 + 11) = 8.2%",472c71ce,0.7560975609756098
34917,9c26c5dcd46a25,a6f81711,"#### <font color=""#114b98"" id=""section_3_1"">3.1. Eboulis des valeurs propres</font>

Afin d'avoir un aperçu du nombre de composantes nécessaire à l'analyse, nous allons projeter l'**éboulis des valeurs propres** :",1bbbb677,0.7560975609756098
34918,514d8de15cb7ef,cf03d14c, ### C. K Nearest Neighbors ,cfe111b2,0.7560975609756098
34919,4246295d91a6c1,688d7f49,Full-test,9138104a,0.7560975609756098
34923,8d0aebab1e5914,52f49d12,### configuring the parameteres,084e671f,0.7560975609756098
34925,62582b8036fbfe,71e67326,"### Important Logs
1. Model trained on unigram
3. Model takes long time to run given its SVM",6c2160db,0.7560975609756098
34931,4ae6a182abac64,07d05d95,* **Training the Random Forest model** ,418676c5,0.7563025210084033
34932,80ad12f326ab70,628af009,* ### Insights on Engagements data,da404a16,0.7564102564102564
34933,4d91e84c564cbe,624ea59b,"Oh, that's right...

To avoid unpleasant surprises like this, we can use the `in` operator to determine whether a list contains a particular value:",355a43e3,0.7564102564102564
34934,897ca904b74a98,9bbd24b4,The SVM model seems to be the best model ,c5844ad4,0.7564102564102564
34936,d4c5aaa4b36810,507a9d5a,Ridge regression has given a slight improvement. Lets try some non linear models to see if we can improve the results. ,65441f28,0.7564102564102564
34937,71c3c1eab0377d,b9dff643,**Import H2O in python **,52b4e360,0.7565217391304347
34939,2dda7facf3c1e0,395e6631,"## Performing inference 

Congratulations! You have now trained your very own AR-CNN model to generate music. Now you can see how well your model will perform with an input melody. 
",45552d2b,0.7567567567567568
34941,b7b1057764fa02,bfac7ace,"# 7. Test the model

We can now test the model on our testing and evaluation images. We are looking to note the difference between how the model performs with the testing images vs the evaluation or real-world images.",5053a192,0.7567567567567568
34942,ac9b48d531bad9,a22a042f,**APPLING BEST PARAMETERS FOR RANDOM FOREST CLASSIFIERS**,95965e35,0.7567567567567568
34949,a6c34cd514e30e,9b121a61,Let's take a quick look at what the data looks like:,bf603ddd,0.7567567567567568
34957,c84925c8171900,38e5425d,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.2 Genre wise Video Games Sales
            </span>   
        </font>    
</h4>",e21ff7ec,0.7570093457943925
34959,2ada0305b68956,88dfa99e,### 129. Palette = 'mako',133e26f4,0.7571428571428571
34963,9c044fa3072552,25451b44,There seems to be some error in the dataset because there's a huge difference between the accidents during the summers and winters. Our dataset consists data from February 2016 to 2020. This could be one of the reasons. Let's how this distribution looks for the year 2019.,1362842e,0.7571428571428571
34965,38b79494ac749e,2be1ab6d,#### L1,39162a40,0.7571428571428571
34971,7f74a04ae75792,1c076718,### What's the Purchase amount pattern with regards to Gender,d01e91da,0.7573529411764706
34972,c65a65d4041018,27224f74,### data visualization libraries,824fb229,0.7573529411764706
34975,70193f0c034b98,92c6d14a,# Model,f8cacd26,0.7575757575757576
34979,f166950fa915f8,04adbe37,### Predict,a7f6ca5e,0.7575757575757576
34980,dbd96dd275dc60,18599c64,"# The 'Valid RMSLE' value is what we are looking for and it is around  0.2452416398953833, which is very close and puts our code in top 30 of the submissions",1ed493a8,0.7575757575757576
34985,7c89a32e3562ca,beb2dd65,# XGBoost,32dd8913,0.7575757575757576
34988,169177b6e9edea,bc6aa9c3,<h3><b>Deep Learning,ca42152f,0.7578947368421053
34993,9ceb7278784462,852a8d9f,## Model Tuning,3768a567,0.7580645161290323
34995,917957c6c4065f,f391ee19,### 2.9. 하루 평균 인기 동영상의 개수  ,55b8ed68,0.7581699346405228
34999,726833f92fb87a,56682116,## Contact,7dc5e1b6,0.7583892617449665
35001,6903d3f38c6a66,7c3bf4fb,"### Qualitative Colormap

A palette of independent colors, often used for categorical variables.

It is recommended to organize up to 10 colors, and to group more and smaller categories with other.

Repeating colors can be confusing, so try to avoid overlapping as much as possible. It's a good idea to change color to color rather than saturation and brightness.",6067ce5e,0.7586206896551724
35003,a4f0a3e1316ff9,c16ae28d,Plot residuals of the last two weeks,53bf0160,0.7586206896551724
35004,fc8e0042411c46,4af090d3,## Last Notable Activity,af476c2a,0.7586206896551724
35006,00001756c60be8,503926eb,Инициализация класса Data,945aea18,0.7586206896551724
35008,f3c6048d1058e3,972a3a9e,## Neural Network Models,1d9056b0,0.7586206896551724
35013,6a1d04e8153df3,2d9bf1ee,"**VOILIN PLOT**
- One of the plot which is more efficient then box plot.",38572b05,0.7586206896551724
35015,45921c50ac56fa,872bc99f,"LDA creates a dictionary of words from the input, and converts the input into document vectors. ",465973eb,0.7586206896551724
35020,a1ba5ffd30dbde,c45d7bda,- 97% accurate in predicting Disease,48e57546,0.7586206896551724
35021,cd10f3afd970b3,8682e50e,### Let's check 3rd file: ../input/index.csv,2db3c8e4,0.7586206896551724
35024,2c5cb484988da2,32a3858c,"km = KModes(n_clusters=4, init='Huang', n_init=1, verbose=1)
clusters = km.fit_predict(df_test_dummies)
kmodes = km.cluster_centroids_
shape = kmodes.shape
for i in range(shape[0]):
    if sum(kmodes[i,:]) == 0:
        print(""\ncluster "" + str(i) + "": "")
        print(""no-skills cluster"")
    else:
        print(""\ncluster "" + str(i) + "": "")
        cent = kmodes[i,:]
        for j in df_test_dummies.columns[np.nonzero(cent)]:
            print(j)",d94f9784,0.7586206896551724
35025,401338428b2d1c,c3ecab09,## Making the Confusion Matrix,e4b768be,0.7586206896551724
35026,656185a18260be,18c28d21,"# Data Recipes, Progressive Resizing, Random Cropping

The function below probably contributed the most to my good score. 

1. **Data Recipe**: I mix, shuffle and concatenate data into a dataset. I tested various recipes (combinations) of datasets and the one below seemed to work pretty well. In the first epoch, I mix chaii data with Hindi MLQA and XQUAD, TyDi (English, Bengali and Telugu) and English SQUAD. In the second epoch, I mix chaii data with Hindi MLQA and XQUAD, TyDi (English, Bengali and Telugu) and Natural Questions. Then I concatenate both epochs. I often topped it off with only chaii at the end, but it didn't always work well. This approach is different from usual pretraining on SQUAD, and it helped me boost the performance.
2. **Progressive Resizing**: One of the hyperparameters for the training is sequence length of combined question and context window. In the first epoch I use sequence length 256, and in the second epoch 384. 
3. **Random Cropping**: In earlier iterations of my pipeline I had a dedicated function that would randomly crop positive fragments around the answer and negative fragments. Later I concluded that if I use a different stride step on every opoch combined with negative sampling, then it will have the same result and simplify my code. 

The important point is that the entire data recipe is fed as a single dataset to the training loop, which means that the optimizer state and learning rate schedule are preserved throughout the training. ",0318cab5,0.7586206896551724
35029,9535bb04ae042c,8d34031b,## ix) Creating Pickle Files for DataFrame and Pipeline,165b6fae,0.7586206896551724
35041,84127ade6fde87,375d9408,"Now our embeddings look like figure 4.7. While doing this manually isn’t really feasible for a large corpus, note that although we had an embedding size of 2, we described 15 different words besides the base 8 and could probably cram in quite a few more if we took the time to be creative about it.",f55d05b6,0.7586206896551724
35042,1cd8be6e679620,340d8252,"---

* **fiveprime**: The unpaired nucleotides at the 5’ end of a molecule/ chain. Name always starts with ‘f’ (e.g. ‘f0’).
* **threeprime**: The unpaired nucleotides at the 3’ end of a molecule/ chain. Name always start with ‘t’ (e.g. ‘t0’)

* **stem**: Regions of contiguous canonical Watson-Crick base-paired nucleotides. By default, stems have at least 2 consecutive basepairs. Always start with ‘s’ (e.g., ‘s0’, ‘s1’, ‘s2’, …)

* **interior loop**: Bulged out nucleotides and interior loops. An interior loop can contain unpaired bases on either strand or on both strands, flanked by stems on either side. Always start with ‘i’ (‘i0’, ‘i1’, ‘i2’,…)

* **multiloop segment**: Single-stranded regions bewteen two stems. Always start with ‘m’. (‘m0’, ‘m1’, ‘m2’…)
    In the current version of forgi, pseudo-knots and exterior loops segments between stems are treated as multiloop segments.

* **hairpin loop**: Always starts with ‘h’.
",3ce15a43,0.7586206896551724
35046,b61ab8f81dc03d,990a8779,"<a id=""random_forest_regressor""></a>
## Random Forest Regressor
A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. 
I've got the definition and parameters from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html",64d05394,0.7588652482269503
35048,4daf6153275cbf,a2c5213a,"Well, test says the opposite!",51db1961,0.7590361445783133
35054,e0f03003a69819,b6849712,"We see some jumps in data in low ages and higher ages , which is expected behaviour as these age groups are more suceptible to health concerns .
The opposites may also be true as we see some instances of old gae people with dcreasing trend in insurance charges. 

But we cannot ignore this as the data has some consistent values beyond the jump.

We need more data to focus our study in these age groups , and this data is not suficient .",609ad1f4,0.7592592592592593
35056,9ad9a97e628bfa,0e27ad67,**Null Value estimation**,0a7e1136,0.7592592592592593
35060,ba4b3bd184acbb,04658d6f,"### Join

The `join` method is for adding columns of one DataFrame to another based on index values which provides a subset of the functionality of `merge`.",0f5de724,0.7593984962406015
35067,b10bd75889dad9,2fd40167,#### loc_og_t2m_mou_8 has very high p-value,ee00ceee,0.76
35071,10c5a39a87c47e,dbe5a326,### Printing Classification report,09c7337a,0.76
35072,7dd46c750653eb,32a666eb,"**Inference**
* 2019 had the highest number of Divorces (47995) and 2012 had the lowest number of Divorces (41928)",c2644713,0.76
35075,519e936017c30a,641f8211,"Los resultados obtenidos en el gráfico de barras, nos permite observar que el género ""Action"" no es el más vendido, esto recae sobre ""Shooter"" por un valor superior a los 250 millones de dólares.

Una vez visualizado que género es el que más ingresos ha generado, a continuación vamos a examinar que videojuegos son los que más ventas presentan.
",dc34915d,0.76
35076,916a9275d9326e,d7804b75,"### 可視化を深堀してみよう

ここまでの可視化で得た示唆をもとにもっと深堀した分析をする

1. 8時と18時の貸し出し数が極端に多いのは出勤・退勤と被るから→休日はそうでもないのでは？
2. 極端に貸し出し数が少ない日がいくつかあるが、何が起こっているのか？",cbe4b24b,0.76
35077,67b7354e96113a,d336c3c9,"**Knn**
",dca94250,0.76
35085,50d4ddf1953997,e500571d,"Movies seem to be getting shorter through the years, but TV shows have discrete number of seasons, so a swarmplot could be more appropriate.

",90bdddd6,0.76
35087,37e461081e47c5,79419e41,"Note that none of the average, min, max item sales or shop sales appear to be important factors. Lagged data is the most important in predicting future sales. ",b3e6549e,0.76
35090,cb570c7b7f0501,1adf0f85,# ــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ,a200a0ec,0.76
35093,21c1e34efd71b8,95ee0d3d,Lets take a look at the Classification report and the Confusion Matrix for NearMiss and SMTOE cross validations,23b2cdd6,0.76
35098,2bd6c370695ea7,cb738f0b,## Entity and Desc,cbe6aec8,0.76
35099,4945eab98d7d39,0c7ab924,number of shows in particular year,46258ffb,0.76
35101,cee088a6840708,4bceac75,# Step 10 -> Build the model,55463e1c,0.76
35102,30fdc4a6e3c1db,5cd9c4c9,### Plotting sales over the days of month,6111ddee,0.7602339181286549
35103,5ce12be6e7b90e,c9ee3417,"### Access: Slicing
  
We can slice lists just like we did with strings, to get partial lists.  
For example:",c0ab62dd,0.7602339181286549
35104,738bfced935b69,ea1490f0,The distribution of price is skewed to right.,2d3c592d,0.7602739726027398
35105,fdc9f4863744b1,8d123748,Here is our prediction model (single regression) using Gross Square Feet as a predictive variable.,b4529365,0.7602739726027398
35106,2f47abddfd1928,93526419,"Looking to the distribution of survived passengers per title, we can clearly see that there is a different proportion of survived depending on your title, therefore this feature will help us.

We can see that Mr title, that are the male over that are not children anymore, have a very high death rate.

But on the other hand, the titles for young people (Master and Miss) show a higher survival rate, similar to married women (Mrs).

Opposite to what we could expect, soldier and upper class does not show a high survival rate, although the popuplation on these groups is small.",ae33cc0b,0.7603305785123967
35117,b86bda7afe3ac3,a051b67a,ofensiveness,16197934,0.7606837606837606
35118,7e89d387feb9f5,da802ac7,"#### Обоснование: рейтинг ресторана может зависеть не только от того, сколько времени прошло между размещением последних двух отзывов, но и от того сколько дней прошло с момента размещения последнего отзыва до текущей даты.",989e3a1b,0.7608695652173914
35119,72d528df923403,c735a9f0,"Let's analyze the global performance of each product:
- We can notice that each department has a few items with higher performance than the rest.",d51c8e8e,0.7608695652173914
35120,0e2a23fbe41ca9,ccc436bb,"Observations:
- both ```avg_purchases_lag12``` and ```avg_sales_lag12``` have a few outliers in the extremes ",64e4762c,0.7608695652173914
35121,b01ee6cb674fa3,6c6fd0e8,"# General Dynamics

A private USA company",a8ffd35e,0.7608695652173914
35125,a6b9837940ee38,6ce6d1f1,# Submit,52d2acc7,0.7608695652173914
35128,a5a419dc7245b0,1a946309,"**1.add method of object classifier to add layers.<br>
2.Dense function will take care of the first step of ANN i.e. randomly intializing weights of synapses to small number close to 0 (but not 0); done with init = 'uniform' (initialize weights based on uniform distribution) 8 input nodes we know from our dataset; hence input_dim = 8.<br>
3.Forward-propagation by applying the activiation function. Neuron applies the activation fn to the sum of weights inputs. The closer the activation fn value is to 1, the more activated the neuron, and the more activated the neuron, the more it passes on the signal.<br>
4.Use rectifier activation fn for hiddern layers; activation = 'relu' units i.e the output dimensions is set = 6 which is the chosen number of nodes in this hidden layer.<br>
TIP: no rule of thumb to choosing ouput dimensions; can choose average of the number of nodes in the input layer and the number of nodes in the output layer.**",4279726e,0.7610619469026548
35129,c9b4e282e4e2c1,5e52a2bd,I don't think there is a real relation between the cloudy weather and more injuries. ,f44d339f,0.7610619469026548
35137,a4aa36df07fd53,f9b180f7,"# Model Building

Sekarang mari kita coba membuat model matematis untuk memprediksi kemungkinan seseorang termasuk top 20% dari segi gaji.
Kita review lagi data yang kita punya:",d2f42b6d,0.7611940298507462
35147,2473d004f92592,6d2b6704,**Results and Accuracy**,18d3b6ee,0.7619047619047619
35151,e16860fce156b0,963a4631,#Correlation between two attributes with line of best fit and most influential points.,2054f1ce,0.7619047619047619
35153,55a5e31d03df9f,15ef56eb,## <a name='kerasresult'> Evaluating Keras Application Model performance </a>,06dce00f,0.7619047619047619
35157,15eb884262ba09,bb942395,"Slight improvements on the Eurasian front, Hawaii is now a continent.

It appears that on a flat earth clustering continents won't work..",d703bdab,0.7619047619047619
35158,06ecf7a304c309,ea879955,"### 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders

이번 케이스는 sequence to sequence 예측입니다. 앞의 예시에서는 기본적으로 2차원 데이터였고, 이번에는 sequence 데이터는 1차원 데이터입니다.

이런 시퀀스 데이터의 예시에는 시계열 데이터와 문자열 데이터가 있습니다. 이 예시는 기계 번역 등에 적용할 수 있습니다. 이미지에 CNN을 사용했다면, 이 케이스에서는 LSTM을 사용합니다.

대부분의 코드는 아래의 글에서 참조했다고 합니다.

Most of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him.

Reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/

#### Autoencoder Architecture

이 케이스의 오토인코더에서도 입력을 변환하는 인코더와 타겟으로 변환하는 디코더가 존재할 것입니다.
우선 LSTM이 이 구조에서 어떤식으로 작동하는지 알아봅시다.

- Long Short-Term Memory, LSTM은 내부 루프로 구성된 반복적 신경망입니다.(RNN)
- 다른 RNN과 다르게 backpropagation throught time, BPTT를 활용하여 효과적으로 훈련하고, 사라지는 그래디언트 문제를 방지합니다.
- LSTM layer에서 메모리 유닛을 정의할 수 있고, layer에 속하지 않은 각 유닛은 셀의 상태를 나타내는 c와 숨겨진 상태이자 출력인 h 등이 있습니다.
- Keras를 사용하면, LSTM 레이어의 출력 상태와 LSTM 레이어의 현재 상태에 모두 접근 할 수 있습니다.

이제 학습과 생성을 하는 오토인코더 구조를 만들어봅시다. 2가지의 요소로 이루어집니다.

- 시퀀스를 입력으로 받아들이고 LSTM의 현재 상태를 출력으로 반환하는 인코더 아키텍처
- 시퀀스 및 인코더 LSTM 상태를 입력으로 받아 디코딩 된 출력 시퀀스를 반환하는 디코더 아키텍처
- LSTM의 숨겨진 상태와 메모리 상태를 저장하고 (숨겨진 그리고 상태들을) 접근하므로,보이지 않는 데이터에 대한 예측을 생성하는 동안 LSTM을 사용할 수 있습니다.

우선, 고정 길이의 무작위 시퀀스를 포함하는 시퀀스 데이터 세트를 생성합니다. 우리는 무작위 순서를 생성하는 함수를 생성 할 것입니다.

- X1은 난수를 포함하는 입력 시퀀스 의미합니다.
- X2는 시퀀스의 다른 요소를 재생산하기 위해 시드로 사용되는 패딩 된 시퀀스를 의미합니다.
- y는 대상 시퀀스 또는 실제 시퀀스를 나타냅니다.",714de627,0.7619047619047619
35163,1fac5edd4063ba,120d54e4,"#Johanna Döbereiner

a Brazilian Inventor, whose studies were essential for the development of Proalcool, Brazilian Alcohol Program, made Brazil one of the largest soybean producers in the world. She also carried out research with biological nitrogen fixation (BNF) that allowed to increase the productivity of food in the country.https://www.ifia.com/news/brazilian-women-inventors/",04bc01e0,0.7619047619047619
35165,e04e5204572e7e,1e13af61,### Here shoot up the most important tasks of the notebook 😎... ***Training and Validation***,6c888be9,0.7619047619047619
35166,adb8441ad28019,2dd603c4,"<div class=""alert alert-success"">
    <h1 align='center'>Stochastic Gradient Boosting</h1>
</div>",d89de993,0.7619047619047619
35168,8985a124d4b657,87584a14,"Below, we will do a lot of visualizations to understand our data using various scatterplots, jointplots, pairplots, heatmap and correlation. ",586d1846,0.7619047619047619
35172,898d18d501f68d,73909ddb,pairs of variables give the idea between the cover types. So pair plotting will be done on on the dfnew dataset,d8bdea2d,0.7619047619047619
35175,60d500d196eb42,1a2af169,Correlation matrix:,2ad55f3f,0.7619047619047619
35177,a758983a68c014,cfbc8860,"Here I use DataLoader from PyTorch, it loads data by batches. That's very useful tool, if you are not familiar with it I encourage you to have a look to avoid unnecessary coding. My dataset is very small, so I don't need DataLoader here, it's added just for fun, that's why batch_size equals the dataset's number of rows. I simply use the whole dataset for one iteration, here one iteration == one epoch. Pay attention, that `get_input_tensor` function defined above is used only for input layer, that's because CrossEntropyLoss expect true outputs as vector in long format, provided by DataLoader.",ab89f181,0.7619047619047619
35179,53f302571cd4ac,e32b0829,"## Abstraction:
Abstraction refers to hiding the internal details or implementations of a function and showing its functionalities only. <br>
This is similar to the way we know how to turn ON or OFF a light using a switch but we do not care about what is happening behind the socket.",62c28443,0.7619047619047619
35181,1084376bc4897c,cf16f294,# 4.4 Random forest,1b598487,0.7619047619047619
35185,6471597c5d2f66,49512025,Correlation matrix:,a41b4abe,0.7619047619047619
35186,a76e0e8770b7a0,1b5795ee,What about before millenium,02863d3b,0.7619047619047619
35192,659f5f3ef8aa0e,be2eef37,Correlation matrix:,3654c2d0,0.7619047619047619
35193,6f4795cfdc96c7,19d81e38,Correlation matrix:,1f3ab82f,0.7619047619047619
35194,b74076b2f8ba1d,17d253a0,Correlation matrix:,9ace22d4,0.7619047619047619
35196,c818250dd720eb,85cf5f11,"# Data Pre-Processing

The training data above needed to be pre-processed into a form that could train a model. Iafoss' great tiling [technique](http://www.kaggle.com/iafoss/panda-16x128x128-tiles) which he made public early on in the challenge was utilised by a lot of contenders. I experiemented with some resizing methods and edge detection using convolutions but having seen the success of the tiling technique I decided to utilise it with an adjustment to obtain twice as much data. 

I dropped any suspicious image ids from our training data i.e for reasons of mislabelling, pen marks or missing masks.
I then accessed the intermediate layer of the data and padded it to so that both dimensions were multiples of 224, I then selected the 24 224x224 px boxes with the most tissue. I repeated the process with cropping instead of padding to obtain twice as much data. I conducted the same process with the masks to obtain 224x224 label images for each tile. The tiles were saved as png files and my code for the process (padding version, cropping version differs only in two lines) can be found [here](http://www.kaggle.com/dararc/panda-step-1-tiling)

",68ee40de,0.7619047619047619
35200,5ffe6aa38958a1,7484e458,"## 4.4 K-Fold Cross Validation

K-Fold cross validation splits the training set into K sub-sets (folds). K-iterations of training and validation are run while K-1 sets are chosen for training and 1 for validation each time. ",11f5412e,0.7625
35201,fdbbd573ba31c2,573af19c,## Different Models,f7c28d74,0.7625
35203,3dd4294f903768,20a52d8b,Now our data is ready for prediction models. Let's start.,0d89d098,0.7625
35208,a81661cc35d8d2,d68301ab,Creating Logistic Regression Models on our datasets,3331f113,0.7627118644067796
35215,a44368590e878a,c2ea40b4,### City,77743ba8,0.7627118644067796
35218,b9bc7dc9f582e5,c41e6ef2,"Let's try to improve the performance of XGBoost using GridSearchCV by tuning the features with their best. Here, i will try GridSearchCV with minimal setting as kaggle kernal has limitaions. In local environment, one can tune various features.",15cc4d28,0.7627118644067796
35219,2ada0305b68956,2db2f102,### 130. Palette = 'mako_r',133e26f4,0.7628571428571429
35226,a1dcd92986bc84,4c1f41e7,"### Retrieve relevant images

In this example, we use exact matching by computing the dot product similarity
between the input query embedding and the image embeddings, and retrieve the top k
matches. However, *approximate* similarity matching, using frameworks like
[ScaNN](https://github.com/google-research/google-research/tree/master/scann),
[Annoy](https://github.com/spotify/annoy), or [Faiss](https://github.com/facebookresearch/faiss)
is preferred in real-time use cases to scale with a large number of images.",730acaaa,0.7631578947368421
35227,6cade0b6a41ba2,91beb97d,## 4.2. Splitting Train & Test Data,e6110293,0.7631578947368421
35229,c950cff74e51ac,e3137507,"From the data above, room type which is most available in NYC is entire home, and most of it rented in Manhattan. Brooklyn is the neighbourhood where private room most rented.",d59bf323,0.7631578947368421
35234,31b564f11ef638,d57263d0,### Random Forest Regression Model (Apply Grid Search),424f9692,0.7631578947368421
35236,52ee792e228d54,0c9fec12,### Naive Bayes,5096094e,0.7631578947368421
35239,b10bd75889dad9,fe0e0e26,#### Lets check the VIF for multicollinearity,ee00ceee,0.7633333333333333
35240,ee23a565163388,43cd8113,"Training score is 1.0, which is ideal. But the testing score is 0.7 which clearly says that the model is overfitting on the training dataset.",88aacbc4,0.7633587786259542
35244,5a8c553e21c70f,98fe0f29,"## Training and Validation

A neural network model is created using functional API of Keras with TensorFlow backend. Since we are dealing with a severely imbalanced data set, the performance metrics are precision and recall.

A learning rate schedule is defined. First, it starts very low, then ramps up linearly upto max learning rate. Neural network is initialized with random numbers. At first epochs, it is better to have small learning rates for the adaptation of weights. After reaching max, learning rate decays exponentially.",9ebd9d8f,0.7636363636363637
35248,c01049afb6d307,4dfd6ef3,* The person who increased total the absenteeism time in hours is 9.,d37d3b5d,0.7638888888888888
35250,593d1d3d1df05a,b29b09d8,## Loading the Pre-Trained Model(remove the Comments and run it),bc682ffe,0.7638888888888888
35253,166a62ebb4fc3a,56232f84,Finding shape of split data,db48a079,0.7638888888888888
35254,69ac33d79f5130,2f1fa97f,### Consider to include Mapquest data only.,9d760d2a,0.7638888888888888
35260,510b8303776bb6,8580c0be,## Plotting the residual plot for the model,18080db8,0.7641509433962265
35265,64169805aacf17,c78a559f,# Video of the evolution of the paint during the optimization,1f12ded0,0.7647058823529411
35266,629f2918807a9b,2f6b8937,These are trends for buying books with respect to months: January with (7332) and december with (5331) are top of the lisk while march with (137) and February (96) are at the bottom,be56dc84,0.7647058823529411
35269,a0a5baa6c7e12a,3a2f76b5,"As a result, we see that *'Soil_Type15'*, and *'Soil_Type7'* have just one value in every training records. Therefore it won't make any sense to use such features in the model training down the road.

'Id' feature is also a nominal identifier, and therefore it should be excluded from the training set in the model training time down the road. ",551d41de,0.7647058823529411
35271,71b75664517244,fd34daff,José Mourinho perform best at season 2004-2008,fc905af5,0.7647058823529411
35273,ab657da5329e3f,c58ebb06,# Confusion matrix,021526f8,0.7647058823529411
35276,0e662a463309e7,01044c21,Cosine Similarity,84c57e51,0.7647058823529411
35277,c8dbc957870a27,91ecee6c,# Training Final Model,d7e84ab7,0.7647058823529411
35281,99821bc6a45be6,dc75ecc9,### Analysis:,b9d59346,0.7647058823529411
35284,e93a41c03638fe,33706057,# 4. Preparing test data for submission:,7363527b,0.7647058823529411
35289,fa02c409161192,477eeb56,### 1.4.3 How the learning rate effects the performance ,e97077f7,0.7647058823529411
35292,6aaee7fdbc7945,9cd04f8d,# **Question 4**,dae653ae,0.7647058823529411
35293,1d1598b6fa2aa7,5996f6ce,### Custom Buttons in Python,e066accf,0.7647058823529411
35294,907f08f9a2c6cf,f7fdfde5,### Final Results,aa84c325,0.7647058823529411
35300,842547b2def18c,65ba9c07,"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).

Note the confidence score generated by the model based on our training dataset.",b8efde6d,0.7647058823529411
35301,6b7c80ed7bd03d,68db26a8,"# 4. Full Conversion
We can get fully converted dataset via following procedure.",7bba27db,0.7647058823529411
35304,02b7e38902069e,1c84dd3c,#Python Object to Document,726a03a0,0.7647058823529411
35305,7cfd96218dd933,cc0acad4,"#### **ATTENTION**
* THE POINT WITH FRP LATITUDE: 40.024
* THE POINT WITH FRP LONGITUDE: -120.914
* THE POINT WITH FRP DATE: 2021-07-23
* THE POINT WITH FRP TIME: 18.55 FOR TURKEY",7c34d96c,0.7647058823529411
35310,52cfd66e9ec908,ca710ee9,"This is mainly me using the Lyft baseline model and training it, for the purpose of demonstrating how we can fit to a PyTorch model with the provided dataset format. Also, I am using the tool neptune.ai for monitoring the epoch progress.

Also using wonderful ML library catalyst and hydra, which helps your PyTorch training greatly. For now, let's get started on the modelling with some basic import setup:",c74adcdf,0.7647058823529411
35313,c3dfa835621ac4,020a931c,"# Interactive CA Tool

Almost there. The final step is to create an interactive UI integrating all the components above for exploring the tasks and rules ",0126bdad,0.7647058823529411
35319,5f32117bcd5255,6ae95650,#### HST OPTICAL FOUR,85882abf,0.7651006711409396
35325,ff3a8ce61fab6a,fb991ecb,"<hr>
### Exampel 2 ",9afe1654,0.765625
35332,4c47839b067546,7cf9525c,### age,1f517b02,0.7659574468085106
35334,04e6b0d3c70f46,c0851743,### Split data into train and test,56344f77,0.7659574468085106
35336,5f674175839b32,76ce0a66,*Above are the top 10 publishers with highest sales.Nintendo has around 1750 million sales.*,53a2e343,0.7659574468085106
35344,c13f73168789c2,8fec2a4f,"## Subsetting by boolean conditions<a id='29'></a>
You can use boolean conditions to obtain a subset of the data from dataframe.",16175052,0.7662337662337663
35349,722cd844dfbe8f,ab1696c7,"## <span style=""color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold"" id=""section_3_5"">Define CNN Multi-inputs model</span>",0cedb385,0.7662337662337663
35354,3c2033cc99c12c,2f509b2c,#### The visualization of SVM  ,dfa22a54,0.7664233576642335
35356,4bada947d597ac,d408109b,# Preparation to get our final result for test data,eab5094a,0.7666666666666667
35362,be616f0785c32d,26089268,"[Go Top](#top)


<div id='PartDcat1'></div>
#### D.2 Results

##### D.2.1 Subtyping major approaches in vaccine research

###### D.2.1.1 Homology-based approach

**Example 1.** 181b7b57851e6f58a601b68e613d10c10616f774.json. used conserved sequence with SARS-COV (2003 version of SARS), which already have experimentally validated antigenic sequences.

**Example 2.** a2a6e262098539eb875a26800d9f6d3d0d5d1875.json. tested an epitope of Ebola in mouse, and suggested that this epitope is conserved in COVID-19.

**Example 3** 74b00f19c3af87d1081644f02490ba250f57b7ca.json used conserved sequences between COVID-19 and human Coronavirus (HCov-HKU1) to identify epitope.

<div id='PartDcat2'></div>
###### D.2.1.1 Immunoinformatics

Docking/molecular dynamics/protein structures and immunoinformatics such as antigenticity

**Example 1** 73c8af41cfdbf52c0dfba37727e3b94cb56b495e.json used antigenicity Prediction, Docking simulation structural prediction

**Example 2** b38ed62b303eaa444d188deb2ab0b23bbdb79211.json used structure prediction.

Note: Many studies use a combination of the above approaches. 

<div id='PartDepi'></div>
##### D.2.2 Compiled list of epitopes across the above publications
|Epitope|Protein|T/B cell|MHC class|
| --- | --- | --- | --- |
|ILLNKHID|N|T cell|I|
|AFFGMSRIGMEVTPSGTW|N|T cell|NA|
|MEVTPSGTWL|N|T cell|I|
|GMSRIGMEV|N|T cell|I|
|ILLNKHIDA|N|T cell|I|
|ALNTPKDHI|N|T cell|I|
|IRQGTDYKHWPQIAQFA|N|T cell|NA|
|KHWPQIAQFAPSASAFF|N|T cell|NA|
|LALLLLDRL|N|T cell|I|
|LLLDRLNQL|N|T cell|I|
|LLNKHIDAYKTFPPTEPK|N|T cell|NA|
|LQLPQGTTL|N|T cell|I|
|AQFAPSASAFFGMSR|N|T cell|II|
|AQFAPSASAFFGMSRIGM|N|T cell|NA|
|RRPQGLPNNTASWFT|N|T cell|I|
|YKTFPPTEPKKDKKKK|N|T cell|NA|
|GAALQIPFAMQMAYRF|S|T cell|II|
|MAYRFNGIGVTQNVLY|S|T cell|II|
|QLIRAAEIRASANLAATK|S|T cell|II|
|FIAGLIAIV|S|T cell|I|
|ALNTLVKQL|S|T cell|I|
|LITGRLQSL|S|T cell|I|
|NLNESLIDL|S|T cell|I|
|QALNTLVKQLSSNFGAI|S|T cell|II|
|RLNEVAKNL|S|T cell|I|
|VLNDILSRL|S|T cell|I|
|VVFLHVTYV|S|T cell|I|
|DVVNQNAQALNTLVKQL|S|B cell||
|EAEVQIDRLITGRLQSL|S|B cell|
|EIDRLNEVAKNLNESLIDLQELGKYEQY|S|B cell|
|EVAKNLNESLIDLQELG|S|B cell|
|GAALQIPFAMQMAYRFN|S|B cell|
|GAGICASY|S|B cell|
|AISSVLNDILSRLDKVE|S|B cell|
|GSFCTQLN|S|B cell|
|ILSRLDKVEAEVQIDRL|S|B cell|
|KGIYQTSN|S|B cell|
|AMQMAYRF|S|B cell|
|KNHTSPDVDLGDISGIN|S|B cell|
|MAYRFNGIGVTQNVLYE|S|B cell|
|AATKMSECVLGQSKRVD|S|B cell|
|PFAMQMAYRFNGIGVTQ|S|B cell|
|QALNTLVKQLSSNFGAI|S|B cell|
|QLIRAAEIRASANLAAT|S|B cell|
|QQFGRD|S|B cell|
|RASANLAATKMSECVLG|S|B cell|
|RLITGRLQSLQTYVTQQ|S|B cell|
|EIDRLNEVAKNLNESLIDLQELGKYEQY|S|B cell|
|SLQTYVTQQLIRAAEIR|S|B cell|
|DLGDISGINASVVNIQK|S|B cell|
|FFGMSRIGMEVTPSGTW|N|B cell|
|GLPNNTASWFTALTQHGK|N|B cell|
|GTTLPK|N|B cell|
|IRQGTDYKHWPQIAQFA|N|B cell|
|KHIDAYKTFPPTEPKKDKKK|N|B cell|
|KHWPQIAQFAPSASAFF|N|B cell|
|YNVTQAFGRRGPEQTQGNF|N|B cell|
|KTFPPTEPKKDKKKK|N|B cell|
|LLPAAD|N|B cell|
|LNKHIDAYKTFPPTEPK|N|B cell|
|LPQGTTLPKG|N|B cell|
|LPQRQKKQ|N|B cell|
|PKGFYAEGSRGGSQASSR|N|B cell|
|QFAPSASAFFGMSRIGM|N|B cell|
|QGTDYKHW|N|B cell|
|QLPQGTTLPKGFYAE|N|B cell|
|QLPQGTTLPKGFYAEGSR|N|B cell|
|QLPQGTTLPKGFYAEGSRGGSQ|N|B cell|
|TFPPTEPK|N|B cell|
|RRPQGLPNNTASWFT|N|B cell|
|SQASSRSS|N|B cell|
|SRGGSQASSRSSSRSR|N|B cell|
|AGLPYGANK|N|T cell|
|AADLDDFSK|N|T cell|
|QLESKMSGK|N|T cell|
|QELIRQGTDYKH|N|T cell|
|LIRQGTDYKHWP|N|T cell|
|RLNQLESKMSGK|N|T cell|
|LNQLESKMSGKG|N|T cell|
|LDRLNQLESKMS|N|T cell|
|SVLNDILSR|S|T cell|
|GVLTESNKK|S|T cell|
|RLFRKSNLK|S|T cell|
|QIAPGQTGK|S|T cell|
|TSNFRVQPTESI|S|T cell|
|SNFRVQPTESIV|S|T cell|
|LLIVNNATNVVI|S|T cell|
|MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLPNNTAS|N|B cell|
|RIRGGDGKMKDL|N|B cell|
|TGPEAGLPYGANK|N|B cell|
|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD|N|B cell|
|SKMSGKGQQQQGQTVTKKSAAEASKKPRQKRTATKAYN|N|B cell|
|KTFPPTEPKKDKKKKADETQALPQRQKKQQ|N|B cell|
|LTPGDSSSGWTAG|S|B cell|
|VRQIAPGQTGKIAD|S|B cell|
|YQAGSTPCNGV|S|B cell|
|QTQTNSPRRARSV|S|B cell|
|VYQVNNLEEIC|
|SMATYYLFDESGEFK|orf1ab|
|MATYYLFDESGEFKL|orf1ab|
|ATYYLFDESGEFKLA|orf1ab|
|DSATLVSDIDITFLK|orf1ab|
|SNPTTFHLDGEVITF|orf1ab|
|NPTTFHLDGEVITFD|orf1ab|
|PTTFHLDGEVITFDN|orf1ab|
|DGEVITFDNLKTLLS|orf1ab|
|EVRTIKVFTTVDNIN|orf1ab|
|VRTIKVFTTVDNINL|orf1ab|
|RTIKVFTTVDNINLH|orf1ab|
|HEGKTFYVLPNDDTL|orf1ab|
|EGKTFYVLPNDDTLR|orf1ab|
|GKTFYVLPNDDTLRV|orf1ab|
|KTFYVLPNDDTLRVE|orf1ab|
|DLMAAYVDNSSLTIK|orf1ab|
|LMAAYVDNSSLTIKK|orf1ab|
|MAAYVDNSSLTIKKP|orf1ab|
|AAYVDNSSLTIKKPN|orf1ab|
|YREGYLNSTNVTIAT|orf1ab|
|REGYLNSTNVTIATY|orf1ab|
|IINLVQMAPISAMVR|orf1ab|
|VAAIFYLITPVHVMS|orf1ab|
|AAIFYLITPVHVMSK|orf1ab|
|PDTRYVLMDGSIIQF|orf1ab|
|DTRYVLMDGSIIQFP|orf1ab|
|TRYVLMDGSIIQFPN|orf1ab|
|RLTKYTMADLVYALR|orf1ab|
|TMADLVYALRHFDEG|orf1ab|
|TKRNVIPTITQMNLK|orf1ab|
|YEAMYTPHTVLQAVG|orf1ab|
|YDHVISTSHKLVLSV|orf1ab|
|SQSIIAYTMSLGAEN|S|
|SNNSIAIPTNFTISV|S|
|AIPTNFTISVTTEIL|S|
|IPTNFTISVTTEILP|S|
|PTNFTISVTTEILPV|S|
|TNFTISVTTEILPVS|S|
|VKPSFYVYSRVKNLN|E|
|KPSFYVYSRVKNLNS|E|
|PSFYVYSRVKNLNSS|E|
|ATKAYNVTQAFGRRG|N|
|KAYNVTQAFGRRGPE|N|
|YTGAIKLDDKDPNFK|N|


[Go Top](#top)",b78e18aa,0.7666666666666667
35364,6a1ae8234c7653,6102d938,"![](http://)Lowest value so far 25.927
but validation mae is 17805",2d643c72,0.7666666666666667
35365,f6488772605bb5,baa93459,## **Testing with individual images**,068d4697,0.7666666666666667
35370,62487bcd70b199,8cde8328,"KNeighborsClassifier doesn't support sample_weight, hence cannot be used with adaboost",f6ae50af,0.7666666666666667
35375,864302b10e7730,8db3455e,# 2. ScatterPlot,e9dd1d2d,0.7666666666666667
35378,91eaec994e0c6f,0b7b8053,## 3.2 Statistical Method: ARIMA,376aef10,0.7666666666666667
35381,6998861ff6ff01,6790d488,"Yep, it looks like we did parse our dates correctly & this graph makes good sense to me. Why don't you take a turn checking the dates you parsed earlier?",ea9e72cf,0.7666666666666667
35383,9276fa5cc2fef6,5b21a17b,"## Tagging Parts Of Speech And More Feature Engineering..

I suspect that the insincere questions have significant adverbs/adjective that makes them toxic. I am hopeful that these features might model understand various POS structures in the question_text


![POS](https://cdn-images-1.medium.com/max/1600/1*fRjvBbgzo90x0MZdXZT82A.png)",24aa6a52,0.7666666666666667
35384,061d6757dfbce0,bf20cbe6,"## Column Types

Let's look at the number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/) . ",c0c2915a,0.7666666666666667
35391,a2176d4653ef60,ce19d876,# Data Evaluation - KNN,ac908675,0.7669902912621359
35408,22bd95f4807a23,59af40b1,"* For all age groups apart from 35-40 and 80-90, the median ratings score for trendy clothes have remain low.
* Jeans have a high median score for all age groups, except for 80-90.
* Blouses dipped in median scores for the age group *18-25* and *30-35*
* Median rating of swim wear dips for the age group *60-70* ,*40-50* and *70-80*
",c05d356f,0.7674418604651163
35410,dbd96dd275dc60,d6ad2016,# Make preds on test data,1ed493a8,0.7676767676767676
35411,3cd78d8d6d56e4,c48b8b9f,"## Peparing Test Data
The test data for the competition needs to be prepared as well as did with the training data set. Therefore the trained pipeline (trained only on the training dataset) will be used.",9f632e94,0.7678571428571429
35414,585c280865b46e,3c04cd4b,# Search top correlated to histone genes among all top expressed genes ,4d6056f1,0.7678571428571429
35423,548f961125248d,edbc8fb2,Contribution of all the features and all the instances of the features towards predictions are shown using the red/blue dots. ,d8c5e8b8,0.7681159420289855
35426,b01ee6cb674fa3,03edddea,"# Martin Marietta
private US company

Later fused with Lockheed ",a8ffd35e,0.7681159420289855
35430,8336d84cf3ff6b,f6cbd9a5,# Extra Tree Classifier Baseline ,b96b58a0,0.7681159420289855
35432,fd4017c1514157,e32d5b08,Only the first three rows are available ; the full test.csv is in the hidden test set.,fd8f0896,0.7682926829268293
35434,840534f2908a9c,1b8e7c38,"Dropoffs to Bronx and Staten island are long trips. In Manhattan the pickup and dropoffs fare amount has similar distribution. Let us add a field, is_lower_manhattan as we had seen above that dropoffs to lower manhattan had higher trip distance but lower fare",8081c3cc,0.7684210526315789
35436,f91f58d488d4af,d68bd081,"To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0, so our accuracy for each item can be calculated (using broadcasting, so no loops!)",5df1bbf3,0.7684210526315789
35438,ab6da5994949a3,4bddf28a,## Visualising the Naive Bayes Training set results,fae6b91d,0.7685185185185185
35440,2ada0305b68956,bbc9d2d3,### 131. Palette = 'nipy_spectral',133e26f4,0.7685714285714286
35441,2f47abddfd1928,2e4ba084,"## 4.2. Surname

From name feature we can also extract the surname.

It will not be used on the prediction model, but it will be useful to get more powerful features.",ae33cc0b,0.768595041322314
35446,af6556ced704f6,088089ae,"***Missing value***
* We can **drop** missing/nan values using **dropna()**
* We can **filling** with mean or most using value ",881577c0,0.7692307692307693
35454,c115e287523aab,1a13557d,"# Grad-CAM
Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Grad-CAM provides visual explanations to better understand image classification problems.

<img src=""http://gradcam.cloudcv.org/static/images/network.png"" width=""800"">",feb1288b,0.7692307692307693
35456,c970849d1f6da2,6de60f04,"# Visualize embeddings
* We can attempt to understand the language model by visualizing its embeddings that are around input words
* Modify the input word to explore the model further
* Eg. for the input word ""virus"", the closest embeddings within the language mdoel include * coronavirus, human, viral, viruses, infection * etc. However, there are many adverbs such as *finally, however, additionally, and similarily*, which makes sense, because these words occur frequently in the same context, given this paper is research oriented. One explore further cleaning up the text of these kinds words prior to training",056e3955,0.7692307692307693
35457,9eed0fae1c7958,96e7cbfd,# Save model,3fb1438e,0.7692307692307693
35459,03048e86a6d806,061ab3b0,### Programming Language,1285c231,0.7692307692307693
35465,020c28a360b0cd,02567672,"# Reflection
",2ba397f0,0.7692307692307693
35468,897ca904b74a98,3e7daad4,## Optimisation,c5844ad4,0.7692307692307693
35478,d07915a6e6992e,c93b1bea,# Creating a Model,2b912140,0.7692307692307693
35487,aa46e9376825a5,61e65c35,### Find the aircraft types and their occurrences in the dataset,57792d96,0.7692307692307693
35488,9b42412e75d640,425985ea,"For rest of the classifiers I will reppeat the same procedure:
-run cross validation to check on ""possible"" accuracy
-run GridSearch and cross validation in order to tune hyperparameters.",b616570a,0.7692307692307693
35492,aae204e78a48d1,443b5fb3,It doesn't look like we're offering a lower line to attrited customers so now let's check out whether they are using their line less.,53ab6133,0.7692307692307693
35500,163ceeb80d6923,255cfcac,## Classify a piece of text,4adfbb90,0.7692307692307693
35501,3cb96bd8eb364b,fcea1199,### Definitions,3157af7e,0.7692307692307693
35507,f2f2db16a2f86c,b26ccea9,"This is the comparision between the predicted(left) and the original(right) values corresponding to the test set.

Some values are very close while some have a significant difference.",ffc6a115,0.7692307692307693
35515,49ee86d074de69,8e816a1f,"<a id = ""16""></a><br>
## Finding The Intercept & Coefficients",71ccc6d3,0.7692307692307693
35516,09751c520b0616,fc1f2593,## 4. Modeling,a4d0c7e9,0.7692307692307693
35519,a915263bc207da,e81c0d80,"A pretty good match!

### Building Recommendation engine
- Similar movies to the movies rated by the user will be found out using the corelation matrix.
- The similarity score will be boosted for higher ratings and will be penalized for lower ratings.
- A list of desired number of recommenations are presented excluding the ones already seen by the user.


##### Now we will recommend movies for the person with user ID '0' based on his ratings on various movies",b17ebcda,0.7692307692307693
35525,4bbe953f82d29b,454434da,"## Воскресенье - радостный день

Добавим признак, является ли день выходным. Строка получит 0, если день рабочий, и 1, если выходной. Учитываются государственные праздники.",772301f2,0.7692307692307693
35528,ac1abfe1dfe815,f873870e,"Oversampling (Smote), although I didn't use it with all of the models, it was worse the the imbalanced data",6529dbcb,0.7699115044247787
35529,c9b4e282e4e2c1,05070bf3,3-Relation between type of play and type of injury.,f44d339f,0.7699115044247787
35530,a5a419dc7245b0,6b5e6d81,##### Adding the output layer,4279726e,0.7699115044247787
35532,83df814455f06c,62e42afc,### Predict the Test set results with criterion entropy,c9cff71a,0.77
35534,14defffcd250f3,6973210a,# Building Model,3a683b94,0.7701149425287356
35536,2c5cb484988da2,9536c5b3,"Parameters
    -----------
    n_clusters : int, optional, default: 8
        The number of clusters to form as well as the number of
        centroids to generate.
    max_iter : int, default: 300
        Maximum number of iterations of the k-modes algorithm for a
        single run.
    cat_dissim : func, default: matching_dissim
        Dissimilarity function used by the algorithm for categorical variables.
        Defaults to the matching dissimilarity function.
    init : {'Huang', 'Cao', 'random' or an ndarray}, default: 'Cao'
        Method for initialization:
        'Huang': Method in Huang [1997, 1998]
        'Cao': Method in Cao et al. [2009]
        'random': choose 'n_clusters' observations (rows) at random from
        data for the initial centroids.
        If an ndarray is passed, it should be of shape (n_clusters, n_features)
        and gives the initial centroids.
    n_init : int, default: 10
        Number of time the k-modes algorithm will be run with different
        centroid seeds. The final results will be the best output of
        n_init consecutive runs in terms of cost.
    verbose : int, optional
        Verbosity mode.",d94f9784,0.7701149425287356
35540,62037c5832129c,3e2fff47,"* You choose different metric depending on your business scenario.
* **Accuracy = (TP + TN) / (FP + FN + TP + TN)**
* In the case of tumor analysis we want to detect as many malignant tumors as possible but at the same time we don't want to worry patients by misclassifying benign tumors. 
* In case of spam filter classification it might be different where misclassfication might not be as important.
* **Precision and recall** are two extremely important model evaluation metrics. 
* While precision refers to the percentage of your results which are relevant. (True Positive/ Predicted Condition Positive)
    **Precision = TP / (TP + FP)**
* Recall refers to the percentage of total relevant results correctly classified by your algorithm. (True Positive/ Condition Positive)
    **Recall = TP /(TP + FN)**
* Unfortunately, it is not possible to maximize both these metrics at the same time, as one comes at the cost of another. 
* **F-1 score**, which is a **harmonic mean of precision and recall**. For problems where both precision and recall are important, one can select a model which maximizes this F-1 score. 
* For other problems, a trade-off is needed, and a decision has to be made whether to maximize precision, or recall.",61474350,0.7702702702702703
35542,e4525eb0c96f28,587cc98e,"It seems that a small portion of the parameters are actually significantly different from zero. This means that some of our interactions may have a big effect on what the model might predict. Let's now test our model to see how the total sales will change each year depending on what kind of game it is. Here, we will calculate the average yearly change in total sales for a game in every genre, country, and esrb combination using dummy game values so we can get a good picture. Since there are too many different combinations between each category possible, we will only look at American games that are rated T for Teen.",2093a1f1,0.7702702702702703
35544,601e18072783b4,f2109714,## Time Series - Average IMDb rating on Netflix region wise,36b2b1fa,0.7704918032786885
35547,0858e1bb3cbaca,f6d63e77,"Finally, we can draw graphs with these data by using 

**.plot()**",78548374,0.7704918032786885
35548,918040fad252ec,6e20499c,Menampilkan tabel grafik Loss,966fcd8f,0.7704918032786885
35551,5cfb546af2b8ce,0b6a9387,Now we're going to try to use our locational data to create a map ,e8730ad1,0.7708333333333334
35554,8c7e00ca3dc5a7,59710385,## Log Transformation of the SalePrice,c83346e4,0.7708333333333334
35555,2a377ced98d67a,b6a4b23e,## 6. Training ,262231a8,0.7708333333333334
35556,3b5903412fe741,8bbb8a9c,Suppose we'll buy any wine that's made in Italy _or_ which is rated above average. For this we use a pipe (`|`):,ad231969,0.7708333333333334
35557,95656e8d666b16,d497088a,80% accuracy using RandomForestClassifier,65e88599,0.7708333333333334
35559,e82462cdc998a7,27bc04fa,"# Training

    def train_session(model, train_data, valid_data, epochs=EPOCHS, scheduler=None, optimizer=None,
                      loss=None, session_name=None, early_stopping_steps=EARLY_STOPPING_STEPS, output=None):
                  
    def train_kfold(model, optimizer, loss, x, y, initialization, epochs=EPOCHS, prefix="""", output=None,
                    batch_size=BATCH_SIZE):
",b39bf244,0.7708333333333334
35563,ee23a565163388,b9364868,## **Random Forest Classifier**,88aacbc4,0.7709923664122137
35564,4daf6153275cbf,e426bc4a,"H0: There is no difference between local cuisines and other cuisines in terms of rating.

H1: There is difference.",51db1961,0.7710843373493976
35566,fc8e0042411c46,310aa470,- 'SMS Sent' is strong symbol for positive lead ,af476c2a,0.7711598746081505
35567,9169c4e9c33c90,66919b7e,### Create a dataframe for each year,725bf880,0.7711864406779662
35568,1294fb4c86f993,55fdb0b6,#### Data Wrangling the new dataframe `df_merge`,4471e513,0.7711864406779662
35571,f50dc95483c98f,4295c6de,"## **Making the Confusion Matrix**

A confusion matrix is a technique for summarizing the performance of a classification algorithm. Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset. Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.

For More on the Confusion Matrix, you can go on with the [`Machine Learning Mastery Blog on Confusion matrix`](https://machinelearningmastery.com/confusion-matrix-machine-learning/)",cd9e9621,0.7714285714285715
35573,bbaa07ad21cf4e,471e93e5,### LightGBM Classifier,3ab6b254,0.7714285714285715
35579,2730840089c8eb,ba85f9a3,Or to change the value associated with an existing key,34d27dac,0.7714285714285715
35581,04bac111ffbe9c,5fc88396,## Confusion matrix and accuracy score,82576b17,0.7714285714285715
35584,9c044fa3072552,6ac9bd33,### For the year 2019,1362842e,0.7714285714285715
35585,ca73f3d2e25b47,aa6a5eb4,# XGBoost,4cd11efe,0.7714285714285715
35589,0fa9979b5690e9,cfcd492c,"Os resultados melhoraram substancionalmente e são confiáveis, visto que foram avaliados numa estratégia de validação cruzada com 5 partições.",c26eea94,0.7714285714285715
35590,867a9f977fa945,7a896e6d,CONTEXT TABLE BY TREE,2740fcca,0.7714285714285715
35591,6b65d81a5743dd,ce314169,"
Will start with XGBoost",4080a2d2,0.7714285714285715
35593,81712ee7510ac5,99dda3b0,"**if,elif, else Statements**",c4685e79,0.7714285714285715
35595,7a058705183598,a14893bc,By plotting the graph of error rates determining most accurate value of n_neighbors,b0ead917,0.7714285714285715
35596,b3e48999ed0d00,d76eeb2d,## 10 Most important features,fe9ada0f,0.7714285714285715
35599,f4b603905215b7,cb54fbb4,# 5. Tomek Link,efe1d587,0.7714285714285715
35600,7454fdc444df16,4d2c7022,### Transorming RGB image paths to single scale image array using PCA,a7818ef5,0.7714285714285715
35601,47a1b1fe51b4ad,214e6870,"### Model Evaluation

Analysis Loss and Accuracy with Evolution",331ded2f,0.7714285714285715
35607,726833f92fb87a,59fe5cc4,"There is no hierarchy among these features. However since there are just 2 features excluding 'unknown', we could impute these by 1 and 0. <br>
We will replace the rows with unknown contact with nan values.",7dc5e1b6,0.7718120805369127
35609,c3498779cda661,9b0755d6,# Modelo de Agrupación Jerárquica (Hierarchical Clustering),0f531b65,0.7719298245614035
35611,5ce12be6e7b90e,7cd34a1e,"### Exercise: Lists

- Use the lists `birds` and `snakes` to create a single list of strings with the animal names. 
- Add the string `Cobra` to the list. 
- Remove the `Owl` from the list. ",c0ab62dd,0.7719298245614035
35613,30fdc4a6e3c1db,f5c48356,Overall sales are primarily higher in the first two weeks of a month,6111ddee,0.7719298245614035
35615,c2a9f2fb3e1594,f8aaad92,"<a id=""ch10""></a>
## 5.13 Tune Model with Feature Selection
As stated in the beginning, more predictor variables do not make a better model, but the right predictors do. So another step in data modeling is feature selection. [Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) has several options, we will use [recursive feature elimination (RFE) with cross validation (CV)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV).",53411c04,0.7719298245614035
35622,7f74a04ae75792,b4a901ee,"### On average, which group of gender has the higher latest purchase amount?
",d01e91da,0.7720588235294118
35627,e19e307b3fd188,6831ea90,#### Select categorical features,2173955b,0.7723577235772358
35630,0cb9adc158b705,8f9053cc,"Okay, done with training! Lets look at some predictions . . .",3abf056e,0.7727272727272727
35634,a6eb631926a4d7,1156be33,Support Vector Machine,0d502aab,0.7727272727272727
35636,450fda47b03baa,927ab67e,Her Kategorideki ortalama Rating oranlarını hesaplayalım ve görselleştirelim.,62c04adb,0.7727272727272727
35638,e323e594ef918f,8288f996,We can now visualize the grid! Let's look at the CAM map for our centrosome class. ,6e829ab6,0.7727272727272727
35644,5083d7a61f2426,66a6ab30,Metrics of the model,541a0fec,0.7727272727272727
35645,0a918602a04693,2d94be1d,"As, the dataset is in unbalanced nature, so lets use SMOTE algorithm to handle it",c1ef0e95,0.7727272727272727
35648,f269d2fbd5f1be,697adec2,"**Commentary:**
* There is a discernible difference in the salary between between players rated 90 and above, and players rated 75 and below
* For players in the middle rating category (75-89) and over the age of 30, there seems to be an inverse relationship between number of years in NBA and Salary",1264c440,0.7727272727272727
35652,5e1d001f8764e0,0890cb7b,# ADDING DROPOUT LAYER,62be464c,0.7727272727272727
35653,be2f4d8a6b73ca,980dc796,**Applying One Hot Encoding and standardization using column transformer**,5d8ce40a,0.7727272727272727
35655,da199f8fb59439,fc6e2ed6,#### **Tv-show with most number of seasons**,baaa665d,0.7727272727272727
35657,ae058c3f1439c3,7b18d617,"> After looking at the above cloud of words which states the most available Responsibilities, We can Conclude that Some of the most Important Responsibilities Include Planning, Team Work, Development, Secuity, Managment, Strategy, Maintenance etc.",965da99d,0.7727272727272727
35665,930cd79ca51204,bf666dc8,Let's increase the size of the graph and fonts of the labels. Remove the legend.,5506779a,0.7727272727272727
35671,4ae6a182abac64,4bd3315e,* **Predict our model**,418676c5,0.773109243697479
35672,c4386b8a01d66e,4c86721e,# Modeling,dc732bf5,0.773109243697479
35675,063a35f644e3c5,f309248b,"### Create a Predictive model to predict whether a vehicle is SmartWay vehicle or not using atleast two Classification Algorithm of your choice. Also conclude, which is the best model and why not other.
",1c30fb0a,0.7731958762886598
35676,2a123b4e8f9433,064c21b1,Distribution of validation scores,0a082218,0.7731958762886598
35683,7e1da639035ac5,2cedab71,# <a id='14'>14. Strong Family-Community Ties analysis</a>,120b6c23,0.7733333333333333
35684,cb570c7b7f0501,9930e92f,"# One more interested Question. (If a patient doesn't attend his first appiontment. but he booked another one in another day. 
what is the chance of not attending the second appionment !?

is it high or low !?",a200a0ec,0.7733333333333333
35685,b10bd75889dad9,53f3e0ed,#### Build another Logistic Model,ee00ceee,0.7733333333333333
35686,91eaec994e0c6f,192535c9,<b>ARIMA</b> stands for <b>A</b>uto<b>R</b>egressive <b>I</b>ntegrated <b>M</b>oving <b>A</b>verage.,376aef10,0.7733333333333333
35692,f015d0147e8fbf,51bfe9e2,"### Perform Cross Validation Using LightGBM's Built-In CV (all folds run in parallel)

The advantages of using LightGBM's built-in CV are that it not only trains all folds in parallel, but that it also keeps track of the *average* ROC AUC score *across all folds* for each boosting round. This allows me to know the exact round when the average ROC AUC score across all folds was at its maximum.

Unfortunately, lightgbm.cv doesn't support the kind of preprocessing of data within its folds that would be necessary to perform target encoding in a way that doesn't lead to data leakage. Nonetheless, with enough tuning of the parameters related to LightGBM's default handling of categorical features (in particular, reducing 'max_cat_threshold' from 32 to 4), I was able to get the CV score of lightgbm.cv to within 0.0002 below the CV score of my serial CV that computed the average best score across all folds.

Due to its much more rapid training time, I found it helpful to use lightgbm.cv while adding/dropping/engineering new features and tuning hyperparameters. By enabling 'verbose_eval', lightgbm.cv gives me the clearest possible picture of how my choice of learning rate is affecting how my model learns overall. This was invaluable in helping me to decide on a good temporary learning rate to use when tuning all the other hyperparameters, as well as finding the optimal learning rate (low enough, but not too low) to use for final training and test prediction generation. 

For this competition, I ultimately chose to use target encoding, which required me to implement standard, serial stratified K-Fold CV to get my final local CV score. I explain more about that decision below. ",518954fb,0.7735849056603774
35695,0ad8d416b89b78,ffb68898,"# Random Forests:
An initial 'baseline' Random Forest classifier was created to identify a rough estimate before conducting hyperparameter tuning",0b0562f0,0.7735849056603774
35701,87e94f864d74be,93d664dd,> Most of the movies were released in the year 2017 followed by 2018 and 2016.,294bfe9f,0.7738095238095238
35703,71c3c1eab0377d,4d42069e,**Convert Data from Pandas Data Frame to H2O data Frame**,52b4e360,0.7739130434782608
35706,c0ddb77bf32e2b,744eaab1,"That's impressived.  Just can't wait to use tensorflow !!
![release the kraken](https://imgur.com/wJiKf1b.jpg)
[And recently Google release TPU free trial on colab, theoretically it can run much more faster than GPU.](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb)

Back to LSTM.

[For the power of RNN, please have a look at Andrej Karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

[For how LSTM works, please have a look at colah's blog Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)




![Common RNN](https://imgur.com/USXFzHV.jpg)
> Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.

(From Andrej Karpathy blog)

We take One vector to predict another One vector, aka next hour PM2.5, so ""n_outputs""  is one. And We since we have lager n_units size of inputs ,we also apply dropout to force all the neuron participating training.

For the tuning, I use default hyperparameter. [Klaus Greff, Rupesh K. Srivastava, Jan Koutn´ık, Bas R. Steunebrink, Jurgen Schmidhuber(2017), LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)

NIG: No Input Gate

NFG: No Forget Gate

NOG: No Output Gate

NIAF: No Input Activation Function

NOAF: No Output Activation Function

CIFG: Coupled Input and Forget Gate

NP: No Peepholes

FGR: Full Gate Recurrence
![Odyssey](https://imgur.com/FegnNXg.jpg)
> This paper reports the results of a large scale study on
variants of the LSTM architecture. We conclude that the
most commonly used LSTM architecture (vanilla LSTM)
performs reasonably well on various datasets. None of the eight
investigated modifications significantly improves performance.
However, certain modifications such as coupling the input and 
forget gates (CIFG) or removing peephole connections (NP)
simplified LSTMs in our experiments without significantly
decreasing performance. These two variants are also attractive
because they reduce the number of parameters and the
computational cost of the LSTM.
",a0cb45f7,0.7741935483870968
35709,78998e078eaaa1,7731f4b2,## XGBoost trained on all training images,2b29364c,0.7741935483870968
35710,56e58d53ac9c57,bf0530cb,"even though not much, there are some imbalance in recommended column as well",90e2ab8e,0.7741935483870968
35713,0cb456a5456cf9,d6dbc2c4,"# **The characteristic of random forest is that we do not need to select features maunally, random forest generate several decision tree and combine the classification result of each tree to obtain the final result.**<br>采用随机森林作为新的分类器，随机森林的优点在于我们不用手动选择特征。",5701729c,0.7741935483870968
35720,0d9a2067267ba1,f4bec274,"### Modeling;

In this notebook we will test out only catboost model, we can enhance performances by stacking it with other models .

I will use cross evaluation method:
* The training predictions are generated with out-of-folds technique

for more informqtion qbout this method check out this [article](https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/#:%7E:text=An%20out%2Dof%2Dfold%20prediction,example%20in%20the%20training%20dataset.) 
> An out-of-fold prediction is a prediction by the model during the k-fold cross-validation procedure. That is, out-of-fold predictions are those predictions made on the holdout datasets during the resampling procedure. If performed correctly, there will be one prediction for each example in the training dataset

The test prediction is the mean of CV-models predictions

",abc194fb,0.7741935483870968
35725,b59b5aaeedb1fb,30829404,#### Generating the Heatmap of the correlations,1ad63faf,0.7741935483870968
35726,ad26c020235dfc,4a99b3a1,Fill missing feature values by mean.,bf766e48,0.7741935483870968
35732,2ada0305b68956,3a9cf9ae,### 132. Palette = 'nipy_spectral_r',133e26f4,0.7742857142857142
35733,fc8e0042411c46,9b80563f,## **Results**,af476c2a,0.774294670846395
35734,ba4b3bd184acbb,0836417f,***,0f5de724,0.7744360902255639
35736,71b75664517244,7ca3924f,"#### Comparison With Team

Since most of his team is Chelsea, we are going to compare him with chelsea performance",fc905af5,0.7745098039215687
35740,3d77c1560bd16e,84239c85,### Vaccine Providers,87c141ca,0.7746478873239436
35743,631cd434fc3aa2,200132b5,#### Skewed features,2b74febb,0.7746478873239436
35745,06c7ba9203293f,51299b8c,# Predicting missing 'price' values,1e1a2b48,0.7746478873239436
35748,1011899b959f44,09da615f,"# Frequency Counts
This is yet another way to find the size of a particular group. For instance, if we wanted to count the number of attacker kings throughout all the years, we can do so through the frequency count method:
* .groupby().size()",0b112382,0.775
35749,37b09262279764,4e435a8f,#### DecisionTreeClassifier,37c4c417,0.775
35753,3dd4294f903768,71bab32e,***,0d89d098,0.775
35759,254cccd5145725,110fc8c0,# Modeling with XGboost and LightGBM,a49b4037,0.775
35762,62487bcd70b199,1d74ad2c,## <a id='8.2.1.1.'>8.2.1.1. LogisticRegression</a>,f6ae50af,0.775
35765,04ff2af52f147b,6c644a85,"We see that survival rates are quite different between the various age bins with only the $[0, 16)$ and $[34, 40)$ age groups with a survival rate $>50\%$.  The same plot as above for *Age* is now provided for *Fare*.  Keep in mind again that the x-axis labels are the upper boundary for the age range bin ($[0, 7.55)$, $[7.55, 7.85)$, etc.).",d5f37be9,0.7752808988764045
35767,312135b445bd23,ccdcf221,Let's visualize the results:,8ced381f,0.7752808988764045
35769,0e2a23fbe41ca9,2dacf430,"Observations:
- due to large values, the plot with the whole data was not proper, so i filtered the avg_sales_lag3 between -10 and 10
- it seems to be between 0 and 10
- Need to handle the outliers
- there is not apparent pattern between the columns though",64e4762c,0.7753623188405797
35770,b01ee6cb674fa3,74d423d9,"# Yuzhmash

Ukrainian state-owned aerospace manufacturer",a8ffd35e,0.7753623188405797
35772,f1e162ddd14f11,7d9220df,RandomizedSearchCV helps us to find the best parameters for our data. It's also faster then gridsearchCV,cdb2e771,0.7755102040816326
35775,2343dc02ffb96a,8abca8c5,"# Now that we have a model, let's make predictions.",29aa95a4,0.7755102040816326
35777,12f4d16fc21645,98908b22,<h1 style='color:blue'>View Accuracy</h1>,c7752038,0.7755102040816326
35781,c84925c8171900,cd406e3d,"<center><img src = ""https://i.ibb.co/GR3mSjR/Theo-Kate.gif""></center>",e21ff7ec,0.7757009345794392
35783,98f2708375307b,e64fb902,its found that best rmse constant value is 66,9ccac242,0.7758620689655172
35789,1cd8be6e679620,90b5f47a,"## Generating Graph Matrices from the Structures
   * [Referance](https://www.kaggle.com/theoviel/generating-graph-matrices-from-the-structures)",3ce15a43,0.7758620689655172
35794,1750367e54f407,0c86dbed,"First, I test my entire prediction pipeline on the validation set as we have little visibility over the test set.",a8e655b2,0.7758620689655172
35795,84127ade6fde87,c222cabc,![image.png](attachment:image.png),f55d05b6,0.7758620689655172
35797,21413205980558,9a930442,"# Redrawing makes it clearer to see the correlation between months and the number of marketing activities, as well as success stories
# 重新作图可以更清楚的让我们看到月份与营销活动次数，以及成功案例的相关性",84197de0,0.7761194029850746
35806,869a39a3d4dea2,66fc5723,"## Histograms <a id=""histogram""></a>",9020daf8,0.7764705882352941
35813,98a6794067932a,0c8b7a9d,"**2.6 Analyse des destinations desservies**

Maintenant que nous avons été en mesure de déterminer les états où le volume des expéditions était le plus élevé soit la Californie, le Texas et New York, il nous semblait intéressant d'aller raffiner nos recherches au sein de ces états. Pour ce faire, nous avons décidé d'aller observer le comportement des principales villes se trouvant dans ces états. ",08600fe2,0.7766990291262136
35818,09751c520b0616,1e4ea1d7,### Select training and target variable,a4d0c7e9,0.7769230769230769
35821,63b44c85e32c1f,2b3ffadd,### Built In Tuple functions,fb9b9562,0.777027027027027
35824,fc8e0042411c46,7134b2fb,"
Based on the univariate analysis we have seen that many columns are not adding any information to the model, hence we can drop them for frther analysis",af476c2a,0.7774294670846394
35825,c349ee5a821411,628cbb81,"Here we can visualize that the data in Europe and Africa is very scattered so the average Happiness Score will be tricky.
As we will see in some lines, in the top Happier Countries there is a lot of countries from Europe but it doesn't mean that is one of the happier countries. This is because the data is scattered and the biggest groups (countries) are between 5-6 points of the Score.",572b269d,0.7777777777777778
35826,d128317750d689,7d9da009,"Now the network is trained, and we can take a look at the performance of the network. Blue line represents the loss function, and the orange one represents the accuracy. The data comes from the logs I've defined before.",d87f7428,0.7777777777777778
35834,c1984e64b35234,8f98eb25,"1. CG shown to have increased risk of coronary artery disease...
![image.png](attachment:image.png)",1811225b,0.7777777777777778
35835,cf39cde80e66b7,fd7713d3,"# <div class=""h3"">Adjusted R²</div>
<a id=""m5""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.7777777777777778
35838,08b5b53e94599f,9da3b7c2,Let's try getting zip codes out of the lat long information we have.,31609f4d,0.7777777777777778
35845,46778b77b8d195,b3410a94,# Analysis,ec849695,0.7777777777777778
35851,9ad9a97e628bfa,d88e18ab,"**먼저 가장 Null Value가 적은 embarked부터. **총 2개뿐이 Null value이므로 가장 많은 수를 차지하는 ""S"" 로 다체하겠다. ",0a7e1136,0.7777777777777778
35853,0f5085b162bd9f,ef3a2651,"# Gaussian mixture

* Tried w covariance_type='tied' acc = 0.9, 'full' DEFAULT acc = 0.97,  'diag' acc = 0.93,  'spherical' acc = 0.89",a3d989ee,0.7777777777777778
35854,80664f474fe2ef,1f822265,# Model,bd82c7eb,0.7777777777777778
35857,7baeb0ffc6659e,bbbbdc0f,**Cabin**,8cbebba9,0.7777777777777778
35859,c01049afb6d307,78e8fc1e,### (9) Diseases of the circulatory system,d37d3b5d,0.7777777777777778
35866,2b39f4ff896f97,00573550,Plot the train and val curve,3ddfe182,0.7777777777777778
35867,135122550b6483,6e1536f4,Feature Engineering Ref: https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series#Fine-tuning,6592d6d8,0.7777777777777778
35869,b3e0b7e9ff6849,aa7cfdc7,"Look at the new columns that we estimate average spending for each customer in a year. By using this information, you can make many business decisions!

This model can also estimate the retention rate and the probability of the customer is alive. Let's plot the probability matrix together and see what will happen!",f6e4bb0d,0.7777777777777778
35870,20e523830aab51,7165e9fb,# Training/Results,810b8785,0.7777777777777778
35871,d6cbd7160961dc,56f71220,---,36d74664,0.7777777777777778
35872,56cc8fb47bef6a,9ec0ed64,"<p>&nbsp; <span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Fit using Naive Bayes</span></span></p>",652d6670,0.7777777777777778
35876,69ac33d79f5130,2b0bb58c,### Start latitude and longitude analysis.,9d760d2a,0.7777777777777778
35877,9085cba2265204,727f7283,### 1. Random Forest ,de766eb3,0.7777777777777778
35883,b39684e6670dd7,2fdde0dd,# transform unseen data using RobustScaler,83de9873,0.7777777777777778
35884,b6c0ad74f95b8c,47131d50,"The distances? (The ones shown are for current NFL QBs minus Jacoby Brisset). It's nice to see alot of mobile QBs (Wentz, Wilson, Newton) match up with Watson; even though I didn't taking any rushing stats into account. ",5de5b241,0.7777777777777778
35888,d77e6d61ad2e8b,b507c283,# Predict on unseen data,03fd0e96,0.7777777777777778
35889,917957c6c4065f,b0245031,"하루 평균 168개 정도의 영상이 인기동영상에 포함되고, 102~200의 범위를 가지네요.  
한편 중복포함과 중복제외 인기동영상의 개수를 확인해보니,  
하루 평균 약 100개 정도의 다른 일자의 인기동영상이 포함되어있습니다.  ",55b8ed68,0.7777777777777778
35896,6a05614abce6d9,210059cb,## Model Development,c0c9da16,0.7777777777777778
35897,fce6f1b02867e3,5c607e88,## Drop the column,3fb572c2,0.7777777777777778
35905,613bf7bfdcb9e3,178aaec4,### axis = None  => most common values of all items,32beb65d,0.7777777777777778
35906,3597174a998d4d,9d4e5f87,#### 2.2.2.5 number of days that booking in advance,276892ed,0.7777777777777778
35918,f998cece696659,dd67e121,"<a id=""5""></a><br>
# Decision Tree Regression",7964297e,0.7777777777777778
35919,49ac6594c8f5cf,99fca06f,**Does the Accuracy Really Give a Good Picture ???**,6f19f28a,0.7777777777777778
35923,2e0fd6e937bf79,92bd9874,# predict and make submission csv ,6acb965d,0.7777777777777778
35924,c9dc8d00773da4,71321de8,# Create image data,d9aa2f85,0.7777777777777778
35934,dd3721cb49c1fd,87c8cef4,"<a id='10'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
    
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center""><b>10. Plotting the different components 📊</b></h1>
  </div>
</div>",1a53fdd9,0.7777777777777778
35938,2597e45509d551,50763cf9,"AUC of 0.501 is very low, and statistically indistinguishable from a perfectly shuffled train/test split. Nonetheless, for the sake of an exercise, let's try to see if we can find which features are the most responsible for the discrepancy. In order to do this, we'll resort to calculating SHAP values, which can be done directly on GPUs with the version 1.3 of XGBoost. this is not very useful with this problem, but it's a good to show how it wouold be done with a much more imbalanced train and test datasets.",4ef1bf15,0.7777777777777778
35943,1883198d6d8c3c,bc04c159,We can select some columns of the dataframe using column name,69a1d458,0.7777777777777778
35944,2500c5fe8497ee,e104b23c,This is show tha WLD is the largest meat consumption Worldwide,855355f0,0.7777777777777778
35947,c8bf959b9608cf,9ae3e0d9,"## Define a function to feed the generated image and get the output - loss and gradient.
We define function, that takes input as generated image, and return combined loss and grad values using the function f_outputs that we have defined above. ",155e3672,0.7777777777777778
35948,30fdc4a6e3c1db,9564f038,### Plotting sales over the month for the 3 categories,6111ddee,0.7777777777777778
35949,df2a7968c08ee4,c1afcd45,"### Restore Top Model Parameters

In the following cell we are restoring the model parameters that scored best on the validation dataset. See line 70 in the cell above to see how we save model parameters.",a2ba0a72,0.7777777777777778
35952,b9328fe3b0cefc,0a497c1a,### Game detail infomation visualization(比赛详细统计数据可视化),3a35eb23,0.7777777777777778
35953,4883314a96dc34,0fc4f097,### Evaluate Model Performance (2),50d36836,0.7777777777777778
35956,1014e6be391084,9f553d4f,The log transformed data of Amount will be used for further analysis since the Fraud cases have no outliers and the genuine transactions have lesser outliers compared to other charts,46f9168f,0.7777777777777778
35959,5f32117bcd5255,352a411e,### OBSERVATION IMAGE DETAIL,85882abf,0.7785234899328859
35961,ee23a565163388,4c39a1fc,Random Forest is an ensemble method which is collection of multiple decision trees. The predictions from different decision trees are aggregated to obtain the final prediction.,88aacbc4,0.7786259541984732
35966,99bf357eaf61f1,35f80b83,# Model Building,9d92fafe,0.7788461538461539
35968,44f6a002ecd033,d53205e3,"It looks like our first model is about 80-85% accurate which is not bad at all, but we will work to do see if a better model can be achieved starting with a DecisionTreeClassifier.",70bbe106,0.7788461538461539
35975,c13f73168789c2,1de27fd4,## 1. Select rows based on column value<a id='30'></a>,16175052,0.7792207792207793
35979,663bbc9eaf267b,4328cc59,# Modelling,32445529,0.7792207792207793
35981,2cb457b60dd246,e1a38d2a,### Plot the Training Curves,339367df,0.7792207792207793
35987,eb0ecd6bebeb15,b66e33b1,total.length'in ortalama değerini yazdıralım. ,d7b93a60,0.7794117647058824
35988,156bbcff05dcea,61e79360,The Evaluator object has been used to calculate logloss as it seemed that doing the same with the previously used class was tedious. The original class has still been used to show how logloss can be calculated using that for completeness's sake.,66ad1fe9,0.7794117647058824
35996,c4bca5d86a38c3,3fca468e,Funciones para graficar modelo y calcular precision,e23d297c,0.7796610169491526
36002,1294fb4c86f993,b6f6ffcf,Dropping the repeated key column,4471e513,0.7796610169491526
36005,f2e5e9fb9eaaf7,98c205b2,"<a id=""6.2.2""></a>
### 6.2.2 Minimum of features
Create a new feature `min` that calculate the minimum value in a row. There is a very small improvement in the model from `0.801517` to `0.801538`.",048e0d08,0.7796610169491526
36008,6a80f915608fc2,5ec5f0c1,### Choose a ML classifier to use: XGB,636938eb,0.7797619047619048
36011,2ada0305b68956,e61d1a18,### 133. Palette = 'ocean',133e26f4,0.78
36015,91eaec994e0c6f,e260ad95,"Types:
- <b>ARIMA</b>: Non-Seasonal. 
- <b>SARIMA</b>: Seasonal ARIMA.
- <b>SARIMAX</b>: Seasonal ARIMA with eXogenous variables.",376aef10,0.78
36026,0e09587faffa8f,40b89bda,**Toyota** vehicles of body type **4DSD** accounted for the maximum number of summons,0d563d61,0.7804878048780488
36029,786475feda0190,7fa3d4b8,**Traditionally while creating this for production the dead noises should be cleaned out before using these files for modelling. By dead noises I mean the trail (Straight dead lines in the graph) of the graph.**,e4663d97,0.7804878048780488
36030,62582b8036fbfe,0aaac64c,### Submission,6c2160db,0.7804878048780488
36031,fd4017c1514157,5b6d739d,---,fd8f0896,0.7804878048780488
36032,8cefb86a675e5d,94c8f012,#  **~ Decision Tree Regressor: Fit a new regression model to the training set**,79f9e69b,0.7804878048780488
36034,74a03887600114,0ee274cd,## Recommender System,c0ffb2f0,0.7804878048780488
36035,47b2c9be5e31cb,7af214c1,Let's take a quick look at what the data looks like:,7d4afe56,0.7804878048780488
36037,8d70dcae7f40a3,efd5afc6,"### **Đánh giá mô hình bằng Precision và Recall**

#### **Precision:** Tỷ lệ dự đoán đúng trên tổng số dự đoán của Positive.(Người có bệnh)
#### **Recall:** Tỷ lệ dự đoán đúng trên tổng số người bệnh thực tế. (Cũng là TPR)

### **Wrap Up:** *2 tỷ lệ trên đều nằm trong khoảng [0:1], chúng nên được cân bằng với nhau vì nếu một trong 2 tỷ lệ nhỏ hơn nhiều so với cái còn lại thì mô hình không có ý nghĩa*

#### **Ex:** 
#### + Nếu Precision cao nghĩa là số người mình dự đoán bị bệnh đúng nhiều, khi đó nếu Recall thấp thì nghĩa là mình chỉ dự đoán đúng phần nhỏ số người bệnh trên thực tế.
#### + Nếu Recall cao nghĩa là đã dự chính xác số người bệnh trên thực tế cao, khi đó nếu Precision thấp suy ra mô hình đã dự đoán nhầm rất nhiều người không bị bệnh thành bị bệnh. Mô hình trở nên vô nghĩa. Kiểu như câu nói ""Em tin chắc cả thế giới này cũng có người yêu anh."" ",472c71ce,0.7804878048780488
36042,6cade0b6a41ba2,f55b8b39,## 4.3. Data Scaling,e6110293,0.7807017543859649
36047,738bfced935b69,1c754e38,The distribution of mileage is skewed to left.,2d3c592d,0.7808219178082192
36048,91473a39b85068,7645b6a3,### Fitting Logistic Regression with OneVsRest Classifier,6e3d91c2,0.7808219178082192
36058,93f5423667b9d5,69825b23,# 対戦表の作成,55bdf071,0.78125
36060,3cc097a5859dc1,da0f43fd,# **Splitting data**,14380d73,0.78125
36061,9daf8b4a46725e,40094f95,### Making Predictions,7d9cc411,0.78125
36062,51a46d0a7597f5,9aa8c896,"Reactions, Ballcontrol, Composure, Dribbling, & ShortPassing -> are top 5 attributes that drive the market Value of a Player",e9e25b17,0.78125
36063,117fc0956643d0,efd52523,"<div id=""step5""></div>",68cef9fd,0.78125
36066,c85c94076e9c3a,71b97ce0,## Kids_Teen_at_home & Total_Spent,3ea0c443,0.78125
36067,49f2274c1dd516,b033d4e5,"# HIFLD
Homeland Infrastructure Foundation-Level Data (HIFLD) provides National foundation-level geospatial data within the open public domain that can be useful to support community preparedness, resiliency, research, and more.",06b0ffee,0.78125
36075,bd0e173abb7b52,9684d9aa,"**9. What is the maximum number of hours a person works per week (*hours-per-week* feature)? How many people work such a number of hours, and what is the percentage of those who earn a lot (>50K) among them?**",9bce3b0d,0.78125
36077,2c3a6969252dc0,c0b4c79c,> # Modelling,d30f10ce,0.78125
36079,c4386b8a01d66e,157b587b,### Logistic Regression,dc732bf5,0.7815126050420168
36080,d5f78aa381f58d,dff6305c,## Decision Trees,d60f358f,0.7816091954022989
36081,14defffcd250f3,b694667e,**Train Test Split**,3a683b94,0.7816091954022989
36084,6191c1f476437a,4bbd46da,# Plot Result,9dba3159,0.7818181818181819
36085,f0fab078f8533b,4952cc93,## d. Bar plot showing number of content (movie and TV show) added per year,bdb5ea32,0.7818181818181819
36089,ba4b3bd184acbb,cfa8473e,"# Analysis

We will walk through two examples of useful analyses that can be performed on the cleaned data.

### Most Expensive Category

This first analysis will find the 5 app categories with the highest average price.

We can start reading from the compressed csv.",0f5de724,0.7819548872180451
36096,fdc3afd309b850,eb6f3352,"<a id=""dp""></a>
## 8.3 Distribution Plot",966bde38,0.7824074074074074
36097,57bad3860b0fa4,27b4cab2,# Batch Normalization Model,05138a5e,0.782608695652174
36099,90ead00a8ee283,b576ff0d,Save this `DataFrame` to disc as a `csv` file with the name `cows_and_goats.csv`.,612efa48,0.782608695652174
36105,33d736abb432d0,74757dc5,## 发现分词效果并不理想，向其中加入用户词典相关代码,d64052a2,0.782608695652174
36107,0e2a23fbe41ca9,f704c8ea,### 9. numerical_1 vs other columns,64e4762c,0.782608695652174
36115,548f961125248d,a1c6f4da,"### Cross validation to check for overfitting

* Explanation of what cv() function in Catboost does: ""The dataset is split into N folds. N–1 folds are used for training, and one fold is used for model performance estimation. N models are updated on each iteration K. Each model is evaluated on its' own validation dataset on each iteration. This produces N metric values on each iteration K. """,d8c5e8b8,0.782608695652174
36117,b01ee6cb674fa3,2e618664,# Douglas,a8ffd35e,0.782608695652174
36119,2b434130adf886,195be32f,# Get predictions,0c4afeca,0.782608695652174
36120,69d50f5e1373f1,0a518f7e,"The `output_id` is the `id` of the task, followed by the index of the `test` input that you should use to make your prediction. The `output` is the predicted output of the corresponding `test` input, reformatted into a string representation. (You can make three predictions per `output_id`, delineated by a space.) Use the following function to convert from a 2d python list to the string representation.",ec7545ee,0.782608695652174
36121,a1a31459abf078,6f1ec93a,"# Random Forest & XGBoost Models<a name=""model""></a>",66fc0f54,0.782608695652174
36122,fe118026267a88,fdb80673,"In the cell below, write code to save this DataFrame to disk as a csv file with the name `cows_and_goats.csv`.",612efa48,0.782608695652174
36123,e3fb4c6300cb56,a6fe5a4b,"<a id=""12""></a> 
## Count Plot",8ebbdf89,0.782608695652174
36133,9d9da6c439b96b,ab8afa10,the distribution still have outlier but it seems much better,361cc7d9,0.782608695652174
36139,835a7b4e660d23,e747d560,### Setting İndexing,53bc7a6e,0.7831325301204819
36140,eda49464dd6d1b,f33a39ac,"## Dealing with Unbalanced Data
y_train contains all the responses, but it is not in the right format for the proper tensorflow model.  If this were a balanced dataset, we could make a model with 1 output neuron to represent probability.  But the data is 88% ""0"", very unbalanced, and we just saw that the random forest predicted ""0"" for everybody.  Tensorflow will do the same thing.  The way around this problem is to create a model with 2 output neurons, one for ""0"" and one for ""1"", weighting them according to the proportions of each one available.  But for 2 output neurons, the output data needs to be 2-dimensional, not 1-dimensional as y_train is.  We can make y_train and y_test 2-dimensional below.",8421f81f,0.7832167832167832
36143,cf4d1c1ad1476c,70348817,# Creating the model,768c1a59,0.7833333333333333
36144,b10bd75889dad9,839e8b3a,#### All p-values are within .05 hence all the variable are significant,ee00ceee,0.7833333333333333
36146,712198370d5521,b9936291,"**Income vs  spending plot shows the clusters pattern**
* group 0: high spending & average income
* group 1: high spending & high income
* group 2: low spending & low income 
* group 3: high spending & low income  

Next, I will be looking at the detailed distribution of clusters as per the various products in the data. Namely: Wines, Fruits, Meat, Fish, Sweets and Gold",5882e04c,0.7833333333333333
36148,b547f0f38f7744,b4841045,## Train Model,b6ba66b3,0.7833333333333333
36150,c18267b203f28a,dbab693e,"# 保存模型,用于预测提交kernel

* TPU版本直接存全模型要用GCS,用本地磁盘有问题,注意点: [Saving to file a model within TPUStrategy
#36447](https://github.com/tensorflow/tensorflow/issues/36447)

* 参考别人,还是直接先存权重,预测时使用代码定义结构

* TPU上保存 https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/148930#835254

* 又参考别人,发现直接save即可. 比较优雅,Done. 之前不行可能跟一些模型的写法有关.

",09ca8efb,0.7833333333333333
36157,063a35f644e3c5,3009c2e9,### Train Test Split,1c30fb0a,0.7835051546391752
36158,8ec771f5600a61,31e77ad8,# Feature scaling: FOR ORIGINAL TEST DATA,48364c1f,0.7835051546391752
36160,5ce12be6e7b90e,f1b2844c,"## `for` loops

Say we want to print each element of our list:",c0ab62dd,0.783625730994152
36164,2dda7facf3c1e0,b96b9da1,"### How to change the *inference parameters* when you perform inference 

The model performs inference by sampling from its predicted probability distribution across the entire piano roll. 

Inference is an iterative process. After adding or removing a note from the input, the model feeds this new input back into itself. The model has been trained to both remove and add notes, so it can improve the input melody and correct mistakes that it may have made in earlier iterations.

You also can change the *inference parameters* to observe differences in the quality of the music generated: 

- Sampling iterations (`samplingIterations`): The number of iterations performed during inference. A higher number of sampling iterations gives the model more time to improve the input melody.

- Maximum notes to remove (`maxPercentageOfInitialNotesRemoved`): The maximum percentage of notes that can be removed during inference. Setting this value to 0% prevents the model from removing notes from your input melody.

- Maximum notes to add (`maxNotesAdded`): The maximum percentage of notes that can be added during inference. Setting this value to 0% means no notes will be added to your input melody

>**NOTE:** If you restrict your model's ability to add and remove notes, you risk creating poor compositions. 

- Creativity, `temperature`: To create the output probability distribution, the final layer uses a softmax activation. You can change the temperature for the softmax to produce different levels of creativity in the outputs generated by the model.
",45552d2b,0.7837837837837838
36165,27d5291d6365ba,9b41517c,# Grouping customers by their Mean Balance and Transaction Amount,96b30229,0.7837837837837838
36169,63b44c85e32c1f,1394d2e7,**count()** function counts the number of specified element that is present in the tuple.,fb9b9562,0.7837837837837838
36178,62037c5832129c,e60ef892,## Optimizing the precision and recall of a classification model,61474350,0.7837837837837838
36183,d1ff7e10ee0102,ba3dffb7,"Next, please...",2cc71c3c,0.7840909090909091
36184,a566b5b7c374e7,911b5975,## REM Sleep Time,b3dc5545,0.7841726618705036
36187,7cfd96218dd933,f8c0a71f,"#### **ATTENTION**

* HERE IS CHICO
* THE HIGHEST VALUE IN THE WORLD WAS IN THE AMERICA
* THIS SITUATION MAY PROVE THAT THERE IS NO ANOMALY IN THE MEDITERRANEAN LOCAL",7c34d96c,0.7843137254901961
36189,fa02c409161192,76275b2d,"In this section, we will see how the learning rate the performance of the NN. The NN will be trained of the same data set for each iteration and the NN will have the same structure (equal number of hidden layers, same activation functions, etc). Then we will see how the learning rate effects the performance. The NN will ne train on the entire MNIST data set.

We will plot the results, I am not sure what the outcome will be. I predict that for a small learning rate NN will converge slowly and the convergence will improve (convergnce to $100\%$ performance), there may be a decrease in performance for larger learning rates due to the fit being unstable. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that more hidden nodes takes longer to train, I'm guessing here.",e97077f7,0.7843137254901961
36191,917957c6c4065f,7961d7fe,### 2.10. 게시되고 어느 정도 지나서 인기 영상에 갔는가  ,55b8ed68,0.7843137254901961
36192,842547b2def18c,7bc88656,"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.

Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).

- Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.
- Inversely as Pclass increases, probability of Survived=1 decreases the most.
- This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.
- So is Title as second highest positive correlation.",b8efde6d,0.7843137254901961
36193,1a0bd2f72bbe36,b6f5264d,## 03.Multivariate Analysis:,2fa311dc,0.7843137254901961
36194,4fa553c2b837d4,fa056bdb,"**Compare the MAE between**
        1. Numerical predictors, where we drop categoricals.   
        2. One-hot encoded categoricals as well as numeric predictors",c65a23e9,0.7843137254901961
36195,32ddc45133f77b,9e2e0f7c,**Preparing for Submission**,3c0d6831,0.7843137254901961
36196,52cfd66e9ec908,de70d27f,"Now we pretty-print the structure of our configuration. Hydra allows you to easily manage your configuration structure and schema whenever you train a machine learning model - it is also, as an additional benefit, part of the official **Pytorch Ecosystem.**",c74adcdf,0.7843137254901961
36198,523123dad03177,a4fdc202,# 10. Why does the Group E score better than other groups?,48a5e4e6,0.7843137254901961
36200,485de87c50af82,eaf91aa9,### Model Evaluation,a5bd438e,0.7846153846153846
36201,a4f8ad33c823c5,e318b949,Looking at the range of the vital signs would help to classify the vital signs better.,fcd48307,0.7846153846153846
36204,03048e86a6d806,a4fd4b5d,"Getting involved with data most of the time requires programming to deal with it. Let's see what programming languages are regularly used by the data folks. These numbers were derived from dividing the number of people from each job title selecting that programming language, by total population of each job title.",1285c231,0.7846153846153846
36205,f2f2db16a2f86c,f40168a6,### **Measuring the Accuracy**,ffc6a115,0.7846153846153846
36207,a8c042af6b7245,9d22ea86,"## Feature engineering

#### Creating dummy variables

The value of the categorical variables do not represent any order or magintude. For instance, category 2 is not twice the value of the category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original vaiable.",2487ac62,0.7846153846153846
36214,0caaec057f7184,69c4dd3f,"There's one case with negative price, check the prices of the item sold in the shop.",b875533e,0.7849462365591398
36215,c84925c8171900,6516564c,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.3 Top 5 Games by Genre
            </span>   
        </font>    
</h4>",e21ff7ec,0.7850467289719626
36216,2f47abddfd1928,b2b2d1fc,"## 4.3. Male Survived

We have seen during the exploration that males have less survival rate than females.

We can combine sex feature with surname and survived features to create a new feature that shows if within the family there is any male that survived.

If the male in a family survived, it is more likely that the women or children survive.",ae33cc0b,0.7851239669421488
36218,5f32117bcd5255,7aae2d80,#### TRANSFORMATION,85882abf,0.785234899328859
36219,726833f92fb87a,78b0b4e9,## Month,7dc5e1b6,0.785234899328859
36221,67efe818cb2372,b4149982,and then use the model to make some predictions,f28a2a34,0.7857142857142857
36223,953ab4b6631ba8,4f1ad4da,"We see that AUC scores are quite stable across all folds. </br>
The average AUC is : 
0.78631449527.</br> Quite good for our first model!
</br>
</br>
</hr>
Many people will start this kind of problem with a tree-based model, such as random forest. For applying random forest in this dataset, instead of one-hot encoding, we can use label encoding and convert every feature in every column to an integer as discussed previously.
",8bf6bfa1,0.7857142857142857
36232,2d40f383473fa4,adc3d39c,"## 5.1 Model Interpretability

It's not just build the model and make it a black-box, it's necessary to make him interpretable to undestand how the features would contribute to the predictions, therefore, time to see the feature importance.",1da1eff0,0.7857142857142857
36233,565ad413cd802f,84fed7f5,Let's try predicting the labels for some sample images,397b074e,0.7857142857142857
36241,b8ffad33f2b369,572fa89a,"## 3. Unsupervised Outlier Detection

Now that we have processed our data, we can begin deploying our machine learning algorithms.  We will use the following techniques: 

**Local Outlier Factor (LOF)**

The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a 
given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the 
object is with respect to the surrounding neighborhood.


**Isolation Forest Algorithm**

The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting 
a split value between the maximum and minimum values of the selected feature.

Since recursive partitioning can be represented by a tree structure, the number of splittings required to 
isolate a sample is equivalent to the path length from the root node to the terminating node.

This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.

Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees 
collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.",484e5560,0.7857142857142857
36242,b7298d6aaff625,65f491c6,## Making Predictions on Test Data,bdf24bf7,0.7857142857142857
36243,0c57e3132ae184,5ded08a2,#### We already have columns ```has_washer``` and ```has_dryer``` so there is no need to also include ```has_Washer_Dryer``` as it is redundant information. Also upon examination ```host_total_listings_count``` is identical to column ```host_listings_count``` so we should consider removing it.,f6bac298,0.7857142857142857
36245,c54ea4523bd49c,77058d95,Finish training on the rest of the data,097ccba2,0.7857142857142857
36247,38b79494ac749e,95865d37,#### L2,39162a40,0.7857142857142857
36257,2ada0305b68956,1cf93170,### 134. Palette = 'ocean_r',133e26f4,0.7857142857142857
36258,31268b33de97b5,2edbf92d,Using joint plot we can draw a plot of two different variable and see how they are related to each other,1e6f7d14,0.7857142857142857
36259,92e9fc3a0ff5c0,d1505429,"## **So as you can see neutral statements are of size 1500 approx. in both the datasets which is quite large, we will drop them all**",d53da425,0.7857142857142857
36260,916ccf243827f1,ca69a0f8,## 12. Predict,5147f4d2,0.7857142857142857
36264,27778055896e17,29b5afa3,# Ada Boost With Random Forest,1dbe0165,0.7857142857142857
36272,b290039151fb39,e249502f,## Training,1836a79c,0.7857142857142857
36274,62ae2b200f6b36,da72745c,"Mean absolute error for the number of casual bikes is around 25, whereas r2 score is 0.44. The above plot shows the scatter plot of true values and fit line.",7da4ea31,0.7857142857142857
36283,1c381451c17150,0deba539,"# Generating New Monty Python Scripts
This block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.

Pretty much this text generator *tries* to accurately duplicate the Monty Python script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Monty Python *like* scripts 

## The Loopbreaker
This is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.

Changing this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.
",e79b530f,0.7857142857142857
36289,9c33d1955302bf,13720b99,# **Choosing K value**,0d9cfc89,0.7857142857142857
36290,8017d8ece39e95,e0cbf492,# Submission,868ff74e,0.7857142857142857
36292,20c9a2456e494a,8494e153,# Training the CNN,3e487f55,0.7857142857142857
36298,8dd655515e7d18,fa7d1239,"### Openness

It indicates how open-minded a person is. 

A person with a high level of openness to experience in a personality test enjoys trying new things. They are imaginative, curious, and open-minded. 

Individuals who are low in openness to experience would rather not try new things. They are close-minded, literal and enjoy having a routine.",895f41cf,0.7857142857142857
36308,98a6794067932a,12006c72,"La cellule de code ci-dessous ne sert pas à effectuer directement des analyses, mais elle permet d'effectuer des manipulations qui seront utiles pour la suite de nos analyses. Dans un premier temps, le code permet de créer une variable ""adresse"" afin que celle-ci soit utilisée dans Nominatim par la suite. Dans un deuxième temps, Nominatim permet de trouver la longitude, la latitude et l'altitude des différentes adresses clients afin de pouvoir les repérer rapidement sur une carte. Ensuite, le code permet de regrouper ces informations dans un dataframe ayant une colonne pour la longitude, la latitude et l'altitude. Une colonne 'Point' est également créée afin de de regrouper les trois composantes de l'emplacement exact du client en une seule valeur. Finalement, comme nous utilisons une version gratuite de Nominatim, nous devons mettre un délai minimum d'une seconde par transaction. Pour éviter que ce délai soit présent à chaque fois que la variable 'adresse' est utilisée, nous avons créé la variable 'adresse_backup' qui rapporte le dataframe 'adresse' qui a été créé avec Nominatim.",08600fe2,0.7864077669902912
36313,37e461081e47c5,1d2d9855,# Model 5: Random Forest,b3e6549e,0.7866666666666666
36315,7e1da639035ac5,8eb78e09,### <a id='14.1'>14.1 Strong Family-Community Ties % distribution</a>,120b6c23,0.7866666666666666
36317,67b7354e96113a,51ce6161,**Decision Tree**,dca94250,0.7866666666666666
36320,b10bd75889dad9,c3dd3e4c,#### Lets check VIF,ee00ceee,0.7866666666666666
36322,7f74a04ae75792,0802da62,"### What is the distribution of the status of our customers
",d01e91da,0.7867647058823529
36325,979f1e99f1b309,6ebce87d,## Try Decision Tree Model,d1bfebbf,0.7868852459016393
36328,bcd7e398c4d0ec,20574349,# 3. Model Training,77a143f6,0.7868852459016393
36331,ab6da5994949a3,e7010078,## Visualising the Naive bayes Test set results,fae6b91d,0.7870370370370371
36332,2f0f808765fc67,01f42a40,# **F. Random Forest**,fd1f6494,0.7870370370370371
36334,f6648e47713411,e55a3e4c,"## Learning rate finder
https://towardsdatascience.com/the-learning-rate-finder-6618dfcb2025",f4af4d1c,0.7872340425531915
36337,3f25b363afec54,81d00561,"### Data concatination for all camps
",bbdaae25,0.7872340425531915
36338,b61ab8f81dc03d,b3a43972,"<a id=""decision_tree_classifier""></a>
## Decision Tree Classifier
DecisionTreeClassifier is a class capable of performing multi-class classification on a dataset.
I've got the definition and parameters from https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html",64d05394,0.7872340425531915
36339,73893f0467d5e3,eb904ddc,# Train Test Split,279787c6,0.7872340425531915
36340,4c47839b067546,52bd5c33,## price,1f517b02,0.7872340425531915
36342,c7e5f658090347,b0720c6c,<a id='Model_Eval'></a>,43c78e7d,0.7872340425531915
36345,5ffe6aa38958a1,80e4dbd0,# 5. Random Forest: Advanced Optimizations,11f5412e,0.7875
36346,3dd4294f903768,9833c950,# Linear Regression,0d89d098,0.7875
36347,fdbbd573ba31c2,7e92cfaa,## LinearRegression,f7c28d74,0.7875
36348,254cccd5145725,1ea07d96,Here we are gonna use the two best classification models but for the final submission we will use LightGBM as it produces a better score ,a49b4037,0.7875
36349,c9b4e282e4e2c1,5c3f5b2c,"* The type of play that provokes more injuries is pass play, specially knee and ankle injuries. ",f44d339f,0.7876106194690266
36350,a5a419dc7245b0,f7996540,##### Compiling the ANN,4279726e,0.7876106194690266
36352,fdc9f4863744b1,c53f5948,"<ul>
    <li>Final Predictive Model using Single Linear Regression </li>
    <li>Yhat=a+bx </li>
    <li>a is intercept</li>
    <li>b is slope</li>
    <li> final formula == SALE PRICE = 2422.1667333157084 + 0.00095306 * GROSS SQUARE FEET  ",b4529365,0.7876712328767124
36355,a2444ab5d5f147,c132f20a,### Some more look into one_word_review (Phrases),10617755,0.7878787878787878
36357,2f964d08c25d93,f16931f5,### Let's check 3rd file: /kaggle/input/data/review/Austin_Animal_Center_Intakes_Outcomes.csv,1f2e4468,0.7878787878787878
36361,dbd96dd275dc60,16739a43,# Make predictions over test dataset,1ed493a8,0.7878787878787878
36362,a2573183738753,8289a749,# for testing evaluation ,f6429599,0.7878787878787878
36363,4b64dc653fb7eb,f0ac3f71,Separating Train and Test Comments using the index stored earlier.,57675cc2,0.7878787878787878
36364,b241b847319d13,d3c61815,Download these files too (json.dumps and the pbtxt file) for training purpose,0fb698f0,0.7878787878787878
36366,efd44ce2c08541,be420ef8,### Training,ebc2d00c,0.7878787878787878
36367,9169c4e9c33c90,394e179e,### Look at the authors that made multiple appearances in a single year,725bf880,0.788135593220339
36369,869a39a3d4dea2,9bdf2e4a,"Open CV way to compute histograms<br/>**using cv2.calcHist(images,channels,mask,histSize,ranges)**",9020daf8,0.788235294117647
36372,c80939c7c626cf,d52a6097,"# Modelling
This is the most important step in ML",b9ac31e2,0.7883211678832117
36373,3c2033cc99c12c,09636595,"**Findings:** *From the above visualization, under the situation of using SVM on PC1 & PC2, Linear Kernel and RBF have relatively better performance.*",dfa22a54,0.7883211678832117
36375,99bf357eaf61f1,afbfac34,### Linear Regressor,9d92fafe,0.7884615384615384
36376,6f1481148352e9,f4467e2c,"**The highest number of fires was in Goias - 925, Sao Paulo - 915 и Paraiba - 856.**",7cfbdb8f,0.7884615384615384
36385,e19e307b3fd188,432f09aa,#### Handling numerical and categorical features,2173955b,0.7886178861788617
36391,bddd799cdbbae8,e2b4a998, # <a id='8'> 8. Evaluation</a>,b44e3c08,0.7887323943661971
36392,b0c2805cd5c087,63f84f0b,Image cbinsights.com   - 100 startups in different stages of R&D,0446f327,0.7888888888888889
36393,d6cbd7160961dc,24bef340,"# 6. Application 3: Fibonacci Sequence (Type of analysis aggregated)
* 6.1. Preparing the data set
* 6.2. Running the script on the data set
* 6.3.1. Results: First digit
* 6.3.2. Results: Second digit
* 6.3.3. Results: Third digit",36d74664,0.7888888888888889
36394,3597174a998d4d,12a5e405,The lead_time variable can be used to describe number of days that booking in advance. It can be found that there is a positive correlation. ,276892ed,0.7888888888888889
36397,49ac6594c8f5cf,bf10bc52,However we got around 92% accuracy. It doesnt paint a great picture abt the overall performance of this data since the number of placed students are way more than candidates who are not placed as we can see below.,6f19f28a,0.7888888888888889
36398,bd380b97b5c894,69410040,## CNN,66f2562a,0.7889908256880734
36401,d8fb26c4197325,5a96208f,# Finish,b190ac50,0.7894736842105263
36404,f14f6708035916,be21a252,"### Group money spent with days of week
How much money is spent per day on average",ca22d04b,0.7894736842105263
36405,e03eb63c1f725d,d8ad3b7a,"<a id=""14""></a>
<font size=""+2"" color=""blue""><b>Comparison of various BOW methods </b> </font><br>",e204b7e3,0.7894736842105263
36408,bef2347846e476,91b79b17,In the following piece of code first we classify the Content Rating types.And then we plotted the scatter graph with the dimensions Rating and Reviews.Reviews were initially in the type of object(str) but we converted it float with the to_numeric() method.We can say that Mature 17+ class gives better reviews but less values at the ratings than others( with green spots at the graph ).And the best ratings and best reviews are coming from the teenagers.,cb93bf51,0.7894736842105263
36409,52ee792e228d54,99e7c85a,### K Nearest Neighbours,5096094e,0.7894736842105263
36410,0d8df2c2983694,db061177,### Simple Logistic Regression,9bf7fa4e,0.7894736842105263
36411,757fa8de4edc4c,15da2443,To check correlation between scaler coupling and fc i have used this [kernal](https://www.kaggle.com/artgor/using-meta-features-to-improve-model/notebook),87211008,0.7894736842105263
36412,9e27af2600925c,fdaf0965,Let's also plot the cost function and the gradients.,9b556435,0.7894736842105263
36413,5ce12be6e7b90e,defce4b0,"Python’s `for` loop syntax allows us to iterate over the elements of a `list`, or any `iterable` value. Python's `for` is similar to the `foreach` statement in other languages, rather than `for(i=0; i<n; i++)`:

```py
for loop_variable in iterable:
    statement1
    statement2
    statement3
    ...
```",c0ab62dd,0.7894736842105263
36414,f05342aabe2b59,621ae305,### Look at saved snapshots,cfbb391f,0.7894736842105263
36416,9d27afa9ca3f96,e0473ade,train yolov5,2d86a18d,0.7894736842105263
36417,c950cff74e51ac,e4fd6277,**Frequency of AirBnB Reviews in NYC**,d59bf323,0.7894736842105263
36423,29437539745aa5,a7d2c9e7,"<h3 style=""text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;"">4.2 INFER</h3>

---
",c17b490a,0.7894736842105263
36425,9f3710be6aea65,1557773c,### Logistic Regression,ae9bda88,0.7894736842105263
36426,89afa0f49378c7,8369afc1,"#### :)
",32dbe10b,0.7894736842105263
36429,caa0ce2715bf34,08e6939f,"## Observations
1. The silhouette_score is highest with 3 numbers of clusters and lowest for 9 & 10 number of clusters.
2. The Minimum silhouette Width, all the values for all the clusters in analysis are negatives, with minimum value at n = 9 and maximum at n = 2.

**Hence lets form an analysis with n_clusters = 3.**
## KMeans with clusters = 3",78a5dc51,0.7894736842105263
36431,038abade89e59f,abca3a6a,# Check Distribution,cb32a3fe,0.7894736842105263
36435,f35ee6e9fab592,e5fe9949,"The distribution of video game releases (i.e., the video games reviewed; however it also gives a great idea of video game releases in general)",b15f7073,0.7894736842105263
36436,d93a87fdbdb3d2,689a3c9c,#### Tokenize,30d079c3,0.7894736842105263
36443,6b955982396c14,ec0290e0,"## 문제3
- 데이터셋(basic1.csv)의 'age'컬럼의 이상치를 더하시오!
- 단, 평균으로부터 '표준편차*1.5'를 벗어나는 영역을 이상치라고 판단함",2b4cb71b,0.7894736842105263
36444,30fdc4a6e3c1db,cfd3fd14,"What we get:
* Food Items are bought in both the first and second week of a month
* Hobbies and Household are only primarily bought in the first 3 days of the months.",6111ddee,0.7894736842105263
36446,54004b32784b68,3251948f,**> Price and Age Relation**,27213ca9,0.7894736842105263
36448,b01ee6cb674fa3,21a73b15,"# Agenzia Spaziale Italiana - ASI

",a8ffd35e,0.7898550724637681
36450,7e89d387feb9f5,49f4ea62,### Добавленный признак №17. Числовая часть ID_TA.,989e3a1b,0.7898550724637681
36451,4ae6a182abac64,77211952,* **Accuracy of the model**,418676c5,0.7899159663865546
36455,83df814455f06c,d250e710,### Check accuracy score with criterion entropy,c9cff71a,0.79
36457,faa8e6c8ab9246,37147256,There are no outliers in the test dataset,2bea1419,0.7901234567901234
36458,4883314a96dc34,f2ad6521,Performance metrics on ***train set***:,50d36836,0.7901234567901234
36465,04bac111ffbe9c,9c9c70c5,## Creating O/P file,82576b17,0.7904761904761904
36466,55a5e31d03df9f,bba6b94e,So far this has been our best model to perform with the pooling layer we reduce a little bit the overfitting and got around 67% performance on the test set. This is the way to go.,06dce00f,0.7904761904761904
36473,806ce45c8fa303,6fdda49e,## Non-linear Classifier,3e5c34dc,0.7906976744186046
36474,22bd95f4807a23,4423d256,# Sentiment Scores,c05d356f,0.7906976744186046
36476,1660daf8867980,9f78c904,# 2.4 Q-learning,42d7cffc,0.7906976744186046
36486,21413205980558,5df66f98,"# It can be concluded that:
# 可以综合得知：
> # Marketing activities are mainly concentrated in January, April and August;
# 营销活动主要集中在1月、4月、8月；",84197de0,0.7910447761194029
36488,a4aa36df07fd53,289a7473,Kita hapus dulu variable yang jelas jelas merupakan target. Tidak valid model untuk memprediksi gaji tapi meminta inputan gaji kan?,d2f42b6d,0.7910447761194029
36492,2ada0305b68956,14059dd3,### 135. Palette = 'pink',133e26f4,0.7914285714285715
36496,a3ae04b78e45b5,acf67641,**BOX PLOT**,4195da8b,0.7916666666666666
36497,62487bcd70b199,5d0b9698,## <a id='8.2.1.2.'>8.2.1.2. GaussianNB</a>,f6ae50af,0.7916666666666666
36501,e82462cdc998a7,2580a59c,### Tabnet pretraining,b39bf244,0.7916666666666666
36502,56a583a039b57c,64911aed,## Displaying top 10 addresses with highest ever recorded native balances/Stake,c0526ea5,0.7916666666666666
36505,e2a907e1c7d7f9,aa917e91,## Train Model,f09fb692,0.7916666666666666
36508,2c8119a4061997,1c2b55e2,"I have performed a check of different optimizers and schedules on a [similar task](https://www.kaggle.com/c/Kannada-MNIST/discussion/122430), and [Over9000 optimizer](https://github.com/mgrankin/over9000) cosine annealing **without warm-up** worked the best. Freezing the backbone at the initial stage of training didn't give me any advantage in that test, so here I perform the training straight a way with discriminative learning rate (smaller lr for backbone).",1836a79c,0.7916666666666666
36509,95656e8d666b16,ec413307,### Metrics,65e88599,0.7916666666666666
36513,28a1ff0f223da9,e1814081,"# Task-4
**Which research areas were most common in Pakistan?**",c945b27d,0.7916666666666666
36514,923e97b05be00b,163aa589,"## 4. Evaluating training results and model performance

Once training is finished, **evaluating the training curves** can help us **tune our hyperparameters** in subsequent experiments by telling us if we're **underfitting** or by showing us when the model begins to **overfit**.

Finally, we will demonstrate the model output on the test data set (in this case, one of each type of radiograph).

>At the end of your experiments, you want to evaluate network performance on an **independent, *held-out* test data set**, as the model will be *indirectly* exposed to the validation data set over successive experiments, potentially **biasing the model hyperparameters** toward overfitting to the validation data.",3a4a22dd,0.7916666666666666
36522,2a377ced98d67a,378e0915,"As all the features are positively correleated with target, here simple linear regression is experimented.",262231a8,0.7916666666666666
36528,593d1d3d1df05a,54db73a8,### Checking if the model is the same,bc682ffe,0.7916666666666666
36538,c13f73168789c2,8c01696a,"### 1.1 To select all rows whose column contain the specified value(s)<a id='31'></a>
Syntax 1 : `df.column_name == value`

Syntax 2 : `df.loc[df.column_name == value]`

Syntax 3 : `df[df.Open == value]`

Syntax 4 : `df[df.column_name.isin([value1, value2, ...])]`",16175052,0.7922077922077922
36539,90691864eb68c7,0c70c341,# 5. Train and Test Split,3555ef9b,0.7922077922077922
36540,75adb7945ef9bd,1068d167,"Findings:
- 'keyword_target' is the top positive coefficient, meaning the keyword column made a good feature
- hiroshima both as text and hashtag made the top 20 positive coefficients
- Punctuation count and stop word count are among top 20 negative coefficients
- None of the bigrams made the top features",785c5095,0.7922077922077922
36541,4ae464582bac51,ae25cb95,## SORTING OUT BASE TRAIN/TEST**,ca6a52ce,0.7922077922077922
36543,241cf32abb22d8,0bde8955,"The optimal Naive Bayes model based on the full features yields a meal AUC score of 0.889, while the optimal one based on the top 10 features has a mean AUC score of 0.931.",47157066,0.7922077922077922
36546,09751c520b0616,f0676ced,### train_test_split,a4d0c7e9,0.7923076923076923
36550,a070fd03ae8ed2,51761f28,"# 7. Анализ четвертой модели (4)
---
## 7.1 Расчет прогноза дефолта ",c0ec4138,0.7924528301886793
36555,510b8303776bb6,5b76897c,"Since the distribution of points is mainly randomly around the trendline, Therefore the XGBoost Regression machine learning model is appropriate on this data set.",18080db8,0.7924528301886793
36556,614ba9f0c62677,3ddbcdbb,"<a id=""17""></a>
### Evaluate the model
* Test Loss visualization
* Confusion matrix",b8551335,0.7924528301886793
36557,23df07a474aaae,f85f6b5f,**Define a checkpoint callback**,0ea40276,0.7924528301886793
36561,9c26c5dcd46a25,4b648ad0,"**Les 2 premiers plans factoriels couvrent une inertie d'un peu plus de 77%**. Une analyse sur $F_1$ et $F_2$ semble donc cohérente.

Projetons à présent le **cercle des corrélations** :

#### <font color=""#114b98"" id=""section_3_2"">3.2. Cercle des corrélations</font>",1bbbb677,0.7926829268292683
36563,fd4017c1514157,542ba5fc,# Let's Explore Audio Data,fd8f0896,0.7926829268292683
36565,5fc2f23dfbeeb1,3cbedccd,### Prediction on Train Data,f37b4110,0.7931034482758621
36575,a1ba5ffd30dbde,44f9e3eb,<h2 align='center'> Gradient Boost </h2>,48e57546,0.7931034482758621
36576,1dd9c6aa74d289,7c6734f5,"#### List 5 of the most popular crags in Spain, USA, France",5ef9a1be,0.7931034482758621
36580,18a96bb5711ed9,4687f68a,"# Reasons of incidents <br>

Here are 19 major causes of death from our dataset. ",e79768db,0.7931034482758621
36583,bb8f5d7807718b,51790dc1,"# 9 Matplotlib XKCD Plots

Well, if you want to add some twist to your matplotlib plots, you can simply call the xkcd()method on the pyplot object as follows. Here we are working with GDP dataset of India, which shows the GDP growth rate percentage from 2010 from 2019.",181ec286,0.7931034482758621
36585,00001756c60be8,032e2820,"Создаем список признаков, используемых в модели - отбор признаков",945aea18,0.7931034482758621
36588,4b7039cb44a54c,60f84052,# XGBoost,24e806af,0.7931034482758621
36589,6b54e39f86bdb5,0d7da4b1,"## Postprocessing

We need to convert the results in a csv files for submission for the kaggle competition. The exact shape should have the id of the test exmaple and the class predcited.",198084bc,0.7931034482758621
36590,5ea840754577e3,8b342974,"There is a skew in the distribution of fare paid and it is to the left. When distribution of fare is plotted but separated by survival, we can see both distributions are skewed to the left. But the density of the non-survived plot is higher than the survived plot. Indicating people who paid a higher fare had a higher chance of survival.

The 3rd quantile value of the fair is 31. And it can be seen that a higher proportion of people survived who had paid more than that amount.

The spike in the beginning of the not survived distribution can be accounted to the disproportionate amount of male passengers.",9cf9b73f,0.7931034482758621
36593,20e1ba19eb9b5e,bacf4970,## 2.5 Split training and valdiation set,4569bfc1,0.7931034482758621
36594,858da4bb312f67,323aa968,#### CBB to HEALTHY,9cca4391,0.7931034482758621
36596,84127ade6fde87,ae2c6e81,"As you’ve probably already guessed, this kind of work can be automated. By processing a large corpus of organic text, embeddings similar to the one we just discussed can be generated. The main differences are that there are 100 to 1,000 elements in the embedding vector and that axes do not map directly to concepts: rather, conceptually similar words map in neighboring regions of an embedding space whose axes are arbitrary floating-point dimensions.",f55d05b6,0.7931034482758621
36597,14defffcd250f3,f5eb848c,We don't need train test split as we have test dataset so we'll not be using it anymore,3a683b94,0.7931034482758621
36600,25ed87d1f0cb06,35b79720,"# Using a two-class model (DR and No_DR)<a class=""anchor"" id=""7""></a>

As we have seen before, the prediction are very accurate to predict if someone has Diabetic Retinopathy or not. Nevertheless, it is not good at predicting the intensity of Diabetic Retinopathy when it is present. Maybe because it is subjective to the doctor to rate the intensity degree and different doctors don't have the same way to evaluate them. Maybe there are other factors taken in consideration to evaluate the intensity, which are independent from the pictures. As this point, without knowing more about the data, we can only speculate.

In this chapter, we'll reduce the label to a two-class model, because we can imagine that the most important part of this analysis is to find out if someone has Diabetic Retinopathy or not.",7ca68782,0.7931034482758621
36604,fb5c6021d127ef,079e5017,"What you should see here is that the model is underestimating the number of rides by quite a bit. 

## 7) Exercise: Average daily rides per station

Either something is wrong with the model or something surprising is happening in the 2018 data. 

What could be happening in the data? Write a query to get the average number of riders per station for each year in the dataset and order by the year so you can see the trend. You can use the `EXTRACT` method to get the day and year from the start time timestamp. (You can read up on EXTRACT [in this lesson in the Intro to SQL course](https://www.kaggle.com/dansbecker/order-by)). ",dd05cbd3,0.7931034482758621
36610,e9b9663777db82,40a9b6bb,Now we can start the prediction.,648e8507,0.7933884297520661
36613,f3d5d8917ce5df,d5b8d93b,"# Train and predict! <img src = ""https://cdn.ventrata.com/image/upload/ar_1.5,c_fill,dpr_1.0,f_jpg,w_600/v1543512847/puu4dyjmualnnrtppag0.png"" align = ""right"" width = 200>
Ok, finally we're ready to train the model and get some predictions",e45112f8,0.7936507936507936
36615,8985a124d4b657,0de3c056,The above plot gives us the idea that these two conentrations are positively correlated with very few outliers.,586d1846,0.7936507936507936
36617,06ecf7a304c309,a0c0dd2f,이제 케라스에서 모델을 만들어봅시다.,714de627,0.7936507936507936
36619,225b4fe5d3894a,3eed4b41,"<a id=""8""></a>
## 8. Fine-Tune the Model",4b4197b3,0.7938144329896907
36622,2a123b4e8f9433,f728ffb6,# Submit best model,0a082218,0.7938144329896907
36623,ee23a565163388,e3b04896,The training score is 1.0 whereas the testing score is 0.77.,88aacbc4,0.7938931297709924
36626,21bce4ec54b3fa,779cda2b,"Ok, so the score decreased slightly, but we reduced input dimension by 80% and prediction time twice!
Of course here top 20 was chosen arbitrarily, we could do a GridSearch approach and get multiple scores for different number of top variables and see how does the trade-off landscape between accuracy and complexity looks like.

In general, in-built selection is preferrable, because importances are calculated directly when fitting trees, so it saves computation and additional code lines, but in general it's good to know alternative methods for later use with algorithms that don't have in-built feature importance calculation.",35546e30,0.7941176470588235
36627,9cec5ddf8b6f49,af696153,## Quick Visualization for Hyperparameter Optimization Analysis,d39fc8e7,0.7941176470588235
36628,3d905ce4828057,9eb0ea48,"# COMMENT
When we look at 1-Month and 12-Month cltv forecast values, 12-Month clv values are estimated over 1-Month clv and there is not much difference between these reports. From this, we can deduce that the clv values of the customers have increased steadily, not exponentially, over time.",5b006cc3,0.7941176470588235
36629,e4c6dd957eb5ce,2320b8ed,"Cool!!! 
It's a good chart to understand the pattern of competitions on Kaggle and their distribution throught the time

We can note that something has changed after September, 2017.... Maybe new politician by Google or what? <br>
To keep at it, I will take a look at prizes distributions.",2e383665,0.7941176470588235
36634,395ed8e0b4fd17,f99053e9,# <center>AREA PLOTS</center> ,7573ea31,0.7941176470588235
36636,02b7e38902069e,f873dcbe,#Tokenization and Sentence Segmentation,726a03a0,0.7941176470588235
36637,c65a65d4041018,29474c86,"Nothing unexpected here. R users use ggplot2, Python - matplotlib/seaborn and plotly.

Tableau is quite a popular tool to visualize data in companies.",824fb229,0.7941176470588235
36643,8f50c9c16db95f,cf900f62,"The hypothesis is, based on data from average people, most of the players are right footed ([6](https://www.psychologytoday.com/us/blog/the-asymmetric-brain/202002/5-scientific-facts-about-left-footedness#:~:text=Most%20people%20are%20right%2Dfooted,et%20al.%2C%202020)), and they most likely choose to position at the left back side of the ball before the approach.

This is because players need to place their support leg, which is most likely the left leg at the left side near the ball ([7](https://ftvs.cuni.cz/FTVS-2332-version1-the_biomechanics_of_kicking_in_soccer_a_review.pdf)). If they come from the right side, the route will be more curved and make the kicking action awkward.",26cc763a,0.7941176470588235
36660,91473a39b85068,4e796e12,"We have a very high dimensional data and we need to built many models in a binary representation. To tackle this, I have taken the help of Logistic Regression with One vs Rest classifier. The classifier takes each of the labels and train 1000 logistic regression models. Training a Logistic Regression model is very cheap and easy when compare to other models like Support Vector Machines (SVM), Random Forest etc..and it performs really well on high dimensional data.",6e3d91c2,0.7945205479452054
36661,3d08ca7656dec0,c8a1b4ad,# KNN,bd3f87e3,0.7945205479452054
36664,34fff8ce731b03,2168111f,"Os dados de teste são gerados e armazenados em *X_test*, excluindo a coluna *defaulting* que desejamos predizer. O modelo faz a predição e tem como saída os valores da predição em *y_test*.",6f9e5b2e,0.7948717948717948
36668,d4c5aaa4b36810,8866a5b2,"Clearly we are over-fitting and need to try Ridge and Lasso regression.

### Polynomial Lasso Regression",65441f28,0.7948717948717948
36673,897ca904b74a98,cf094bd3,We will try to optimize the SVM model with a gread search,c5844ad4,0.7948717948717948
36674,4d91e84c564cbe,cc03febd,"There are a few more interesting list methods we haven't covered. If you want to learn about all the methods and attributes attached to a particular object, we can call `help()` on the object itself. For example, `help(planets)` will tell us about *all* the list methods: ",355a43e3,0.7948717948717948
36680,80ad12f326ab70,975a2b49,"The trend tends to break, which may be due to the summer break from June to Mid August. The trend also moves up afterwards.",da404a16,0.7948717948717948
36684,4daf6153275cbf,a72490b7,"There seems to be no difference, so I do not have enough evidence to reject the null hypotesis. Two-sided 2-sample t-test with %95 significance results:",51db1961,0.7951807228915663
36686,30fdc4a6e3c1db,1ad9982e,# 5. Impact of Events and SNAP days on sales,6111ddee,0.7953216374269005
36700,f269d2fbd5f1be,0beed5db,# Distribution of Salary by Age Group,1264c440,0.7954545454545454
36705,3c2033cc99c12c,0db60af0,### Comparison between two machine learning method ,dfa22a54,0.7956204379562044
36713,f35bf4df70d310,be1ee66a,### Generate Training accuracy & Validation accuracy curves,10bb859a,0.7959183673469388
36716,e69a496109e7d8,fa5afd38,"Considering the upper matrices,Age and No.of.AxillaryNodes have better information than other two",1c640591,0.7959183673469388
36721,fc8e0042411c46,d0e0c75f,## Data Preparation,af476c2a,0.7962382445141066
36723,fdc3afd309b850,225ab5d3,"<a id=""ltaps""></a>
## 8.5 Log Transformation and Preprocessing Scale ",966bde38,0.7962962962962963
36725,233cb23d9e01b9,58ce9cc7,## ethnicity error,ffa56c19,0.7962962962962963
36732,c9b4e282e4e2c1,e4fd14d8,4-Relation between Temperature and number of injuries.,f44d339f,0.7964601769911505
36733,ac1abfe1dfe815,f82d5604,# Modelling,6529dbcb,0.7964601769911505
36735,a077820f7ab459,0ef9243f,### make_gradcam_heatmap,05a43104,0.7966101694915254
36736,a81661cc35d8d2,73b5c570,"<b><font size=""4"">Random Forest</font></b>",3331f113,0.7966101694915254
36737,bb0905d33ae417,094a6544,# Model interpretation,25fd1965,0.7966101694915254
36741,ed8009f482b380,42a61634,So the optimal number of n_neighbors is 2,e99941fa,0.7966101694915254
36742,dac3c8204a2d1b,bb9959e7,Above the graph we can see here highest price of book sold out in 2013 and 2014.,b0d2d0dc,0.7966101694915254
36743,2a56d6b0e153f2,08d3b579,# TUNING PARAMETER,8dc315e6,0.7966101694915254
36746,1294fb4c86f993,06c8688b,Dropping irrelevant census columns,4471e513,0.7966101694915254
36747,a44368590e878a,d501a5c7,### Province,77743ba8,0.7966101694915254
36748,b10bd75889dad9,a0150ccf,#### Lets build Logistic Regression model again,ee00ceee,0.7966666666666666
36751,ff3a8ce61fab6a,c0499e8c,"<hr>

<div>
    🔺 <b>Alert</b><br><br>
    <b>datatype equavilant</b> which we talk about it at first             <b>alert</b> ☝ has same importance in <b>dividation</b>.
</div>


<p>
    Let's talk about <b>pow</b> function We can use it to  get a value of <b>x<sup>y</sup>.</b>
</p>

<h3>x<sup>y</sup></h3><hr>

### Exampel 1 ",9afe1654,0.796875
36754,ba4b3bd184acbb,96f3731b,"Next we can group by Category and take the mean of each.

The `groupby` method is an essential DataFrame tool that is used quite often.",0f5de724,0.7969924812030075
36757,b01ee6cb674fa3,a34adbf3,# US Air Force,a8ffd35e,0.7971014492753623
36759,0e2a23fbe41ca9,e74ab740,"Observations:
- ```avg_sales_lag``` columns show similar patterns according when plotted corresponding to ```numerical_1```.
",64e4762c,0.7971014492753623
36760,a1a31459abf078,3523eecc,"This competition has been unique for me because we don't have visibility into the entire test dataset at once. We have to call an API and fetch batches of test data, iterate over test data in loops and make predictions for each of the batches. Because we don't have visibility into test data any errors in the code are visible only once we make a submission, which sometimes takes hours to run. Therefore running multiple iterations and experimenting has been a challenge in this competition.

### Evaluation Metric : ROC
Found this great [link](https://towardsdatascience.com/understanding-the-roc-and-auc-curves-a05b68550b69) here to read about the evaluation metric for this competition - ROC

#### What is ROC - AUC Curve?
AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. The ROC curve is plotted with True Positive Rate(TPR) against the (False Positive Rate)FPR where TPR is on y-axis and FPR is on the x-axis.


![](https://miro.medium.com/max/542/1*pk05QGzoWhCgRiiFbz-oKQ.png)",66fc0f54,0.7971014492753623
36761,598b6228760590,0df57a49,- Our recall rate has increased by one point,be30ab66,0.7971014492753623
36762,7e275c8d5ff2a0,76ed2a5c,"# Recovered Cases
",b3afcc98,0.7971014492753623
36763,8336d84cf3ff6b,91827164,# Neural Network ,b96b58a0,0.7971014492753623
36767,2ada0305b68956,f8aca6fd,### 136. Palette = 'pink_r',133e26f4,0.7971428571428572
36769,e4525eb0c96f28,fe0cafff,"From our results, it seems that within all the American T-Rated games, the genres that seem to increase the most in sales are Board Games, Educational Games, and Visual Novels, rising at about 220 thousand USD per year. On the other hand, the genres that seem to decrease the most in sales within the same scope are MMO and Music Games, decreasing at about 200 thousand USD per year. However, these calculations won't really mean anything if the residuals are still terrible. First, let's observe the data once more with our new model plotted on top just so we can see what it looks like.",2093a1f1,0.7972972972972973
36771,63b44c85e32c1f,6896da07,**index()** function returns the index of the specified element. If the elements are more than one then the index of the first element of that specified element is returned,fb9b9562,0.7972972972972973
36774,917957c6c4065f,8bf7698b,인기동영상들은 동영상을 평균적으로 게시하고 다음 날 인기동영상이 되었습니다. 빠르면 당일에 인기동영상이 되네요.,55b8ed68,0.7973856209150327
36775,5d2a3e82679cf3,bda27bff,# LASSO REGRESSION,9e60b1e3,0.7974683544303798
36778,87e94f864d74be,d3992d33,> Most of the TV Shows were released in 2020 followed by 2019 and 2018,294bfe9f,0.7976190476190477
36779,6a80f915608fc2,304873f0,"### Do the fits, use x4 with added noise ",636938eb,0.7976190476190477
36782,04ff2af52f147b,712a3bb7,"We can see a much clearer trend for *Fare* than we could for *Age*.  The survival rate consistently trends upwards as *Fare* increases (with the exception of the outlier $[27.00, 39.69)$ range).  

The number of bins for discretization of *Age* and *Fare* is not too large as the bins are able to uniquely capture the various behaviour of how these features affect *Survived*.",d5f37be9,0.797752808988764
36786,4c47839b067546,b55d61ba,"Распределение имеет тяжелый левый хвост,чтобы это исправить воспользуемся логорифмированием.",1f517b02,0.7978723404255319
36787,dbd96dd275dc60,17d9f9d6,"### test_preds= ideal_model.predict(df_test)

This will not work as it has not been manipulated, filtered or cleaned",1ed493a8,0.797979797979798
36792,3fb15e6e48aec2,4237c420,# Models,9d1f4358,0.7982456140350878
36794,6cade0b6a41ba2,eabd9167,# 5. Data Modelling,e6110293,0.7982456140350878
36797,9ceb7278784462,b00df495,## <a id='22'> 18.Random Forests</a>,3768a567,0.7983870967741935
36801,726833f92fb87a,e82635fc,We can encode the month feature by ordinal label encoding.,7dc5e1b6,0.7986577181208053
36802,fc8e0042411c46,44890c00,### Converting some binary variables (Yes/No) to 1/0,af476c2a,0.799373040752351
36809,fdbbd573ba31c2,10e01b0d,## Ridge,f7c28d74,0.8
36810,0687cd5c8597db,b1db486a,### **Confusion Matrix for evaluating predictions on Test dataset.**,4edec76a,0.8
36811,b547f0f38f7744,64ee5c46,### Create DataLoader,b6ba66b3,0.8
36814,5626e84c4e6bf8,8b3a2921,"**Well, we can certainly have more defined clusters through optimization.**",e2ecb669,0.8
36820,e78f177ca86768,2f7139a2,# Top 60,120e25c1,0.8
36825,c115e287523aab,169a0072,"# **Wandb** Logger
Log:
* Best Score
* Grad-CAM",feb1288b,0.8
36839,d905cde3391d2b,59dea569,"For this particular data, standard deviation and IQR are pretty close by, although it won't be the scenario always.

## Distribution graph

Let's plot the frequency distribution graph for `win_by_wickets` data since we can have values from 1 - 10.",067dba39,0.8
36840,ad121e0531afa4,d5168df6,<h1 align='center' style='color: #8532a8; font-family: Segoe UI; font-size: 1.5em; font-weight: 300; font-size: 32px'>5. KFolds Model Training</h1>,a3492905,0.8
36842,7a058705183598,cc750904,"The lowest error rate is at n_neighbors = 7,8,9,11 & 12 so running K Neighbors Classifier with n_neighbors = 7",b0ead917,0.8
36845,71c3c1eab0377d,00f6675a,**Check the type of the data after conversion **,52b4e360,0.8
36846,f89f8540df580e,e55cb712,# Viewing the Tokens only for the **< mask >**,83579ee7,0.8
36847,2b36742b49c7bc,494c766f,"## Сургалт эхлүүлэх
- **`[Model]`** 
    - Backbone моделийг олон янзаар сольсон ба олон туршилтын эцэст BERT-ийг сонгов. [дэлгэрэнгүй](https://huggingface.co/tugstugi/bert-large-mongolian-uncased)
    - Шууд pooler_output ашиглахын оронд зөвхөн synset word ийн embedding-ийг цааш сургав. [дэлгэрэнгүй](https://github.com/bayartsogt-ya/mlub-muis-soril/blob/main/models.py#L36)
- **`[Optimizer]`** BERT-Large model үүд converge хийхгүй байх үед higher level layer үүд дээрх learning rate үүдийг багаар оноох арга хэрэглэв. [дэлгэрэнгүй](https://github.com/bayartsogt-ya/mlub-muis-soril/blob/main/optimizers.py#L3)
    - *BERT-base модель ийм trick ашиглах шаардлагагүй болно.
- **`[Learning Rate Scheduler]`** HuggingFace Trainer-ийн default-г (`torch.optim.lr_scheduler.LambdaLR`) шууд ашиглав. [дэлгэрэнгүй](https://huggingface.co/transformers/main_classes/trainer.html#id1)",c8f8a96d,0.8
36849,0fa9979b5690e9,58b87b27,"Um último passo é integrar na validação cruzada a pesquisa pelos melhores parâmetros, algo semelhante ao que foi feito anteriormente. Novamente, o Scikit-Learn tem uma função para facilitar o processo chamada *GridSearchCV*. A dificuldade nesse ponto aparece ao perceber como será formado o fluxo, afinal não pode-se procurar os melhores parâmetros a cada execução da validação cruzada, uma vez que a validação cruzada tem o objetivo de avaliar um modelo e a mudança de parâmetro a cada execução criaria um modelo novo. Isso criaria um loop infinito de tentativas de avaliar um modelo.

A solução é separar um conjunto de dados para fazer a busca pelos melhores parâmetros e formar um modelo. Assumindo que esse conjunto utilizado para esse propósito é representativo, avalia-se então com a validação cruzada qual o desempenho desse modelo. ",c26eea94,0.8
36854,254cccd5145725,473af301,# XGBoost,a49b4037,0.8
36858,d07915a6e6992e,599abcf5,"**Cross Validation Strategy**
![CV.png](attachment:CV.png)

Cross Validation is one of the most powerful tool in Data Scientist's tool box. It helps you to understand the performance of your model and fight with overfitting. As we all know that Learning the model parameters and testing it on the same data is a big mistake. Such a model would have learned everything about the training data and would give result in a near perfect test score as it has already seen the data. The same model would fail terribly when tested on unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. 

The general approach is as follows:

1. Split the dataset into k groups
2. For each unique group:
        a. Kee one group as a hold out or test data set
        b. Use the remaining groups as training data set
        c. Build the model on the training set and evaluate it on the test set
        d. Save the evaluation score 
3. Summarize the performance of the model using the sample of model evaluation scores

You can access following link and read about Cross Validation in detail.

https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833
https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/",2b912140,0.8
36859,72f72525e8a7fb,a242eea9,"Time series model building

>Naive Forecasting
",e6380e0b,0.8
36860,6cacdcf8daf400,42305d45,**Training**,83939b53,0.8
36863,bfe6c7096b1ad0,a679d7b7,## Предсказываем цены и выгружаем в файл,fffd95e0,0.8
36864,b7452d87e4abfe,c1fc1e76,**We can also buil a string with the formations and print it:**,9c75a86d,0.8
36865,3cb96bd8eb364b,ba36688b,### Model construction,3157af7e,0.8
36872,bbad077c274022,e6a4153d,The outdoor steps have more variance than the indoor ones.,3c2e3dea,0.8
36874,7341f069d9b2ee,5e22773c,Select and Train a Model,e0a49e62,0.8
36878,7454fdc444df16,87b1200d,### Fit the pixel PCA object,a7818ef5,0.8
36879,36c35f0a9f70f7,a0e31ffd,## Random Forest classification,67358bc7,0.8
36881,e5dd725b8fa422,e746b0b6,# Predicting using ARIMA,14675d8b,0.8
36883,70c1c70437ce86,e70b31f6,## KNN,e94552a4,0.8
36890,5cb7f999fd1ecb,f9b34f05,# Plotly - Map Box ( Interactive ),88b54f70,0.8
36891,3a6274ed72cc00,546a9be9,## <a id='6.'> 6. Modelling</a>,51369a2a,0.8
36894,171494b45650a2,cc1067e3,### Features pairplot,9c8cc578,0.8
36896,b4e238fbc6464c,761114b0,### customers is an important column so we cant drop it as the column in not present in test csv[](http://),fd1a6cba,0.8
36901,5d5c9480b5a0a3,572430c8,"Whereas almost all combinations are negatively relation. There are some that are positive and provide insight.
For example: Pclass and Embarked are positively correlated to survival.",04d82e2d,0.8
36902,5ffe6aa38958a1,58cce6ea,"
We can use Randomforest classifier and perform more advanced optimizations to further improve the accuracy

## 5.1 Feature Importance

Feature importance can be obtained in random forest which gives you an idea of how much weight each feature carries. This can be useful to eliminate less important or noisy features ",11f5412e,0.8
36906,2730840089c8eb,a2fedf37,Python has *dictionary comprehensions* with a syntax similar to the list comprehensions we saw in the previous tutorial.,34d27dac,0.8
36908,1011899b959f44,628d51a0,"10. Find the number of battles fought by each attacker king, respectively. ",0b112382,0.8
36913,bbaa07ad21cf4e,180e5137,"## 9. BERT  <a class=""anchor"" id=""10""></a>

",3ab6b254,0.8
36915,e3c0b55ed519e2,57bdb71c,# Locations with *Lowest* ICU beds per projected infected individuals who need ICU care.,9f51352e,0.8
36918,f91f58d488d4af,0e408ace,We have to create a DataLoader from our dataset.,5df1bbf3,0.8
36920,9c044fa3072552,064162d9,Aha! This distribution seems good. Accidents seem to be more frequent during the winters.,1362842e,0.8
36923,6338f6b0178d13,6edd4737,# Calculating jaccard score,ae81b18b,0.8
36930,8cd6656a65e6e7,2b773cb1,## Training,c8e1697a,0.8
36933,8696921d9adc93,11a46ec1,Model will be trained on 90% of the data and randomly chosen 10% of the data has been kept aside for testing the model.,b8908b23,0.8
36935,cb6f349e54c2a1,a28eac62,"# Estimator

TensorFlow Estimators is a High-level TensorFlow API that greatly simplifies machine learning programming. The design goals can be summarized as automating repetitive and error-prone tasks, encapsulating best practices, and providing a ride from training to deployment. The tf.estimator Quickstart web page gives a good reason to use it: “TensorFlow’s high-level machine learning API (tf.estimator) makes it easy to configure, train, and evaluate a variety of machine learning models”

A more extensive list is the Advantages of Estimators page that list a few points of particular interest:

* Run on CPU, GPU or TPU without reordering your model
* Safely distributed training loop to build a graph, initialize variables, start queues, create checkpoints, save summaries to TensorBoard
* Export for serving

![image.png](attachment:image.png)",962bade8,0.8
36940,60da9bbfe39c4b,c9f64d94,"# So, we can show that in July 2021, Bangladesh was the most deaths by COVID-19.",b0dd8ad6,0.8
36941,3cea0f929a2035,da80700d,"Considering the top 3 countries, let's see the yearly trend of the numbers of suicides in these countries.",04cfbade,0.8
36943,0d58c434c7db1e,476faca2,"China is first with '7' GM title.

Russia has '6' GM title.

Georgia has '5'. GM title",517e01d3,0.8
36946,312d2a3c7547f1,08fc00d9,Training and Predicting,8fd1efcd,0.8
36953,63d0d9b9a8c7d2,e5fbe0e4,***SVM***,e32e5933,0.8
36954,d6cbd7160961dc,1e73b5b5,"For this example, we will simulate a data set with the first 1500 numbers in the sequence of fibonacci. Then, let's select a sample size of 1000 and use our code to analyze the frequency of 0 to 9 in the first, second and third digits of these 1000 values.

In this case we will type 1 for type of analysis, since we are only interested on the aggregated analysis.",36d74664,0.8
36955,9f0ccf5b9e8f03,8d2df694,Rossin and his codes that I wrote in many Notebooks: rossinEndrew SHAP VALUES Visualization. https://www.kaggle.com/endrewrossin/fast-initial-lightgbm-model-to-detect-exam-result/comments,66691203,0.8
36960,892be0a523578c,1ed2660a,"By inspecting the trend of total steps, intensity and calories from morning to night for each cluster, we can rougly describe the features of each cluster:
* Cluster 0: customers in this group may exercise twice each day, generaly in the afternnon and evening, and the exercise intensity is moderately high
* Cluster 1: Total steps in this group distributes more evenly than in other groups, which may indicate customers in this group don't exercise too much, it's just their job require them to walk more
* Cluster 2: customers in this group exercise in the morning, and the exercise intensity is fairly high",b0e8d7c0,0.8
36962,f2f2db16a2f86c,c56f0d9b,Using the **R-Squared** metric.,ffc6a115,0.8
36963,ff9142eb631dd5,5908319b,## Get Feature Importances on Time Split Mean Squared Error,453131ac,0.8
36964,55a5e31d03df9f,7d2f0214,"## Predicting with a new image

Our final objective was to predict any Lego image, with that mindset I created a folder with some extra lego-test-images to test with our new algorithm and show how we can work when we want to inference new images.",06dce00f,0.8
36968,f4b603905215b7,9b41774c,![image.png](attachment:image.png),efe1d587,0.8
36971,869a39a3d4dea2,e8cc6494,"Gray Scale Histogram <a id=""grayhist""></a>",9020daf8,0.8
36977,ff83da40bcdb19,630a38d1,"**PART 7:**
*Predicting and plotting*

Predicting: Just give the image data and take the resulst.

Plotting Part: It's a bit complicated.",36b5ec8c,0.8
36983,10c5a39a87c47e,5cc47e01,"## Step 13: Plotting ROC AUC Curve<a id='step-13'></a>
In this step we will first compute Are Under Curve (AUC) based on which we plot ROC curve",09c7337a,0.8
36988,4fd4b6a80d40e3,6a886941,"## Identity Function

![image.png](attachment:image.png)",f6913cc3,0.8
36991,edc19e349fe80a,87436464,"Again, check its shape and data",7882221a,0.8
36995,be616f0785c32d,55a0238d,"<div id='PartDconsolidate'></div>

##### D.2.3 Compiling list of consolidated epitope groups by partial subsequence overlap
Now we consolidate the epitopes from various publications by partial sub-sequence overlap. The rationale is to identify the cases where one epitope is a subsequence of another, or if one overlaps to the other by more than 5 consequtive amino acides, and only the outside flanking ragion is not overlapping. In these cases, two epitodes would be considered in the same group.

These are the consolidated epitope groups that have so far been published:


|Group|Epitope 1|Epitope 2|Epitope 3|Epitope 4|Epitope 5|Epitope 6|Epitope 7|Epitope 8|Epitope 9|
|---|---|---|---|---|---|---|---|---|---|
|1|AADLDDFSK|||||||||
|2|AAIFYLITPVHVMSK|||||||||
|3|AATKMSECVLGQSKRVD|||||||||
|4|AAYVDNSSLTIKKPN|||||||||
|5|AGLPYGANK|TGPEAGLPYGANK||||||||
|6|AIPTNFTISVTTEIL|||||||||
|7|ALNTLVKQL|DVVNQNAQALNTLVKQL||||||||
|8|ALNTLVKQL|QALNTLVKQLSSNFGAI|DVVNQNAQALNTLVKQL|||||||
|9|ALNTLVKQL|QALNTLVKQLSSNFGAI||||||||
|10|ALNTPKDHI|||||||||
|11|AMQMAYRF|GAALQIPFAMQMAYRF|GAALQIPFAMQMAYRFN|PFAMQMAYRFNGIGVTQ||||||
|12|AMQMAYRF|GAALQIPFAMQMAYRF|GAALQIPFAMQMAYRFN|||||||
|13|AMQMAYRF|MAYRFNGIGVTQNVLY|PFAMQMAYRFNGIGVTQ|||||||
|14|AQFAPSASAFFGMSR|KHWPQIAQFAPSASAFF|QFAPSASAFFGMSRIGM|AQFAPSASAFFGMSRIGM||||||
|15|AQFAPSASAFFGMSR|KHWPQIAQFAPSASAFF||||||||
|16|ATKAYNVTQAFGRRG|KAYNVTQAFGRRGPE|YNVTQAFGRRGPEQTQGNF|||||||
|17|ATKAYNVTQAFGRRG|YNVTQAFGRRGPEQTQGNF||||||||
|18|ATYYLFDESGEFKLA|||||||||
|19|DGEVITFDNLKTLLS|||||||||
|20|DLGDISGINASVVNIQK|||||||||
|21|DLMAAYVDNSSLTIK|||||||||
|22|DSATLVSDIDITFLK|||||||||
|23|DTRYVLMDGSIIQFP|||||||||
|24|EGKTFYVLPNDDTLR|||||||||
|25|EVRTIKVFTTVDNIN|||||||||
|26|FIAGLIAIV|||||||||
|27|GAGICASY|||||||||
|28|GKTFYVLPNDDTLRV|||||||||
|29|GMSRIGMEV|AQFAPSASAFFGMSR|QFAPSASAFFGMSRIGM|AQFAPSASAFFGMSRIGM||||||
|30|GMSRIGMEV|FFGMSRIGMEVTPSGTW|QFAPSASAFFGMSRIGM|AFFGMSRIGMEVTPSGTW|AQFAPSASAFFGMSRIGM|||||
|31|GMSRIGMEV|MEVTPSGTWL|FFGMSRIGMEVTPSGTW|AFFGMSRIGMEVTPSGTW||||||
|32|GSFCTQLN|||||||||
|33|GTTLPK|LPQGTTLPKG|QLPQGTTLPKGFYAE|QLPQGTTLPKGFYAEGSR|QLPQGTTLPKGFYAEGSRGGSQ|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD||||
|34|GTTLPK|LQLPQGTTL|LPQGTTLPKG|QLPQGTTLPKGFYAE|PKGFYAEGSRGGSQASSR|QLPQGTTLPKGFYAEGSR|QLPQGTTLPKGFYAEGSRGGSQ|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD||
|35|GTTLPK|LQLPQGTTL|LPQGTTLPKG|QLPQGTTLPKGFYAE|QLPQGTTLPKGFYAEGSR|QLPQGTTLPKGFYAEGSRGGSQ|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD|||
|36|GTTLPK|SQASSRSS|LPQGTTLPKG|QLPQGTTLPKGFYAE|SRGGSQASSRSSSRSR|PKGFYAEGSRGGSQASSR|QLPQGTTLPKGFYAEGSR|QLPQGTTLPKGFYAEGSRGGSQ|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD|
|37|GVLTESNKK|||||||||
|38|HEGKTFYVLPNDDTL|||||||||
|39|IINLVQMAPISAMVR|||||||||
|40|ILLNKHID|ILLNKHIDA|LNKHIDAYKTFPPTEPK|LLNKHIDAYKTFPPTEPK||||||
|41|ILLNKHID|TFPPTEPK|ILLNKHIDA|LNKHIDAYKTFPPTEPK|LLNKHIDAYKTFPPTEPK|KHIDAYKTFPPTEPKKDKKK||||
|42|ILSRLDKVEAEVQIDRL|||||||||
|43|IPTNFTISVTTEILP|||||||||
|44|KAYNVTQAFGRRGPE|YNVTQAFGRRGPEQTQGNF||||||||
|45|KGIYQTSN|||||||||
|46|KNHTSPDVDLGDISGIN|||||||||
|47|KPSFYVYSRVKNLNS|||||||||
|48|KTFYVLPNDDTLRVE|||||||||
|49|LALLLLDRL|||||||||
|50|LITGRLQSL|EAEVQIDRLITGRLQSL|RLITGRLQSLQTYVTQQ|||||||
|51|LITGRLQSL|EAEVQIDRLITGRLQSL||||||||
|52|LITGRLQSL|RLITGRLQSLQTYVTQQ||||||||
|53|LLIVNNATNVVI|||||||||
|54|LLLDRLNQL|LDRLNQLESKMS||||||||
|55|LLLDRLNQL|QLESKMSGK|LDRLNQLESKMS|||||||
|56|LLPAAD|||||||||
|57|LMAAYVDNSSLTIKK|||||||||
|58|LPQRQKKQ|KTFPPTEPKKDKKKKADETQALPQRQKKQQ||||||||
|59|LPQRQKKQ|TFPPTEPK|KTFPPTEPKKDKKKK|YKTFPPTEPKKDKKKK|KTFPPTEPKKDKKKKADETQALPQRQKKQQ|||||
|60|LQLPQGTTL|LPQGTTLPKG|QLPQGTTLPKGFYAE|QLPQGTTLPKGFYAEGSR|QLPQGTTLPKGFYAEGSRGGSQ|||||
|61|LTPGDSSSGWTAG|||||||||
|62|MAAYVDNSSLTIKKP|||||||||
|63|MATYYLFDESGEFKL|||||||||
|64|MAYRFNGIGVTQNVLY|MAYRFNGIGVTQNVLYE|PFAMQMAYRFNGIGVTQ|||||||
|65|MAYRFNGIGVTQNVLY|MAYRFNGIGVTQNVLYE||||||||
|66|MEVTPSGTWL|FFGMSRIGMEVTPSGTW|AFFGMSRIGMEVTPSGTW|||||||
|67|NLNESLIDL|EVAKNLNESLIDLQELG|EIDRLNEVAKNLNESLIDLQELGKYEQY|||||||
|68|NLNESLIDL|RLNEVAKNL|EVAKNLNESLIDLQELG|EIDRLNEVAKNLNESLIDLQELGKYEQY||||||
|69|NPTTFHLDGEVITFD|||||||||
|70|PDTRYVLMDGSIIQF|||||||||
|71|PSFYVYSRVKNLNSS|||||||||
|72|PTNFTISVTTEILPV|||||||||
|73|PTTFHLDGEVITFDN|||||||||
|74|QGTDYKHW|LIRQGTDYKHWP|IRQGTDYKHWPQIAQFA|||||||
|75|QGTDYKHW|QELIRQGTDYKH|IRQGTDYKHWPQIAQFA|||||||
|76|QGTDYKHW|QELIRQGTDYKH|LIRQGTDYKHWP|IRQGTDYKHWPQIAQFA||||||
|77|QIAPGQTGK|VRQIAPGQTGKIAD||||||||
|78|QLESKMSGK|LNQLESKMSGKG||||||||
|79|QLESKMSGK|RLNQLESKMSGK|LNQLESKMSGKG|LDRLNQLESKMS|SKMSGKGQQQQGQTVTKKSAAEASKKPRQKRTATKAYN|||||
|80|QLESKMSGK|RLNQLESKMSGK||||||||
|81|QLESKMSGK|SKMSGKGQQQQGQTVTKKSAAEASKKPRQKRTATKAYN||||||||
|82|QLIRAAEIRASANLAAT|QLIRAAEIRASANLAATK||||||||
|83|QQFGRD|||||||||
|84|QTQTNSPRRARSV|||||||||
|85|RASANLAATKMSECVLG|||||||||
|86|REGYLNSTNVTIATY|||||||||
|87|RIRGGDGKMKDL|||||||||
|88|RLFRKSNLK|||||||||
|89|RLNEVAKNL|EVAKNLNESLIDLQELG|EIDRLNEVAKNLNESLIDLQELGKYEQY|||||||
|90|RLTKYTMADLVYALR|||||||||
|91|RRPQGLPNNTASWFT|GLPNNTASWFTALTQHGK|MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLPNNTAS|||||||
|92|RRPQGLPNNTASWFT|GLPNNTASWFTALTQHGK||||||||
|93|RRPQGLPNNTASWFT|MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLPNNTAS||||||||
|94|RTIKVFTTVDNINLH|||||||||
|95|SLQTYVTQQLIRAAEIR|||||||||
|96|SMATYYLFDESGEFK|||||||||
|97|SNFRVQPTESIV|||||||||
|98|SNNSIAIPTNFTISV|||||||||
|99|SNPTTFHLDGEVITF|||||||||
|100|SQASSRSS|PKGFYAEGSRGGSQASSR|QLPQGTTLPKGFYAEGSRGGSQ|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD||||||
|101|SQASSRSS|SRGGSQASSRSSSRSR|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD|||||||
|102|SQASSRSS|SRGGSQASSRSSSRSR|PKGFYAEGSRGGSQASSR|GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD||||||
|103|SQSIIAYTMSLGAEN|||||||||
|104|SVLNDILSR|AISSVLNDILSRLDKVE||||||||
|105|TFPPTEPK|KTFPPTEPKKDKKKK|YKTFPPTEPKKDKKKK|KHIDAYKTFPPTEPKKDKKK|KTFPPTEPKKDKKKKADETQALPQRQKKQQ|||||
|106|TFPPTEPK|KTFPPTEPKKDKKKK|YKTFPPTEPKKDKKKK|LNKHIDAYKTFPPTEPK|LLNKHIDAYKTFPPTEPK|KHIDAYKTFPPTEPKKDKKK|KTFPPTEPKKDKKKKADETQALPQRQKKQQ|||
|107|TFPPTEPK|KTFPPTEPKKDKKKK|YKTFPPTEPKKDKKKK|LNKHIDAYKTFPPTEPK|LLNKHIDAYKTFPPTEPK|KHIDAYKTFPPTEPKKDKKK||||
|108|TKRNVIPTITQMNLK|||||||||
|109|TMADLVYALRHFDEG|||||||||
|110|TNFTISVTTEILPVS|||||||||
|111|TRYVLMDGSIIQFPN|||||||||
|112|TSNFRVQPTESI|||||||||
|113|VAAIFYLITPVHVMS|||||||||
|114|VKPSFYVYSRVKNLN|||||||||
|115|VLNDILSRL|AISSVLNDILSRLDKVE||||||||
|116|VLNDILSRL|SVLNDILSR|AISSVLNDILSRLDKVE|||||||
|117|VRTIKVFTTVDNINL|||||||||
|118|VVFLHVTYV|||||||||
|119|VYQVNNLEEIC|||||||||
|120|YDHVISTSHKLVLSV|||||||||
|121|YEAMYTPHTVLQAVG|||||||||
|122|YQAGSTPCNGV|||||||||
|123|YREGYLNSTNVTIAT|||||||||
|124|YTGAIKLDDKDPNFK|||||||||



Code for compiling list of consolidated epitope groups by partial subsequence overlap.



[Go Top](#top)",b78e18aa,0.8
36996,0885edf1a61429,e686319d,"I repeat, discovering missing labels and retraining **did not improve score** for me.",cf8ce75e,0.8
37000,7dd46c750653eb,bbb21660,"**Inference**

* Over the years March has the most number of Divorces. after that December has the most number of Divorces",c2644713,0.8
37005,fdc3afd309b850,8f066cb0,"As seen above, the graphs are not similar to a Gaussian distribution. Also the scale of the numbers are very different as we expected. 

For the normalization we will use numpy.log and for scale we will use preprocessing scale.",966bde38,0.8009259259259259
37007,5ce12be6e7b90e,ff0cdebd,![Python loop](http://2.bp.blogspot.com/-7lXe1_Gou3k/UX92PWche3I/AAAAAAAAAFA/JxD4u8St-9g/s1600/python+loop.jpg),c0ab62dd,0.8011695906432749
37008,fdc9f4863744b1,a3a9043f,Conclusion: We can use single regression to create a predictive model GROSS SQUARE FEET AND OR LAND SQUARE FEET as the predictive variable.,b4529365,0.8013698630136986
37010,56785caebaa256,3a281725,"## 5.3. Results of all tuning<a class=""anchor"" id=""5.3""></a>

[Back to Table of Contents](#0.1)",a792961a,0.8014184397163121
37013,c65a65d4041018,a46971da,### data bases,824fb229,0.8014705882352942
37014,7f74a04ae75792,bfc6d700,"### What is the average number of purchase made in the last 3 years among different customer status
",d01e91da,0.8014705882352942
37015,ee23a565163388,b62104c7,## **K-Neighbors Classifier**,88aacbc4,0.8015267175572519
37016,e9b9663777db82,83c1142f,# Pipelines,648e8507,0.8016528925619835
37019,510b8303776bb6,cb30ecb2,# Predicting output over the testset.,18080db8,0.8018867924528302
37022,c09fac3c943d51,f4982ea2,# User-level,678d076d,0.8023255813953488
37023,d96642860ab3dd,849bb700,## Modeling ML,98419d48,0.8023255813953488
37026,faa8e6c8ab9246,028fd766,"One Hot Encoding is used to convert categorical data into numerical data.
",2bea1419,0.8024691358024691
37030,631cd434fc3aa2,e6b2aca5,"Here we'll apply Box-Cox tranformation for highly skewed features.
We use the scipy function _boxcox1p_ which computes the Box-Cox transformation of _1+x_.",2b74febb,0.8028169014084507
37033,bddd799cdbbae8,2b32d42d,**Support Vector Machine classifier**,b44e3c08,0.8028169014084507
37035,2ada0305b68956,a6ba42a0,### 137. Palette = 'plasma',133e26f4,0.8028571428571428
37036,3c2033cc99c12c,a3ecdcb6,#### The confusion matrix of two models,dfa22a54,0.8029197080291971
37039,a2444ab5d5f147,ce54bdd4,### let's see which words are mostly used for all emotions,10617755,0.803030303030303
37040,ce9ed5e2d601d7,c3480f58,# Submission,f58a2f43,0.8031496062992126
37042,918040fad252ec,6574dbce,Menampilkan tabel grafik Accuracy,966fcd8f,0.8032786885245902
37044,0858e1bb3cbaca,6f74bae5,"we can draw many different types of plots with pandas, for example:",78548374,0.8032786885245902
37045,601e18072783b4,e73a9d97,## Distribution of movie duration,36b2b1fa,0.8032786885245902
37061,523123dad03177,9433a8cb,Parent's education?,48a5e4e6,0.803921568627451
37062,52cfd66e9ec908,a683dfd4,This is basically initializing the rasterization process for the training data which basically functions to pass the .zarr files to our model and make it a coherent data format easily modular with our PyTorch setup.,c74adcdf,0.803921568627451
37063,629f2918807a9b,39f1fe03,"Hence Proved, No records were found as order cancelled in 2019,and 2020 has only 3 records. Amazing !!!",be56dc84,0.803921568627451
37069,7cfd96218dd933,2590da56,"#### **ATTENTION**
* THE FIRST POINT WITH BRIGHTNESS LATITUDE: 40.042
* THE FIRST POINT WITH BRIGHTNESS LONGITUDE: -121.383
* THE FIRST POINT WITH BRIGHTNESS DATE: 2021-07-20
* THE FIRST POINT WITH BRIGHTNESS TIME: 18.25 FOR TURKEY



* THE SECOND POINT WITH BRIGHTNESS LATITUDE: 40.044
* THE SECOND POINT WITH BRIGHTNESS LONGITUDE: -121.369
* THE SECOND POINT WITH BRIGHTNESS DATE: 2021-07-20
* THE SECOND POINT WITH BRIGHTNESS TIME: 18.25 FOR TURKEY",7c34d96c,0.803921568627451
37071,842547b2def18c,6baa7c0b,"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).

Note that the model generates a confidence score which is higher than Logistics Regression model.",b8efde6d,0.803921568627451
37077,225b4fe5d3894a,8f2cc690,"<a id=""8a""></a>
### a. Grid Search",4b4197b3,0.8041237113402062
37078,8ec771f5600a61,ecee68b4,# Traning and Testing of data by logistic regression,48364c1f,0.8041237113402062
37079,eda49464dd6d1b,1b018117,"### A variety of variations were tried to the following properties:
* Epochs
* Batch size
* Activation function (relu or sigmoid)
* Validation split
* Hidden layer neuron number

The optimum arrangement was found below",8421f81f,0.8041958041958042
37081,72d528df923403,9dec0afd,"Some quick observations:
- 95% of the items are sold in batches of up to 10 units per assistance.
- the FOODS category has more some more extreme products, even so they only represent 5% of the total items.
- The mean and median of the quantities have a certain difference and this is due to the degree of asymmetry involved.
- 95% of the items have a standard deviation of 8 units per transaction.
- Most of the items have positive skewness, which justifies the superior performance of some products.",d51c8e8e,0.8043478260869565
37082,7e89d387feb9f5,5e96ac3d,### Добавленный признак №18. Нормализованный диапазон цен в зависимости от положения ресторана среди всех ресторанов города. Идея: вряд ли хорошие отзывы будут у не самых крутых ресторанов с самыми крутыми ценами и наоборот.,989e3a1b,0.8043478260869565
37085,b01ee6cb674fa3,e955918c,"#  National Centre for Space Studies - CNES

Centre national d'études spatiales",a8ffd35e,0.8043478260869565
37099,fd4017c1514157,9c5e877f,"### *Librosa vs Scipy*
I have chosen 'Librosa' because It normalizes the data while reading/loading audio file in the range 1 and -1 where as 'scipy' doesn't.",fd8f0896,0.8048780487804879
37101,e19e307b3fd188,cc533078,#### Select TRAIN and TEST data,2173955b,0.8048780487804879
37109,5169abdc647412,992e9a97,### r2 score,28efc68d,0.8048780487804879
37116,241cf32abb22d8,18d62034,"# Performance Comparison <a class=""anchor"" id=""4""></a> ",47157066,0.8051948051948052
37118,2cb457b60dd246,4b02f363,### Create a Confusion Matrix,339367df,0.8051948051948052
37121,90691864eb68c7,fd8d0a9a,"Our train data splitted as train and test data in order to feed in a Neural  Network correctly.
Train data is 80% and test data 20% of the House Prices Dataset.",3555ef9b,0.8051948051948052
37123,a5a419dc7245b0,4eb22633,"**1.optimizer is the algo used to find the optimal no of weights in the NN (until now weights have only been initialized); 'adam' is a type of SGD algo loss deals with the loss function within the SGD algo which needs to be optimized(minimized);<br>
2.loss fn for SGD going to be the same as that for logistic regression (logarithmic loss); since sigmoid fn used as activation fn we use log loss fn acuracy metric ensure that accuracy increases batch by batch; metrics parameter expecting a list so 'accuracy' added in []**
",4279726e,0.8053097345132744
37124,ac1abfe1dfe815,c9d182b6,## K-Neighbors Classifier,6529dbcb,0.8053097345132744
37127,f18e737fcc4b06,ae40d627,"
# Missing Value¶
* Find Missing Value
* Fill Missing Value",087b8637,0.8055555555555556
37131,ab6da5994949a3,79919ee3,"# Decision Tree Classification Model
## Fitting Decision Tree classifier to the Training set",fae6b91d,0.8055555555555556
37135,268a610bbc64b4,e1795dc2,# 5. Number of users who reactivate their account per month?,8a16f301,0.8055555555555556
37137,cf39cde80e66b7,493503a1,"![](https://miro.medium.com/max/955/0*1jxDmwoJF8R4tOVq.png)
source: http://www.haghish.com/statistics/stata-blog/stata-programming/adjusted_R_squared.php

> A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared.
",aed4bc9b,0.8055555555555556
37140,166a62ebb4fc3a,c60e70b7,"Now, let's build logistic model on our data set",db48a079,0.8055555555555556
37152,1014e6be391084,d5c2f624,# Splitting the model into training and testing part,46f9168f,0.8055555555555556
37159,a2176d4653ef60,23ba82f0,# Data Modeling - Neural Networks,ac908675,0.8058252427184466
37161,98a6794067932a,ae4db6ac,"Encore une fois, cette cellule de code ne permet pas d'effectuer des analyses, mais plutôt à préparer les données pour nos analyses à venir. Cette fois-ci le code permet dans un premier temps de sélectionner uniquement les données pour lesquelles nous avons la longitude et la latitude de l'emplacement client. Les clients pour lesquels nous n'avons pas réussi à générer l'une ou l'autre de ces valeurs ne seront donc pas pris en compte étant donné que ces valeurs sont essentielles afin que nous puissions représenter les clients sur une carte avec l'outil Folium. Ensuite, le code permet de réinitialiser la colonne 'Index' maintenant que certaines adresses incomplètes ont été enlevées et il permet également d'afficher le dataframe corrigé. ",08600fe2,0.8058252427184466
37162,e58e68e4eeefe5,4aacda67,#### Trying with different sets of features based on the observations.,a87662ce,0.8059701492537313
37167,21413205980558,ddbc04f8,# Section Ⅴ Prediction Model,84197de0,0.8059701492537313
37171,0caaec057f7184,8bba01a8,"From the data above, we can assume that maybe it's wrong keyed in, we can replace the the price with the same month price = 1249. Next, we take a little closer look and do some analysis on the item.",b875533e,0.8064516129032258
37173,0925f172b5eb74,9dd4d326,# Training,ec34cd72,0.8064516129032258
37175,535da6591a3246,7cb053cc,# 3 different models for predicting placement status,9ef8d0c7,0.8064516129032258
37177,0d9a2067267ba1,179d4a5c,"The imnplementation code sample was inspired from [Mathurin Ache's notebook starter](https://www.kaggle.com/mathurinache/starter-wids2022/notebook) **thanks to upvote it**

And you can check  for all his great contributions : his profile is fulled of great content",abc194fb,0.8064516129032258
37178,098fedfcd07456,8aa44310,"# We are fitting the Model here
1. Epochs indicates the number of passes of the entire training dataset need back and forth.
1. The batch size is the amount of samples you feed in your network 
1. Validattion data determines how your data showed be partitioned into training and validation sets.",052ece26,0.8064516129032258
37180,16862cb02d73d5,d2a66de1,"Identify anomalies for individual metrics and plot the results. 

**X axis -  date 
Y axis - Actual values and anomaly points.**

Actual values of metrics are indicated in the **blue line** and anomaly points are highlighted as **red points**.

In the table, background **red** indicates high anomalies and **yellow** indicates low anomalies.",d7ffa1a6,0.8064516129032258
37193,ad26c020235dfc,82700da5,Plot the correlation matrix.,bf766e48,0.8064516129032258
37195,0cb456a5456cf9,215a7780,# **PART 1 Data preprocessing**<br>数据预处理,5701729c,0.8064516129032258
37198,b10bd75889dad9,1f809678,#### p-values are all fine,ee00ceee,0.8066666666666666
37199,4ae6a182abac64,7d9beb3a,* **Training Accuracy\ Testing Accuracy**,418676c5,0.8067226890756303
37200,c4386b8a01d66e,c25d80ed,### K Nearest Neighbours,dc732bf5,0.8067226890756303
37201,d1ff7e10ee0102,c772aae8,"Ok, now we are dealing with the big boss. What do we have here?

* Something that, in general, presents skewness.
* A significant number of observations with value zero (houses without basement).
* A big problem because the value zero doesn't allow us to do log transformations.

To apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.

I'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'.",2cc71c3c,0.8068181818181818
37202,73d8e56bc709b1,0a1bf076,This seems a positive correlation between height and weight.,78ec3cce,0.8068181818181818
37209,6cade0b6a41ba2,6e2f7b31,## 5.1. Data Modelling on Trained Data,e6110293,0.8070175438596491
37210,5ce12be6e7b90e,85e15330,"### Looping over strings
",c0ab62dd,0.8070175438596491
37212,30fdc4a6e3c1db,a6126f6e,## Gauging impact of events,6111ddee,0.8070175438596491
37229,71d3e4aee86e3e,14dd7c44,> ## 3. Deaths Reported,69706f0b,0.8076923076923077
37230,a915263bc207da,ed56ddaa,First lets see how the user with ID number 0 has rated the movies,b17ebcda,0.8076923076923077
37234,1bd6cc83c02681,d725aa59,# Making the recommendation function,17ff92f0,0.8076923076923077
37236,44f6a002ecd033,96f7d7d1,"Not as accurate as the logistic regression model, but we will see how a random forest classifier handles the dataset.",70bbe106,0.8076923076923077
37237,6f1481148352e9,a5f05457,# Seasonality,7cfbdb8f,0.8076923076923077
37239,582cb872d19026,bb28ce1f,"
******************************************************************************************************************************
#### As we can see in this pie chart, most of the games in this dataset are available to users for free.
******************************************************************************************************************************",8d966d69,0.8076923076923077
37247,d0f6276d5b628c,62106021,So the fin_data contains all the consumer who are very light or happy behavorial and not making any degrdataion while creating or punlishing any content. ,c64f5ce5,0.8076923076923077
37248,d78988cb5a1b02,1910d4d4,**The roc curve gives some threshold values. We have to select the correct threshold value which gives more model accuracy.**,233f3a92,0.8076923076923077
37254,2facf256353117,cd1974cf,## save the automatically updated image ,18f579be,0.8076923076923077
37256,cf08b03b002c13,cb3cab31,"Apparantly, only 2% of all NSFW comments get deleted ",104d416f,0.8076923076923077
37258,dbd96dd275dc60,ca4272a2,## Preprocessing the data(getting the test dataset in the same form of our training dataset),1ed493a8,0.8080808080808081
37262,91473a39b85068,f164ccb6,"### One Vs Rest Classifier

Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier.

This strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i, j] is 1 if sample i has label j and 0 otherwise.

https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html",6e3d91c2,0.8082191780821918
37264,fdc9f4863744b1,ff9428ae,Let's see if we can use the BOROUGH,b4529365,0.8082191780821918
37266,62487bcd70b199,02bcb536,## <a id='8.2.1.3.'>8.2.1.3. DecisionTreeClassifier</a>,f6ae50af,0.8083333333333333
37267,73893f0467d5e3,d9bfb41c,# Logistic Regression,279787c6,0.8085106382978723
37268,c7e5f658090347,a3eeb824,"## Evaluating the Final Model

For the 4 models trained above, we will now evaluate their performances on the validation set using the area under the receiver operating characteristic curve (""roc_auc_score"" in sklearn). 

For more insight into their performance we will also generate a confusion matrix on the validation results for each model.  

##### A Reminder on How to Interpret a Confusion Matrix:
* True Negatives (Top-Left Square): This is the number of **correct classifications** of the **No Fraud** Detected class.
* False Negatives (Top-Right Square): This is the number of **incorrect classifications** of the **No Fraud** Detected class.
* False Positives (Bottom-Left Square): This is the number of **incorrect classifications** of the **Yes Fraud** Detected class
* True Positives (Bottom-Right Square): This is the number of **correct classifications** of the **Yes Fraud** Detected class.


#### Final Model Performance Comments
Looking at the above four models we see a general trend of... 
Do they all perform incredibly similarly? ",43c78e7d,0.8085106382978723
37271,04e6b0d3c70f46,aed6daec,### Build Model,56344f77,0.8085106382978723
37276,5f674175839b32,4b849b5f,*The above heatmap shows games released for every genre in each year.*,53a2e343,0.8085106382978723
37277,2ada0305b68956,4dd22b01,### 138. Palette = 'plasma_r',133e26f4,0.8085714285714286
37280,e4c6dd957eb5ce,af6f1f6e,"# Votes in Kernels - Total unique users votes 
The idea of it is take all votes and combine the tables until we get the total of votes per unique users to Kagglers. 

- The objective is understand the distribution of unique users that have voted in a other user
- We can get an index of purity in the kernel votes or detect posible frauds?
- How is the result of total kernels and total unique votes received? 
",2e383665,0.8088235294117647
37286,9cec5ddf8b6f49,dd10292b,#### Visualize the optimization history. ,d39fc8e7,0.8088235294117647
37287,eb0ecd6bebeb15,168caca0,total.length'in standart sapma değerini yazdıralım.,d7b93a60,0.8088235294117647
37288,3d905ce4828057,44bb0698,"
# MISSION 3

**1. For 2010-2011 UK customers, divide all your customers into 4 groups (segments) according to 6-month CLTV and add the group names to the dataset.**

**2. Does it make sense to divide customers into 4 groups based on CLTV scores?Should it be less or more? Please comment.**

**3. Make short 6-month action suggestions to the management for 2 groups you will choose from among 4 groups.**",5b006cc3,0.8088235294117647
37291,e67925694c07d3,d7e16b06,### BUREAU Data,83af4c4a,0.8089887640449438
37293,04ff2af52f147b,2f3eff15,"**Encode Features:**

Now we encode our desired features.  We will begin using ordinal encoding for the *Deck* and *Title* features.",d5f37be9,0.8089887640449438
37294,ee23a565163388,7041d8d9,The K-Neighbors Classifier predicts the target label by considering the k-neighbors.,88aacbc4,0.8091603053435115
37304,87e94f864d74be,61712b64,## Distribution of Movie duration,294bfe9f,0.8095238095238095
37308,b6e698d389d0d3,7745f6e6,# predict test data target using model,f02f68b5,0.8095238095238095
37310,a758983a68c014,739e0f12,"Let's have a look at our embeddings. W2 is transposed to get shape [vocab_size, embedding_dims]. Weights are tensors, so we should convert it to numpy.",ab89f181,0.8095238095238095
37311,c818250dd720eb,b9237323,"# Data Labelling

(Up until now I had got by with an introduction from Andrew Ng's Machine Learning course, plenty of python tutorials - particularly several DataCamp courses, learning from other entrants' notebooks and many hours of trial and error. I completed Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning, a four week course on the Coursera platform. This gave me a grounding in flowing image data to Neural Networks using the Keras API.)

As mentioned in the introduction I trained three seperate pre-trained RESNET50 models to detect the three patterns of prostate cancer: Gleason 3, Gleason 4, and Gleason 5. Radboud data provided us with labels as to whether Gleason 3, Gleason 4 or Gleason 5 was present in a particular region so for each of our training tiles I could read the corresponding mask file and determine whether each of the patterns was present in the tile. Detailed code similar to the final code used can be found [here](http://www.kaggle.com/dararc/eda-on-tile-labels)

Karolinska data only labelled tissue as cancerous or benign. So I only used Karolinska slides which contained only one pattern. That is to say only tiles coming from images which were either purely benign, only gleason 3, only gleason 4 or only gleason 5 were used as training data for the RESNET50 models.",68ee40de,0.8095238095238095
37313,b4ecd6e4277e3c,d1b62c9f,"### Training

The method for training is borrowed from https://www.kaggle.com/hengzheng/pytorch-starter",94d79d5f,0.8095238095238095
37314,e16860fce156b0,59b07e9e,"#<b><mark style=""background-color: #9B59B6""><font color=""white"">plot_missing(): analyze missing values</font></mark></b>

They provide an API plot_missing to analyze the pattern and impact of missing values. At the first glance, it shows the position of missing values, which allows the user to be aware of data quality for each column or find any underlying pattern of missing values.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.8095238095238095
37319,04bac111ffbe9c,0341b349,## Random Forest Classifier,82576b17,0.8095238095238095
37322,6a80f915608fc2,9e571c8a,"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
### Continue with the <a id=""HyperSearch"">Hyper-Parameter Search</a> that follows
#### Or skip the whole Hyper-Parameter section, go to <a href=""#FeatureImportance"">Feature Importance</a> <br>
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",636938eb,0.8095238095238095
37328,76c8afe761adc1,a8875993,# Inference,8258944b,0.8095238095238095
37337,066c5ee1ef39e6,0ce3c1d2,## Predict on Test Data,0f394e1b,0.8095238095238095
37344,0d59a3e0130db0,3569db7f,Let's look at the graph of the network error change by epoch.,285f04b2,0.8095238095238095
37345,2f47abddfd1928,62a9702e,"Again we can identify insight in this new feature. Basically as it was expected, it translates in that if in your family there is a male that survived, your chances to survive are higher.",ae33cc0b,0.8099173553719008
37346,e9b9663777db82,bb7c17f3,"As this is a regression problem we are going to use famous regression models -
   1. LinearRegression without Feauture Selection Method
   2. LinearRegression with SelectKBest, F_Regression
   3. LinearRegression with RFE
   4. LinearRegression with Mutual Info Regression
   5. LinearRegression with Select From Model
   6. LinearRegression with PCA
   7. Gradient Boosting Regressor
   8. Random Forest Regressor
    ",648e8507,0.8099173553719008
37347,b10bd75889dad9,eacfc34a,#### Lets check the VIF ,ee00ceee,0.81
37348,4cd25e50c7e007,eb5ad8ef,"##### As we see that p_value < 0.05 and vif value is < 5% ,now we can say model is good",ceb0c525,0.81
37349,83df814455f06c,c5b80369,"### Compare the train-set and test-set accuracy


Now, I will compare the train-set and test-set accuracy to check for overfitting.",c9cff71a,0.81
37353,3c2033cc99c12c,bee30b3a,"Confusion matrix : also known as the error matrix, allows visualization of the performance of an algorithm :
+ true positive (TP) : Diabetic correctly identified as diabetic
+ true negative (TN) : Healthy correctly identified as healthy
+ false positive (FP) : Healthy incorrectly identified as diabetic
+ false negative (FN) : Diabetic incorrectly identified as healthy",dfa22a54,0.8102189781021898
37354,c80939c7c626cf,52f75989,"# 9 Cross Validation (K-fold)
In ML we cannot fit the model on training data also we cannot say model will work on final data
We must assure our model is not too noisy and got correct pattern",b9ac31e2,0.8102189781021898
37360,84127ade6fde87,1edb9168,"While the exact algorithms used are a bit out of scope for what we’re wanting to focus on here, we’d just like to mention that embeddings are often generated using neural networks, trying to predict a word from nearby words (the context) in a sentence. In this case, we could start from one-hot-encoded words and use a (usually rather shallow) neural network to generate the embedding. Once the embedding was available, we could use it for downstream tasks.",f55d05b6,0.8103448275862069
37361,a09e20bb9b5259,8984d64b,# Predicting on the test set,99475ae5,0.8103448275862069
37369,917957c6c4065f,6b49f161,"게시 후   
당일에 인기동영상이 되는 경우가 3%  
2일째에 인기동영상이 되는 경우가 90%  
3일 이내에 인기동영상이 되는 경우가 95%입니다.
게시 후 3일 이내에 인기동영상이 되지 못하면, 인기 동영상이 되기 힘들다고 보입니다.",55b8ed68,0.8104575163398693
37381,63b44c85e32c1f,47a21705,"---
## [](http://)Sets",fb9b9562,0.8108108108108109
37383,a6c34cd514e30e,b6bac55a,Distribution graphs (histogram/bar graph) of sampled columns:,bf603ddd,0.8108108108108109
37384,ac9b48d531bad9,99bdddcc,# Explanation of the Model,95965e35,0.8108108108108109
37387,b7b1057764fa02,48ebd66e,"We see that our model does rather well with the testing images which came from the same batch as the training data. this is unsurprising given that our model performed well with the training data. On the evaluation images however, our model does rather poorly with less than 50% accuracy. This tells us that we need to include more varied data in our analysis to get better results with real-world images. A great way to visualize the successes and errors of our model is to plot confusion matrices, which I will do next.

# 8. Confusion Matrices

A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known. Each row of the matrix represents the instances in an actual class while each column represents the instances in an predicted class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).

I will plot confusion matrices for both the testing and evaluation data. Since I have to go through the same code twice, it makes sense to first define a function that helps us plot these matrices. The function below does exactly that. The code `y = np.argmax(y, axis = 1)` converts the 2D array y into a 1D array with number labels for each image. I do this for both the true and the predicted values. Additionally, plotting the confusion matrix with a `cmap` automatically fills it with a colour gradient showing how many of the values were correctly predicted. The `for` loop at the end of the function fills each cell of the matrix with the number of predictions that correspond to that cell.",5053a192,0.8108108108108109
37390,bbb3f4b76a4559,16b6cfb9,### Downsampling and Bagging,75185823,0.8108108108108109
37391,d76896b30cebd3,816a3eb8,"Relation between Backers and Pledged Amount(USD)

",1b4e8f34,0.8108108108108109
37393,d6cbd7160961dc,47281c84,## 6.1. Preparing the data set,36d74664,0.8111111111111111
37394,3597174a998d4d,bf0731f1,I'd like to construction a new variable named new_lead_time.,276892ed,0.8111111111111111
37404,0ad8d416b89b78,f482aaf7,**Hyperparamater tuning** of the Random Forest Classifer utilising 'GridSearchCV',0b0562f0,0.8113207547169812
37406,510b8303776bb6,a23592db,## Reading test file,18080db8,0.8113207547169812
37409,f015d0147e8fbf,170e1074,"### Stratified 5-Fold Cross Validation (performed serially, fold-by-fold, using target encoding)

Categorical feature target encoding is performed five times (for each of the five validation folds). This is necessary in order to prevent data leakage.

### Why I decided to use target encoding:
Performing serial cross validation is obviously much slower than using LightGBM's built-in CV, which trains and predicts on all five folds in parallel. However, after making the apples-to-apples comparison of performing serial CV, first using target encoding, and then again using LightGBM's default handling of categorical features, I found that I got a higher (by `0.001`) CV score when using target encoding. This was enough for me to conclude that accepting the limitations of not using lightgbm.cv was worth it as a tradeoff for the higher score I got using target encoding vs. not doing target encoding, all other conditions being held constant.

The single biggest trade-off, of course, is that using serial CV probably does a slightly worse job of approximating the true ideal number of boosting rounds for training my model, in that I can only know the *average round* of best score, as opposed to *the single round* when the *average score across* all 5 folds was best.

### However, knowing what I know now:
As I mentioned in my comments at the beginning of this kernel, it would ultimately turn out that training a predictor using LightGBM's default categorical feature handling, where the number of boosting rounds was determined by the results of running lightgbm.cv, would result in a private LB score only just `0.0001` lower than the private LB score earned by my model that used target encoding and had its number of boosting rounds determined by the results of running serial CV. 

My conclusion is that with the proper hyperparameter tuning, there may not be much advantage to using target encoding with LightGBM, at least not for the Home Credit competition's dataset.",518954fb,0.8113207547169812
37414,ea4e559a86d613,464e3a7b,**1.Crop**,eff47843,0.8115942028985508
37415,b01ee6cb674fa3,2307bfc6,"# CECLES

I could find no info about it, so lets look at the data",a8ffd35e,0.8115942028985508
37418,598b6228760590,d7541cc3,- RF,be30ab66,0.8115942028985508
37419,7e275c8d5ff2a0,6bdb7200,Just like we predict the no. of confirmed Cases we are gonna predict Recovered and Death Cases,b3afcc98,0.8115942028985508
37421,0e2a23fbe41ca9,c9fe8c66,"Observations:
- ```avg_purchases_lag``` columns show similar patterns according when plotted corresponding to ```numerical_1```.
",64e4762c,0.8115942028985508
37431,49ee86d074de69,84edcecd,"* Whichever weights/coefficient is bigger, its correspending feature is more important.

<hr> ",71ccc6d3,0.811965811965812
37432,ba4b3bd184acbb,38c6f077,Sort the results by Price in descending order.,0f5de724,0.8120300751879699
37433,726833f92fb87a,3e25c859,## Poutcome,7dc5e1b6,0.8120805369127517
37434,5f32117bcd5255,995a9d61,#### SATO,85882abf,0.8120805369127517
37435,386c42a7fb27a4,51f64106,#### Categorical Columns,9e9f6974,0.8125
37438,49f2274c1dd516,953570d7,"# IHME
Data describes the forecasting carried out by the IHME on the COVID-19 impact on hospital bed-days, ICU-days, ventilator days and deaths by US state in the next 4 months",06b0ffee,0.8125
37445,3cc097a5859dc1,538b215e,# **Applying LogisticRegression Model**,14380d73,0.8125
37447,c85c94076e9c3a,ac658755,## Kids_Teen_at_home & Total_Purchases,3ea0c443,0.8125
37450,9ec2fb131cf677,78da1c21,# Season wise view counts,211ea6bd,0.8125
37458,5e02999ca74e7e,b19fc38d,### **Visualization for Actual and Predicted Sales in Validation Data**,b69da28e,0.8125
37460,2a724fb7835cdc,172b8b31,**Train the Model:**,c38ac61d,0.8125
37478,13c7672da1b571,60bd8d59,"Finally, the best parameters will be used to the generate predictions on the entire training set.",002d3ec0,0.8125
37479,5cfb546af2b8ce,b4ea66e7,Note that an image of the output is provided below as it is too difficult to link shp file,e8730ad1,0.8125
37480,fdbbd573ba31c2,3a79d2e5,## Lasso,f7c28d74,0.8125
37484,8c7e00ca3dc5a7,0e235fc7,"After applying OneHotEncoder, the data is at different scales now. It needs to be normalized.",c83346e4,0.8125
37486,117fc0956643d0,b07c2e35,"## Step 5. Run over All 10 COVID-19 Questions

We have run the same approach for all the questions currently posted on Kaggle. Again, we have run the code with GPU in Google Colab due to the time constraint. Remove the files from ""../input/covid19-top-articles"" if you prefer to run on your local machine. ",68cef9fd,0.8125
37489,68cceffe5bb8ec,cb7ea97a,# Evaluate,dcbfcd6e,0.8125
37490,3b5903412fe741,424e864b,"`pandas` comes with a few pre-built conditional selectors, two of which we will highlight here. The first is `isin`. `isin` is lets you select data whose value ""is in"" a list of values. For example, here's how we can use it to select wines only from Italy or France:",ad231969,0.8125
37491,ffc9490c4f6c38,f0e6a1a7,"Next, I drop columns whose data range is under 15.",ae7bbbb3,0.8125
37494,30fdc4a6e3c1db,79fae35a,### Plotting daily sales for the year 2012 to see the pattern of impact of events,6111ddee,0.8128654970760234
37495,a566b5b7c374e7,8aeb64f8,## Restfulness Score,b3dc5545,0.8129496402877698
37499,2bd6c370695ea7,1e4891ae,## Sequences,cbe6aec8,0.8133333333333334
37500,91eaec994e0c6f,6ad726ca,"order = (p,d,q)
- p: AutoRegression (AR) order.
- d: Trend Differncing order.
- q: Moving Average (MA) order. <br>",376aef10,0.8133333333333334
37504,67b7354e96113a,4321274f,**Random Forest Classifier**,dca94250,0.8133333333333334
37506,7e1da639035ac5,660169b8,### <a id='14.2'>14.2 Strong Family-Community Ties ratings statistical analysis</a>,120b6c23,0.8133333333333334
37511,a81661cc35d8d2,f5fb4ba2,Creating Random Forest models on our datasets,3331f113,0.8135593220338984
37512,dac3c8204a2d1b,ad9279b4,# Most Expensive Authors,b0d2d0dc,0.8135593220338984
37513,bb0905d33ae417,11b9ec98,"At this stage, we would like to check the effectiveness of the `learn` model against our validation set (which is automatically generated by the `ImageDataBunch` object). We will use the following methods to evaluate the effectiveness.
1. Confusion matrix.
2. Accuracy.
3. ROC-AUC, as dictated in the competition evaluation.",25fd1965,0.8135593220338984
37515,1294fb4c86f993,62c4bb9a,Renaming the columns for easy access,4471e513,0.8135593220338984
37516,c4bca5d86a38c3,73a29775,Función para crear modelo,e23d297c,0.8135593220338984
37521,f2e5e9fb9eaaf7,1f561dfc,"<a id=""6.2.3""></a>
### 6.2.3 Maximum of features
Create a new feature `max` that calculate the maximum value in a row. Adding `max` to the model doesn't change the model performance.",048e0d08,0.8135593220338984
37527,71b75664517244,3d26a21a,These comparison shows that Chelsea can still perform well no matter who is their managers,fc905af5,0.8137254901960784
37528,4c47839b067546,f02dcfc6,# 4. Feature engineering,1f517b02,0.8138297872340425
37532,1660daf8867980,a094313e,"**Theory**
* In SARSA/TD0, we back-up our action values with the succesor action value
* In SARSA-max/Q learning, we back-up using the maximum action value. ",42d7cffc,0.813953488372093
37533,22bd95f4807a23,2bb4beb3,"## Distribution of Sentiment scores across product types and age groups
For assessing the sentiment scores, we will use a lexicon based approach. Each word in a  text document is associated with a score. A positive score for a word that signifies a positive sentiment and negative score for the word that signifies a negative sentiment. Neutral words like 'a','the' etc get a score of zero. These scores are then summed over a text document. For this analysis , we will be using is the AFINN lexicon by Finn Årup Nielsen.",c05d356f,0.813953488372093
37535,72d393488311b6,5e91e305,# Prediction,80663df0,0.813953488372093
37537,c09fac3c943d51,ac5eee7a,"Make one user one object:
* all features are averaged
* we hope, that categorical features do not change for one user (that's not true :/ )
* categoricals labels are averaged (!!!) and are treated as numerical features (o_O)
* predictions are averaged in multiple ways...",678d076d,0.813953488372093
37540,a5a419dc7245b0,611c2c08,##### Fitting the ANN to the Training set,4279726e,0.8141592920353983
37542,c9b4e282e4e2c1,03b5438e,"Since a temperature of -999 Fº seems very unlikely, I'm going to obviate that value.",f44d339f,0.8141592920353983
37544,2ada0305b68956,385c8ac7,### 139. Palette = 'prism',133e26f4,0.8142857142857143
37546,fe6750354fb64f,d9ac2dbd,## Time Series Forecasting,271741f0,0.8142857142857143
37550,9c044fa3072552,c9798977,## Start Latitude & Longitude,1362842e,0.8142857142857143
37551,38b79494ac749e,7f294cb5,#### Summary,39162a40,0.8142857142857143
37552,063a35f644e3c5,66d9b6e5,## Logistic Regression,1c30fb0a,0.8144329896907216
37553,8ec771f5600a61,adf11e1b,## creating obj for the logestic regression implementation,48364c1f,0.8144329896907216
37559,f4b9042e693b6c,6adeea5d,"Finally, we define a main function that we will run on each of the 8 cores of the TPU.",676cacc9,0.8148148148148148
37560,2500c5fe8497ee,6b8cb76c,# Q3) What years had the most worldwide meat consumption?,855355f0,0.8148148148148148
37565,4392956f62c040,fbc75296,"## PREDICT AND SUBMIT

Hindi: 0.47 average jaccard score on our validation set   
Tamil: 0.50 average jaccard score on our validation set",c3ed519d,0.8148148148148148
37568,4883314a96dc34,60e37d12,Performance metrics on ***test set***:,50d36836,0.8148148148148148
37572,6d29650083cbde,66e0fbf1,"Although my group average is well below the top 10% target (730), it seems like I am still picking up on valid characteristics and that the players do on average, ceteris paribus, very well. 

I encourage you to look into the disapointing players, and you'll often find interesting stories (e.g. Carrasco moved to China after just a few months of the season).",e65fd993,0.8148148148148148
37578,c1984e64b35234,ddc123d2,**Alzheimers:** ![image.png](attachment:image.png),1811225b,0.8148148148148148
37581,c6f8ff61a5fa87,3cf66774,"# <span style=""color:blue;""><strong>8.Blending</strong></span>",3eea586b,0.8148148148148148
37583,ddcdecdd6a3b6d,01635637,"NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。  
NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。   
NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。  ",90831448,0.8148148148148148
37591,1667a100fc8b42,babcfcf0,"Now, let's try the same but using data with PCA applied.",6c8cd6b6,0.8148148148148148
37596,c968dbd8d49ae6,a5f2408e,"As seen, the dataset is highly un-balanced. We must take that into account when we look at the results, either giving more weight to one class or by balancing the data.",dfb2684d,0.8148148148148148
37602,738bfced935b69,e5bc41d7,The distribution between price & year is skewed to left.,2d3c592d,0.815068493150685
37606,09751c520b0616,b5ea5aeb,### Standardization of train and test data,a4d0c7e9,0.8153846153846154
37607,a8c042af6b7245,1ea54553,#### Creating interaction variables,2487ac62,0.8153846153846154
37613,03048e86a6d806,fbeefd12,"Most of these data-related job titles use Python as the most regulary used programming language. Meanwhile, Statisticians mostly use R and Database Engineers use SQL, and Python is the second most regularly used programming language for both job titles.",1285c231,0.8153846153846154
37616,d07915a6e6992e,c70ba251,"Now we have the training and test datasets available and we can start training the model. We will build couple of base models and then will use Grid Search method to optimize the parameters. There are several classification you can select.
We are trying following to develop a baseline - 

        1. K Nearest Neighbour
        2. Linear Discriminant Analysis
        3. Support Vector Classifier
        4. Multi-layer Perceptron classifier
        5. Extra Trees Classifier
        6. Logistic Regression
        7. Decision Trees
        8. Random Forest
        9. Gradient Boosting Classifier
        10. AdaBoost Classifier
",2b912140,0.8153846153846154
37618,6a80f915608fc2,cfbe0119,"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
###  Done with <a href=""#HyperSearch"">Hyper-parameters</a> above
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",636938eb,0.8154761904761905
37624,b61ab8f81dc03d,7ffe5565,"<a id=""random_forest_classifier""></a>
## Random Forest Classifier
A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.
I've got the definition and parameters from https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html",64d05394,0.8156028368794326
37626,a1dcd92986bc84,965ad849,"Set the `query` variable to the type of images you want to search for.
Try things like: 'a plate of healthy food',
'a woman wearing a hat is walking down a sidewalk',
'a bird sits near to the water', or 'wild animals are standing in a field'.",730acaaa,0.8157894736842105
37629,3fb15e6e48aec2,9db89792,# Full Survival prediction,9d1f4358,0.8157894736842105
37634,52ee792e228d54,2b8bedc1,### SVM,5096094e,0.8157894736842105
37635,31b564f11ef638,09288df6,### Random Forest Regression Model is the Best among four models!,424f9692,0.8157894736842105
37640,ee9ddc756b2d4a,183b253e,### Regressione logistica per mettere insieme nel migliore dei modi i due risultati,e367eab3,0.8160919540229885
37642,14defffcd250f3,de55f589,**Training and Prediction**,3a683b94,0.8160919540229885
37645,7f74a04ae75792,c3fb6a27,"### What is the average number of purchase made in the last 5 years among different customer status
",d01e91da,0.8161764705882353
37649,eb33e05704d647,d6f3bf1b,Build the Model,cd80436d,0.8163265306122449
37650,ffd1df95ca5289,755c144b,"mean of kfold accuracy is 81% let's check f1 score, precission and recall of dependent variable ",db00c338,0.8163265306122449
37651,e69a496109e7d8,5d2531cc,# Multi-Variate Analysis,1c640591,0.8163265306122449
37652,12f4d16fc21645,745f35e8,<h1 style='color:blue'>Compare train and test set accuracy</h1>,c7752038,0.8163265306122449
37657,541d0fa0e26b80,9ef3797d,# Visualization With respect to Grades,a29e0f29,0.8166666666666667
37660,712198370d5521,ac483005,"
From the above plot, it can be clearly seen that cluster 1 is our biggest set of customers closely followed by cluster 0.
We can explore what each cluster is spending on for the targeted marketing strategies.
",5882e04c,0.8166666666666667
37662,5b92c712910a11,689d89a0,"# Decontracted
* Expanding the chat words like ""i've --- I have""",e1d17100,0.8166666666666667
37665,cf4d1c1ad1476c,be45cc8f,# training the Model,768c1a59,0.8166666666666667
37666,c18267b203f28a,48427603,"# Evaluating our model
The first chunk of code is provided to show you where the variables in the second chunk of code came from. As you can see, there's a lot of room for improvement in this model, but because we're using TPUs and have a relatively short training time, we're able to iterate on our model fairly rapidly.",09ca8efb,0.8166666666666667
37669,b10bd75889dad9,349a2ce1,"#### All VIF values are also fine < 5, there is no more multicollinearity",ee00ceee,0.8166666666666667
37670,37b09262279764,cdb8a31c,#### LogisticRegression,37c4c417,0.8166666666666667
37683,81712ee7510ac5,f9ac84dd,**For Loops**,c4685e79,0.8171428571428572
37684,0caaec057f7184,38134729,"##### About the negative-price item - sales, price, shop",b875533e,0.8172043010752689
37687,99bf357eaf61f1,df58e06c,### Random Forest Regressor,9d92fafe,0.8173076923076923
37688,71c3c1eab0377d,8bc4ed3a,"**NOTE : Always remember to convert the Target variable in training Data Set as Factor or categorical variable.
Else the the H2O's GBM algorithm will not create a classification algorithm. **",52b4e360,0.8173913043478261
37691,63b44c85e32c1f,799a181d,"Sets are mainly used to eliminate repeated numbers in a sequence/list. It is also used to perform some standard set operations.

Sets are declared as set() which will initialize a empty set. Also set([sequence]) can be executed to declare a set with elements",fb9b9562,0.8175675675675675
37693,241cf32abb22d8,1ae76906,"In this section, I fit the optimal models from the above analyses on the test data with 5-fold stratified cross validation and 3 repetitions. Then, I compare the performance of models by paired t-test:
* DT (full features) vs. DT (top 10 features)
* KNN (full features) vs. KNN (top 10 features)
* NB (full features) vs. NB (top 10 features)

Full Features:
* DT vs. KNN
* DT vs. NB
* KNN vs. NB

Top 10 Features:
* DT vs. KNN
* DT vs. NB
* KNN vs. NB",47157066,0.8181818181818182
37698,722cd844dfbe8f,336b8805,Then we **train the multi-input model** with the network defined above:,0cedb385,0.8181818181818182
37699,bb3d1b4b9f1248,59e7822a,Average PerCapita analysis 2019 vs 2020 for wine consumption,bf7de324,0.8181818181818182
37700,1466e61d45b718,3f17ccb8,"Oh, no! There are no automatic insights available for the file types used in this dataset. As your Kaggle kerneler bot, I'll keep working to fine-tune my hyper-parameters. In the meantime, please feel free to try a different dataset.",b062d92b,0.8181818181818182
37701,b241b847319d13,638ae84d,# Checking TFRecords,0fb698f0,0.8181818181818182
37704,be2f4d8a6b73ca,be7f916c,"<div style=""color:white;
           display:fill;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana;
           letter-spacing:0.5px"">

<p style=""padding: 25px; color:white; text-align:center""><b> Model Building</b></p>
</div>",5d8ce40a,0.8181818181818182
37707,016abae0483764,956415db,### Random Forest Classifier,bc9f289b,0.8181818181818182
37709,3a15bac33f2a74,efc80a35,**lgbm**,25204b71,0.8181818181818182
37711,5083d7a61f2426,fdd17495,"After you train and test your model, with the data that you already had, you want to predict future data, which is, I think, the trully interresting thing about recurrent networks.

So in order to make this, you need to start predicting the values from one day after your final date in your original dataset, using the model (which is trained with this past data). Once you predict this value, you do the same thing, but considering the last values predict, and so on.

The fact that you are using a prediction to make others predictions, implies that is much more difficult to get good results, so is common to try to predict short ranges of time.",541a0fec,0.8181818181818182
37713,132fa9714f2046,cfeab247,** See if you can resize the plot by adding the figsize() argument in plt.subplots() are copying and pasting your previous code.**,3bb1775f,0.8181818181818182
37716,da199f8fb59439,35061d9e,![](https://cdn1.edgedatg.com/aws/v2/abc/GreysAnatomy/showimages/64f1df58afb7276e875ee449e76e8635/1200x627-Q80_64f1df58afb7276e875ee449e76e8635.jpg),baaa665d,0.8181818181818182
37717,450fda47b03baa,36fb15bb,Her kategorinin yüklenme oranlarını hesaplayalım. En çok yüklenen ilk 15 kategoriyi görselleştirerek gösterelim.,62c04adb,0.8181818181818182
37718,1691c9f2c2f656,4f667a82,"# Add Donald Trump mask

![https://free.clipartof.com/1583-Free-Clipart-Of-Donald-Trump.jpg](https://free.clipartof.com/1583-Free-Clipart-Of-Donald-Trump.jpg)",70433e76,0.8181818181818182
37724,4ae464582bac51,8d6837db,# TESTING UNDERSIMPLING,ca6a52ce,0.8181818181818182
37730,0a918602a04693,4bdadf57,"# **Pipeline Creation without PCA**
1. Process data using standard scaler
2. Apply classifier

# **Pipelines Creation with PCA**
1. Process data using standard scaler
2. Apply PCA
3. Apply classifier",c1ef0e95,0.8181818181818182
37731,b1684dfa49524a,955f43c5,- then we can guess the max temp and the min temp for the early years,60a2599b,0.8181818181818182
37733,347c7b0f48c53f,6733268c,"# Preprocessing
The first preprocessing step is to remove references from the article. Wikipedia, references are enclosed in square brackets. The following script removes the square brackets and replaces the resulting multiple spaces by a single space. 

# Removing Square Brackets and Extra Spaces

The article_text object contains text without brackets. However, we do not want to remove anything else from the article since this is the original article. We will not remove other numbers, punctuation marks and special characters from this text since we will use this text to create summaries and weighted word frequencies will be replaced in this article.

To clean the text and calculate weighted frequences, we will create another object. 

# Removing special characters and digits

Now we have two objects article_text, which contains the original article and formatted_article_text which contains the formatted article. We will use formatted_article_text to create weighted frequency histograms for the words and will replace these weighted frequencies with the words in the article_text object.

# Converting Text To Sentences
At this point we have preprocessed the data. Next, we need to tokenize the article into sentences. We will use thearticle_text object for tokenizing the article to sentence since it contains full stops. The formatted_article_text does not contain any punctuation and therefore cannot be converted into sentences using the full stop as a parameter.

# Find Weighted Frequency of Occurrence
To find the frequency of occurrence of each word, we use the formatted_article_text variable. We used this variable to find the frequency of occurrence since it doesn't contain punctuation, digits, or other special characters.

In the script above, we first store all the English stop words from the nltk library into a stopwords variable. Next, we loop through all the sentences and then corresponding words to first check if they are stop words. If not, we proceed to check whether the words exist in word_frequency dictionary i.e. word_frequencies, or not. If the word is encountered for the first time, it is added to the dictionary as a key and its value is set to 1. Otherwise, if the word previously exists in the dictionary, its value is simply updated by 1.

Finally, to find the weighted frequency, we can simply divide the number of occurances of all the words by the frequency of the most occurring word.

# Calculating Sentence Scores
We have now calculated the weighted frequencies for all the words. Now is the time to calculate the scores for each sentence by adding weighted frequencies of the words that occur in that particular sentence. 

n the script above, we first create an empty sentence_scores dictionary. The keys of this dictionary will be the sentences themselves and the values will be the corresponding scores of the sentences. Next, we loop through each sentence in the sentence_list and tokenize the sentence into words.

We then check if the word exists in the word_frequencies dictionary. This check is performed since we created the sentence_list list from the article_text object; on the other hand, the word frequencies were calculated using the formatted_article_text object, which doesn't contain any stop words, numbers, etc.

We do not want very long sentences in the summary, therefore, we calculate the score for only sentences with less than 30 words (although you can tweak this parameter for your own use-case). Next, we check whether the sentence exists in the sentence_scores dictionary or not. If the sentence doesn't exist, we add it to the sentence_scores dictionary as a key and assign it the weighted frequency of the first word in the sentence, as its value. On the contrary, if the sentence exists in the dictionary, we simply add the weighted frequency of the word to the existing value.

# Getting the Summary
Now we have the sentence_scores dictionary that contains sentences with their corresponding score. To summarize the article, we can take top N sentences with the highest scores. The following script retrieves top 7 sentences and prints them on the screen.

In the script above, we use the heapq library and call its nlargest function to retrieve the top 7 sentences with the highest scores.",c58305ba,0.8181818181818182
37739,45cf7099ebd023,511010ff,# Points,0d292462,0.8181818181818182
37740,f0fab078f8533b,ec256df7,## e. The bar plot below shows the total number of Movies and TV Shows per ratings respectively,bdb5ea32,0.8181818181818182
37741,d83e5b44d1b80d,bb08c0e7,"***In all top responded contries the preference for programming language order similar. Python ranks first and followed by R, SQL C, C++ etc. Interestingly Java, JS are the least prefered when it comes to Datascience/Machine learning***",62845930,0.8181818181818182
37742,5a8c553e21c70f,898b5346,Each neural network model is trained on a seperate **bootstrap sample**. Validation is done using **OOB** samples.,9ebd9d8f,0.8181818181818182
37745,dd02a9b545f742,ed638306,# Phase 3 | Commissioning mode,7116cd2d,0.8181818181818182
37746,32e04b08ff52eb,dcfd66ed,ErrorBar,8d5b86e0,0.8181818181818182
37748,2f47abddfd1928,251f98b1,"def find_female_child_died(surname):
    data = df_all[((df_all['Surname'] == surname) & (df_all['Sex'] == 1) & (df_all['Age'] > 14)) | ((df_all['Surname'] == surname) & (df_all['Age'] < 14))]
    woman_child_survive = 0
    i = 0
    if data.shape[0] > 1:
        while ((i <= data.shape[0]-1) & (woman_child_survive == 0)):
            if data['Survived'].iloc[i] == np.nan:
                continue
            elif data['Survived'].iloc[i] == 0:
                woman_child_survive = 1
            i += 1
    else:
        return 0
    return woman_child_survive

df_all['Family_woman_child_died'] = df_all['Surname'].apply(lambda x: find_female_child_died(x))
df_all[df_all['Family_woman_child_died'] == 1]## 4.4. Female with died child

In the same way we can compute which women passenger have in their family a died children.

At that time the women were the major responsible of taking care of the children, therefore if a woman have a children dead in his family it will be more likely for her to be dead.",ae33cc0b,0.8181818181818182
37758,73d8e56bc709b1,9fd2a93d,2. Height vs Overall,78ec3cce,0.8181818181818182
37767,70193f0c034b98,9154e163,# Validation function,f8cacd26,0.8181818181818182
37768,c2be02442e8cfd,60476354,"# Observations:

1. people having higher survival rate in the operated year 1966, compared to other years like 1959-1961",2d364acc,0.8181818181818182
37772,663bbc9eaf267b,4fbee490,## Decision Tree Regressor,32445529,0.8181818181818182
37774,9ceb7278784462,7128edea,## Model Tuning,3768a567,0.8185483870967742
37776,5ce12be6e7b90e,c64e701b,### Exercise: string loop,c0ab62dd,0.8187134502923976
37781,b01ee6cb674fa3,a60b9962,"Wow!- 
Some info we've got here!

CECLES has 2 countries, France and Australia. 

The initial idea is to check for more results with 'Australia' ",a8ffd35e,0.8188405797101449
37782,7e89d387feb9f5,8cd10f0b,"### Добавленный признак №18. Количество людей в городе, приходящихся на 1 отзыв может характеризовать популярность ресторана.
## Почему-то не сработало. Точность модели немного ухудшилась. Дружно кекаем!!!",989e3a1b,0.8188405797101449
37791,4c47839b067546,2d72d5bb,"Создадим новые признаки:

- mileage_per_year: с помощью the productionDate и mileage columns получим информацию,сколько км проехал автомобиль за год;
- rarity: был ли автомобиль произведен ранее 1960;
- older_3y: старше ли автомобиль трёх лет;
- older_5y: старше ли автомобиль пяти лет;",1f517b02,0.8191489361702128
37792,f6648e47713411,68acf05e,"Dựa vào biểu đồ, chúng ta có thể chọn learning rate của Adam = 10^-3. ",f4af4d1c,0.8191489361702128
37794,4daf6153275cbf,3889dd61,"H0: There is no difference between local cuisines and other cuisines in terms of number of reviews.

H1: There is difference.",51db1961,0.8192771084337349
37800,593d1d3d1df05a,a81c408c,# Predicting the Output and Showcasing the Result ,bc682ffe,0.8194444444444444
37801,c01049afb6d307,a1796a9b,### (12) Diseases of the skin and subcutaneous tissue,d37d3b5d,0.8194444444444444
37810,b10bd75889dad9,e0d8e098,### Lets Make Predictions,ee00ceee,0.82
37813,2ada0305b68956,f1867d00,### 140. Palette = 'prism_r',133e26f4,0.82
37816,10c5a39a87c47e,422e6ae9,### Computing Area Under Curve (AUC),09c7337a,0.82
37817,7dd46c750653eb,b6f20f25,**Adoption and Name Change **,c2644713,0.82
37818,4cd25e50c7e007,a0a2803b,## R-squared:	0.82,ceb0c525,0.82
37829,80ad12f326ab70,2799b421,"The lineplot above shows the percentage access each day of the month. Interactions are quite low on the weekends. Engagement reached peek in months January and September, the trend starts dipping from february and takes a sharp drop in July.",da404a16,0.8205128205128205
37830,c8c4705cca1ebb,7ca35576,# 2. Model Train ,6d9d7107,0.8205128205128205
37831,e424c111c44669,aa9a5d08,**Network**,d9fccfba,0.8205128205128205
37833,4d91e84c564cbe,6381450e,"Click the ""output"" button to see the full help page. Lists have lots of methods with weird-looking names like `__eq__` and `__iadd__`. Don't worry too much about these for now. (You'll probably never call such methods directly. But they get called behind the scenes when we use syntax like indexing or comparison operators.) The most interesting methods are toward the bottom of the list (`append`, `clear`, `copy`, etc.).",355a43e3,0.8205128205128205
37836,50b03ce5b1a286,ff32717e,Above I got ValueError: multiclass format is not supported,d49896a5,0.8205128205128205
37837,897ca904b74a98,fe4e2003,"To optimize the model performances, we will use a Grid search CV to test multiple hyperparameter combinations ",c5844ad4,0.8205128205128205
37839,d4c5aaa4b36810,d260884b,"Appears to be fitting something close to a linear model. 

### Polynomial Ridge Regression",65441f28,0.8205128205128205
37842,49ee86d074de69,81e7b0c3,### Interpreting The Logistic Regression Coeff.,71ccc6d3,0.8205128205128205
37846,43e60eb1362f5c,6b2d8c7b,# Model fitting and results,87934234,0.8207547169811321
37848,21413205980558,80dacfb4,"# Because it is a classification event to decide whether a person will have a deposit, we think of the construction model in machine learning. One is the simplest logical regression model based on the correlation of numerical variables, and the other is the random forest model, in which the relevant numerical variables and classification variables are included in the model.
# 由于决定一个人是否会有存款是属于一个分类事件，所以我们在机器学习中想到的建构模型，一个是最简单的根据数值变量的相关性建立逻辑回归模型，另外一个则是随机森林模型，将相关的数值变量与分类变量都纳入到模型中预测。
",84197de0,0.8208955223880597
37854,f91f58d488d4af,c655b815,#### Basic training loop for an epoch.,5df1bbf3,0.8210526315789474
37856,e19e307b3fd188,4f5b8f8e,#### List of ML models,2173955b,0.8211382113821138
37858,f4514ec092a771,2c79b896,The postprocess script have to handle this file to get a submission file.,3739ab1e,0.8214285714285714
37859,3cd78d8d6d56e4,cbf7530f,## Creating Competition File,9f632e94,0.8214285714285714
37862,6a80f915608fc2,b25ed5df,"## <a id=""FeatureImportance"">Feature Importance</a>
Back to <a href=""#Index"">Index</a>",636938eb,0.8214285714285714
37866,b290039151fb39,573cfb90,"I have performed a check of different optimizers and schedules on a [similar task](https://www.kaggle.com/c/Kannada-MNIST/discussion/122430), and [Over9000 optimizer](https://github.com/mgrankin/over9000) cosine annealing **without warm-up** worked the best. Freezing the backbone at the initial stage of training didn't give me any advantage in that test, so here I perform the training straight a way with discriminative learning rate (smaller lr for backbone).",1836a79c,0.8214285714285714
37868,565ad413cd802f,c53e0542,## Creating a submission file,397b074e,0.8214285714285714
37869,e78e7edae89049,f70ab07b,# LSTM,9cef1d94,0.8214285714285714
37872,8dd655515e7d18,b23b2f36,"### Conscientiousness

A person scoring high in conscientiousness usually has a high level of self-discipline. These individuals prefer to follow a plan, rather than act spontaneously. Their methodic planning and perseverance usually makes them highly successful in their chosen occupation.
",895f41cf,0.8214285714285714
37875,0dd3ac2d55efd7,479ee95e,"Now we can use different methos to increase the f1 score. Using StratifiedKFold definitely gives better results, Data Augmentation can also be tried and many such more techniques. We can vary the hyperparams such as batch size, learning rate and see how the model performs.",e9aa2cc2,0.8214285714285714
37879,76d94f2011a1cb,d542313a,"Resnet converges to 100% accuracy very quickly.
Plotting losses and watching confusion matrix is of no need.",9820aca8,0.8214285714285714
37880,98fd05fcc5c3e3,683981a6,### We'll select 3 as the number of clusters,55fe7ece,0.8214285714285714
37883,7686f42e1f28d2,2fcd74f7,"Some of the words (""delicious"", ""great"", ""good"") suggest a positive review while others (""but"", ""bland"", ""sorry"") suggest a negative review",6c128859,0.8214285714285714
37887,b8849a04581d32,f0ac4423,#### The XGBClassifier and RandomForestClassifier models showed the highest accuracy. Let's try to select parameters for them that will further increase their accuracy,b8a568cd,0.8214285714285714
37893,fdc9f4863744b1,695f985b,"Based on the data exploration, I can use GROSS SQUARE FEET, LAND SQAURE FEET as predictive variable for Single or Multi Linear Regresstion data model. Important data variables are;

<ul>
    <li>Residential Units</li>
    <li>Land Square Feet</li>
    <li>Gross Square Feet</li>
    <li>Age of the Building Sale</li>
",b4529365,0.821917808219178
37899,c8bf959b9608cf,731e7b24,"### Define Class Evaluator
In the above function 'eval_loss_and_grads', it returned the combined loss and grad value. Here, we define a class which has separate 'loss' function and 'grads' function, that will seperate the loss and grad values. They will return those values separately.",155e3672,0.8222222222222222
37900,c73e07ad6d25c5,cc1d38e9,### Add Family Feature,3ab391fb,0.8222222222222222
37906,892be0a523578c,17d364c9,"By counting the number of customers in each cluster, the number of regular exercisers (customer in cluster 0 or cluster 2) is 12 out of 26, about 50%.",b0e8d7c0,0.8222222222222222
37910,49ac6594c8f5cf,a0c65097,"This is a classic case of Imbalanced dataset.
Now next we will move on to balance it.",6f19f28a,0.8222222222222222
37911,4fd4b6a80d40e3,37232ac8,"## Softmax Function

![image.png](attachment:image.png)",f6913cc3,0.8222222222222222
37913,d96e03a9e7c030,8db65234,"Let's produce similar graphs as the two above, but instead of the size of each bubble representing *all* additional expected testtakers at a school, we can look at the estimated number of additional testtakers from underrepresented groups

### Expected Additional Testtakers - Black / Hispanic",d2b72ced,0.8222222222222222
37918,c84925c8171900,8cd1e4da,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.4 Region Wise Sales per Genre
            </span>   
        </font>    
</h4>",e21ff7ec,0.822429906542056
37926,56785caebaa256,e7e43163,"## 6. Prediction <a class=""anchor"" id=""6""></a>

[Back to Table of Contents](#0.1)",a792961a,0.8226950354609929
37941,7cfd96218dd933,7eac9c3c,"#### **ATTENTION**
* HERE IS CHICO
* THE HIGHEST VALUE IN THE WORLD WAS IN THE AMERICA
* THIS SITUATION MAY PROVE THAT THERE IS NO ANOMALY IN THE MEDITERRANEAN LOCAL",7c34d96c,0.8235294117647058
37947,52cfd66e9ec908,dc8a657f,"~~Unhide the above cell if you want to see the model, it's basically Peter's original work. All I am doing here is modifying the training pipeline to be Catalyst-compatible.~~

~~Update 17-09-2020: The model is roughly based on what kkiller has accomplished with PointNet and is basically me using a modified FPN network - to first segment (encode and decode) - and then use a simple MLP to classify. The model is roughly simple but it gives me pretty good results on loss, maybe something worth looking into?~~

The model is not eactly meant to segment + classify, that was my mistake in clarifying. It is simply a resnet50 + CBR block (Conv - BatchNorm - ReLU) + final part to get preds.",c74adcdf,0.8235294117647058
37951,4ae6a182abac64,d9a7101a,### 5- Evaluating the performance of the model,418676c5,0.8235294117647058
37955,b779c3ce7b671a,5f9c6042,"# Exercises for reader

This concludes the guide on simulating a planet orbiting a star. As you may notice, it isn't perfect. A few improvments are left as an exercise for the reader:
> 1.  Evaluate the accuracy of the simulation by setting up a model solar system and comparing with real data
2. Experiment with different star masses and initial planet velocities
3. Implement moons that orbit planets
4. Create a binary binary star system 

---

### More planets animation",ca778770,0.8235294117647058
37956,e170d33ee1da8c,3d9d78bc,## Cross validation,253cee3c,0.8235294117647058
37957,99821bc6a45be6,5e340f18,## COVID-CXNET Model:,b9d59346,0.8235294117647058
37958,02b7e38902069e,9b2fd35a,"#Use the tokenizer just for sentence segmentation. To access segmented sentences, simply use",726a03a0,0.8235294117647058
37959,a0a5baa6c7e12a,769e06c9,"## <div style=""font-size:20px;text-align:center;color:black;border-bottom:5px #0026d6 solid;padding-bottom:3%"">Express EDA Analysis with AutoViz</div>

We are going to invoke *AutoViz*, one of the prominent freeware Pythonic Rapid EDA tools, to quickly draw the basic insights about the data",551d41de,0.8235294117647058
37960,36d0d4cb9c7993,ff36ad21,## Finding accuracy,34b93e27,0.8235294117647058
37962,1d1598b6fa2aa7,114cfc2d,### Animated figures,e066accf,0.8235294117647058
37965,71b75664517244,bca0f805,### Arsène Wenger,fc905af5,0.8235294117647058
37966,907f08f9a2c6cf,580a1fcd,### Test it out!,aa84c325,0.8235294117647058
37970,21bce4ec54b3fa,c83f257d,"# Permutation importance

The final technique that I'm going to consider in this post is permutation importance, which as argued (https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py) is superior to default impurity based feature importance.

Additionally, I'll wrap feature selection part into Sklearn pipeline, such that it can be integrated into end-to-end solution.",35546e30,0.8235294117647058
37973,55c34673c1f760,241246b3,## Saving,2663c47f,0.8235294117647058
37974,156bbcff05dcea,4fd300a1,# Model Summary,66ad1fe9,0.8235294117647058
37980,842547b2def18c,cfe4567b,"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).

KNN confidence score is better than Logistics Regression but worse than SVM.",b8efde6d,0.8235294117647058
37981,fa02c409161192,05bb4667,"Below we will plot the relationship between number of hidden nodes against performance. From the plot we can see that the NN does extremely badly for smal numbers of hidden nodes but plateaus at around a $100$ nodes where the performance levels out. Note the convergence of the performance to $100\%$ and that the number of hidden nodes has to be exponetially larger to improve the preformance at the high end.

Seems that my guess was somewhat correct. The performance is extremely bad for a small learning rate and increases sharply to a maximum at around $1.0$ and drops off for larger learning rates.

If I am not mistaken then this is what is known as *hyperparameter tuning*.",e97077f7,0.8235294117647058
37982,917957c6c4065f,1403566d,"게시 후 1,000일이 지나 인기동영상이 된 사례들입니다.  
16123은 2018년 2월 18일 방영된 효리네민박에서 언급되어 하루 지난 19일에 인기동영상이 된 걸로 생각합니다.  
6684는 2017년 12월 17일 샤이니 종현 사망 사건 이후 22일에 인기동영상이 된 것 같습니다.  
154와 15823은 인기동영상이 된 이유를 모르겠습니다.   ",55b8ed68,0.8235294117647058
37985,410285582f4f7e,8514dfac,"**Preprocessing Data 
**

*Normalizing Numerical Features*

Only numerical data of age, year, date, day is normalized",d026266b,0.8235294117647058
37988,dc0b0e1cb46c6f,845a1ef0,"<a id='4'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;"">4. World Summary</p>",47b17a7b,0.8235294117647058
37994,4fa553c2b837d4,dcad8b95,"# XGBoost
XGBoost is an implementation of the Gradient Boosted Decision Trees algorithm.

![](https://i.imgur.com/e7MIgXk.png)
We go through cycles that repeatedly builds new models and combines them into an ensemble model. We start the cycle by calculating the errors for each observation in the dataset. We then build a new model to predict those. We add predictions from this error-predicting model to the ""ensemble of models.""

To make a prediction, we add the predictions from all previous models. We can use these predictions to calculate new errors, build the next model, and add it to the ensemble.

There's one piece outside that cycle. We need some base prediction to start the cycle. In practice, the initial predictions can be pretty naive. Even if it's predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.

This process may sound complicated, but the code to use it is straightforward. We'll fill in some additional explanatory details in the model tuning section below.",c65a23e9,0.8235294117647058
37995,fdc3afd309b850,00ebdca4,"<a id=""c""></a>
## 8.6 Correlation Plot",966bde38,0.8240740740740741
37997,ab6da5994949a3,e4596756,## Decision Tree Training Results,fae6b91d,0.8240740740740741
37998,5f4ae633cfd090,6a0a56ea,Almost all of _rank variables except better_rank are empty. I'll keep it and remove the rest,a30a16e2,0.8241758241758241
37999,62037c5832129c,2eed10d7,## Plotting a receiver operating characteristic (ROC) Curve,61474350,0.8243243243243243
38003,e4525eb0c96f28,725fed8e,"This model might look a bit weird at first glace, but we must remember that this model is no longer a singular straight line, and that it is now a model with various interactions of multiple categorical variables. Different as it may seem, the model still looks to be around the same place as before. Therefore, we can pretty much guess that this model is not that much of an improvement from the previous one. But, let's look at our residuals just to be sure.",2093a1f1,0.8243243243243243
38004,ee23a565163388,1e86a127,The accuracy on the training set is high and testing dataset is pretty low.,88aacbc4,0.8244274809160306
38009,5ce12be6e7b90e,9361a66d,Count the number of spaces in the sentence below.,c0ab62dd,0.8245614035087719
38012,9e27af2600925c,eb7cb29b,"**Interpretation**:
You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. ",9b556435,0.8245614035087719
38014,54004b32784b68,9bca027e,As we can see some old houses are more expensive than new houses.,27213ca9,0.8245614035087719
38017,c2a9f2fb3e1594,e9358bb8,"<a id=""ch11""></a>
# Step 6: Validate and Implement
The next step is to prepare for submission using the validation data. ",53411c04,0.8245614035087719
38019,e03eb63c1f725d,e0643ecb,"<a id=""15""></a>
<font size=""+2"" color=""blue""><b>LSTM</b> </font><br>",e204b7e3,0.8245614035087719
38025,3c2033cc99c12c,76dfa044,#### The Learning Curve of two models,dfa22a54,0.8248175182481752
38026,c80939c7c626cf,14094e9d,# 10 K- Nearest Neighbours,b9ac31e2,0.8248175182481752
38032,9f0ccf5b9e8f03,7fd2d718,Below code from Thor God of Thunder,66691203,0.825
38036,62487bcd70b199,3ddf5923,## <a id='8.2.1.4.'>8.2.1.4. default AdaBoostClassifier</a>,f6ae50af,0.825
38038,5ffe6aa38958a1,43868cd9,"## 5.2 Confusion Matrix

Confusion matrix gives an indication of which class is being misclassified more. 
The matrix below suggests that there is some imbalance in the data",11f5412e,0.825
38043,fdbbd573ba31c2,420b3cd6,## ElasticNet,f7c28d74,0.825
38045,eda49464dd6d1b,0e7705a4,"# Weights and Biases

For those curious, this is how to extract weights and biases from the DNN model.  The neural network is a black box, and works in mysterious ways.  The weights do not clearly identify which variables are the most important.  It would take a great deal more investigation to learn what specific job each neuron is doing.",8421f81f,0.8251748251748252
38047,98a6794067932a,bfa090a6,"Le code ci-dessous permet de localiser les différents clients de l'entreprise sur une carte des États-Unis. Avec l'aide de la fonction Folium, le code sélectionne la latitude et la longitude de chacun des clients présents dans le dataframe 'adresse' et il indique sur une carte du monde avec l'aide d'un marqueur l'emplacement exact de ces clients. Ensuite, le code permet d'afficher la carte qui vient d'être créée. Cette analyse pourra être utilisée par les dirigeants de l'entreprise afin de vérifier l'entendu du territoire couvert par les activités de l'entreprise.",08600fe2,0.8252427184466019
38053,06ecf7a304c309,4f59e614,각 모델을 확인하면 다음과 같습니다.,714de627,0.8253968253968254
38054,726833f92fb87a,2b041045,The great majority of customers have missing values for poutcome (over 7000) meaning that most of the customers have not been previously contacted. We will encode the feature by one hot encoding and drop the rows with 'poutcome='other' since it is not clear what it means.,7dc5e1b6,0.825503355704698
38059,2ada0305b68956,db4e37f4,### 141. Palette = 'rainbow',133e26f4,0.8257142857142857
38060,0e2a23fbe41ca9,b3894b17,"### 10. Correlation in lag columns

",64e4762c,0.8260869565217391
38061,59236ba162ab7b,ff59923c,4. Train a model,ace5b0ef,0.8260869565217391
38062,3319c5c562f607,4ed33dd6,# Visualize the Most Common words in review,f298250a,0.8260869565217391
38064,77f958b3f41a70,34a808c3,# Community measures,2ad9bb69,0.8260869565217391
38085,b01ee6cb674fa3,00189b4a,"Australia has 3 companies that launched from them
- AMBA
- CECLES
- RAE 

lets do a search for some info on them!

RAE stands for Royal Australian Engineers

AMBA has no association with rockets, though the satelite, WRESAT, was the first australian sat

CECLES remains a mistery, still no info on it, even its satelites, STV's 1,2,3

So, to conclude, the three companies will be set as australian's public agencies, and a new search onto CECLES will be conducted due to an association with France",a8ffd35e,0.8260869565217391
38096,57bad3860b0fa4,249f447a,"Basically model 1, however I put in some batch normalizations and some drop outs to help improve it, usually gives an accuracy of about 93.5%.",05138a5e,0.8260869565217391
38097,e9b9663777db82,9d2a2f39,#### LinearRegression without Feauture Selection Method,648e8507,0.8264462809917356
38099,087e21401d7dfc,d70140df,# Plot Confusion Matrix.,42000489,0.826530612244898
38102,d6ddbe57f59cf7,e6fae051,# Stripplot (used to plot data over categorical axis),504a3cda,0.8266666666666667
38104,37e461081e47c5,5cb5da88,# Model 6: XGBoost,b3e6549e,0.8266666666666667
38108,e5dd725b8fa422,9c8e23fd,# Validating the predictions,14675d8b,0.8266666666666667
38120,ba4b3bd184acbb,5b77ee9c,And output the top 5.,0f5de724,0.8270676691729323
38127,fb5c6021d127ef,27898b91,Write your query below:,dd05cbd3,0.8275862068965517
38129,6903d3f38c6a66,0ecbd44c,"### Sequential Colormap

This palette is appropriate for variables with numbers or sorted values.

Used a lot in comparison of figures. Especially effective for expressing density. Take advantage of map graphs for better visualization.

Similar to diverging, but with a slightly different part because each endpoint is a color criterion, not the median. It usually indicates that light values are dark on dark backgrounds and dark values on light backgrounds.
",6067ce5e,0.8275862068965517
38131,cd10f3afd970b3,65d33037,Let's take a quick look at what the data looks like:,2db3c8e4,0.8275862068965517
38135,45921c50ac56fa,18e79208,"There are a few parameters:
1. Number of Topics - Based on your use case, you can change the number of topics.
2. Passes - Number of passes you want to the algorithm to run for. ",465973eb,0.8275862068965517
38138,84127ade6fde87,cbb030b8,One example is word2vec: https://code.google.com/archive/p/word2vec.,f55d05b6,0.8275862068965517
38142,9535bb04ae042c,56fd8138,# **Step.2 Create a Streamlit Web App and Deploy it on Heroku**,165b6fae,0.8275862068965517
38143,656185a18260be,f87cb676,"# Cutout - Random Masking

In images, we sometimes cut out a fragment of an image to introduce variety and increase robustness. We can do the same in NLP by randomly masking some tokens. I think this should be quite helpful if we train for many epochs with the same small dataset. Here we already have quite a lot of variation and little repetition, so let's keep the cutout rate small at 3%. ",0318cab5,0.8275862068965517
38148,5ea840754577e3,fd9372a3,"## Feature: Name

The feature Name is pretty much useless, but we can use the salutations to have a better picture and maybe create a new feature. But first the salutations has to be extracted and analyzed.",9cf9b73f,0.8275862068965517
38158,98f2708375307b,95583dca,"This is the optimal RMSE score that we can get with only a constant prediction and using all data available.
We therefore call it the best 'Naive baseline'
A model should at least perform better than this RMSE score.",9ccac242,0.8275862068965517
38159,6a1d04e8153df3,65e57324,"**Observation(s):**
- Inside the voilin figure there is boxplot where a white spot denote 50 percentile of data.
- The conclusion is the same as boxplot .
- It's easy to visualize.",38572b05,0.8275862068965517
38169,979f1e99f1b309,4f4f899b,## Finally using VotingRegressor Model,d1bfebbf,0.8278688524590164
38172,ff3a8ce61fab6a,b7b8d6de,"1. <hr>
1. ### Exampel 2 ",9afe1654,0.828125
38173,0932046e1f485d,8b042f0a,"Spliting the dataset between the Positive, Neutral and Negative sentiments.",218cc7a3,0.828125
38178,55a5e31d03df9f,ee48b46e,"We now made some predictions on a totally new set of images, as you can see it isn't perfect yet however we could precisely predict some new images like R2-D2, Darth-Vader, Harry Potter.

For the other images it appears that they're getting confused, in the case of Obi-wan the lightsaber and clothes are the same to Mace windu and also Cara Dune could be the % of black and some blue related to Emperor Palpatine.

As we have seen before there is much room to improve to our current 'best' model and we could even input more variety of images to generalize even more the set of images we have.",06dce00f,0.8285714285714286
38182,fe6750354fb64f,5ee544b3,# 4. Facebook's Prophet Model,271741f0,0.8285714285714286
38191,f50dc95483c98f,2cfa01b0,### **Accuracy of the model**,cd9e9621,0.8285714285714286
38194,7454fdc444df16,876c75c7,### Apply PCA to the pixel dimension of image array,a7818ef5,0.8285714285714286
38198,2730840089c8eb,38f30963,The `in` operator tells us whether something is a key in the dictionary,34d27dac,0.8285714285714286
38207,72e098fe5b2a04,bb12492c,# electra nopt 462,5399eebd,0.8285714285714286
38209,9c044fa3072552,43f3ee2d,"Now, let's look at a distribution based on the location of accidents (latitude and longitude).",1362842e,0.8285714285714286
38211,738bfced935b69,1cb374d1,"The larger the engine size, the higher the price of the car, in addition to the year of manufacture.",2d3c592d,0.8287671232876712
38212,fdc9f4863744b1,14520809,## Data Preperation,b4529365,0.8287671232876712
38214,49ee86d074de69,8f8e86da,* log(odds) = intercept + b1x1 + b2x2 + b3x3 + .... + b14x14,71ccc6d3,0.8290598290598291
38218,8d70dcae7f40a3,38555f09,"#### *Công thức*
<math xmlns=""http://www.w3.org/1998/Math/MathML"" display=""block"">
  <mtext>AP</mtext>
  <mo>=</mo>
  <munder>
    <mo data-mjx-texclass=""OP"">&#x2211;</mo>
    <mi>n</mi>
  </munder>
  <mo stretchy=""false"">(</mo>
  <msub>
    <mi>R</mi>
    <mi>n</mi>
  </msub>
  <mo>&#x2212;</mo>
  <msub>
    <mi>R</mi>
    <mrow data-mjx-texclass=""ORD"">
      <mi>n</mi>
      <mo>&#x2212;</mo>
      <mn>1</mn>
    </mrow>
  </msub>
  <mo stretchy=""false"">)</mo>
  <msub>
    <mi>P</mi>
    <mi>n</mi>
  </msub>
</math>",472c71ce,0.8292682926829268
38221,8cefb86a675e5d,82c9cd83,**Perform Prediction using Decision Tree Regressor**,79f9e69b,0.8292682926829268
38225,47b2c9be5e31cb,27094033,Distribution graphs (histogram/bar graph) of sampled columns:,7d4afe56,0.8292682926829268
38228,0b01138ad120fc,83273f1d,1772 values - I'll split at 1400 = ~80%,0b4b72e6,0.8292682926829268
38231,0e09587faffa8f,e2e4dce6,The general violation location number **19** was issued the maximum number of tickets,0d563d61,0.8292682926829268
38241,3f25b363afec54,3d6757a6,For ```second_camp``` Health_Score column is difined in a different way let's first replace the name of column.,bbdaae25,0.8297872340425532
38246,5f674175839b32,137c65b0,"# Answering some questions.





",53a2e343,0.8297872340425532
38249,4cd25e50c7e007,509f5e6e,# Residual Analysis of the train data,ceb0c525,0.83
38250,917957c6c4065f,ff6a2075,### 2.11. category 별 동영상의 수  ,55b8ed68,0.8300653594771242
38252,23df07a474aaae,3dfe0741,**Training Model**,0ea40276,0.8301886792452831
38255,510b8303776bb6,3a6af4be,## Checking and removal for null values,18080db8,0.8301886792452831
38258,a070fd03ae8ed2,76415505,## 7.2 Расчет бинарного вектора прогноза (targets_pred) из вероятностного прогноза (pred_proba),c0ec4138,0.8301886792452831
38261,30fdc4a6e3c1db,4faf8815,"If we closely look at the sales for 2012 we see that people** prefer buying on the weekends(Saturdays preferably) *before the Events rather on the days of the events***. So we don't see a increase in the sales on the days of the events. But the increase in sales on the weekends before that can be attributed to that event. We see some exception though like Labour Day which is a Monday, we still see a peak on that day. Another one is Thanksgiving which is on a Thurday but we see a peak on the day before which is a wednesday",6111ddee,0.8304093567251462
38268,a077820f7ab459,e07c6e21,### Predict,05a43104,0.8305084745762712
38271,a44368590e878a,5d28adf3,### Visit,77743ba8,0.8305084745762712
38280,c115e287523aab,27cd4382,"# Train Model
* Cross-Validation: 5 fold
* **WandB** dashboard is shown end of the each fold. So we don't need to plot anything. We can select best model from here.",feb1288b,0.8307692307692308
38283,a4f8ad33c823c5,663a1737,"In the above plot on the left, we can observe the relationship between the mean and the range of the glucose concentration measured during the first 24 hours of Diabetes and non-diabetes patients. The range of glucose concentration observed for diabetes patients sees a wider distribution of values and majority of diabetic patients' glucose results sees a fluctuation of 50-80 mmol/L and a mean of close to 150 mmol/L is observed among diabetic patients.",fcd48307,0.8307692307692308
38285,09751c520b0616,b3c73d78,### Fitting modeling algorithm,a4d0c7e9,0.8307692307692308
38286,3cb96bd8eb364b,e0c4ea37,### Model Training,3157af7e,0.8307692307692308
38288,f2f2db16a2f86c,1f44c4bc,This is the accuracy of the model in predicting the housing prices given the parameters. ,ffc6a115,0.8307692307692308
38289,03048e86a6d806,82631717,### ML Algorithms Regularly Used,1285c231,0.8307692307692308
38292,7f74a04ae75792,c80c7eee,"### What is the distribution of the inactive number of months since last purchase?
",d01e91da,0.8308823529411765
38297,631cd434fc3aa2,e5bff94d,#### Create dummy variables,2b74febb,0.8309859154929577
38306,75adb7945ef9bd,e25524e3,We then pick up the 1133 selected features to do Grid Search CV to find optimal hyperparameters,785c5095,0.8311688311688312
38310,835a7b4e660d23,981b414b,### Pivoting,53bc7a6e,0.8313253012048193
38311,2ada0305b68956,d8890bb3,### 142. Palette = 'rainbow_r',133e26f4,0.8314285714285714
38312,312135b445bd23,65a4d91b,Most of the results looks informative and relevant to the research topic.,8ced381f,0.8314606741573034
38315,04ff2af52f147b,d5808457,"Next, we use one hot encoding on our remaining categorical variables.",d5f37be9,0.8314606741573034
38320,ac1abfe1dfe815,adad9628,---,6529dbcb,0.831858407079646
38321,a5a419dc7245b0,d2a5cc5f,"**1.Weights upated via batch learning so batch size needs to be specified (no rule of thumb)<br>
2.When the whole training set is passed throught the ANN, that makes an epoch. <br>
3.Epoch size needs to be specified (no rule of thumb)**",4279726e,0.831858407079646
38324,4ae6a182abac64,4f48fc3d,* **Confusion Matrix**,418676c5,0.8319327731092437
38325,ee23a565163388,fc7566f0,## **Support Vector Classifier**,88aacbc4,0.8320610687022901
38326,3c2033cc99c12c,93ba0724,**Note:** *In this part I will compare the training score and the validation score during the process and trying to seek any correlation between the two curves.*,dfa22a54,0.8321167883211679
38332,712198370d5521,1e24ba2e,Let us next explore how did our campaigns do in the past.,5882e04c,0.8333333333333334
38333,02773bdc5d3c7a,e880b0ce,"*7. The accuracy, precision, recall and f1 score were compared for all the above methods for 2 algorithms: a) Logistic Regression b) Decision Tree*",86245f35,0.8333333333333334
38335,b547f0f38f7744,5e6bd6b7,### Create Params ,b6ba66b3,0.8333333333333334
38345,898d18d501f68d,fc3f7e74,work with Wilderness variable,d8bdea2d,0.8333333333333334
38349,d52eea97c2aee5,8a10aae2,"# Calculate P, R, f1",c9fa2014,0.8333333333333334
38352,dd3721cb49c1fd,dc9744cb,"<a id='11'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1e88e5;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center""><b>11. Printing the summary</b></h1>
  </div>
</div>",1a53fdd9,0.8333333333333334
38353,d4cdedc1cd6d7a,5523a8d9,**Seasonal Plot Draw it**,84c60e71,0.8333333333333334
38358,f7436bc492474c,b2084d42,Fit a model for one dependent at a time:,328fd235,0.8333333333333334
38360,b10bd75889dad9,dffb7908,### Predictions with Random Cut-Off,ee00ceee,0.8333333333333334
38362,de577c910a687d,fb19a381,"# Make Submission #

Our predictions are binary 0 and 1, but you're allowed to submit probabilities instead. In scikit-learn, you would use the `predict_proba` method instead of `predict`.",c3ebadbc,0.8333333333333334
38364,b3e0b7e9ff6849,e34eff7b,"# 5. Train Gamma Gamma Model (Expected average spending)

Now, we will train the gamma-gamma model to estimate average spending for future transactional events. Then, it will be used for calculating the customer lifetime value. ",f6e4bb0d,0.8333333333333334
38368,1084376bc4897c,08ee815f,# 4.5 XGBoost,1b598487,0.8333333333333334
38370,a76e0e8770b7a0,0220004a,No Iraq and no Afganistan it is interesting.,02863d3b,0.8333333333333334
38379,fbb1f9d3818830,c3c8e4fe,"# Results

As we know that a CNN model performs heirarchial feature extraction i.e. the earlier layers extract local features & the deeper layers use those local features to extract abstract concepts in a way that helps the model to nail the downstream task. 

Since we make use of conv_pw_2_relu layer therefore I expected the model to extract some fairly low level features. For easy deciphering of results, I thought to keep number of clusters to 2.

1. In the first example the MobilNet was able to differentiate between Sherlock's suit & hand. Since it had only 2 clusters the model felt keeping suit & background as the 2 most seperate clusters. (mainly I feel pixel intensity i.e. dark (suit, hair, flowers in background etc.) & light (Cumberbatch's face, yellow walls, the glass etc.)).

2. It is a tricky picture but the again the model decided to use pixel intensity as the differentiating factor i.e dark (suit, the floral background on Mrs Hudson's wall & the dark regions in skull) & light(shirt, face, yellow part of the wall, white part of skull etc.).

3. Again the same thing : light ( ""I am sherlocked"", faces) & dark (sherlock's back).

4. This is the result I was looking for, the model was able to differentiate between the background vs Sherlock & Moriarity.

5. Again clothes were differentiated from lighter background.


*Pixel Intensity quite an important feature for MobileNetV1's earlier layer*",c7027f86,0.8333333333333334
38392,fdc3afd309b850,2b3a045f,"<a id=""M""></a>
# 9 Modeling",966bde38,0.8333333333333334
38393,7e89d387feb9f5,3e4edfab,### Тренируем и тестируем модель,989e3a1b,0.8333333333333334
38397,a3ae04b78e45b5,8bceadf5,**SWARM PLOT**,4195da8b,0.8333333333333334
38402,4d91e84c564cbe,516265ef,"## Tuples

Tuples are almost exactly the same as lists. They differ in just two ways.

**1:** The syntax for creating them uses parentheses instead of square brackets",355a43e3,0.8333333333333334
38403,9ad9a97e628bfa,ceedeaf7,**Categorical Data 를 다듬고 Label encoding 하기**,0a7e1136,0.8333333333333334
38407,5f544a32fb2ce9,2b41c4ab,# Review Output,616f33af,0.8333333333333334
38408,ea2763c0f6c6a0,4076ecfb,"# Wordcloud - year wise

*""Cyber threat actors are states, groups, or individuals who, with malicious intent, aim to take advantage of vulnerabilities, low cyber security awareness, or technological developments to gain unauthorized access to information systems in order to access or otherwise affect victims' data, devices, systems, and networks""*",e5812ac1,0.8333333333333334
38410,a69d41047fdd3e,8d108df5,"Thinking about the question above, there are a few columns that appear to have geographic data. Look at a few values (with the `list_rows()` command) to see if you can determine their relationship.  Two columns will still be hard to interpret. But it should be obvious how the `location` column relates to `latitude` and `longitude`.",b1f28647,0.8333333333333334
38418,245c89d02f3f5f,5d1329a6,## NoFrameskip-v0,61a1eacd,0.8333333333333334
38419,98ea617d18c9cc,cd5957a5,# Plotting Loss and Accuracy Graphs,e6316d11,0.8333333333333334
38425,9085cba2265204,70cad404,"### To be cont.
## 2. Logistic Regression
## 3. K Nearest Neighbor
## 4. Gaussian Naive Bayes
## 5. Stochastic Gradient Descent (SGD)
## 6. Linear Support Vector Machine
## 7. Decision Tree
## 8. Perceptron",de766eb3,0.8333333333333334
38429,2f0f808765fc67,2dea32ae,The best result given by Random Forest across A-H.,fd1f6494,0.8333333333333334
38431,87e94f864d74be,95e0fcdc,> It is evident that majority of the movies have duration ranging from 85 min to 120 min.,294bfe9f,0.8333333333333334
38432,c91c137284976f,cb434c7f,# 4. Final model,c6888c0a,0.8333333333333334
38440,2d40f383473fa4,c6369c66,"We can see that `Family_Name` give a good gain to model prediction, followed by `Honorific` and `Sex`. <br>
Let's re-train with the 6 top features and see if we could got any improvements.",1da1eff0,0.8333333333333334
38445,916ccf243827f1,44bdf07e,## 13. Split Dataset in Batches,5147f4d2,0.8333333333333334
38446,2a377ced98d67a,77ee7390,## 7. Test and Evaluate,262231a8,0.8333333333333334
38455,bc058fe14d3d1b,0669aafb,## 2. Modeling,d0273670,0.8333333333333334
38460,999258a81ba32a,7d14e067,"# Test prediction model
We use `random_state=None` in order to add a degree of chance to this, just like the real lottery. Every run should be different!",48cd3d21,0.8333333333333334
38464,7ff97196d5db8c,637d706b,# Predicting For the New Data,3d82be43,0.8333333333333334
38465,49ac6594c8f5cf,8ebed57c,Installing Imblearn ,6f19f28a,0.8333333333333334
38468,166a62ebb4fc3a,f28fe92c,Predict on the top of test data,db48a079,0.8333333333333334
38472,6b2776f151ed9c,46a3816a,# Values of k ,40406d5c,0.8333333333333334
38474,31268b33de97b5,22fa3702,# Box Plot too detect outliers ,1e6f7d14,0.8333333333333334
38475,864302b10e7730,c8883d67,# 3. RegPlot,e9dd1d2d,0.8333333333333334
38482,28a1ff0f223da9,befab3e8,This graph is showing the most common areas of researchs in Pakistan.,c945b27d,0.8333333333333334
38486,b3681fd423741d,35e91a98,# 6. Apakah terdapat outlier pada kolom Kilometers_Driven? Sertakan argumen yang mendukung jawaban.,1aec06ce,0.8333333333333334
38488,7ba63a2d9abb58,15c4e249,<h2>Covid-19 Trend in top 5 countries by cases</h2>,821a261f,0.8333333333333334
38489,95d896e75f9a50,9543ace5,"Again this makes sense, seems like `feature_0 = 1` is generally a bit higher than the signal for `feature_0 = -1`.

### Pattern 3 - Various strength predictors",2721b6f5,0.8333333333333334
38496,6998861ff6ff01,3f3f6aef,"And that's it for today! If you have any questions, be sure to post them in the comments below or [on the forums](https://www.kaggle.com/questions-and-answers). 

Remember that your notebook is private by default, and in order to share it with other people or ask for help with it, you'll need to make it public. First, you'll need to save a version of your notebook that shows your current work by hitting the ""Commit & Run"" button. (Your work is saved automatically, but versioning your work lets you go back and look at what it was like at the point you saved it. It also lets you share a nice compiled notebook instead of just the raw code.) Then, once your notebook is finished running, you can go to the Settings tab in the panel to the left (you may have to expand it by hitting the [<] button next to the ""Commit & Run"" button) and setting the ""Visibility"" dropdown to ""Public"".

# More practice!
___

If you're interested in graphing time series, [check out this Learn tutorial](https://www.kaggle.com/residentmario/time-series-plotting-optional).

You can also look into passing columns that you know have dates in them  the `parse_dates` argument in `read_csv`. (The documention [is here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html).) Do note that this method can be very slow, but depending on your needs it may sometimes be handy to use.

For an extra challenge, you can try try parsing the column `Last Known Eruption` from the `volcanos` dataframe. This column contains a mixture of text (""Unknown"") and years both before the common era (BCE, also known as BC) and in the common era (CE, also known as AD).",ea9e72cf,0.8333333333333334
38497,cb4ad8ed4cb300,5eb5470a,# 7. Some Thoughts about Goodness and Improvement,7c0f3236,0.8333333333333334
38504,b01ee6cb674fa3,7a9b8395,"The entry 3299, was an europa 2 test flight, so I decided to wikisearch it and found the page 

[https://en.wikipedia.org/wiki/Europa_(rocket)](http://)

Within this page there is a list of rocket launches by the European Launch Development Organization, ELDO. 
Further looking for the list, there is its launch site and they all match with the list above together with the rocket type. So, I decided to check the ELDO page, in frech, and bingo, there it is!

**Centre européen pour la construction de lanceurs d'engins spatiaux ou CECLES **

> The organization consisted of Belgium, Britain, France, Germany, Italy, and the Netherlands. Australia was an associate member of the organization. Initially, the launch site was in Woomera, Australia, but was later moved to the French site Kourou, in French Guiana. The program was created to replace the Blue Streak Missile Program after its cancellation in 1960. In 1974, after an unsuccessful satellite launch, the program was merged with the European Space Research Organisation to form the European Space Agency.

CECLES, an european multinational organization --> a public company

Because the acronim is in french, I will set the country as France, even though it was multinational",a8ffd35e,0.8333333333333334
38505,52cfd66e9ec908,c3817601,## Part 1: Catalyst training,c74adcdf,0.8333333333333334
38511,df2a7968c08ee4,d43d7e0b,"### Data Preparation for Prediction

Now that we have fully trained the model we need to instantiate the Test Images and make predictions on these images. ",a2ba0a72,0.8333333333333334
38512,a4a494c667c673,9437e41b,# Results,397d12f8,0.8333333333333334
38515,ac04ba639d1c93,dcfdaeda,"<a id=""id5""></a> <br> 
# **5. Model** 
",748059d5,0.8333333333333334
38517,1fac5edd4063ba,07c8563e,"#Deepika Kurup
![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSFo3gl706FixUMS95VMcbMsv3UuJ_ObKMkYw&usqp=CAU)news.wef.org",04bc01e0,0.8333333333333334
38529,71c3c1eab0377d,209c435e,**Import H2O's GBM **,52b4e360,0.8347826086956521
38534,063a35f644e3c5,0a0ada1b,## KNeighbors Classifier,1c30fb0a,0.8350515463917526
38540,869a39a3d4dea2,2616b320,"From histogram above, from 200-256 its droppping and it shows that white pixels are minimum and more dark pixels range from 125 - 150 and 175 - 190",9020daf8,0.8352941176470589
38542,513ce405d7f6a3,57627be9,"**This notebook shows how to implement Machine learning and deep learning models to do sentiment analysis even without any data preprocessing and hyperparameter tuning the accuracies of the models are reasonably good.**
",8461e086,0.8352941176470589
38550,1eb62c5782f2d7,9cf1142d,"### Merubah $z$-Score menjadi $x$-Value

$$ 
\begin{aligned}
z &= \frac{x - \mu}{\sigma} \\
z \sigma &= x - \mu \\
\mu + z \sigma &= x \\
x &= \mu + z \sigma
\end{aligned}
$$",bb69f147,0.8356164383561644
38551,a4aa36df07fd53,45dbe589,"## One Hot Encoding

Nah dari hasil eksplorasi tadi jelas terlihat kalau hampir semua data merupakan data kategori. Kebanyakan model tidak dapat memproses variabel dalam bentuk kategori. Salah satu bentuk feature engineering yang paling umum adalah mengkonversinya kedalam bentuk biner 1 dan 0",d2f42b6d,0.835820895522388
38552,21413205980558,cf211338,# 5.1  Logistic Regression Model（逻辑回归模型）,84197de0,0.835820895522388
38561,2105f2c5132866,ad43cdfa,# Model Tuned Performance,bfe8023d,0.8360655737704918
38562,601e18072783b4,d79099c9,"- As expected, Indian movies have a mean duration higher than US movies
- Indian movies seem to have a mean duration around 140 min
- While US movies have a mean duration of around 90 min",36b2b1fa,0.8360655737704918
38563,918040fad252ec,76423889,Membuat variabel untuk memulai prediksi,966fcd8f,0.8360655737704918
38565,5ce12be6e7b90e,c0958c68,"## range

Sometimes we want to loop over consecutive numbers.

This is accomplished using the `range` function.

`range` accepts one, two, or three arguments: the bottom and upper limits and the step size.  
The bottom limit can be omitted - the default is zero - and the step can be omitted, too - the default is one.
The upper limit is __not__ included.",c0ab62dd,0.8362573099415205
38567,016abae0483764,20b3b7c4,"Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.",bc9f289b,0.8363636363636363
38571,44f6a002ecd033,fc9cec93,"This model did better than the decision tree, but still not as good as the logistic regression. We will try one more method which is XGBoost.",70bbe106,0.8365384615384616
38580,f35bf4df70d310,1db2564b,## Result Analysis,10bb859a,0.8367346938775511
38582,0e7ac281a19feb,9c3ed66b,## Evaluation,5b84d10f,0.8367346938775511
38585,f1e162ddd14f11,8130056b,this RandomizedSearchCV takes input as the parameters that we have specified and will automatically select the best parameters for us.,cdb2e771,0.8367346938775511
38591,2ada0305b68956,8c38188f,### 143. Palette = 'rocket',133e26f4,0.8371428571428572
38592,1660daf8867980,80b2cfde,**Implementation**,42d7cffc,0.8372093023255814
38594,743ae010f5e875,ad93b46d,## Aggregate final predictions and write submission file,02c54445,0.8372093023255814
38597,22ba3a8149c2f1,2523dd6b,# 3 stacking,19c82be5,0.8372093023255814
38603,e19e307b3fd188,b6805e3f,#### Fit all ML models and select best,2173955b,0.8373983739837398
38606,fdbbd573ba31c2,f37eaa9b,## SVR,f7c28d74,0.8375
38611,ccabe7a86825ce,3a42fd37,**Use special encoding for cyclic**,d766cbf9,0.8378378378378378
38618,62037c5832129c,5fd8255f,"* AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. 
* ROC is a probability curve and AUC (Area Under the Curve) represents degree or measure of separability. 
* It tells how much model is capable of distinguishing between classes. 
* Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. 
* By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.",61474350,0.8378378378378378
38621,63b44c85e32c1f,f1222398,"elements 2,3 which are repeated twice are seen only once. Thus in a set each element is distinct.",fb9b9562,0.8378378378378378
38625,bdf23d2d396916,45ce588e,## SVM - Modelling and Model Performance,b0e45a49,0.8378378378378378
38630,55a5e31d03df9f,03aa940a,"## End Notes

We completed the purpose of this notebook to teach about the very basics and built some intuiton around deep learning and pass very quickly through some of the most important contents to get started. We trained a set of models starting from the very elemental structure from an MLP to transfer learning, what are we missing then?

* Finally fixing the overfiting, we discussed about ways to reduce the overfit but we did not walk through them except from the transfer learning. 
* Error analysis on the cases that the model is not perfoming well, this will help us guide what efforts need to be make.

This was my effort to share my learnings so that everyone can benefit from it, if you like the content don't doubt to take Daniel Bourcke course he has the first 14 hours for free, check it [here](https://youtu.be/tpCFfeUEGs8).

If this kernel receives love,I plan to keep on bringing the best insights around Deep Learning with TF but this time for NLP and Time-Series. 

<span style=""color:Green"">Please consider upvote it if my efforts help you, it motivates me to write more notebooks explaining future content</span>",06dce00f,0.8380952380952381
38633,bbaa07ad21cf4e,29eb6624,### BertTokenizer and Encoding the Data,3ab6b254,0.8380952380952381
38634,04bac111ffbe9c,7e4cf1bf,## Confusion matrix and accuracy score.,82576b17,0.8380952380952381
38636,9cec5ddf8b6f49,fa34ba1e,#### Visualize individual hyperparameters as slice plot. ,d39fc8e7,0.8382352941176471
38637,99821bc6a45be6,84f04ac3,"The architecture used here is inspired by the paper [**COVID-CXNET: Detecting COVID-19 in Frontal Chest X-Ray Images using Deep Learning**](https://arxiv.org/pdf/2006.13807v2.pdf), which is directly related to our task. 

An overview of this model is visualized below:
![](https://d3i71xaburhd42.cloudfront.net/18a690f7622f41638b42d4f07dabbd0fd45b784a/6-Figure6-1.png)

The base of this model is DenseNet121, which is also pretrained on ImageNet weights. To make this model adapt to our task, the last 6 layers of DenseNet are unlocked to be trained.

The COVID-CXNET, paper discusses advanced data augmentation techniques such as the Bi-histogram Equilization with Adaptive Sigmoid Function (BEASF) to increase the contrast of the X-rays. However, this is complex and for this application we will stick with the augmentations suggested in the VGG16 model. This also allows us to use the reuse the data generator defined before. 
",b9d59346,0.8382352941176471
38641,eb0ecd6bebeb15,d6f501d9,sepal.length'in maksimum değerini yazdıralım.,d7b93a60,0.8382352941176471
38642,c65a65d4041018,932a767e,"Postgress is really popular in Russia.

By the way, I'm surpised that Teradata was not  in the main options.",824fb229,0.8382352941176471
38645,dbd96dd275dc60,ad1a2c40,"# we can find how the columns differ using sets
set(X_train.columns) - set(df_test.columns)",1ed493a8,0.8383838383838383
38646,d07915a6e6992e,e4e52b23,"# Model Evaluation
![model.png](attachment:model.png)",2b912140,0.8384615384615385
38647,09751c520b0616,c4b4d35f,<b> We are using RandomForest algorithm to model and making prediction,a4d0c7e9,0.8384615384615385
38649,a4f8ad33c823c5,ed8433d4,### Final set of variables,fcd48307,0.8384615384615385
38659,0caaec057f7184,ed4c9fa7,monthly total sales of the item in each shop and all shops,b875533e,0.8387096774193549
38661,921fff7d3040db,0ae1c830,# 8. Compare the models,5f36ced9,0.8387096774193549
38662,ad26c020235dfc,7d5420d1,Scale data:,bf766e48,0.8387096774193549
38675,f15eac23fbcc9d,0fecf917,"0.955 is not bad. Though not optimul, but consider how much efforts we make here, it's already pretty good results. ",ea46d8af,0.8387096774193549
38676,396bc36edb95d3,7ea86a15,### Model Evaluation,965e4f8f,0.8388888888888889
38679,1294fb4c86f993,8fc5229e,Create a new column indicates total of <b>registered guns per 1000 person of population</b> for each state as an indication of guns per capita,4471e513,0.8389830508474576
38683,ee9ddc756b2d4a,9d63b17e,> I vettori features che utilizza la regressione logistica sono vettori a due componenti: ad ogni paziente (rappresentato dall'immagine della sua risonanza magnetica) corrisponde un vettore di due predizioni (la probabilità di avere il tumore per i due modelli),e367eab3,0.8390804597701149
38687,f13534449a3750,0f0a7e73,To obtain the second dataset (100% Ship + 0% No Ship) we need to remove all the images without ships from the original dataset.,8b7f3332,0.8392857142857143
38694,c80939c7c626cf,cab19a96,# Above we got the acuracy,b9ac31e2,0.8394160583941606
38696,4883314a96dc34,dcc79ac6,#### *EXTRA: deep error check on individual model*,50d36836,0.8395061728395061
38697,510b8303776bb6,a51428f3,### Checking for null values ,18080db8,0.839622641509434
38698,43e60eb1362f5c,e7ad76d6,# Model Analysis,87934234,0.839622641509434
38699,ee23a565163388,96b9338c,The SVM classifier forms a hyperplane to differentiate between different target labels and predict the outcomes.,88aacbc4,0.8396946564885496
38701,cee088a6840708,ee10bac6,# Step 11 -> Train the model,55463e1c,0.84
38702,8854f72e7e9be0,d7d4ad41,**Prediction For test data**,2a1031b7,0.84
38703,83df814455f06c,6c6bcf9a,### Check for overfitting and underfitting,c9cff71a,0.84
38706,7e1da639035ac5,9e107507,# <a id='15'>15. Trust analysis</a>,120b6c23,0.84
38712,cb570c7b7f0501,4760134f,"now i am interested in the same patients when they book the second appiontment.

you may have to pay attention to these steps:

1- i will go back the dataframe for people have more than one appiontments.

2- i will join this data frame with the '5230' patients ids for people didn't attend their first appiontment

3- i will Keep the the duplicated values.

4- i will keep the first oppiontment for the duplicated values and it will be represented the second appiontmets becasue the data frame is sorted by appiontment_id
",a200a0ec,0.84
38722,bbad077c274022,4416cf2b,**Let's see this variance in hourly basis.**,3c2e3dea,0.84
38723,916a9275d9326e,d2573d20,"土日祝日は8時と18時に極端に多いわけではない  
また、17,18時を頂点として13~21時までが多い傾向にある  
0時付近が平日と比べて多いのは、翌日仕事がないからかもしれない(深堀の余地あり。ここでは触れないが)",cbe4b24b,0.84
38724,0687cd5c8597db,88957076,### **Classification Report of Test Predictions**,4edec76a,0.84
38727,519e936017c30a,669b262d,"Los videojuegos ""Shooter"" que mayor nivel de ingresos han generado son ""Call of Duty: Modern Warfare 3"" por un importe de 14,76 millones de dólares y ""Call of Duty: Black Ops"" por un valor de 14,64 millones de dólares.  ",dc34915d,0.84
38728,caaa6793391520,4d3a4edc,"OK, negative age is a bit too much... But keep in mind that the purpose is not to reconstruct the data, but to avoid amplifying bias in the machine learning model. Let's make sure Age and Cabin now is not null",1e79f342,0.84
38729,7dd46c750653eb,101f4a6d,**Adoptions over the Years**,c2644713,0.84
38731,2a9a149f306b6c,5b28a446,## Interpretation,295c52ce,0.84
38732,50d4ddf1953997,80df9f12,What about the longest film?,90bdddd6,0.84
38737,c4386b8a01d66e,f7a1bcb7,### SVM,dc732bf5,0.8403361344537815
38738,4ae6a182abac64,a577f297,![](https://i1.wp.com/interviewbubble.com/wp-content/uploads/2019/03/1pOtBHai4jFd-ujaNXPilRg.png?resize=936%2C340&ssl=1),418676c5,0.8403361344537815
38739,f6648e47713411,3674294f,# 4. Huấn luyện và đánh giá model,f4af4d1c,0.8404255319148937
38742,0e2a23fbe41ca9,d353c79b,"### 11. looking at the slice of data where ```state_id``` and ```category_2``` are nulls
",64e4762c,0.8405797101449275
38744,ea4e559a86d613,c5f5cab5,**2.Color jitter**,eff47843,0.8405797101449275
38746,b01ee6cb674fa3,328805af,# Royal Australian Engineers - RAE,a8ffd35e,0.8405797101449275
38749,8336d84cf3ff6b,1c5480a4,# Neural Network ,b96b58a0,0.8405797101449275
38752,548f961125248d,df2a67dc,"## Other Boosting Algorithms <a class=""anchor"" id=""ninth""></a>",d8c5e8b8,0.8405797101449275
38754,ac1abfe1dfe815,b6ca9729,## SVC,6529dbcb,0.8407079646017699
38755,c9b4e282e4e2c1,a14eaa90,Higher temperatures are related with more injuries.,f44d339f,0.8407079646017699
38756,a5a419dc7245b0,1fc498e5,"#### Phase 3: Making the predictions and evaluating the model
",4279726e,0.8407079646017699
38758,da199f8fb59439,adcdb8a7,![](https://filmdaily.co/wp-content/uploads/2020/05/NCIS_lede.jpg),baaa665d,0.8409090909090909
38762,f269d2fbd5f1be,4879fc11,"**Commentary:**
* There are more outliers in salary for players under the age of 30
* Median salary for a player above 30 years old is 3-4 times higher than the median salary of a player under 30 years old",1264c440,0.8409090909090909
38769,18ce858f90966d,83fbd506,# **PCA**,09e9caed,0.8409090909090909
38784,62487bcd70b199,7d55e0f9,## <a id='8.2.2.'>8.2.2. GradientBoostingClassifier</a>,f6ae50af,0.8416666666666667
38785,a566b5b7c374e7,ced6c7e8,## Time Series Analysis,b3dc5545,0.841726618705036
38787,99afe9f3af6dbc,b573d04b,"**Visualizing document clusters**

Memvisualisasikan output document cluster menggunakan matplotlib.",cdec9b3a,0.8421052631578947
38788,0d8df2c2983694,6c9b222a,Run the regression and visualize it on a scatter plot (no need to plot the line).,9bf7fa4e,0.8421052631578947
38791,f91f58d488d4af,5aaa838f,"#### For validation accuracy
",5df1bbf3,0.8421052631578947
38794,ba4b3bd184acbb,53fdfb56,"Since each step uses a method based on the datatype returned by the prior step, an identical analysis can also be done in one line.",0f5de724,0.8421052631578947
38803,9e27af2600925c,560b640b,"## 6 - Further analysis (optional/ungraded exercise) ##

Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\alpha$. ",9b556435,0.8421052631578947
38806,f35ee6e9fab592,4bc3937f,"""Each man is capable of doing one thing well. If he attempts several, he will fail to achieve distinction in any"" - Plato
Company tend to allocate work on basis of specialisation; same applies for reviewers. The following code cell outputs a nested dictionary revealing specialisation trends (i.e., which genre is reviewed by which reviewer).  A great example here will be Luke Reilly with 11 ```Racing``` video game reviews with the next most reviewed genre being ```Action``` with just 2",b15f7073,0.8421052631578947
38808,26b93b6f4dc148,f0ae05e0,"### Conclusion:
<b>*It added many extra features to dataset
<br>71 Features can lead to curse of dimension
<br>So need to reduce the Dimension using PCA*</b>",6f667d22,0.8421052631578947
38813,169177b6e9edea,9a814954,"#best_models.append(rand_search.best_estimator_)

for model in best_models:
    scores=cross_val_score(model,X,Y,cv=k_fold,scoring='accuracy')
    print('Model:\t',model)
    print('Mean score:\t',round(np.mean(scores)*100, 2))
    print(60*'-')
    print('\n')",ca42152f,0.8421052631578947
38814,31b564f11ef638,7c628579,## Submit,424f9692,0.8421052631578947
38817,52ee792e228d54,602dd194,"### The highest recall we could get was 68% with an accuracy of 96% from SVM Model.

### Let us try to improve our model.
### Initially, we observed that the data was highly imbalanced with majority people as Non loan customers. Let's balance this data by oversampling the loan classes and then building the models again.",5096094e,0.8421052631578947
38824,54004b32784b68,09710074,# Model Setup,27213ca9,0.8421052631578947
38827,bef2347846e476,52305343,"We have the types Free,Paid,nan,0 as a type of the applications.We'have found the number of each types of applications and plotted a basic bar plot.",cb93bf51,0.8421052631578947
38832,3fb15e6e48aec2,93de9fce,"# Trying Oversampling
* removed as this didnt improve the score",9d1f4358,0.8421052631578947
38835,738bfced935b69,fbf979d9,"Over all engine sizes, the higher mileage, the lower the price of the car.",2d3c592d,0.8424657534246576
38837,ab6da5994949a3,fe56ef02,## Decision Tree Test Results,fae6b91d,0.8425925925925926
38838,2f0f808765fc67,b49235b3,# **G. Gradient Boosting**,fd1f6494,0.8425925925925926
38839,fdc3afd309b850,470da044,"For modeling we are going to create a pipeline that also will tuning our parameters based on a cross validation.

However, before that we will split into Train Tests a df where the categorical values are converted into dummy and the numeric values are normalized and scaled except for the price of the apartaments. Where we will keep the original prices to compare the prediction.
",966bde38,0.8425925925925926
38841,312135b445bd23,a5588bc4,# Extracting Topics From Answers,8ced381f,0.8426966292134831
38848,2ada0305b68956,4234cd1d,### 144. Palette = 'rocket_r',133e26f4,0.8428571428571429
38853,38b79494ac749e,9e502a58,### Excercise 2 - Sparsity (4 points),39162a40,0.8428571428571429
38854,2f47abddfd1928,7a60d1c8,Again we confirm our expectations. This feature shows that being a female if in your family there is a dead child you will very likely not survive.,ae33cc0b,0.8429752066115702
38855,e9b9663777db82,5fbe6aba,#### Feature Selection Methods,648e8507,0.8429752066115702
38857,842547b2def18c,00bf3da9,"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).

The model generated confidence score is the lowest among the models evaluated so far.",b8efde6d,0.8431372549019608
38860,917957c6c4065f,28b39b87,"category 별 인기동영상의 개수를 확인해본 결과입니다.  
1위는 Entertainment로 4073개 26%  
2위는 News & Politics로 3728개 23%  
3위는 People & Blogs로 3379개 21%  

3개의 카테고리가 70%를 차지하네요.  
위의 3가지 카테고리에 속한 카테고리를 사람들이 많이 조회한다고 보입니다.",55b8ed68,0.8431372549019608
38863,629f2918807a9b,eb01da4b,"### Observations::
There is again no transparent correlation between any time of year and order returned or cancelled, However it can be concluded as:

1. 2021 is So far so good for guftugu publications, There are around 6272 successful orders and 3 cancelled while 477 returned books. 


2. 2020 was pretty good for guftugu publications, despite of pandemic they have sold around 23,784 copies with ""انٹرنیٹ سے پیسہ کمائیں"" was the best seller.

3. Most cancelled orders are in august while Feb, march, april and june has no cancelled orders in whole dataset.

4. Most returning orders are in 2020, with  count = 1882; Most in December and January",be56dc84,0.8431372549019608
38867,64169805aacf17,6da00f0f,# Video of the final painting stroke by stroke,1f12ded0,0.8431372549019608
38869,d0080e3a39bc5c,1bd33f04,**FINAL PREDICTION FOR SUBMISSION**,2fcde4cf,0.8431372549019608
38873,4daf6153275cbf,0485cf06,"In my final hypotesis, I am making comparison in terms of number of reviews, and obviously local restaurants have more rating. Test results are below.",51db1961,0.8433734939759037
38878,49f2274c1dd516,a0c79c50,"# Johns Hopkins
These are the data powering the 2019 Novel Coronavirus Visual Dashboard operated by the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). The data sources include the World Health Organization, the U.S. Centers for Disease Control and Prevention, the European Center for Disease Prevention and Control, the National Health Commission of the People’s Republic of China, local media reports, local health departments, and the DXY, one of the world’s largest online communities for physicians, health care professionals, pharmacies and facilities.",06b0ffee,0.84375
38882,3cc097a5859dc1,96c68891,# **Applying SGDClassifier Model**,14380d73,0.84375
38883,0932046e1f485d,46a37e30,Preparing review text for a wordcloud.,218cc7a3,0.84375
38885,9daf8b4a46725e,7d1a9b2f,### Checking Accuracy,7d9cc411,0.84375
38889,96c4c0e36b8ec0,af28c4e9,"What the graphs tell us:
* Passengers who have a higher number of parents or children on board are more likely to perish",4dd6de8c,0.84375
38891,51a46d0a7597f5,d65d212e,"Striker, GoalKeeper & Center Back are top three Positions in terms of number of Players",e9e25b17,0.84375
38895,c85c94076e9c3a,9c0f7fb9,## Dt_Customer,3ea0c443,0.84375
38896,b61ab8f81dc03d,524c2774,Comparing the accuracy from the models above the best one for those conditions is **XGBoost Classifier**,64d05394,0.8439716312056738
38901,4ae464582bac51,902df64d,"## As we can see in the graph, the base is unbalanced",ca6a52ce,0.8441558441558441
38902,663bbc9eaf267b,622c4b82,## Random Forest Regressor,32445529,0.8441558441558441
38903,90691864eb68c7,a95353e7,# 6. Neural Network : Keras Model,3555ef9b,0.8441558441558441
38911,d905cde3391d2b,473040ab,"* Blue line = Mean
* Red line = Mean $\pm$ std.dev",067dba39,0.8444444444444444
38912,6fad63bfd45ef9,f9178f22,# Evaluation,b3c6f1d6,0.8444444444444444
38915,3597174a998d4d,2e016836,#### 2.2.2.6 number of stay nights,276892ed,0.8444444444444444
38916,892be0a523578c,496ab07c,Next I want to know the exercise frequency of regular exercisers. It seems that most of them do exercise every day,b0e8d7c0,0.8444444444444444
38918,b0c2805cd5c087,ce3265f6,Image cra.org,0446f327,0.8444444444444444
38919,188731d7fa0604,7d695033,## 모델 선택 및 결과 출력,7cc543d3,0.8444444444444444
38921,396bc36edb95d3,c7957b53,#### Confusion Matrix and Classification Report for Training data - ANN,965e4f8f,0.8444444444444444
38922,5be39e4e35cec7,3e198591,"<a id = ""10""></a><br>
## Fill Missing Value
* Embarked has 2 missing value
* Fare has ony 1 missing value",14d617c9,0.8444444444444444
38926,d77e6d61ad2e8b,117b9832,# Saving the model,03fd0e96,0.8444444444444444
38927,4fd4b6a80d40e3,c6470527,![image.png](attachment:image.png),f6913cc3,0.8444444444444444
38930,63b44c85e32c1f,329b6c1e,### Built-in Functions,fb9b9562,0.8445945945945946
38932,98a6794067932a,56666f55,"Comme la carte générée ci-dessus était plutôt difficile à interpréter lorsque plusieurs points de clients se retrouvaient au même endroit, nous avons créé la ""Heat map"" ci-dessous afin de mieux représenter la situation. Encore une fois avec la fonction Folium, le code permet de prendre en compte la latitude et la longitude de chacun des clients présents dans le dataframe 'adresse' et il indique leur emplacement sur une carte du monde en utilisant des zones cette fois-ci. Le code nous permet de sélectionner le rayon désiré dans lequel les différents clients sont regroupés. Après avoir essayé des rayons de 5-10-15-20 et 25, nous avons déterminé que le rayon le plus adéquat était celui de 15. Si la couleur d'une région est chaude donc se rapprochant du rouge, cela signifie qu'il y a une forte densité de clients dans cette région. À l'inverse, si la couleur est plutôt froide donc se rapprochant du vert, cela signifie qu'il y a une faible densité de clients dans cette région. Finalement, le code permet d'afficher la carte qui vient d'être créée. Cette analyse pourra donc être très utile afin d'évaluer la densité des marchés desservis par l'entreprise. De cette manière, les dirigeants pourront orienter leurs décisions dans le but de viser des marchés de clients précis où la densité est plus élevée. En visant ce type de marchés denses, cela permet à une entreprise de limiter ses déplacements et de ce fait ses coûts de transport tout en ayant accès au plus grand volume de clients possible.",08600fe2,0.8446601941747572
38934,84127ade6fde87,03e5b1a3,"One interesting aspect of the resulting embeddings is that similar words end up not only clustered together, but also having consistent spatial relationships with other words. For example, if we were to take the embedding vector for apple and begin to add and subtract the vectors for other words, we could begin to perform analogies like apple - red - sweet + yellow + sour and end up with a vector very similar to the one for lemon.",f55d05b6,0.8448275862068966
38938,1cd8be6e679620,5a0740f1,### Example-2 (Test Data),3ce15a43,0.8448275862068966
38941,20e1ba19eb9b5e,680750b5,"# 3. XGBoost
## 3.1 Parameter Tuning",4569bfc1,0.8448275862068966
38944,00001756c60be8,8554b284,"**Обучение модели на CatBoostRegressor**

Вычисления гиперпараметров модели при помощи randomized_search() learning_rate=0.1 iterations=1150 depth=8",945aea18,0.8448275862068966
38945,a1ba5ffd30dbde,f83f91ed,- 94% accurate in predicting Disease,48e57546,0.8448275862068966
38962,c65a65d4041018,c4e0bb1a,### types of data used,824fb229,0.8455882352941176
38963,7f74a04ae75792,f655c4b4,"### What is the average number of purchase made in the last 3 years among different gender
",d01e91da,0.8455882352941176
38964,5f32117bcd5255,85025a66,#### NEGATIVE DIAGRAM,85882abf,0.8456375838926175
38968,4c47839b067546,0d133a96,Снова тяжелый хвост. Логорифмируем:,1f517b02,0.8457446808510638
38969,34fff8ce731b03,b2e3d107,"A saída do modelo é salvo em um arquivo csv, contendo as colunas ""client_id"" e ""inadimplente"". Estas colunas serão utilizadas para avaliar a acurácia do modelo. Por isso, o resultado da predição em *y_test* é adicionada em uma nova coluna (inadimplente) do DataFrame df_test.",6f9e5b2e,0.8461538461538461
38971,3f451680b1857b,0275fcb0,"## Pretrained Checkpoints:

| Model | AP<sup>val</sup> | AP<sup>test</sup> | AP<sub>50</sub> | Speed<sub>GPU</sub> | FPS<sub>GPU</sub> || params | FLOPS |
|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |
| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B
| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B
| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B
| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B
| | | | | | || |
| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/tag/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B
| | | | | | || |
| [YOLOv3-SPP](https://github.com/ultralytics/yolov5/releases/tag/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B",56c45a1b,0.8461538461538461
38973,33398ae40da63d,783a7184,"Let's save our data frame as a new csv file called ""real_news.csv"".",7bd02b88,0.8461538461538461
38975,80ad12f326ab70,32406114,The engagement per month has similar trend with the percentage access. Engagement took an uptrend in March and then dropped April.,da404a16,0.8461538461538461
38981,cf08b03b002c13,00af935c,Who is responsible for filtereing NSFW vs Mass Orinited content?,104d416f,0.8461538461538461
38989,4bbe953f82d29b,9ab69c25,### Автоматизация,772301f2,0.8461538461538461
38990,3e325daf577158,0bf557e3,## Prediction,c873dfec,0.8461538461538461
38993,d4c5aaa4b36810,4411edb9,"These polynomial models are only marginally more accurate, if at all, than their linear counterparts. Ridge and Lasso regression appear to be forcing to zero or minimiseing most of the polynomial coefficients respectively.

Lets see if a Support Vector Machine or Random Forrest can better capture these relationships. ",65441f28,0.8461538461538461
38995,6f1481148352e9,16c37868,**The graph shows that almost 70%f the fires occurred in the 2nd half of the year. We can make a conclusion about the seasonality of fires by half-year. It is also interesting to look at the number of fires in the states by half a year..**,7cfbdb8f,0.8461538461538461
38996,99bf357eaf61f1,38b20121,"# Interpret The Model

Now the model has generated a LinearRegression model for us. Recall that a LinearRegression model consist of coefficient(s) and intercept. We can now have a look at the intercept and coefficients for our model and interpret them.",9d92fafe,0.8461538461538461
38998,8ac70416723897,6ad3900a,Fitting and training the models,d32fd8f6,0.8461538461538461
38999,aa46e9376825a5,a84d00be,"### Remove all the observations where aircraft names are not available
",57792d96,0.8461538461538461
39001,50b03ce5b1a286,09e68c77,#Below: Let's try models without specifying the kind (Regression or classification).,d49896a5,0.8461538461538461
39009,3536195ad632ee,10ac8e26,**Summarize ensemble results and create submission file**,3c26aafc,0.8461538461538461
39010,09751c520b0616,6fbb8703,- Fitting model,a4d0c7e9,0.8461538461538461
39011,e1a69c71c2c282,86241703,# Split and Model,b2ca7d3a,0.8461538461538461
39012,a8c042af6b7245,b37f3e07,"## Feature Selection

#### Removing features with low or zero variance

Personally, I prefer to let the classifier algorithm chose which features to keep. But there is one thing that we can do ourselves. That is removing features with no or a very low variance. Sklearn has a handy method to do that: VarianceThreshold. By default it removes features with zero variance. This will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps. But if we would remove features with less than 1% variance, we would remove 31 variables.",2487ac62,0.8461538461538461
39017,866b157128a09c,e6be44c6,## Training Loop,0909f459,0.8461538461538461
39027,9b42412e75d640,83cc0203,"# Part 4
# Results summary",b616570a,0.8461538461538461
39033,f2f2db16a2f86c,8c0e8711,Using the **Mean Absolute Error** metric,ffc6a115,0.8461538461538461
39034,020c28a360b0cd,f5aa97d7,"Here, write a 1 paragraph reflection answering the following questions:

*   What was the hardest part of this project?
*   What was the easiesy part of this project?
*   How did you utilize your peer feedback?
* What was different between your plan and the final project?
* How well did you plan? Was your idea too easy? Too hard? Just right?
* If you had unlimited time, what would you add to this project?",2ba397f0,0.8461538461538461
39035,d07915a6e6992e,7acff081,"

Evaluating multiple models using GridSearch optimization method. 

Hyper-parameters are key parameters that are not directly learnt within the estimators. We have to pass these as arguments. Different hyper parameters can result in different model with varying performance/accuracy. To find out what paparmeters are resulting in best score, we can use Grid Search method and use the optimum set of hyper parameters to build and select a good model.

A search consists of:

1. an estimator (regressor or classifier)
2. a parameter space;
3. a method for searching or sampling candidates;
4. a cross-validation scheme; and
5. a score function.",2b912140,0.8461538461538461
39038,582cb872d19026,5bd129a9,## PieChart of Free Games by Categories,8d966d69,0.8461538461538461
39042,9a96e1588410ee,19c9cb44,## Finally building the submission file,3fa6cf92,0.8461538461538461
39043,03048e86a6d806,6e3596dc,"Machine Learning is now widely used to solve some targets which requires data. These machine learning methods might help to get prepared of what to learn when we are assigned to solve data-related targets. These numbers were also derived from dividing the number of people from each job title selecting that algorithm, by total population of each job title.",1285c231,0.8461538461538461
39046,0a1fcda859252c,93ee5d9e,"When a particular problem includes an imbalanced dataset, then accuracy isn't a good  metric to look for. For example, if your dataset contains 95 negatives and 5 positives, having a model with 95% accuracy doesn't make sense at all. The classifier might label every example as negative and still achieve 95% accuracy. Hence,  we need to look for alternative metrics. **Precision** and **Recall** are really good metrics for such kind of problems. 

We will get the confusion matrix from our predictions and see what is the recall and precision of our model.",13a38774,0.8461538461538461
39047,aae204e78a48d1,ff5df32a,"We can clearly see that the attrition base is skewed to 0% utilisation; i.e. they are not using their card at all.

**Hypothesis 4: proven**",53ab6133,0.8461538461538461
39050,6e8f3e8ed1c241,ea56def0,"I also have a basic floodfill algorithm found on the internet that isn't in the current version, but we plan on experimenting to see if we can get similar results",c2cfb626,0.8461538461538461
39054,2bd6c370695ea7,76319bb7,## XGBoost,cbe6aec8,0.8466666666666667
39055,c80939c7c626cf,b15024ee,# Score,b9ac31e2,0.8467153284671532
39059,513ce405d7f6a3,e5408a82,# Let's see model performance after cleaning the data,8461e086,0.8470588235294118
39061,869a39a3d4dea2,3b77550e,"Color Histogram <a id=""colorhist""></a>",9020daf8,0.8470588235294118
39063,593d1d3d1df05a,0bbb10c4,# Segmented characters and their predicted value,bc682ffe,0.8472222222222222
39066,fdc3afd309b850,577d42fd,"<a id=""tts""></a>
## 9.1 Train Test Split",966bde38,0.8472222222222222
39070,a81661cc35d8d2,8bbaa56e,"<b><font size=""4"">Logistic Regression with class weights</font></b>",3331f113,0.847457627118644
39072,b9bc7dc9f582e5,828b7d7e,# Predicting Test Data,15cc4d28,0.847457627118644
39074,dac3c8204a2d1b,8e198aa2,We extract most expensive authors books based upon the price here.,b0d2d0dc,0.847457627118644
39079,f2e5e9fb9eaaf7,457a85c5,"<a id=""6.2.4""></a>
### 6.2.4 Sum of features
Create a new feature `sum` that summing up all features values in a row. It seems adding up the `sum` of all features slightly decrease the model performance from `0.80151796` to `0.80151795`.",048e0d08,0.847457627118644
39080,c4bca5d86a38c3,ff900851,Haciendo separación de la data en conjunto de entrenamiento y conjunto de validación.,e23d297c,0.847457627118644
39084,55a5e31d03df9f,d54a0a69,"## Visualizing our overall progress with TensorBoard

We have in all the models used the TensorBoard callback this is because TensorBoard can keep track of the experiments and you just need to use:
```
%load_ext tensorboard
%tensorboard --logdir <log_directory>
```

Unfortunately, Kaggle doesn't support TensorBoard right now, however I download the logs and uploaded them 
in this link: https://tensorboard.dev/experiment/cDtJEN5fQue6Nac2H5mv1g/#scalars 

**📝 Note:** Keep in mind that whatever you upload to TensorBoard.dev becomes public. If there are training logs you don't want to share, don't upload them.",06dce00f,0.8476190476190476
39086,04bac111ffbe9c,7b80bf0d,## Creating O/P file,82576b17,0.8476190476190476
39088,7a058705183598,219328d8,6. DNN classifier with tensorflow estimators,b0ead917,0.8476190476190476
39090,b01ee6cb674fa3,41fa2445,"# UT

Doing a search for the name, nothing showerd up, so the next step was to search for the rocket, LAMBDA 4S - Bingo! A japanese rocket developed by the UNIVERSITY of TOKYO

A public company from Japan",a8ffd35e,0.8478260869565217
39091,72d528df923403,231eed60,In the next graph we can observe a positive relationship between units sold mean and units sold standar deviation which involves that items with most sold items have bigger fluctuations.,d51c8e8e,0.8478260869565217
39099,30fdc4a6e3c1db,32e67285,We can see that Super Bowl event was on Sunday and we have mapped it to the previous Saturday and Sunday via the column weekend_precede_event,6111ddee,0.847953216374269
39100,5ce12be6e7b90e,41d3074d,"### Exercise: primality check

Implement a simple primality check for the variable `n=97` (or some other value of your choice).

For each number `k` between 2 and `n` (or some other range if you prefer), check if `k` is a divider of `n` (using the modulo operation, right?).
If `k` is a divider you can break the loop using `break`.

**Note** `for` can have an `else` clause that will be executed if we exited the `for` normally, without a `break` or an exception.",c0ab62dd,0.847953216374269
39103,efd44ce2c08541,bfa4e4ff,# Predict ,ebc2d00c,0.8484848484848485
39104,adf419444a59df,bf9eafd4,## Training and evaluation,3a275e7f,0.8484848484848485
39109,2f964d08c25d93,eaed0e93,Let's take a quick look at what the data looks like:,1f2e4468,0.8484848484848485
39111,f166950fa915f8,b573caf8,### Confusion Matrix,a7f6ca5e,0.8484848484848485
39112,7c89a32e3562ca,e02410db,# SVM,32dd8913,0.8484848484848485
39114,a2573183738753,08786f9d,# validation vol,f6429599,0.8484848484848485
39116,2ada0305b68956,c91ad531,### 145. Palette = 'seismic',133e26f4,0.8485714285714285
39122,614ba9f0c62677,a84b6da6,"<a id=""18""></a>
## Conclusion
* http://scs.ryerson.ca/~aharley/vis/conv/flat.html
",b8551335,0.8490566037735849
39125,4dd47072617594,24905f92,# Random Forests,44ff1d11,0.8490566037735849
39134,fdc9f4863744b1,6b03e2ae,"""SALE PRICE"" is the dependent variable meaning , we are trying to predict the sale price in our ",b4529365,0.8493150684931506
39135,1eb62c5782f2d7,98b4340f,"### Contoh 3
### Mencari nilai $x$ sesuai dengan $z$-Score

- Seorang dokter hewan mencatat berat dari kucing-kucing yang dirawat di klinik.
- beratnya berdistribusi normal, dengan mean $9$ pounds dan standard deviasi $2$ pounds
- Temukan berat $x$ yang sesuai dengan $z$-scores dari $1.96$, $-0.44$, dan $0$
- Interpretasikan jawabanmu",bb69f147,0.8493150684931506
39140,c9b4e282e4e2c1,33d30390,**C)PlayerTrackData**,f44d339f,0.8495575221238938
39141,a5a419dc7245b0,01200c1a,##### Predicting the Test set results.,4279726e,0.8495575221238938
39154,b10bd75889dad9,8daec212,#### Metrics beyond simply accuracy,ee00ceee,0.85
39161,bc058fe14d3d1b,4385f6e8,### 2.1 SVM,d0273670,0.85
39171,edc19e349fe80a,5e21b84e,### 5. Write our answer to ChanceOfAdmit column in test data,7882221a,0.85
39173,09bac0c221388e,50773391,"The final summary is made with the nlargest function from heapq module, which efficiently returns the **k sentences with the highest score**.",bea4aa2e,0.85
39174,fdbbd573ba31c2,2f37e4ad,## AdaBoostRegressor,f7c28d74,0.85
39178,fc3adf9d45953e,1f37fb1a,"#Can't provide quality of life is an acceptable reason for not having Children.

#If they change their minds, I hope their children can learn AI, ML and still have FUN!",e3789b90,0.85
39179,1011899b959f44,f881954c,"# Plots 
This tool is great for visualizing the data we have gathered. There are a multitude of plots to pick from including: bar, line, pie, and scatter. The method for plots is:
* .plots()
When picking a type of plot, you simply add it before the parenthesis like this: .plot.bar()
* you can also label your y and x values accordingly!",0b112382,0.85
39182,0d58c434c7db1e,03286f0d,Youngest GM Title Holder?,517e01d3,0.85
39185,5ba4207c371899,b1bf8f3f,Analyzing our predictions,187b1451,0.85
39186,6e28c4f557f736,7da72468,"### Model Evaluation
https://www.kaggle.com/mjvakili/glove-bidirectional-lstm",021fdf75,0.85
39192,5ffe6aa38958a1,45c0a5f2,"# 5.3 Improving the model

There are a few tricks used here: 

n_estimators: complexity of random forest model 
oob_score: this uses an OOB train and cross validation method to improve the training
class weight: since ourput dataset has more examples of class: 0 (not survived), we can adjust for it in the training of random forest

  
OOB score: This is a metric similar to cross validation score computed at training time",11f5412e,0.85
39193,2bace980aeb34c,2a08aa2d,Run the next code cell without changes to save your results to a CSV file that can be submitted directly to the competition.,dc05ef6c,0.85
39195,c84925c8171900,38ba5b28,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.5 Genre WordCloud
            </span>   
        </font>    
</h4>",e21ff7ec,0.8504672897196262
39197,14defffcd250f3,e68f709a,**Logistic Regression**,3a683b94,0.8505747126436781
39199,d5f78aa381f58d,57f1c944,## KNN,d60f358f,0.8505747126436781
39210,3f25b363afec54,1ed832e6,```concat()``` all three camps data together.,bbdaae25,0.851063829787234
39213,b61ab8f81dc03d,be7a738c,"<a id=""xgboost""></a>
## XGBoost
XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.
I've got the parameters from https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters",64d05394,0.851063829787234
39214,5f674175839b32,51d1a197,"Q1)In which genre there is maximum sales?


Ans: The action has the best sales with 1722 million.

In action genre the no. of games released is also maximum.

",53a2e343,0.851063829787234
39215,04e6b0d3c70f46,9ce3baf2,"### Training The Model
Depending on features, we will train the model on TPU/GPU or CPU",56344f77,0.851063829787234
39216,f6648e47713411,5258e2a4,"### 4.1 Các biểu đồ của mô hình
> 1. Loss
> 2. Accuracy
> 3. F1",f4af4d1c,0.851063829787234
39218,73893f0467d5e3,1d9e6510,"## Accuracy check for test and train
",279787c6,0.851063829787234
39221,e9b9663777db82,fb902dfe,"#### - LinearRegression with SelectKBest, F_Regression",648e8507,0.8512396694214877
39222,2f47abddfd1928,19571091,"## 4.5. Family Size

It might be also useful for the model to have a combind feature of Parch + SibSp + 1 to represent the total size of the family.",ae33cc0b,0.8512396694214877
39223,e4525eb0c96f28,0b13e755,"As guessed, it seems that we do not have much improvement from our previous model, as our residual plot looks exactly the same as before. Our residuals are still heavily skewed right for a lot of our years past 1995 and they still have the same, almost symmetrical shape, with means well above zero for the years 1995 and before. Although the distribution of residuals for each year after 1995 seem to be taking approximately the same shapeform, there is still a lot of variance of spread amongst them as the skewedness for each year is still different. 

We must note that the years at and before 1995 seems to take on a totally new shape when compared to the later years. The residual distributions for these earlier years seem to be more spread out than before, with means larger than zero. It seems that both models have underestimated a lot of the total sales of games that existed back then. There is a suspicion that this might be due to overfitting the model to the majority of the data without regard to how different the games from the earlier years did. Let's try only fitting the model on games that came out in 1996 or later.",2093a1f1,0.8513513513513513
39226,27d5291d6365ba,049833be,# Customer Segmentation- 2D,96b30229,0.8513513513513513
39229,81712ee7510ac5,87a7aeef,**While loops**,c4685e79,0.8514285714285714
39232,6d29650083cbde,956135fe,"**5. FORECASTING THE 2018-2019 TALENTS**

Using the same random forest, I use the 2017-2018 data to forecast the high potential players of the incoming season:",e65fd993,0.8518518518518519
39237,faa8e6c8ab9246,40c8b7f2,"Now, lets split the data.",2bea1419,0.8518518518518519
39241,2b39f4ff896f97,587dd90b,Model Accuracy,3ddfe182,0.8518518518518519
39246,d128317750d689,5fc8455a,We can also take a look at some data samples and compare the image with prediction of the network. ,d87f7428,0.8518518518518519
39251,ddcdecdd6a3b6d,a8c3bc3e,"# GoogLeNet
1. 由Inception基础块组成。  
2. Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   
3. 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 
![1576146282%281%29.png](attachment:1576146282%281%29.png)",90831448,0.8518518518518519
39258,b809d07ddd17ed,d7f79a86,## Plot the loss and metric,e32bf3b1,0.8518518518518519
39262,fce6f1b02867e3,3b8a24cc,## Rename the column name,3fb572c2,0.8518518518518519
39265,07544ba83da480,10c00ef1,# Question 5: What product sold the most?,dc2f52b1,0.8518518518518519
39270,7baeb0ffc6659e,6921ef6f,**Feature / Label**,8cbebba9,0.8518518518518519
39275,71c3c1eab0377d,e784b317,**Instantiate the H2OGradientBoostingEstimator Class as model with some important parameters **,52b4e360,0.8521739130434782
39276,d1ff7e10ee0102,915690d8,### In the search for writing 'homoscedasticity' right at the first attempt,2cc71c3c,0.8522727272727273
39278,73d8e56bc709b1,6824b309,3. Value vs Wage?,78ec3cce,0.8522727272727273
39279,726833f92fb87a,3fc6d69f,We encode the remaining features with one hot encoding:,7dc5e1b6,0.8523489932885906
39284,0858e1bb3cbaca,ef0c9898,# Analysis,78548374,0.8524590163934426
39286,601e18072783b4,f75ca5db,## Top movie directors,36b2b1fa,0.8524590163934426
39289,840534f2908a9c,89735623,"The distribution of trip distance and fare amount for Lower Manhattan pickups and dropoffs is very different. Also, slope of linear realtionship for pickups for Lower Manhattan is higher than that for Rest of Manhattan",8081c3cc,0.8526315789473684
39291,21bce4ec54b3fa,5dbe1954,"fit base estimator, in next step get fitted estimator and apply feature importances",35546e30,0.8529411764705882
39297,395ed8e0b4fd17,14dea638,### Ethereum(ETH) Volume Traded for last 50 rows,7573ea31,0.8529411764705882
39298,629f2918807a9b,5090ae61,"### Hidden Patterns that are Counter-Intuitive For a Lay man

There are some questions which I have created, and I think it would be fascinating to know:

1. Which day is suitable for marketing of books on social media ?


2. Which book is being returned/ cancelled mostly ?


3. Which book is best best seller for top book buying Cities ?",be56dc84,0.8529411764705882
39300,ab657da5329e3f,6b9398b5,# Predictions,021526f8,0.8529411764705882
39301,8f50c9c16db95f,423b224e,"In this case, if the player wants to kick the ball to his right, his body needs to be pivoted at a larger angle, which is confirmed by our data as shown below.",26cc763a,0.8529411764705882
39302,02b7e38902069e,54bd729e,"#Tokenization without Sentence Segmentation

""Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\n\n) in the raw text. Just set tokenize_no_ssplit as True to disable sentence segmentation. Here is an example""

https://stanfordnlp.github.io/stanza/tokenize.html",726a03a0,0.8529411764705882
39303,71b75664517244,795a62e1,"#### Comparison With Team

This loyal manager always with Arsenal",fc905af5,0.8529411764705882
39306,156bbcff05dcea,e96407bc,# Feature Importance,66ad1fe9,0.8529411764705882
39312,e170d33ee1da8c,5f2fea0a,Let's fit models on remaining folds and save all the prediction.,253cee3c,0.8529411764705882
39314,99821bc6a45be6,8d786dc4,### Model Definition:,b9d59346,0.8529411764705882
39315,52cfd66e9ec908,202b3755,This takes a very small subset of the data that we have here (mainly because this training pipeline is purely for demonstration purposes with regards to Catalyst).,c74adcdf,0.8529411764705882
39318,67b7354e96113a,2b9331b3,**Gradient Boosting Classifier**,dca94250,0.8533333333333334
39319,e5dd725b8fa422,96f7dae2,"# [Sarimax Model](http://)
",14675d8b,0.8533333333333334
39320,7e1da639035ac5,05de274d,### <a id='15.1'>15.1 Trust % distribution</a>,120b6c23,0.8533333333333334
39328,74a03887600114,61ae7040,Now we will make a function which will recommend the movie with their correlation score.Note that higher the correlation more the movie related to each other,c0ffb2f0,0.8536585365853658
39330,514d8de15cb7ef,5b652ddd,## Comparing all these methods and finding the most suitable for both parameters,cfe111b2,0.8536585365853658
39333,8d0aebab1e5914,2aaea69a,"# PCA for dimensionality redcution (not for visualization)
Distribution of explained variance for each principal component gives a sense of how much information will be represented and how much lost when the full, 64-dimensional input is reduced using a principal component model (i.e., a model that utilizes only the first N principal components).",084e671f,0.8536585365853658
39336,e19e307b3fd188,a89820e4,"In this specific case, **XGBoost** showed better results compared to the other models.",2173955b,0.8536585365853658
39338,9c26c5dcd46a25,ae67decd,"#### <font color=""#114b98"" id=""section_3_3"">3.3. Projection des produits sur les plans factoriels</font>

On peut à présent visualiser la projection des individus sur ces premiers plans factoriels et donc en 2D :",1bbbb677,0.8536585365853658
39342,5169abdc647412,d1d9a5b2,### NOTE:,28efc68d,0.8536585365853658
39344,30fdc4a6e3c1db,7866dc1b,### First let's look at impact of different types of events and then we will look at specific events,6111ddee,0.8538011695906432
39347,d07915a6e6992e,f87321ba,"**AdaBoost classifier** 

Adaboost begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.",2b912140,0.8538461538461538
39350,312135b445bd23,ca055a8d,## Aggregate all sentences into a dataframe to use LDA on relevant sentences to see if distincitve topics emerge,8ced381f,0.8539325842696629
39351,04ff2af52f147b,89335513,"**Remove Unnecessary Features:**

Finally, we can remove any features that have become redundant, or simply not helpful for our purposes.",d5f37be9,0.8539325842696629
39354,3c2033cc99c12c,52e8aa4e," **Findings:** *From the above experiment, we could find that the accuracy on the testing set is a bit lower than the validation set, I think that the main reason lies on that the data distribution of the validation set is more similar to the training set, which may cause the occurence of overfitting, the solutions contains methods like data augmentation, shuffling and regularization.*",dfa22a54,0.8540145985401459
39356,28a1ff0f223da9,119034d4,"# Task-5
**How does Pakistan Student to PhD Professor Ratio compare against rest of the world, especially with USA, India and China?**",c945b27d,0.8541666666666666
39357,eb0854a6601407,0dc55f79,"If we plot the number of assets by time alongside the average target by time, it becomes evident that when there are less assets, the target oscillates more with prevalently higher targets. The correlation of assets number and target is negative, in fact. ",6d107747,0.8541666666666666
39361,386c42a7fb27a4,0990c35f,### Numeric Columns,9e9f6974,0.8541666666666666
39362,3b5903412fe741,037b8854,"The second is `isnull` (and its companion `notnull`). These methods let you highlight values which are or are not empty (`NaN`). For example, to filter out wines lacking a price tag in the dataset, here's what we would do:",ad231969,0.8541666666666666
39368,2ada0305b68956,1eebe1d3,### 146. Palette = 'seismic_r',133e26f4,0.8542857142857143
39373,6191c1f476437a,aee56efb,# Verify Images After Every Epoch,9dba3159,0.8545454545454545
39375,5a8c553e21c70f,28c25e4c,"Neural networks output probabilities. In binary case, we need a threshold to convert a probability to one of two classes. OOB samples can be used to determine the thresholds for each classifier. Precision-recall curves are drawn to find optimal operating point and threshold. We find the precision-recall pair which is giving the highest F1-score.",9ebd9d8f,0.8545454545454545
39378,49ee86d074de69,31c1dc12,"### A feature is not particularly important:
* if its coeff is around 0
* if its odds ratio is around 1

#### So you can consider to drop these features: Education, Month Value, Distance to Work
<hr>",71ccc6d3,0.8547008547008547
39382,c0ddb77bf32e2b,5583cd9c,"Like k-fold, we can do some monthly-fold, use all month exclude $i$ to train, and use month $i$ to test it. ",a0cb45f7,0.8548387096774194
39384,ee23a565163388,9aaaa35d,The SVM model gives pretty decent training and testing score.,88aacbc4,0.8549618320610687
39385,598b6228760590,ba8f8dd0,"- Looking at f1_score, the overall effect of random forest tuning has increased to a certain extent.",be30ab66,0.855072463768116
39387,0e2a23fbe41ca9,7b4bf675,"### 12. Deduping merchant data based on ```merchant_id```

work in progress",64e4762c,0.855072463768116
39388,17a24d566ffa59,effb7149,###### IF TIME PERMITS ,89049e56,0.855072463768116
39395,b01ee6cb674fa3,fc4bf592,"# Yuzhnoye Design Office - OKB-586

Soviet design bureau

",a8ffd35e,0.855072463768116
39396,548f961125248d,fd1d2e85,### Xgboost,d8c5e8b8,0.855072463768116
39403,3597174a998d4d,38f97020,The number of stay nights can be calculated with the formula: total_stay_night = stays_in_weekend_nights + stays_in_week_nights.,276892ed,0.8555555555555555
39406,2a123b4e8f9433,c43815e4,Logistic regression model,0a082218,0.8556701030927835
39408,063a35f644e3c5,b37b5634,### So from above Two model we can see that KNN performs better...,1c30fb0a,0.8556701030927835
39417,738bfced935b69,a8d238b0,"The Manual & Automatic transmissions are higer Mileage in the Diesel & Petrol fuel, while the Semi-Auto transmission are lower in the Petrol & Diesel & Hybrid fuel, & Other transmission are very low Mileage in the all fuel type.",2d3c592d,0.8561643835616438
39419,917957c6c4065f,4acc49cb,"Entertainment와 News & Politics는 평균 2일 째에, People & Blogs는 평균 3일 째에 인기동영상이 되네요.",55b8ed68,0.8562091503267973
39427,659f5f3ef8aa0e,f534d447,Scatter and density plots:,3654c2d0,0.8571428571428571
39434,87e94f864d74be,68b56a62,> We can say that mostTV Shows end by season 3,294bfe9f,0.8571428571428571
39436,0d59a3e0130db0,f7430290,"Finally, let's look at the predictions

The neural network does a good job on the test set",285f04b2,0.8571428571428571
39438,6e9b4020644836,79588475,### No data is very clean.,5ad41fc6,0.8571428571428571
39439,1a285e4c830f3f,4a76c6a4,"Für die beste Parameterkombination werten wir den Klassifikator auf einem Validierungsset aus. (Streng genommen wird hier auf dem Validierungsset auch trainiert. Die Overfitting- bzw. Data Snooping-Gefahr besteht, aber in einer milden Art, die wir für einmal akzeptieren.",360b50e9,0.8571428571428571
39440,c13f73168789c2,92442763,"### 1.2 Rows that match multiple column conditions<a id='32'></a>
Syntax : `df[(df.column_name == value) | (df.column_name == value)]`",16175052,0.8571428571428571
39441,7454fdc444df16,08bf022c,"We managed to reduce our total number of pixels from 2500 to 50, pretty great if you ask me! But before we go on celebrating this feat, let's check how much of the variance we managed to capture.",a7818ef5,0.8571428571428571
39443,e04e5204572e7e,f73df99c,"### Boss: I want results, not how you trained and validated the model!! All I care about is ***Classification metrics***
### Me: Alright, *señor*...😑",6c888be9,0.8571428571428571
39447,171494b45650a2,9c701ca6,## ***5. Encoding Categorical Data***,9c8cc578,0.8571428571428571
39450,be357c1e2c975d,d83cbe37,### Task 7: Represent Model as JSON String,2486a061,0.8571428571428571
39452,6b65d81a5743dd,337b47d6,We got 72.4% Accuracy from XGBoost,4080a2d2,0.8571428571428571
39453,e69a496109e7d8,7233e7c1,Majority of people who had survived is from 45-65 and having nodes 0-2,1c640591,0.8571428571428571
39456,6471597c5d2f66,b385c7ec,Scatter and density plots:,a41b4abe,0.8571428571428571
39457,ffd1df95ca5289,d738d275,## Random Forest,db00c338,0.8571428571428571
39458,1fac5edd4063ba,9183d9f9,"#Deepika Kurup

Deepika Kurup (born 1998) is an inventor, scientist, and clean water advocate. She is the recipient of the 2012 Discovery Education 3M Young Scientist Award. Kurup was awarded the 25,000 Award for her work in developing a new and inexpensive method to clean water using solar power. She also a finalist in the 2014 international Stockholm Junior Water Prize with her project ""A Novel Photocatalytic Pervious Composite for Degrading Organics and Inactivating Bacteria in Wastewater.""

In January 2015, Kurup was named as one of the Forbes 2015 30 Under 30 in Energy. She has also been featured in Teen Vogue for her work.https://en.wikipedia.org/wiki/Deepika_Kurup",04bc01e0,0.8571428571428571
39459,087e21401d7dfc,bdc866ca,So from above images we can see the difference between our result.,42000489,0.8571428571428571
39463,38b79494ac749e,13610613,"Lasso can also be used for **feature selection** since L1 is [more likely to produce zero coefficients](https://explained.ai/regularization/).

Is it indeed happening? 

Please do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`.",39162a40,0.8571428571428571
39464,400bbcc496138f,43ebc22e,The auc is improved from 0.96541 to 0.96548,191b86b8,0.8571428571428571
39465,3cd78d8d6d56e4,b54cb650,## Prediction of Testdata,9f632e94,0.8571428571428571
39467,e16860fce156b0,1ae831fa,"#Missing values from  Specific columns

To understand the impact of missing values from a specific column, users can pass the column name into the parameter. It will compare the distribution of each column with and without missing values from the given column, such that the user could understand the impact of the missing values.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.8571428571428571
39474,066c5ee1ef39e6,71a55b79,## BERT - pretrained roberta model,0f394e1b,0.8571428571428571
39476,adb8441ad28019,5c1d6c17,"<div class=""alert alert-success"">
    <h1 align='center'>XGB Classifier</h1>
</div>",d89de993,0.8571428571428571
39481,0c57e3132ae184,32a95045,"## Lastly, reorganize the order of columns",f6bac298,0.8571428571428571
39483,ba4b3bd184acbb,0351665f,The analysis results can be written to an html file.,0f5de724,0.8571428571428571
39485,3cea0f929a2035,1c15a819,"In contrast to the recent decreasing trend in the number of suicides in Russian Federation and Japan, the number of suicides in the US is increasing. So, to ignore the fact of having more population, and to have better understanding on the number of suicides, we'd better consider suicides per 100k population in each country.",04cfbade,0.8571428571428571
39488,585c280865b46e,6435fd51,# Expression analysis ,4d6056f1,0.8571428571428571
39489,6f4795cfdc96c7,5e9283db,Scatter and density plots:,1f3ab82f,0.8571428571428571
39490,3879ef16f5eb28,0b003e35,# Stem plot,784b8d8e,0.8571428571428571
39496,722cd844dfbe8f,86c39be2,"# <span style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"" id=""section_4"">Test of trained final model</span>

The best model was saved during training. We will therefore load it and test the predictions on the file and the submission images.",0cedb385,0.8571428571428571
39497,ffcfb4bb9a5812,85ad0a11,# Upvote if Like the Work 😋,477e0eb5,0.8571428571428571
39499,99f84fa59cb1da,72e58d0d,### Hurrayyy. We got the same distribution in each fold 🙌🙌🙌🙌,41e95f63,0.8571428571428571
39514,98fd05fcc5c3e3,fae9b0f2,## Model Training,55fe7ece,0.8571428571428571
39516,a76e0e8770b7a0,19d9edb9,Spain has problems in 20th century.,02863d3b,0.8571428571428571
39518,60d500d196eb42,5e208da6,Scatter and density plots:,2ad55f3f,0.8571428571428571
39520,2473d004f92592,e389a993,"**Confusion Matrix**


*A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.*",18d3b6ee,0.8571428571428571
39526,12f4d16fc21645,54f08c63,<h1 style='color:blue'>Check for Overfitting</h1>,c7752038,0.8571428571428571
39528,8dd655515e7d18,f66f6d11,"### Extraversion

It indicates how outgoing and social a person is. 

A person who scores high in extraversion on a personality test is the life of the party. They enjoy being with people, participating in social gatherings, and are full of energy. 

A person low in extraversion is less outgoing and is more comfortable working by himself.",895f41cf,0.8571428571428571
39531,55a5e31d03df9f,e4130919,"## ** New update: Optimizing the our dataset ** 
Recently I discover reading our documentation that we can optimize the performance on training using TF.AUTOTUNE (https://www.tensorflow.org/guide/data_performance), however we need to change the iterator to be a `tf.data.Dataset` instead `image.DataFrameIterator` that is the case with our current `train_ds` and `test_ds`.

**Intially we did this:**

",06dce00f,0.8571428571428571
39537,60da9bbfe39c4b,9a673fed,"# So, we can show that in August 2021, Bangladesh was the most people recovered of COVID-19.",b0dd8ad6,0.8571428571428571
39538,b74076b2f8ba1d,c32f47f4,Scatter and density plots:,9ace22d4,0.8571428571428571
39545,9456df44ab9308,f7a4142c,## Save Files,a8e94298,0.8571428571428571
39546,84d1ef55b89e17,0ae5bbc4,**Draw 3D**,2d0b9d51,0.8571428571428571
39553,4b4117cf42ef8d,44b9a22d,### 3- Elastic Net,457cd6f4,0.8571428571428571
39554,b211c8c107f56d,a5ab6d30,"## Submission  <a name=""step4""></a>

Here's your submission csv that should get you a LB score around .897 ! ",805b90f3,0.8571428571428571
39558,c818250dd720eb,99c1b7d4,"# Training the Models

Each tile was copied to three folders according to whether each of the three possible patterns were present or not. This was done using three seperate notebooks as I ran up against the memory limits when running it all in one notebook. For instance a tile containing gleason 3, and gleason 5 but not gleason 4. Was copied to the three folders: Gleason 3 present, Gleason 4 absent, Gleason 5 present.


This way I used all the data to train each model to ascertain whether each pattern was present or absent in a given tile. Validation accuracy got to roughly 0.7 for these models, training of one can be found [here](http://www.kaggle.com/dararc/gl3-panda-training/). 


After about 25 epochs the model tended towards overfitting. There is definitely room for improvement in this step of the system and I will be experimenting by accessing different tiles, for instance 16 tiles from the lowest layer as well as using augmentation of the training data. This will take a lot of GPU hours so it will probably be late August before these improvements are live.


At this stage we had three models which could ascertain with about 70% accuracy whether each pattern was present in a tile. I decided that a cancerous region could obviously be present in the tile with the 25th most amount (still some) of tissue so I decided to run these models on the top 30 tiles when predicting. These models would give us 3 figures per tiles so we still had to collate these 90 figures into a single ISUP grade. I hard coded some procedures which got a QWK score of 0.19, before realising that several thousand examples of 90 values which predict a single value was a perfect task for machine learning.

I made two small Neural Networks (I am currently unsure if a Neural Network was the best choice for this task, I will be researching this over the coming weeks) one for each data provider. The NN would take 90 values and output a single ISUP grade. I trained it on all the training data (minus suspicious slides). I could make use of all the Karolinska data in this phase as I was now predicting with our RESNET50 models on the top 30 tiles per image, the mask files were irrelevant for this step as all the labels I needed was the final ISUP grade of the whole slide.


I tried to use the QWK as the loss function for this model but did not get this custom loss function finished in time (Another task for the coming weeks). I had to settle for binary cross entropy which doesn't distinguish how far away mis-classifications are from the true value, whereas the final grading for this competition does.",68ee40de,0.8571428571428571
39560,1c381451c17150,72601bc4,"# Let's See the Results
As you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines and scene names seem really plausible as Monty Python dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. In some ways that works wit Monty Python. Still, more AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days. This is why stuff like Shakespeare and poetry are popular by AI generation, abstract language makes imperfect text generation less detectable. 

A final thing I find interesting is that, while typos are generally few when trained well, the AI actually sometimes makes up new words in the same style. I guess this is from the AI making a typo but covering it with something that sounds right to it. So it is making some typos but they are just not overt and they seem like uncommon words. For example, once this script generated the phrase ""I say some perpilly cricket"". While that is a very british/Monty Python sounding phrase, the word perpilly does not appear in the original Monty Python scripts anywhere and is not a word in english at all! Something to look for when generating your now unlimited supply of Monty Python scripts.
",e79b530f,0.8571428571428571
39561,2b36742b49c7bc,912a982f,## Баталгаажуулах (Validation),c8f8a96d,0.8571428571428571
39568,f4b603905215b7,bcd31635,# 6. Synthetic Minority Oversampling Technique( SMOTE),efe1d587,0.8571428571428571
39575,a758983a68c014,277e5255,"Using decomposition, PCA for example, we can visualize our vectors in 2D vector space.",ab89f181,0.8571428571428571
39584,37b09262279764,6ebeb146,### Predicting on Test Data using LGBM,37c4c417,0.8583333333333333
39587,ac1abfe1dfe815,0fa6153d,-,6529dbcb,0.8584070796460177
39588,510b8303776bb6,b28b0754,### Removal of null values ,18080db8,0.8584905660377359
39589,43e60eb1362f5c,a8ea0d8f,"# When Flight_Delays is taken, where I took all the different Delays into Concern.",87934234,0.8584905660377359
39594,869a39a3d4dea2,1fcbb3f9,Just like how we have seen histogram for grayscale we are going to visualize the RGB,9020daf8,0.8588235294117647
39598,d4c5aaa4b36810,0c17753f,### Support Vector Machine,65441f28,0.8589743589743589
39604,631cd434fc3aa2,0f145277,"Before ending this section, we have to split again the data into train and test.",2b74febb,0.8591549295774648
39607,bddd799cdbbae8,ed0ba36b,**Multinomial Naive Bayes**,b44e3c08,0.8591549295774648
39610,0932046e1f485d,6a146b16,Positive reviews WordCloud,218cc7a3,0.859375
39611,ff3a8ce61fab6a,a05586d8,"<hr>
<div>
    🔺 <b>Alert</b><br><br>
    <b>datatype equavilant</b> which we talk about it at first             <b>alert</b> ☝ has same importance when we use <b>pow</b> function.
</div>


<p>
    Let's talk about <b>matmul</b> function  We can use it to   get a value of <b>Matrix multiplication.</b>
</p>

## Matrix multiplication [......] x [......]
<hr>

### Exampel 1 ",9afe1654,0.859375
39617,6cade0b6a41ba2,a48848ec,"##### From the above graph, probability cut off of 0.064 seems to be respectable enough to behave as threshold value above which patient is likely to have stroke",e6110293,0.8596491228070176
39627,9e27af2600925c,1cb4df9d,"#### Choice of learning rate ####

**Reminder**:
In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may ""overshoot"" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.

Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. ",9b556435,0.8596491228070176
39628,5ce12be6e7b90e,39b07b92,# Dictionaries,c0ab62dd,0.8596491228070176
39631,83df814455f06c,b558492c,"We can see that the training-set score and test-set score is same as above. The training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting. 
",c9cff71a,0.86
39634,2ada0305b68956,2560fd4c,### 147. Palette = 'spring',133e26f4,0.86
39639,4cd25e50c7e007,5089f8e4,"### Making Predictions
Applying the scaling on the test sets",ceb0c525,0.86
39641,2bd6c370695ea7,8328031f,## Run LGBM,cbe6aec8,0.86
39643,0caaec057f7184,8c5cb986,monthly average price  and std of all shops of the item,b875533e,0.8602150537634409
39644,7f74a04ae75792,7db3e036,### Check the distribution of customers who responded to ads in past 5 years with By Gender,d01e91da,0.8602941176470589
39648,8539260444e6b5,89772659,# Stacking Classifier,0369463f,0.8604651162790697
39651,d96642860ab3dd,103fb5ca,"### Hyperparameters Tuning
Perform GridSearchCV or RandomizedSearchCV to all the classifiers with optimizing their hyperparameters and thus improving their accuracy.then answer default model parameters the best bet or not?",98419d48,0.8604651162790697
39652,72d393488311b6,dd2c5746,# Evaluation,80663df0,0.8604651162790697
39654,b2e2c792b886ac,9a2a48b8,"Обратите внимание, что метрика, которую необходимо оптимизировать в конкурсе --- f1-score. Вычислим целевую метрику на валидационной выборке.",2a184b39,0.8604651162790697
39662,10b5af05d804ff,8123859c,"We have 0.48% better result! Is it big improvement? Let's see on leaderbord! The 1st place has 0.86428, minus 0.48%...is...  ",4a9b1705,0.8611111111111112
39663,9085cba2265204,a6cf479a,## And then we have K-Fold Cross Validation,de766eb3,0.8611111111111112
39666,ab6da5994949a3,62d625fb,## Visualising the Decision tree Training set results,fae6b91d,0.8611111111111112
39670,69ac33d79f5130,44908874,#### Find the Latitude and Longitude in pairs,9d760d2a,0.8611111111111112
39672,c01049afb6d307,f6cc1ba5,"### (19) Injury, poisoning and certain other consequences of external causes",d37d3b5d,0.8611111111111112
39676,268a610bbc64b4,a5dfe2a4,There are only 36 users in the data who reactivate their account.,8a16f301,0.8611111111111112
39681,0f5085b162bd9f,0d7ab662,# Birch,a3d989ee,0.8611111111111112
39684,cf39cde80e66b7,363d0bc8,"# <div class=""h3"">Residual Sum of Squares (RSS)</div>
<a id=""m6""></a>
[Back to Table of Contents](#top)

[The End](#theend)",aed4bc9b,0.8611111111111112
39689,c80939c7c626cf,5a88700f,# 11 Descison Tree,b9ac31e2,0.8613138686131386
39690,3c2033cc99c12c,22364dba,### Deep Learning Method ,dfa22a54,0.8613138686131386
39692,c115e287523aab,dd9ed69d,# Calculate OOF Score,feb1288b,0.8615384615384616
39698,09751c520b0616,e3f16815,- Making predictions,a4d0c7e9,0.8615384615384616
39699,3cb96bd8eb364b,3f038da9,### Save Model,3157af7e,0.8615384615384616
39702,485de87c50af82,619e5de6,### Trying again with 3 classes instead of 5,a5bd438e,0.8615384615384616
39706,e19e307b3fd188,e1b659dd,## RandomizedSearchCV with the best model (XGBRegressor),2173955b,0.8617886178861789
39707,00d295edcd117e,52d5f7ff,接下来，我们来看看整体的正确率。,f5810f4b,0.8620689655172413
39708,858da4bb312f67,dc7b9557,#### HEALTHY to CBB,9cca4391,0.8620689655172413
39710,bb8f5d7807718b,beba887e,"# 10. CyberPunk Style

[mplcyberpunk](https://github.com/dhaitz/mplcyberpunk), which is a Python package on top of matplotlib to create ‘cyberpunk’ style plots with just three additional lines of code.",181ec286,0.8620689655172413
39713,1dd9c6aa74d289,cc46d55c,### Bouldering log done in countries of:,5ef9a1be,0.8620689655172413
39714,9535bb04ae042c,d858cdab,"## Bring the pickle files for DataFrame and Pipeline in your virtual environment
## The process of deployment has been explained in great detail by Gilbert Tanner: https://gilberttanner.com/blog/deploying-your-streamlit-dashboard-with-heroku",165b6fae,0.8620689655172413
39715,401338428b2d1c,dc8b6400,## Visualising the Training set results,e4b768be,0.8620689655172413
39716,5fc2f23dfbeeb1,4c569cc4,### Prediction on Test Data,f37b4110,0.8620689655172413
39717,a4f0a3e1316ff9,67716b2d,"# Plot Into Future

This is a very basic use of regression to look at the trend into the Future. Please note that currencies are difficult to predict even one day in advance. It cannot be understated how difficult it is to predict movements of a currency and I do not recommend trading based on predicted values.",53bf0160,0.8620689655172413
39724,6a1d04e8153df3,4ae62b34,**CONTOUR PLOT**,38572b05,0.8620689655172413
39726,18a96bb5711ed9,ce7e9f22,"# Cause of death by region <br>

By the interactive plot of cause of death by region, we can analyze reasons of incidents in depth. 
<br>
 
* the majority of people from Mediterranean region were drowned
* the majority of people from North Africa region were dead due to sickness and lack of access to medicines 
* the majority of people from US-Mexico region were missing (unknown) ",e79768db,0.8620689655172413
39731,fb9296ecd0cb2a,b43adfd9,## Making a submission,aa66d98c,0.8620689655172413
39732,c18c37441caa8d,baa4a1bf,> ** Submision **,ca98414d,0.8620689655172413
39737,fc8e0042411c46,a3658d70,# Feature Scaling,af476c2a,0.8620689655172413
39741,6b54e39f86bdb5,4c64690b,"## Save as a CSV file

Finally, we have the data well ordered and everything is ready to create and submit the CSV file.",198084bc,0.8620689655172413
39744,84127ade6fde87,9bbf51fd,"More contemporary embedding models—with BERT and GPT-2 making headlines even in mainstream media—are much more elaborate and are context sensitive: that is, the mapping of a word in the vocabulary to a vector is not fixed but depends on the surrounding sentence. Yet they are often used just like the simpler classic embeddings we’ve touched on here.",f55d05b6,0.8620689655172413
39750,b01ee6cb674fa3,8a1ee62f,"# AMBA --> ABMA

Searching for the Pioneer 3 spacecraft, we get the result that actually, AMBA is misspelled, and is 

> ABMA: American Balistic Missile Agency",a8ffd35e,0.8623188405797102
39752,0e2a23fbe41ca9,1c4953ed,### 13. ```category_2``` vs lag columns,64e4762c,0.8623188405797102
39754,254cccd5145725,eccc862d,Custom Evaluation Metric,a49b4037,0.8625
39755,fdbbd573ba31c2,504dfa12,## GradientBoostingRegressor,f7c28d74,0.8625
39757,3dd4294f903768,3bddc159,***,0d89d098,0.8625
39758,ee23a565163388,1f271537,## **Naive Bayes Classifier**,88aacbc4,0.8625954198473282
39762,523123dad03177,baa5f4f7,Test preparation?,48a5e4e6,0.8627450980392157
39764,917957c6c4065f,8694a919,### 2.12. 채널 별 상황은 어떠한가,55b8ed68,0.8627450980392157
39765,7cfd96218dd933,7f0e8106,"#### **ATTENTION**
* NONE OF THE HIGHEST VALUES HOTSPOTS ARE IN TURKEY.
* THE HIGHEST VALUE IN THE WORLD WAS IN THE AMERICA.
* THIS SITUATION MAY PROVE THAT THERE IS NO ANOMALY IN THE MEDITERRANEAN LOCAL",7c34d96c,0.8627450980392157
39766,fa02c409161192,3e97b839,# Predictions for Kaggle,e97077f7,0.8627450980392157
39770,842547b2def18c,b6dc5ad4,"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron).",b8efde6d,0.8627450980392157
39771,d0080e3a39bc5c,c2bd7cbe,"Since there is some problem with making a CSV to submit using FastAI on Kaggle, I have directly uploaded the CSV file at the end.

Nonetheless, the code for making predictions is given below.",2fcde4cf,0.8627450980392157
39773,81712ee7510ac5,4ee532e0,**range()**,c4685e79,0.8628571428571429
39775,9ceb7278784462,8aa81d16,## <a id='23'> 19.Gradient Boosting Regressor</a>,3768a567,0.8629032258064516
39779,1eb62c5782f2d7,d28aea27,### Solusi,bb69f147,0.863013698630137
39782,6a80f915608fc2,83056aa6,"## <a id=""ROC"">Model Quality and ROC</a>
Back to <a href=""#Index"">Index</a>",636938eb,0.8630952380952381
39783,f91f58d488d4af,26693a73,"That's our starting point. Let's train for one epoch and see if the accuracy increases.
",5df1bbf3,0.8631578947368421
39784,840534f2908a9c,d4d15b7e,Let us now look at datetime features and their realtionship with Fare Amount,8081c3cc,0.8631578947368421
39788,49ee86d074de69,0310d0b6,### Interpreting The Important Features,71ccc6d3,0.8632478632478633
39789,a566b5b7c374e7,cda1b102,### HVR versus Resting Heart Rate,b3dc5545,0.8633093525179856
39790,b10bd75889dad9,91840406,### Plotting the ROC Curve,ee00ceee,0.8633333333333333
39793,e323e594ef918f,476281df,Worked like charm :) How about the other label for this image (0 - nucleoplasm)? ,6e829ab6,0.8636363636363636
39794,6d66ced0028dea,6c896b2a,# 4. Модель,f50aae52,0.8636363636363636
39796,0cb9adc158b705,3d7b1ce0,"Amazing! Lets export the model so that we can deploy it to production 😂. Just kidding, we will (only) use it for inference.",3abf056e,0.8636363636363636
39802,450fda47b03baa,f5256449,En çok sayıda yüklenmiş ilk 10 uygulamayı bulalım ve görselleştirelim.,62c04adb,0.8636363636363636
39803,f269d2fbd5f1be,dc9ba957,# Distribution of Salary by Rating Group,1264c440,0.8636363636363636
39806,d1ff7e10ee0102,b73bb9c4,"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).

Starting by 'SalePrice' and 'GrLivArea'...",2cc71c3c,0.8636363636363636
39808,d83e5b44d1b80d,9a14f23a,***Surprisingly Python is the favourite programming language for all age group people***,62845930,0.8636363636363636
39809,a35cdce61f4059,c9d79bda,it is visible that our accuracy for detection of fraud cases has been improved with underSampling,acc8eab6,0.8636363636363636
39810,b066ab2167199c,362c35f8,- It shows all columns as False means no NULL Values present in dataset.,18a1753d,0.8636363636363636
39812,dd02a9b545f742,3153aaed,"* System mode | Options: (1) Test (2) Commissioning 

      --> Commissioning mode
    

* Regression method for a linear model | Options: (1) Ordinary Least Squares (Cross-validation 10 folds) (2) Bayesian Ridge (Cross-validation 10 folds) (3) Bayesian Ridge

      --> Bayesian Ridge

* Train dataset | Input instances size (0 - 1,048,575 )

      --> 15219

* Return value policy | (1) Prediction Model correspondance: Assign 1-point action value when an actual return corresponds to a predicted return and it is within the confidence interval (2) Maximax correspondance: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return (3) Maximax correspondance biased by tag-confidence perception: Assign 1-point action value when a (+) actual return corresponds to a (+) predicted return. Then, the action value has to be multiplied by the confidence probability. Finally, the result must be rounded to the nearest decision-making boundary, i.e., 0 or 1..

      --> (3)

* Risk exposure to loss | Scenarios | (1) 25% (2) 50% (3) 75% 

      --> (1)

* Investing proficiency | Scenarios | (1) 25%-beginner  (2) 50% (3) 75% (4) 99%-expert

      --> (4)",7116cd2d,0.8636363636363636
39817,7b59fc0ff0d10b,91075891,"We can see that the cloud fraction affects the NO2 measurements inversely. Areas with higher cloud fraction have significantly lower NO2 measurements than adjacent areas with lower cloud fraction, when there are no sources of NO2. Cloud fraction shows a higher than proportional effect at values >0.3.",c192b94c,0.8636363636363636
39818,be2f4d8a6b73ca,7aaa80fd,**Logistic Regression**,5d8ce40a,0.8636363636363636
39819,930cd79ca51204,da347142,Let's add some description to the biggest countries. ,5506779a,0.8636363636363636
39823,9c19668d6b7295,0c771415,## Main,35be7001,0.8636363636363636
39824,8578b9a8d00730,9bc04382,## ***Comparison Between Both Models***,7648721b,0.8636363636363636
39826,ae058c3f1439c3,c8d91b0f,"> After looking at the above cloud of words which states the most demanded Educational Requirements, We can Conclude that Some of the most Important Educational Requirements Include Technical Experience, BA, BS, Computer Science, and Practical Experience etc.",965da99d,0.8636363636363636
39829,da199f8fb59439,2e66978e,#### **Longest documentary on netflix**,baaa665d,0.8636363636363636
39832,98a6794067932a,05b9d5a8,"**2.7 Analyse des clients perdus**

Finalement, la dernière section de notre analyse sera consacrée à l'évaluation des clients qui ont cessé de commander auprès de l'entreprise au fil des années. Comme nos analyses sont grandement orientées vers la prise de décisions par rapport aux emplacements potentiels des centres de distribution, il nous semblait intéressant d'évaluer le nombre de clients ayant cessé de commander auprès de l'entreprise afin de voir où se trouvaient géographiquement ces clients. ",08600fe2,0.8640776699029126
39837,1294fb4c86f993,024aeb8b,Defining a function to plot any census feature against `guns_per_thousand`,4471e513,0.864406779661017
39838,dac3c8204a2d1b,99ddb6d8,# User Rating Scores Of Each Genre,b0d2d0dc,0.864406779661017
39843,ed8009f482b380,92b81bf0,## Comparison,e99941fa,0.864406779661017
39845,a077820f7ab459,7d815f6b,### Create heatmap,05a43104,0.864406779661017
39847,a44368590e878a,57f738a1,### Latitude / Longitude,77743ba8,0.864406779661017
39854,a6c34cd514e30e,9b566cf8,Correlation matrix:,bf603ddd,0.8648648648648649
39857,63b44c85e32c1f,c949b5bd,**union( )** function returns a set which contains all the elements of both the sets without repition.,fb9b9562,0.8648648648648649
39859,b7b1057764fa02,6a74aff9,"With our function now ready we can plot the first confusion matrix. This will be the matrix for the testing data, which gave a high accuracy. We expect to find the diagonal elements to have large values with some values distributed in non-diagonal elements. Note that our the matrix is not normalized, and the total number of testing images per label were 300.",5053a192,0.8648648648648649
39863,62037c5832129c,192b59af,----------------------------,61474350,0.8648648648648649
39867,ac9b48d531bad9,58ec6df8,**Understanding the Random Classifier Model using Permutation importance method**,95965e35,0.8648648648648649
39876,56785caebaa256,ef1f21ac,### Calculation of forecasting errors,a792961a,0.8652482269503546
39880,71d3e4aee86e3e,03de866e,> ## 4. Mortality Rate(per 100) ,69706f0b,0.8653846153846154
39881,95efc1ad1d3e26,59f3bd70,## Sampling,79de1120,0.8653846153846154
39884,44f6a002ecd033,4e7edc0d,"Our xgb did not do as well as the logistic regression model, so we will go ahead and just try to beef up the logistic regression model the best it can be. This starts with seeing if our log transformations helped the model.",70bbe106,0.8653846153846154
39886,5ce12be6e7b90e,8abebb6a,"**Dictionaries** are hashtables or maps: a data structure used to store collections of elements to be accessed with a _key_.
Keys can be of any _immutable_ type - strings, integers, floats, etc.
Each key refers to a single _value_.",c0ab62dd,0.8654970760233918
39887,30fdc4a6e3c1db,cdf38ccf,### Plotting sales of weekends before different event types,6111ddee,0.8654970760233918
39889,4ae6a182abac64,495c9fc8,* **Accuracy_score**,418676c5,0.865546218487395
39891,a4aa36df07fd53,4f948f34,Mari kita perhatikan seksama bentuk data kita setelah one hot encoding,d2f42b6d,0.8656716417910447
39894,ba655a261cc09e,fae684a1,## Random Forest,48cc549a,0.8656716417910447
39896,2ada0305b68956,c46c0143,### 148. Palette = 'spring_r',133e26f4,0.8657142857142858
39902,fd4017c1514157,5a5c37f3,"The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.
The data provided of audio cannot be understood by the models directly to convert them into an understandable format feature extraction is used.
librosa.display is used to display the audio files in different formats such as wave plot, spectrogram, or colormap etc. Waveplots let us know the loudness of the audio at a given time.

",fd8f0896,0.8658536585365854
39904,225b4fe5d3894a,d439f300,"<a id=""8b""></a>
### b. Analyse the Best Models and Their Errors",4b4197b3,0.865979381443299
39917,04bac111ffbe9c,8acf3b5e,## AdaBoostClassifier,82576b17,0.8666666666666667
39918,d6ddbe57f59cf7,dfb5395b,# PairGrid,504a3cda,0.8666666666666667
39919,e78f177ca86768,ba80a532,# Top 50,120e25c1,0.8666666666666667
39923,d6cbd7160961dc,4fe1cd9b,## 6.2. Running the script on the data set,36d74664,0.8666666666666667
39933,6e472c6c591c7d,623fde9c,"# Keep Going
**[Click here](https://www.kaggle.com/dansbecker/as-with)** to learn how to use **AS** and **WITH** to clean up your code and help you construct more complex queries.",65532a3d,0.8666666666666667
39935,c18267b203f28a,2bde7155,"# Making predictions
Now that we've trained our model we can use it to make predictions! ",09ca8efb,0.8666666666666667
39940,b547f0f38f7744,9c01ec2f,### Let's Train,b6ba66b3,0.8666666666666667
39945,49ac6594c8f5cf,82b752e7,OVER-SAMPLING,6f19f28a,0.8666666666666667
39946,b10bd75889dad9,251a7f45,"An ROC curve demonstrates several things:

- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).
- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.
- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.",ee00ceee,0.8666666666666667
39948,d905cde3391d2b,f3823799,"## Mean Absolute Deviation (MAD)

**Mena absolute deviation** is the average distance between mean and each data point.

$$
\begin{align}
Mean\,absolute\,deviation\,(MAD) = {\sum{\lvert x_i - \bar{x} \rvert} \over n}
\end{align}
$$

Let's calculate mean absolute deviation for `win_by_runs`",067dba39,0.8666666666666667
39950,7a058705183598,0293c506,Generating feature_columns,b0ead917,0.8666666666666667
39956,817449886e2cbd,9ac1f6d3,"We observe that the average accuracy is about 69-70%, I test the classifier below to ensure that it does not simply output the dominant winner class (RED with win rate of 2 in 3 matches)",ea681120,0.8666666666666667
39959,be616f0785c32d,da7a23b5,"
[Go Top](#top)


<div id='PartDuniq'></div>

##### D.2.4 Compiling the unique protein regions where epitopes have been identified from various publications by BFS

Now let us find out the unique virus protein regions where epitopes have been identified from various publications by partial sub-sequence overlap. The difference between this section and D.2.3 is the following: When epitode A and B overlap, and B and C overlap, but A and C do not overlap substantially, in the previous section, they are considered as separate groups as we were trying to find out non-overlapping peptides, while in this section, they are considered to be in the same group as they are in the same protein regions.

These are the unique groups of protein regions where epitopes have so far been identified:


Group 1 'ILLNKHID', 'ILLNKHIDA', 'LLNKHIDAYKTFPPTEPK'

Group 2 'AFFGMSRIGMEVTPSGTW', 'MEVTPSGTWL', 'GMSRIGMEV', 'FFGMSRIGMEVTPSGTW'

Group 3 'ALNTPKDHI'

Group 4 'IRQGTDYKHWPQIAQFA', 'QGTDYKHW', 'QELIRQGTDYKH', 'LIRQGTDYKHWP'

Group 5 'KHWPQIAQFAPSASAFF', 'AQFAPSASAFFGMSR', 'AQFAPSASAFFGMSRIGM', 'QFAPSASAFFGMSRIGM'

Group 6 'LALLLLDRL'

Group 7 'LLLDRLNQL'

Group 8 'LQLPQGTTL'

Group 9 'RRPQGLPNNTASWFT', 'GLPNNTASWFTALTQHGK', 'MSDNGPQNQRNAPRITFGGPSDSTGSNQNGERSGARSKQRRPQGLPNNTAS'

Group 10 'YKTFPPTEPKKDKKKK', 'KHIDAYKTFPPTEPKKDKKK', 'KTFPPTEPKKDKKKK', 'LNKHIDAYKTFPPTEPK', 'TFPPTEPK', 'KTFPPTEPKKDKKKKADETQALPQRQKKQQ'

Group 11 'GAALQIPFAMQMAYRF', 'GAALQIPFAMQMAYRFN', 'AMQMAYRF', 'PFAMQMAYRFNGIGVTQ'

Group 12 'MAYRFNGIGVTQNVLY', 'MAYRFNGIGVTQNVLYE'

Group 13 'QLIRAAEIRASANLAATK', 'QLIRAAEIRASANLAAT'

Group 14 'FIAGLIAIV'

Group 15 'ALNTLVKQL', 'QALNTLVKQLSSNFGAI', 'DVVNQNAQALNTLVKQL'

Group 16 'LITGRLQSL', 'EAEVQIDRLITGRLQSL', 'RLITGRLQSLQTYVTQQ'

Group 17 'NLNESLIDL'

Group 18 'RLNEVAKNL', 'EIDRLNEVAKNLNESLIDLQELGKYEQY', 'EVAKNLNESLIDLQELG'

Group 19 'VLNDILSRL', 'AISSVLNDILSRLDKVE', 'SVLNDILSR'

Group 20 'VVFLHVTYV'

Group 21 'GAGICASY'

Group 22 'GSFCTQLN'

Group 23 'ILSRLDKVEAEVQIDRL'

Group 24 'KGIYQTSN'

Group 25 'KNHTSPDVDLGDISGIN'

Group 26 'AATKMSECVLGQSKRVD'

Group 27 'QQFGRD'

Group 28 'RASANLAATKMSECVLG'

Group 29 'SLQTYVTQQLIRAAEIR'

Group 30 'DLGDISGINASVVNIQK'

Group 31 'GTTLPK', 'LPQGTTLPKG', 'QLPQGTTLPKGFYAE', 'QLPQGTTLPKGFYAEGSR', 'QLPQGTTLPKGFYAEGSRGGSQ'

Group 32 'YNVTQAFGRRGPEQTQGNF', 'ATKAYNVTQAFGRRG', 'KAYNVTQAFGRRGPE'

Group 33 'LLPAAD'

Group 34 'LPQRQKKQ'

Group 35 'PKGFYAEGSRGGSQASSR', 'SQASSRSS', 'SRGGSQASSRSSSRSR','GTTLPKGFYAEGSRGGSQASSRSSSRSRNSSRNSTPGSSRGTSPARMAGNGGD'

Group 36 'AGLPYGANK', 'TGPEAGLPYGANK'

Group 37 'AADLDDFSK'

Group 38 'QLESKMSGK', 'RLNQLESKMSGK', 'LNQLESKMSGKG', 'LDRLNQLESKMS', 'SKMSGKGQQQQGQTVTKKSAAEASKKPRQKRTATKAYN'

Group 39 'GVLTESNKK'

Group 40 'RLFRKSNLK'

Group 41 'QIAPGQTGK', 'VRQIAPGQTGKIAD'

Group 42 'TSNFRVQPTESI'

Group 43 'SNFRVQPTESIV'

Group 44 'LLIVNNATNVVI'

Group 45 'RIRGGDGKMKDL'

Group 46 'LTPGDSSSGWTAG'

Group 47 'YQAGSTPCNGV'

Group 48 'QTQTNSPRRARSV'

Group 49 'VYQVNNLEEIC'

Group 50 'SMATYYLFDESGEFK'

Group 51 'MATYYLFDESGEFKL'

Group 52 'ATYYLFDESGEFKLA'

Group 53 'DSATLVSDIDITFLK'

Group 54 'SNPTTFHLDGEVITF'

Group 55 'NPTTFHLDGEVITFD'

Group 56 'PTTFHLDGEVITFDN'

Group 57 'DGEVITFDNLKTLLS'

Group 58 'EVRTIKVFTTVDNIN'

Group 59 'VRTIKVFTTVDNINL'

Group 60 'RTIKVFTTVDNINLH'

Group 61 'HEGKTFYVLPNDDTL'

Group 62 'EGKTFYVLPNDDTLR'

Group 63 'GKTFYVLPNDDTLRV'

Group 64 'KTFYVLPNDDTLRVE'

Group 65 'DLMAAYVDNSSLTIK'

Group 66 'LMAAYVDNSSLTIKK'

Group 67 'MAAYVDNSSLTIKKP'

Group 68 'AAYVDNSSLTIKKPN'

Group 69 'YREGYLNSTNVTIAT'

Group 70 'REGYLNSTNVTIATY'

Group 71 'IINLVQMAPISAMVR'

Group 72 'VAAIFYLITPVHVMS'

Group 73 'AAIFYLITPVHVMSK'

Group 74 'PDTRYVLMDGSIIQF'

Group 75 'DTRYVLMDGSIIQFP'

Group 76 'TRYVLMDGSIIQFPN'

Group 77 'RLTKYTMADLVYALR'

Group 78 'TMADLVYALRHFDEG'

Group 79 'TKRNVIPTITQMNLK'

Group 80 'YEAMYTPHTVLQAVG'

Group 81 'YDHVISTSHKLVLSV'

Group 82 'SQSIIAYTMSLGAEN'

Group 83 'SNNSIAIPTNFTISV'

Group 84 'AIPTNFTISVTTEIL'

Group 85 'IPTNFTISVTTEILP'

Group 86 'PTNFTISVTTEILPV'

Group 87 'TNFTISVTTEILPVS'

Group 88 'VKPSFYVYSRVKNLN'

Group 89 'KPSFYVYSRVKNLNS'

Group 90 'PSFYVYSRVKNLNSS'

Group 91 'YTGAIKLDDKDPNFK'



Code for compiling the unique virus protein regions where epitopes have been identified from various publications by BFS:



[Go Top](#top)


",b78e18aa,0.8666666666666667
39960,80ecc4c67a9f54,ae4c980a,## Salving new dataset with score values,4bbf546c,0.8666666666666667
39961,892be0a523578c,a46eaca9,"Next, I want to know how much time the regular exercisers spend on exericising. By inspecting the data, I find that it is very possible that the intensity value larger than 1 indicates the exercising moment.",b0e8d7c0,0.8666666666666667
39963,62487bcd70b199,9b9c3cec,## <a id='8.3'>8.3. VotingClassifier</a>,f6ae50af,0.8666666666666667
39965,37e461081e47c5,ebb113d3,# Model 7: Neural Network Sequential Model,b3e6549e,0.8666666666666667
39967,051b118f751e77,e510d275,### Performance comparison based on ROC-AUC Value,9fad25fd,0.8666666666666667
39982,712198370d5521,339f5029,"There has not been an overwhelming response to the campaigns so far. Very few participants overall. Moreover, no one part take in all 5 of them. Perhaps better-targeted and well-planned campaigns are required to boost sales. 
",5882e04c,0.8666666666666667
39987,7454fdc444df16,0a6d870e,#### Percentange of Variance Explained Post Dimensionality Reduction (PCA),a7818ef5,0.8666666666666667
39994,ac1abfe1dfe815,a769c63f,## Decision Tree,6529dbcb,0.8672566371681416
39995,087e21401d7dfc,2e63f785,# Neural Network,42000489,0.8673469387755102
39996,4daf6153275cbf,1d012269,#### 7. Local Differences with Other Countries,51db1961,0.8674698795180723
39997,835a7b4e660d23,a5ad313d,### Change İndex,53bc7a6e,0.8674698795180723
39999,7f74a04ae75792,fb4d52e1,#### Plot By gender,d01e91da,0.8676470588235294
40000,eb0ecd6bebeb15,2bdbecdf,sepal.length'i 5.5'den büyük ve türü setosa olan gözlemleri yazdıralım.,d7b93a60,0.8676470588235294
40001,02b7e38902069e,f43fbd3b,#I included my account name in the doc below:,726a03a0,0.8676470588235294
40002,9cec5ddf8b6f49,d1ec5f67,#### Visualize parameter importances.,d39fc8e7,0.8676470588235294
40004,dc0b0e1cb46c6f,15b5d245,"<a id='4.1'></a>
## <p style=""background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;"">4.1 Confirmed cases evolution by continent</p>",47b17a7b,0.8676470588235294
40006,e4c6dd957eb5ce,19fb3fb8,"Cool! Now, we have:

- Total votes by Users
- Total Unique votes by Users
- Total kernels by User

Using the cols of aggregation, we can also create new metrics to better understand the patterns:

mean_unique_ratio = Unique_user_votes / total votes <br>
kernel_votes_mean = Total_Votes / Total_Kernels <br>
kernel_unique_votes_mean = Unique_user_votes / Total_Kernels <br>

",2e383665,0.8676470588235294
40008,2f47abddfd1928,67db5fdd,"## 4.6. Scale and Dummy Variables

Now that we created all the features we should scale the continuous variables and encode the categorical ones.

The model will work better, faster as minimum, and probably will give us a better result.",ae33cc0b,0.8677685950413223
40009,e9b9663777db82,cd827f4c,#### -LinearRegression with RFE,648e8507,0.8677685950413223
40013,23df07a474aaae,50ffcf2b,**Predicting the Streams**,0ea40276,0.8679245283018868
40014,614ba9f0c62677,82fa04e4,## Implementing with Pytorch Library,b8551335,0.8679245283018868
40015,43e60eb1362f5c,986de6b1,I removed the null values present in all the different types of Delays and proceeded with prediction of the Arrival Delays. ,87934234,0.8679245283018868
40017,f015d0147e8fbf,6c888f59,"### List Features in Order of LightGBM Importance (importance_type='split')

I tried using this to get the top 20/50/100 features of lowest importance, and then remove them from my dataset and see if my local CV score increased. However, doing this never improved my CV score.",518954fb,0.8679245283018868
40018,0ad8d416b89b78,76f4ce94,# Gradient Boosted Descent:,0b0562f0,0.8679245283018868
40019,a070fd03ae8ed2,0acb1d7f,"## 7.3 Метрики, матрицы ошибок и кривые",c0ec4138,0.8679245283018868
40022,a1dcd92986bc84,9fe39ae2,"## Evaluate the retrieval quality

To evaluate the dual encoder model, we use the captions as queries.
We use the out-of-training-sample images and captions to evaluate the retrieval quality,
using top k accuracy. A true prediction is counted if, for a given caption, its associated image
is retrieved within the top k matches.",730acaaa,0.868421052631579
40024,d93a87fdbdb3d2,cef0b320,#### Modeling,30d079c3,0.868421052631579
40027,52ee792e228d54,0bcb39c8,### Logistic Regression,5096094e,0.868421052631579
40029,31b564f11ef638,c895ec4e,### Compare train data vs predicted test data distribution ,424f9692,0.868421052631579
40032,fe7360cddc13e5,c116653e,Model parasallıkla frekans arasında bir ilişki olmadığını varsayar. Bunun kontrolünü yapıyoruz. Korelasyon varsa gamma gamma modelini kullanacağız.,8979e423,0.868421052631579
40040,3c2033cc99c12c,3b900e2d,"**Note:** *In this part i will create a neural network to evaluate the performance*  
<b>Steps of applying deep learning model into prediction<b>  
+ *Design the network structure*  
+ *Choose the proper activation function and loss function*  
+ *Prepare the feature and label and transform them into tensor array*  
+ *Check the device and whether CUDA could be used*  
+ *Train the model* 
+ *Evaluate and test the accuracy*",dfa22a54,0.8686131386861314
40041,dbd96dd275dc60,37c179eb,"Finally now our test df has same features as training df, we can make preds",1ed493a8,0.8686868686868687
40047,918040fad252ec,b68aed43,Menyimpan beberapa indeks yang salah diklasifikasikan,966fcd8f,0.8688524590163934
40048,0858e1bb3cbaca,b35935c4,Here are some questions we can answer from analyzing this dataset with pandas.,78548374,0.8688524590163934
40049,87e94f864d74be,90e3073a,## Top10 Genre in Movies and TV Shows:,294bfe9f,0.8690476190476191
40050,565ad413cd802f,4c85c08d,Let us know create a submission file with these predictions,397b074e,0.8690476190476191
40052,c84925c8171900,78623d15,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.5.6 Publisher wise Game Genre
            </span>   
        </font>    
</h4>",e21ff7ec,0.8691588785046729
40053,d07915a6e6992e,e230e597,"**ExtraTrees Classifier**

ET is a meta estimator that fits a number of randomized decision trees on various sub-samples of the dataset and then uses averaging method to improve the predictive accuracy and control over-fitting.",2b912140,0.8692307692307693
40055,a4f8ad33c823c5,41611fe1,"Heartrate and glucose
https://www.health.harvard.edu/heart-health/hows-your-heart-rate-and-why-it-matters ",fcd48307,0.8692307692307693
40064,ea4e559a86d613,ce3072ee,**Random Grey Scale**,eff47843,0.8695652173913043
40065,17a24d566ffa59,3f1477d0,"### Latent Semantic Indexing (LSI)
Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text. LSI is based on the principle that words that are used in the same contexts tend to have similar meanings. A key feature of LSI is its ability to extract the conceptual content of a body of text by establishing associations between those terms that occur in similar contexts.

The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches. Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.

##### Overview
LSA can use a term-document matrix which describes the occurrences of terms in documents; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is tf-idf (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

##### Rank Lowering
After the construction of the occurrence matrix, LSA finds a low-rank approximation to the term-document matrix. There could be various reasons for these approximations:

**The original term-document matrix is presumed too large for the computing resources:** in this case, the approximated low rank matrix is interpreted as an approximation (a ""least and necessary evil"").
**The original term-document matrix is presumed noisy:** for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a de-noisified matrix (a better matrix than the original).
The original term-document matrix is presumed overly sparse relative to the ""true"" term-document matrix. That is, the original matrix lists only the words actually in each document, whereas we might be interested in all words related to each document—generally a much larger set due to synonymy.
The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

{(car), (truck), (flower)} --> {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with polysemy, since components of polysemous words that point in the ""right"" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.


##### Use Cases
- Compare the documents in the low-dimensional space (data clustering, document classification).
- Find similar documents across languages, after analyzing a base set of translated documents (cross language retrieval).
- Find relations between terms (synonymy and polysemy).
- Given a query of terms, translate it into the low-dimensional space, and find matching documents (information retrieval).
- Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions MCQ answering model.
- Expand the feature space of machine learning / text mining systems 
- Analyze word association in text corpus 

Synonymy and polysemy are fundamental problems in natural language processing:
* **Synonymy** is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for ""doctors"" may not return a document containing the word ""physicians"", even though the words have the same meaning.

* **Polysemy** is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word ""tree"" probably desire different sets of documents.

##### Limitations
LSA cannot capture polysemy (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space. For example, the occurrence of ""chair"" in a document containing ""The Chair of the Board"" and in a separate document containing ""the chair maker"" are considered the same. The behavior results in the vector representation being an average of all the word's different meanings in the corpus, which can make it difficult for comparison. However, the effect is often lessened due to words having a predominant sense throughout a corpus (i.e. not all meanings are equally likely).

SOURCE: https://en.wikipedia.org/wiki/Latent_semantic_analysis#cite_note-38",89049e56,0.8695652173913043
40066,598b6228760590,84f70664,# Model stacking,be30ab66,0.8695652173913043
40070,71c3c1eab0377d,9a008e0e,"**Train the model.**

Note1: We are using train not fit as in scikit learn

Note2: Since we are using N fold cross validation, and the size of data set being small; I am not spliting the the training set,
 instead, using the whole training set.
    
Note3: Will check the Bias and the Variance of the model from our N fold Cross Validation's mean and Std Dev of accuracy score.",52b4e360,0.8695652173913043
40073,73ca9abcc2034e,01ba19af,# **Let's see what are the most popular tickers mentioned in the body text**,cec3446c,0.8695652173913043
40080,4913b61a68d355,588773f7,# Errors Sample,6e269c6a,0.8695652173913043
40085,0e2a23fbe41ca9,052480d0,# Historical Data,64e4762c,0.8695652173913043
40092,90ead00a8ee283,d992d00f,"**Exercise 7**: This exercise is optional. Read the following `SQL` data into a `DataFrame`:

![](https://i.imgur.com/mmvbOT3.png)

The filepath is `../input/pitchfork-data/database.sqlite`. Hint: use the `sqlite3` library. The name of the table is `artists`.",612efa48,0.8695652173913043
40095,738bfced935b69,f9700dc0,"The car price with Manual transmission in the Petrol & Diesel fuel are less than 55000 pound, while the car price with Automatic transmission in the Petrol fuel are less than 70000 pound & in the Diesel & Hybrid & Other fuel are less than 45000 pound, and the car price with Semi-Auto transmission in the Petrol fuel are less than 89000 pound & in the Diesel & Hybrid fuel are less than 47000 pound.",2d3c592d,0.8698630136986302
40098,83df814455f06c,b478823f,### Visualize decision-trees,c9cff71a,0.87
40102,241cf32abb22d8,c964f099,"The above results show that all models perform significantly better with the top 10 features selected by F-Score, than with the full features, at a 5% level of significance.",47157066,0.8701298701298701
40103,4ae464582bac51,1b8dcfc7,# LINEAR REGRESSION AFTER UNDER-SAMPLING,ca6a52ce,0.8701298701298701
40106,75adb7945ef9bd,83ef514d,A little improvement from the earlier model,785c5095,0.8701298701298701
40108,663bbc9eaf267b,dd6867f0,## Gradient Boosting Regressor,32445529,0.8701298701298701
40109,ee23a565163388,0d1ba1a7,The Naive Bayes Classifier predicts the outcomes by calculating the conditional probabilities for each of the target label.,88aacbc4,0.8702290076335878
40110,fdc3afd309b850,868de127,"<a id=""pht""></a>
## 9.2 Pipeline Hyperparameter Tuning",966bde38,0.8703703703703703
40111,9ad9a97e628bfa,19009c57,**마지막으로 Age 의 결측치를 채워보도록 하겠다. Age를 가늠할 수 있는 요소는 가족의 수 또는 Initial이 있다. **,0a7e1136,0.8703703703703703
40117,ac04ba639d1c93,bf2da540,# Same Params for all Types,748059d5,0.8703703703703703
40124,f15eac23fbcc9d,a6c82a89,Submit predictions. ,ea46d8af,0.8709677419354839
40125,56e58d53ac9c57,0193cd6d,"without processing the reviews, we cannot extract information from them, so I will do that part only on Deep Learning model.",90e2ab8e,0.8709677419354839
40126,921fff7d3040db,e7042caa,"|                              | Decision tree | KNN     | Naive Bayes |
|------------------------------|---------------|---------|-------------|
| Accuracy in training dataset | 100%          | 100%  | 100%        |
| Accuracy in testing dataset  | 95.27%        | 95.27%  | 98.77%      |",5f36ced9,0.8709677419354839
40127,16862cb02d73d5,e11aa381,"Yes, from the plots we are able to capture the **sudden spikes, dips in the metrics and projects them**. 

Also the **conditional formatted table** gives us insights on cases like data not present(value is zero) captured as high anomaly which could be a **potential result of broken pipeline in data processing** which needs fixing along with highlighting high and low level anomalies.",d7ffa1a6,0.8709677419354839
40129,ad26c020235dfc,274cbbcf,Define train and test data randomly:,bf766e48,0.8709677419354839
40131,0925f172b5eb74,22b422b3,# Building the model: Efficient Noisy Student,ec34cd72,0.8709677419354839
40136,098fedfcd07456,286b1581,"# Let's Look at Train test Accuracy . 
1. Using the History inbuilt function we can test and train accuracy . ",052ece26,0.8709677419354839
40139,a3e8d6ef4c5188,4c4747be,Our Accuracy is 95%,7c8212dd,0.8709677419354839
40143,0cb456a5456cf9,3faaa08a,# **PART 2 Model Training**<br>训练模型,5701729c,0.8709677419354839
40154,2ada0305b68956,1285b275,### 149. Palette = 'summer',133e26f4,0.8714285714285714
40156,2730840089c8eb,e90276a2,A for loop over a dictionary will loop over its keys,34d27dac,0.8714285714285714
40160,38b79494ac749e,12a33e0a,#### L1,39162a40,0.8714285714285714
40164,4d91e84c564cbe,be03b5d3,**2:** They cannot be modified (they are *immutable*).,355a43e3,0.8717948717948718
40166,9eed0fae1c7958,3a9d89d1,"# test on some images from 

```../input/image-classification/test/test/classify```",3fb1438e,0.8717948717948718
40172,80ad12f326ab70,6f3b88d9,"There is more engagment at the begining of the week then it slowly drops as it gets to the weekend. Tuesday has the highest number of average engagment
",da404a16,0.8717948717948718
40174,c8c4705cca1ebb,0d9786e5,# 3. Root Mean Square Error,6d9d7107,0.8717948717948718
40181,49ee86d074de69,7f78ddae,"* Reason_1 : various diseases
* Reason_2 : pragnancy and giving birth
* Reason_3 : poising
* Reason_4 : light diseases",71ccc6d3,0.8717948717948718
40182,897ca904b74a98,bb26fc13,## Precision / Recall Curve,c5844ad4,0.8717948717948718
40184,c09fac3c943d51,3c7539eb,# Meta-models,678d076d,0.872093023255814
40185,ba4b3bd184acbb,23ab082e,"### Well Liked Apps

The second analysis will determine all apps that have a rating of 4.5 or higher and rank them by Sentiment.

We can start by reading from the HDF file.",0f5de724,0.8721804511278195
40186,396bc36edb95d3,29c82954,#### Confusion Matrix and Classification Report for Testing data - ANN,965e4f8f,0.8722222222222222
40187,957e035ba5b9d5,e5fcc796,## Create generators,778ab3d3,0.8723404255319149
40190,5f674175839b32,c3fec6ef,"Q2)In which Year the video game industry experienced the most sales?

Ans:IN year 2008 the this industry experienced highest sales with 678 millions.",53a2e343,0.8723404255319149
40192,f6648e47713411,e8d79dac,### 4.2 Train model,f4af4d1c,0.8723404255319149
40198,726833f92fb87a,f43acaa6,## Train - Test split,7dc5e1b6,0.87248322147651
40199,52cfd66e9ec908,0e4ba9cb,"Few things:
+ Configuration of the data loaders
+ Beginning the Catalyst process
+ Neptune, wandb  initialization",c74adcdf,0.8725490196078431
40207,f0fab078f8533b,7e886319,## f. A function 'genre_actor_or_director' with i/p parameter the dataframe name of actor / director and a bool value indicating Ture for actor or director rspectively plots a pie chart with the total number of movies / TV shows done in each genre by the actor/director,bdb5ea32,0.8727272727272727
40216,b01ee6cb674fa3,478328d6,"case the procedure above indicates AMBA's length > 0, use the procedures bellow",a8ffd35e,0.8731884057971014
40226,5d2a3e82679cf3,4db8cc66,# ELASTIC NET REGRESSION,9e60b1e3,0.8734177215189873
40235,98a6794067932a,c939d656,"La cellule de code ci-dessous permet de créer une liste regroupant tous les clients n'ayant pas effectué une commande au court des x nombre de mois et d'années désirés. Le code a été créé afin de permettre à l'utilisateur de saisir manuellement le nombre de mois et d'années auxquels il veut reculer dans le temps afin de constater les clients qui n'ont pas effectué de commandes auprès de l'entreprise dans ce laps de temps. La date de référence afin de reculer dans le temps est donc la date la plus récente de la base de données. Dans un premier temps, le code établit que la variable 'date_today' équivaut à la date la plus récente de la base de données. Ensuite, le code permet d'évaluer en fonction des paramètres saisis manuellement dans la fonction (nombre x de mois et y d'années dont l'utilisateur veut reculer dans le temps) quels identifiants clients se retrouvant dans la base de données complète n'ont pas effectué de commandes lors de cette période de x mois et y années. Dans un troisième temps, le code associe les clients n'ayant pas effectué de commandes au cours de cette période de x mois et y années à un dataframe. Finalement, le dataframe est représenté sous forme de tableau à une colonne. Les paramètres x et y représentant les mois et les années ont été configurés de cette manière afin que les dirigeants puissent évaluer plusieurs scénarios à leur guise. Cette analyse peut être utile afin d'avoir la liste des clients n'ayant pas effectué de commandes au cours d'une certaine période de temps. En ayant cette information entre leurs mains, les dirigeants de l'entreprise peuvent évaluer s'il s'agit de clients majeurs et si des démarches doivent être faites afin de regagner la confiance de ces clients. Cette analyse sera également très utile par la suite afin de représenter graphiquement ces clients perdus.",08600fe2,0.8737864077669902
40239,c4386b8a01d66e,0a78b145,### Decision Tree,dc732bf5,0.8739495798319328
40244,fc8e0042411c46,497c49bd,# Model Building,af476c2a,0.8746081504702194
40249,436ceac778d184,f2a6be52,"# <sub>3.</sub> <span style='color:#F7765E'><sub>SOUNDSCAPE INFERENCE (TRAIN/TEST)</sub></span>
- <b>Function <code>soundscape_records</code> requires:</b>
    - pathway to <b>soundscape</b> file.
    - model to be used for <b>evaluation (predict)</b>
    - submission option allows one to quickly switch from <b>soundscape investigations/confirmations</b> to <b>submission format</b>
    
    
- <b>The function does the following:</b>
    - Firstly, clears common/temporary <code>mel_soundscape</code> folder used for <b>spectogram</b> export for each individual soundscape file.
    - Reads soundscape audio using librosa & temporary store 600 segment spectrums in folder (each image is a 5 second segment); <code>mel_soundscape</code>.
    - Keras <b>Dataloader/Data Generator is created</b> for the ""test"" set of 600 spectrum segments.
    - <b>Inference is conducted</b>, using <b>imported model</b> using ""test"" data generator (w/ standard augmentation), <b>results (probability)</b> are stored in a common array and extracted individually. 
    - For each segment, if a probability exceeds <b>a threshold</b>, the label corresponding to that probability is stored in dictionary, <b>data</b>. If none of the probabilites exceeds this threshold, a no bird call result is stored <b>nocall</b>.
    - Individual soundscape results are stored in local DataFrame and passed via return, the results for all possible soundscape files are then stored in a unified DataFrame <b>(df_allres)</b>, which is used for submission.
   
   
- <b>Unified submission option (submission=False)</b>:
    - <b>False</b> is used for different training soundscape investigations, eg. comparison of birds correctly predicted / all available birds ..., 
    - <b>True</b> option is used for creating a submission...",db256ccd,0.875
40259,593d1d3d1df05a,3f22ced1,# Seeing the output through Pytesseract,bc682ffe,0.875
40260,166a62ebb4fc3a,04ea0d2f,Creating confusion matrix,db48a079,0.875
40265,73d8e56bc709b1,9fa58f23,"Value and Wage are symmetric about y = x. And most players got value less than 2,000,000 pounds, and wage less than 150,000 pounds.",78ec3cce,0.875
40267,1d5daeca89f48d,b7e2b5c4,"##  After vectorization

    Numeric data
    Tabular format
",48d478bc,0.875
40273,49f2274c1dd516,2dc37667,"# New York Times
The New York Times is releasing a series of data files with cumulative counts of coronavirus cases in the United States, at the state and county level, over time. We are compiling this time series data from state and local governments and health departments in an attempt to provide a complete record of the ongoing outbreak.",06b0ffee,0.875
40277,117fc0956643d0,d12a7f51,"<div id=""step6""></div>",68cef9fd,0.875
40280,3cc097a5859dc1,8ab2fe82,# **Applying SVC Model**,14380d73,0.875
40283,8c7e00ca3dc5a7,a27f2252,"
## Applying Regression Algorithms
Now is the time to apply the regression algorithms! Following algorithms are tried the ElasticNetCV gives the best result.

* ElasticNet
* Ridge
* Random Forest",c83346e4,0.875
40288,f13534449a3750,e7612b7a,"**Split into training and validation groups**
",8b7f3332,0.875
40289,69130a37583a06,0d585b33,### Plotting R_emailDomain :,65a4de1c,0.875
40290,3dd4294f903768,2c7c067e,# Desicion Trees,0d89d098,0.875
40297,fbe822854174fe,5bf03eb5,"model=svm.SVC()
model.fit(x_train,y_train)
prediction2=model.predict(x_test)
print('The accuracy of Aupport Vector Macine is :', metrics.accuracy_score(prediction2,y_test)*100)",1f8e0bdb,0.875
40304,2a377ced98d67a,09c12904,"* The Explained variance regression score is around 0.8167, the best score is 1, lower values are worse.
* Mean Absolute Error in 0.0431 in range of 1, around 0 is best, this was pretty close.
* The lower the Mean Squared Error and close to 0, the better, this model attained around 0.0013
* Median Squared Error is around 0.0313.
* R2 Score close to 1 is best, here this model attained 0.8163.",262231a8,0.875
40305,fdc3afd309b850,2cd552c8,"To choose the best model we will try **Lasso (l1)**, **Kernel Ridge (l2)**, **Elastic NNet**, **Xgb Regressor** or **Lgbm Regressor** . We are going to create a **Pipeline** with **Hyperparameter Tuning** using **Grid Search** and 10 **K-Folds** shuffled. The scored used will be **neg_root_mean_squared_error**. Also for the Grid Search we are going to scale and normalize our dependent variable (y_train).",966bde38,0.875
40306,eb0854a6601407,6dc4410c,"### Features interaction
We will do analysis on a smaller random 1% samle of the dataset to speed up the process.",6d107747,0.875
40308,08e3444f9eddcf,2d841897,# Submission,1d9d4f73,0.875
40315,96c4c0e36b8ec0,8f10deb1,"**Swarm plots of class against fare, and swarm plot of fare against survival,colour coded by survival **",4dd6de8c,0.875
40322,1c5aaf7bea6414,baf835eb,# Labelling RFM Scores,34d8f42d,0.875
40325,a3ae04b78e45b5,b4af667b,**PAIR PLOT**,4195da8b,0.875
40326,5626e84c4e6bf8,0aaecfd2,"As weights are random, every time the map is generated different. I got great distinguished clusters while I was coding with little overlapping. Overlapping should be minimal but minute to no overlapping means the features are very distinguished and images of both labels have little to nothing in common which is not really true in this case.

Now, I will try optimizing two parameters with Hyperopt.",e2ecb669,0.875
40327,44f6a002ecd033,9fa0d76e,### Log Dataset Modeling,70bbe106,0.875
40330,1011899b959f44,aa1aa94e,"11. Display a pie chart to distribute the number of deaths.

From the graph, it is clear that there are more battles in which there are no deaths than there are a maximum of one.",0b112382,0.875
40331,68cceffe5bb8ec,90c69ad6,# Visualize curves,dcbfcd6e,0.875
40345,9ec2fb131cf677,dbe62dc9,"<table style=""border-collapse: collapse; width: 100%; height: 100%; margin-left: 4.6500pt; border: none;text-align:center;"">
<tbody>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; background: #ffd965; border: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 14.0000pt;"">Event</span></strong></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: 1pt solid windowtext; border-bottom: 1pt solid windowtext; background: #ffd965;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 14.0000pt;"">Count of Speaker Occupation</span></strong></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: 1pt solid windowtext; border-bottom: 1pt solid windowtext; background: #ffd965;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><strong><span style=""font-family: Calibri; color: #000000; font-size: 14.0000pt;"">Views</span></strong></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2007</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">27</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">15,178,044</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDNYC</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">19</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">23,612,018</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2002</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">28</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">26,641,297</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED Talks Live</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">20</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">29,917,721</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDWomen 2016</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">25</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">33,508,916</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2003</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">34</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">37,780,871</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDWomen 2015</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">28</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">40,597,242</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2005</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">26</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">42,615,627</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDWomen 2010</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">34</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">44,134,937</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDIndia 2009</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">35</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">47,567,971</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDxBeaconStreet</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">22</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">48,564,891</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDSummit</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">34</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">52,018,727</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2005</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">37</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">66,088,906</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2014</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">50</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">67,124,493</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2010</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">55</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">73,640,042</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2017</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">67</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">79,613,259</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2004</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">31</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">83,502,957</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2007</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">68</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">102,756,885</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2009</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">65</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">109,136,352</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2008</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">57</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">116,202,871</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2011</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">67</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">116,807,745</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2010</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">68</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">122,268,845</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2011</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">70</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">137,750,504</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2016</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">77</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">139,571,905</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2012</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">64</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">144,497,608</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2012</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">70</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">145,070,549</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2009</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">83</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">145,656,385</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2006</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">45</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">147,345,533</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2015</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">75</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">150,826,305</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TEDGlobal 2013</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">66</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">170,554,736</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2014</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">84</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">174,121,423</span></p>
</td>
</tr>
<tr style=""height: 14.3000pt;"">
<td style=""width: 30.5147%; padding: 0.75pt; border-left: 1pt solid windowtext; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""178"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">TED2013</span></p>
</td>
<td style=""width: 46.875%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""263"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">77</span></p>
</td>
<td style=""width: 22.4265%; padding: 0.75pt; border-left: none; border-right: 1pt solid windowtext; border-top: none; border-bottom: 1pt solid windowtext;"" width=""164"">
<p style=""text-align: center; vertical-align: middle;""><span style=""font-family: Calibri; color: #000000; font-size: 11.0000pt;"">177,307,937</span></p>
</td>
</tr>
</tbody>
</table>",211ea6bd,0.875
40346,1014e6be391084,41b52c65,# Drawing confusion matrix and AUC ROC curve,46f9168f,0.875
40351,923e97b05be00b,96e67c10,"This method below will show up heatmaps to make sure that the model is looking at the right thing! It is also worth stating that this particular dataset is not without [criticism](https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/), but the principles here can be applied to anyone hoping to develop a machine learning model that examines medical images.

>We can see that the model is often misclassifying images by looking at the wrong thing. In a subsequent kernel we can look into how we can fix this.",3a4a22dd,0.875
40352,fdbbd573ba31c2,b864e244,## RandomForestRegressor,f7c28d74,0.875
40353,51a46d0a7597f5,87f1754c,"<br/>
<A name=""section1.4"">4. Clubs with the highest median wages (Top 11)?</A>",e9e25b17,0.875
40357,e3c0b55ed519e2,c1497dd5,# # Locations with *Highest* ICU beds per projected infected individuals who need ICU care.,9f51352e,0.875
40365,0932046e1f485d,f922f0c0,Neutral reviews WordCloud,218cc7a3,0.875
40366,fae5023faa435f,c48ba609,# Result Visualization,b37c893b,0.875
40369,c80939c7c626cf,c457ca72,# Score,b9ac31e2,0.8759124087591241
40373,a5a419dc7245b0,99fe68bf,"##### Making the Confusion Matrix
",4279726e,0.8761061946902655
40374,c9b4e282e4e2c1,82145f04,I'm going to merge this dataset with InjuryRecord.csv.,f44d339f,0.8761061946902655
40383,063a35f644e3c5,b5e4e625,"### Also, create a model to predict the City Mileage per gallon(City MPG) using the best features.
",1c30fb0a,0.8762886597938144
40385,04ff2af52f147b,4ec053aa,"**Create Model:**

Now that we are finished with preprocessing the data, we can begin to initialize our model.  We begin by defining our training, validation, and testing features/targets.",d5f37be9,0.8764044943820225
40388,faa8e6c8ab9246,6cf87d86,Feature Scaling is used to standardize the independent variables present in the data in a fixed range.,2bea1419,0.8765432098765432
40389,4883314a96dc34,5b7bd025,## Iteration (3),50d36836,0.8765432098765432
40400,a8c042af6b7245,3caabdaf,"We would lose rather many variables if we would select based on variance. But because we do not have so many variables, we'll let the classifier chose. For data sets with many more variables this could reduce the processing time.

Sklearn also comes with other feature selection methods. One of these methods is SelectFromModel in which you let another classifier select the best features and continue with these. Below I'll show you how to do that with a Random Forest.",2487ac62,0.8769230769230769
40401,1645979263c148,982bd43f,"## Receiver Operating Characteristic Curve (ROC AUC)

here we will be using many algorithms and compare all of them. which algorithm will be giving us a Better result. The following algorithms are below.

1. Logistic Regression (auc: 0.9997333333333333)
2. k-nearest neighbors (auc: 0.8948933333333333)
3. **naive bayes (auc: 1.0)**
4. support vector classification (auc: 0.7623200000000001)
5. DecisionTreeClassifier (auc: 0.99892)
6. RandomForestClassifier (auc: 0.9998933333333333)",fa11663e,0.8769230769230769
40403,09751c520b0616,5b8eed01,### Prediction accuracy,a4d0c7e9,0.8769230769230769
40404,03048e86a6d806,0f9bbcaa,"Data folks use Regression and Decision Trees/Random Forest the most for any job title. The next mostly used algorithms are the neural network groups, such as DNN, CNN, and RNN. For Machine Learning Engineers, CNN is the second most regularly used ML method.",1285c231,0.8769230769230769
40406,f2f2db16a2f86c,ad91e3b1,"Hence, the prediction by this model is on an average off by this value",ffc6a115,0.8769230769230769
40413,2ada0305b68956,b9561605,### 150. Palette = 'summer_r',133e26f4,0.8771428571428571
40415,30fdc4a6e3c1db,64989470,"What we see:
* Weekends before all the event types have much higher sales than the avg. sales per day
* Religious events have the highest impact to sales of the preceding weekend 
* National events have the lowest impact to sales of the preceding weekend",6111ddee,0.8771929824561403
40416,9f3710be6aea65,2b553919,"## Support Vector Machines: 
#### Support Vector Classifier",ae9bda88,0.8771929824561403
40425,5ce12be6e7b90e,42636d74,"In this dictionary, the _keys_ are the coutnries and the _values_ are the capital city. Both are of type `str`.",c0ab62dd,0.8771929824561403
40430,2343dc02ffb96a,74c83f56,# Now try on test data.,29aa95a4,0.8775510204081632
40437,087e21401d7dfc,d587df82,"Though we already got a good accuracy for our model but lets try neural network.
My main aim here is to explain how neural network works. It is not necessary that neural networks are always better than other models.
We will just see the implementation on Artificial Neural Network and test whether it performs better or worse. Ususally ANN needs huge data but lets apply.",42000489,0.8775510204081632
40442,fc8e0042411c46,45be23a6,# kNN (k- Nearest Neighbors),af476c2a,0.877742946708464
40443,3597174a998d4d,d3aa2d2e,# 3. Modelling Details,276892ed,0.8777777777777778
40452,8cefb86a675e5d,198e9039,**For comparision: What is the mean of the expected target value in test set ?**,79f9e69b,0.8780487804878049
40455,fd4017c1514157,4af24f11,"#### <font color = 'orange'>Waveplots</font>
* Waveplots let us know the loudness of the audio at a given time.
* librosa.display.waveplot is used to plot waveform of amplitude vs time",fd8f0896,0.8780487804878049
40456,8d70dcae7f40a3,01373efd,"### **Giá trị AP (Average Precision)**

#### - AP là một giá trị đánh giá chung đường của chúng ta. (Nó giống AUC của ROC)
#### - AP càng gần 1 thì Precision và Recall càng ổn định.
#### - Có thể hiểu AP như độ chính xác trung bình của mô hình. ",472c71ce,0.8780487804878049
40460,0e09587faffa8f,ba4bed1f,The maximum number of tickets were issued for **overspeeding near school premises** violation with vehicles registered in NY ,0d563d61,0.8780487804878049
40461,786475feda0190,f68ffa9c,### Cleaning Audio Files,e4663d97,0.8780487804878049
40463,5169abdc647412,81fbb8c3,R2 score is not same ,28efc68d,0.8780487804878049
40465,47b2c9be5e31cb,7aa2e505,Correlation matrix:,7d4afe56,0.8780487804878049
40467,62037c5832129c,db6926cd,# Debugging algorithms with learning curves,61474350,0.8783783783783784
40468,63b44c85e32c1f,ab840a38,**add( )** will add a particular element into the set. Note that the index of the newly added element is arbitrary and can be placed anywhere not neccessarily in the end.,fb9b9562,0.8783783783783784
40470,e4525eb0c96f28,34c1502a,"Still, it seems that we get the same result. This new plot looks pretty much identical to the previous violin plots. Our suspected problem might not have been the real issue after all. It also makes sense why the plots turned out to be the same because, if you noticed from the scatter plot, there is not that much data on games before 1996. Therefore, ommiting the data before 1996 would not have influenced the model all that much. 

Nevertheless, our residuals are still as unevenly spread as before. This suggests that we might need a different kind of regression model instead of a linear one. Instead, we can definately try a Random Forest Regression. For this, we will be using the respective RandomForestRegressor module from Sci-Kit Learn. 

A Random Forest Regressor is a forest of multiple decisions trees that predict values based on decisions. Imagine each node as a decision you have to make based on your independent variable value. From there, you would have to pick a route to go on based on the decision you made, which leads to another node. A decision tree would make predictions by going through each node's decision, following the paths based on those decisions, until finally landing at a leaf node, which represents a prediction.

The problem is that a singular tree can often times be too brittle. If one were to make a decision tree model, one would have to figure out the decisions that have to be made first: the most important and significant decisions. For us to be able to figure out what the most significant decision is, we would have to maximize information gain. However, this will lead to being more prone to overfitting the model. Therefore, our solution to this is to use multiple trees. 

To make sure the trees in our random forest are not all equally brittle, we can use a certain subset (with overlap) of our data for each tree we make. Sci-Kit Learn has a way of not only doing this, but also allowing us to specify the maximum depth of each tree, the number of trees in our forest, and the number of samples we will allow for the making of each tree. For our purposes, we will be using 5 trees and a maximum of 3000 out of the ~9000 samples for each tree, since the module's default of 100 trees would have been really expensive to compute and would not have made that much of a difference. We will not be specifying the maximum depth of each tree simply because we want the leaf nodes to be pure values.",2093a1f1,0.8783783783783784
40478,3793c438a71b52,e95d4299,**Ridge Regression**,13eb76df,0.8787878787878788
40481,70193f0c034b98,facdb54e,# Plotting curves function,f8cacd26,0.8787878787878788
40484,4b64dc653fb7eb,8f06c444,Merging comments and labels for training data set and ids for test data set.,57675cc2,0.8787878787878788
40487,5f4ae633cfd090,fca6a665,***Splitting the data in training and testing data set***,a30a16e2,0.8791208791208791
40488,5f32117bcd5255,558e65a2,#### MEIJERING,85882abf,0.8791946308724832
40491,e3f3f108cd3869,4a3d07a7,**final Prediction**,2b78de2d,0.8793103448275862
40494,1750367e54f407,149dc9b0,We can also have a better understanding of where this new model misclassifies diseases by plotting a heatmap from the results. Each row in this heatmap is normalised to highlight the classification distribution per disease without being bothered by the fact that the dataset is imbalanced.,a8e655b2,0.8793103448275862
40496,00001756c60be8,2e1a5949,Оценка модели,945aea18,0.8793103448275862
40497,a1ba5ffd30dbde,c2b5cd96,<h2 align='center'> XG Boost </h2>,48e57546,0.8793103448275862
40503,84127ade6fde87,607ee4cd,## Text embeddings as a blueprint,f55d05b6,0.8793103448275862
40506,b61ab8f81dc03d,1bc9b8f4,"<a id=""checking_accuracy""></a>
### Checking Accuracy",64d05394,0.8794326241134752
40508,4daf6153275cbf,47a084a5,"In this section, I will map the differences of price and rating between local countries and other countries. Calculations are made like Local minus Others.

This price difference map below shows that, Mediterranean-European cuisine is paid higher in other countries, so is Danish and Norwegian food.",51db1961,0.8795180722891566
40510,ab6da5994949a3,ea5dd400,## Visualising the Decision Tree Test set results,fae6b91d,0.8796296296296297
40513,0687cd5c8597db,6c804fa1,### **Predicting the digits in the ramdom images and displaying it on the X-axis**,4edec76a,0.88
40514,5cb7f999fd1ecb,7d4f1d56,# Folium - Map,88b54f70,0.88
40532,9395559895004f,5342845f,## Evaluation,b5a0494b,0.88
40534,91eaec994e0c6f,5c3ddb94,## 3.3 FB Prophet,376aef10,0.88
40537,cb570c7b7f0501,99d8620c,lets check random data and let's pray that we get patients with more than one appiontment and were absent in the first one,a200a0ec,0.88
40538,b10bd75889dad9,fb5d24e6,### Finding Optimal Cutoff Point,ee00ceee,0.88
40539,519e936017c30a,ea23aa33,"Para finalizar nuestro estudio, vamos a realizar una comparación entre las plataformas ""XBOX 360"" y ""PS3"",donde examinaremos cual de las dos plataformas ha generado mayores ingresos por zona geográfica.",dc34915d,0.88
40545,7e1da639035ac5,6ec2233e,### <a id='15.2'>15.2 Trust ratings statistical analysis</a>,120b6c23,0.88
40547,7dd46c750653eb,ee92c457,"**Inference**

* Over the years number of adoptions are gradually decreasing ",c2644713,0.88
40548,21c1e34efd71b8,c3abbd30,I would like to see if the Logistic regression is really the best model when we use NearMiss CV.. Lets find out..,23b2cdd6,0.88
40549,49ee86d074de69,855f82dd,"### Test The Model
* Often the test accuracy is 10-20% lower than the train accuracy (due to overfitting) ",71ccc6d3,0.8803418803418803
40556,e58e68e4eeefe5,aa5fb1bd,"**features = {ejection_fraction, serum_creatinine, time}**

The accuracies over here are slightly corrected as it is tested on sampled data.

* Logistic Regression --> 90%
* Random Forest --> 90%
* Gradient Boosting --> 95%",a87662ce,0.8805970149253731
40560,fc8e0042411c46,068b33d8,"It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors.",af476c2a,0.8808777429467085
40562,27778055896e17,269e8594,# Ada Boost with Logistic Regression,1dbe0165,0.8809523809523809
40563,2d40f383473fa4,9e9564ea,"Don't get any improvements, althought the lesser AUC score, the model performed well with only the 6 top features.",1da1eff0,0.8809523809523809
40566,916ccf243827f1,271cd4be,## 14. Neural Network Model,5147f4d2,0.8809523809523809
40575,a76e0e8770b7a0,a18c46ab,Before 2000s America countries has more trouble with terror after old World has smthng to ,02863d3b,0.8809523809523809
40576,898d18d501f68d,e9dd0507,in  the above plot we can obsere that the cover type 2 is is in every area.,d8bdea2d,0.8809523809523809
40577,31268b33de97b5,811954fe,# Box Plot to compare categorical and continuous Features,1e6f7d14,0.8809523809523809
40582,eda49464dd6d1b,e4cb7d46,## Confusion matrix,8421f81f,0.8811188811188811
40584,149cb8d3489224,9e29083b,## Sentiment analysis,116858e7,0.8813559322033898
40590,1294fb4c86f993,1465066c,"In the figures above we got overshooting values of `guns_per_thousand`, I would like to eliminate those to get a better look at the correlations.",4471e513,0.8813559322033898
40594,ed8009f482b380,4330096e,Use classification report to determine which model fits this project best.,e99941fa,0.8813559322033898
40596,f2e5e9fb9eaaf7,6c104556,"<a id=""6.2.5""></a>
### 6.2.5 Multiplication of features
Create a new feature `multiply` that multiply all features values in a row. Adding up `multiply` column increase the model to `0.8019` from `0.8015`.",048e0d08,0.8813559322033898
40597,a81661cc35d8d2,d5811b5a,"<b><font size=""4"">Random forests with class weights</font></b>",3331f113,0.8813559322033898
40599,0caaec057f7184,97d2139e,"## Negative sales
- 3511 / 22K items have returns
- 7259 kinds of varations on items+month+shops
- distribution in each category",b875533e,0.8817204301075269
40600,ce9ed5e2d601d7,abbdfad1,## Unknown Cover Type prediction,f58a2f43,0.8818897637795275
40603,16ca1123840e9f,0ff092f8,Majority of the organisations haven't disclosed their mission cost. Only 964 mission cost is available. Let's do the analysis with the available data.,e8b8f086,0.8823529411764706
40604,1d1598b6fa2aa7,d46451b0,### Visualizing MRU Volume slices,e066accf,0.8823529411764706
40605,7f74a04ae75792,2e78503e,#### Plot By Customer Status,d01e91da,0.8823529411764706
40613,c65a65d4041018,c3442e2a,"It is interestig that images are relatively less used in America, than in other countries.",824fb229,0.8823529411764706
40615,4ae6a182abac64,9ea5e37b,* **Precision_score**,418676c5,0.8823529411764706
40619,6aaee7fdbc7945,f7c4500b,# **Question 5**,dae653ae,0.8823529411764706
40620,55c34673c1f760,5f48a10b,## Loading,2663c47f,0.8823529411764706
40621,a871419285588a,602c4673,## Visualize some tumors,5e08e15f,0.8823529411764706
40622,7cfd96218dd933,46acdbfa,"#### **ATTENTION**
* THE POINT WITH FRP LATITUDE: 40.057
* THE POINT WITH FRP LONGITUDE: -121.374
* THE POINT WITH FRP DATE: 2021-07-20
* THE POINT WITH FRP TIME: 16.40 FOR TURKEY",7c34d96c,0.8823529411764706
40625,a0a5baa6c7e12a,f4cd7301,"****# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">Tackling Imbalanced Target Class Labels</div>

Since we detected extremely imbanalced target class labels, we should take it into account when building ML models down the road. We can choose from one of the conventional methods below

- undersampling
- oversampling (like using SMOTE etc.)
- assigning the differnt class label weights in the ML model training
- Etc.

**Note:** In a nice discussion thread per [Imbalanced classes vs. imbalanced cost](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/294305) it is presented with the arguments on why resampling methods are not likely to work in a less efficient manner vs. class weightening and other model-level tweaks.",551d41de,0.8823529411764706
40631,4fa553c2b837d4,1da99382,"# Partial Dependence Plot

We'll start with 2 partial dependence plots showing the relationship (according to our model) between Price and a couple variables from the Housing dataset. We'll walk through how these plots are created and interpreted.",c65a23e9,0.8823529411764706
40636,6b7c80ed7bd03d,8397c378,"# 5. Summarizing
Now we have finished transforming our dataset to participate in this contest.  
A fully converted dataset is further provided in <a href=""https://www.kaggle.com/yeonghyeon/rsnamiccai-btrc2021"">RSNA-MICCAI BTRC2021</a>.  

If you like this source code, please upvote.  
I hope you get the results you want.  ",7bba27db,0.8823529411764706
40647,869a39a3d4dea2,c0fc00ca,"More light red color is present in the image, which is clearly evident from the hist of red for pixels 240-256 and other channels are equally distributed on the dark range",9020daf8,0.8823529411764706
40653,99821bc6a45be6,f263c89f,### Training:,b9d59346,0.8823529411764706
40655,0e662a463309e7,1a2e08db,Linkage Dendrogram,84c57e51,0.8823529411764706
40657,2ada0305b68956,a2d15d1e,### 151. Palette = 'tab10',133e26f4,0.8828571428571429
40660,4c47839b067546,ab65099b,# 5. Построение моделей ML,1f517b02,0.8829787234042553
40661,5ce12be6e7b90e,b8778233,"### Access
Accessing a dictionary record is similar to what we did with lists, only this time we'll use a _key_ instead of an _index_:",c0ab62dd,0.8830409356725146
40662,30fdc4a6e3c1db,7776957a,### Plotting sales of weekends preceding each event,6111ddee,0.8830409356725146
40663,9ceb7278784462,f52927fd,## Model Tuning,3768a567,0.8830645161290323
40670,c13f73168789c2,3572f789,"### 1.3 Select rows whose column DOES NOT contain specified values<a id='33'></a>
Syntax : `df[~df.column_name.isin([value])]`",16175052,0.8831168831168831
40673,3c2033cc99c12c,c11d5874,#### Prepare the data of the network ,dfa22a54,0.8832116788321168
40680,f6488772605bb5,3f08426e,## **Saving and loading the model**,068d4697,0.8833333333333333
40685,5b92c712910a11,cf6c7212,# Word Cloud ,e1d17100,0.8833333333333333
40692,738bfced935b69,fa31341f,"The distribution between price & mileage is skewed to right, and notice the Semi-Auto transmission Most densely distributed.",2d3c592d,0.8835616438356164
40693,1660daf8867980,29ae8809,**Demonstration**,42d7cffc,0.8837209302325582
40696,22ba3a8149c2f1,1a30ba35,# 4 Submission,19c82be5,0.8837209302325582
40700,22bd95f4807a23,f71ad2c6,"* Trendy clothes are viewed highly by individuals from ages 30 to 35.
* Intimate wear and Outerwear is viewed positively by individuals aged between 18 to 25.
* For individuals between ages 80 to 90 , knitwear and fine gauge wear were viewed positively.
",c05d356f,0.8837209302325582
40701,806ce45c8fa303,0ea7c35d,"## Linear Classifier

Now let's apply linear classifier to classify the data and observe the result. We will use **Logistic Regression** to build the model.",3e5c34dc,0.8837209302325582
40706,a1a31459abf078,511ba5a2,"Features related to mean & deviance of responses from users and question are most important for the model. Some of the flag features around tags, bundles and lectures don't do so well. Its likely that information from these features has already been captured in the features above. 

We see that our XGBoost model performs much better than Random Forest on validation dataset. Therefore we would be using the XGBoost model for final predictions. ",66fc0f54,0.8840579710144928
40708,0e2a23fbe41ca9,e378c04a,"## Sanity Check

- All ```card_id```s in training and testing set
- All ```merchant_id```s in merchant file
- Nulls
",64e4762c,0.8840579710144928
40711,598b6228760590,d970e10b,"- Combine the previous models, and use the logistic regression model for the final base model to see the final effect.",be30ab66,0.8840579710144928
40718,f91f58d488d4af,c47fb736,"Now, lets do for few more epochs",5df1bbf3,0.8842105263157894
40721,e9b9663777db82,c8c680ea,#### -LinearRegression with Mutual Info Regression,648e8507,0.8842975206611571
40723,a915263bc207da,7ed493d3,Looks like the user likes science fiction/action/fantasy movies and hates romantic movies! Lets see how well the recommender performs - (We'll look at the top 10 movie recommendations from the engine excluding the ones rated or watched by the user),b17ebcda,0.8846153846153846
40724,e7237da7cbec10,d2e6a009,"Standardize 정규화 데이터 적용결과
",5fcf5e3d,0.8846153846153846
40726,6f1481148352e9,3a14655a,**It can be seen from the graph that in different semesters the largest number of fires in different states.**,7cfbdb8f,0.8846153846153846
40732,d0f6276d5b628c,cf150de4,As they are giving reviews on more than 4 contents we are predicting he/she is amazed with the content and also the platform. If we can produce the content similar to these people's liked list that would be nice for growth of the platform i.e. for Amazon.,c64f5ce5,0.8846153846153846
40734,3f451680b1857b,cacea31c,"# Selecting Models
In this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models/yolov5s.yaml --weights yolov5s.pt` with the following command:
* `v5s` : `--cfg models/yolov5s.yaml --weights yolov5s.pt`
* `v5m` : `--cfg models/yolov5m.yaml --weights yolov5m.pt`
* `v5l` : `--cfg models/yolov5l.yaml --weights yolov5l.pt`
* `v5x` : `--cfg models/yolov5x.yaml --weights yolov5x.pt`",56c45a1b,0.8846153846153846
40747,aae204e78a48d1,60b51e5c,"# Hypothesis 5: the attrition base have been inactive for longer than the customers who remain
Given the the attrition base is utilising the card less, do they have a sustained period of inactivity?",53ab6133,0.8846153846153846
40750,09751c520b0616,db09b07f,- r2_score,a4d0c7e9,0.8846153846153846
40753,897ca904b74a98,f3428d5c,We will check the precision/recall curves in respect of the threshold and keep the best threshold,c5844ad4,0.8846153846153846
40754,1bd6cc83c02681,d04dc796,# Let’s Recommend some movies!,17ff92f0,0.8846153846153846
40757,af6556ced704f6,76692d58,***Plotting data***,881577c0,0.8846153846153846
40758,d07915a6e6992e,bc96ebec,"**Random Forest Classifier**

Similar to Extra Tree Classifier a Random Forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).

How ET differes from RF - 

1) When choosing variables at a split, samples are drawn from the entire training set instead of a bootstrap sample of the training set.

2) Splits are chosen completely at random from the range of values in the sample at each split.",2b912140,0.8846153846153846
40759,a566b5b7c374e7,638858e4,### Deep Sleep versus Restfulness Score,b3dc5545,0.8848920863309353
40761,ac1abfe1dfe815,eea6ea92,## Logistic Regression,6529dbcb,0.8849557522123894
40772,bcd7e398c4d0ec,76695072,"# 4. Model Tuning
Memory issue, we don't go GridSearch at here.",77a143f6,0.8852459016393442
40773,0858e1bb3cbaca,a9bfca06,1. What are the top 3 popular product categories and how mant of those items were sold?,78548374,0.8852459016393442
40774,ee23a565163388,7f8f0eaf,The training and testing score are low.,88aacbc4,0.8854961832061069
40775,60da9bbfe39c4b,8cbe1ff8,"## Highest number of Cases, Deaths and Recovered in a month",b0dd8ad6,0.8857142857142857
40778,b3e48999ed0d00,05a439d0,## ROC Curve,fe9ada0f,0.8857142857142857
40780,b4e238fbc6464c,82caca2c,"# Optimisation-1

## Hyperparameter Tuning",fd1a6cba,0.8857142857142857
40783,f50dc95483c98f,7f58af1e,"## **Visualising the Training set results and test Set results**

we make use of a scatter plot to plot the actual observations, with x_train on the x-axis and y_train on the y-axis. For the Classification, we will use x_train on the x-axis and then the predictions of the x_train observations on the y-axis.
We add a touch of aesthetics by coloring the original observations.
This feature is given by the library `Matplotlib`",cd9e9621,0.8857142857142857
40784,55a5e31d03df9f,2f5e3abc,"This means every iteration is 32 batches long from a 240 height and 240 width by 3 RGB channels, and the labels are 32 batches by 36 dimensions (one for each possible class).",06dce00f,0.8857142857142857
40794,6b65d81a5743dd,8aa84545,Let's try other classifiers,4080a2d2,0.8857142857142857
40797,5d5c9480b5a0a3,8dccf1e6,# Modelling the Data,04d82e2d,0.8857142857142857
40801,0fa9979b5690e9,4a175163,"O resultado melhorou ainda mais ao pesquisar qual era a melhor configuração de atributos, antes de avaliar a performance do modelo no conjunto de teste. O conceito e a função de Pipeline serão abordados com mais detalhes nos próximos encontros, então não se desespere.

Conclui-se aqui essa breve discussão e apresentação sobre divisão de dados, onde você pode ter aprendido sobre:
* separação dos dados em treino, validação e teste;
* normalização dos dados;
* validação cruzada; e
* busca pelos melhores parâmetros.",c26eea94,0.8857142857142857
40806,f4b603905215b7,e10a8a55,![image.png](attachment:image.png),efe1d587,0.8857142857142857
40808,726833f92fb87a,a01900e5,## Train - Validation split,7dc5e1b6,0.8859060402684564
40816,a35cdce61f4059,ff03995e,"* **OverSampling **<br>
**SMOTE**",acc8eab6,0.8863636363636364
40820,73d8e56bc709b1,e2e9a910,# 7. Logistic Regression,78ec3cce,0.8863636363636364
40821,d1ff7e10ee0102,80ab5e2a,"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.

Now let's check 'SalePrice' with 'TotalBsmtSF'.",2cc71c3c,0.8863636363636364
40828,32e04b08ff52eb,925bd592,Cross validation accuracy and SD as a function of the smoothing parameter,8d5b86e0,0.8863636363636364
40834,957e035ba5b9d5,6bf565c9,## Build Sequential Model and fit,778ab3d3,0.8865248226950354
40836,2a123b4e8f9433,ff67e07b,SVC model,0a082218,0.8865979381443299
40839,8ec771f5600a61,025bb9da,# from randomforest,48364c1f,0.8865979381443299
40840,91eaec994e0c6f,ec050696,FB Prophet requires a column 'ds' (for dates) and a columns 'y' (target variable).,376aef10,0.8866666666666667
40842,2bd6c370695ea7,6d159145,## Post process,cbe6aec8,0.8866666666666667
40844,614ba9f0c62677,475c569f,"Steps of CNN:
1. Import Libraries
2. Prepare Dataset
    * Totally same with previous parts.
    * We use same dataset so we only need train_loader and test_loader.
3. Convolutional layer:
    * Create feature maps with filters(kernels).
    * Padding: After applying filter, dimensions of original image decreases. However, we want to preserve as much as information about the original image. We can apply padding to increase dimension of feature map after convolutional layer.
    * We use 2 convolutional layer.
    * Number of feature map is out_channels = 16
    * Filter(kernel) size is 5*5
4. Pooling layer:
    * Prepares a condensed feature map from output of convolutional layer(feature map)
    * 2 pooling layer that we will use max pooling.
    * Pooling size is 2*2
5. Flattening: Flats the features map
6. Fully Connected Layer:
    * Artificial Neural Network that we learnt at previous part.
    * Or it can be only linear like logistic regression but at the end there is always softmax function.
    * We will not use activation function in fully connected layer.
    * You can think that our fully connected layer is logistic regression.
    * We combine convolutional part and logistic regression to create our CNN model.
7. Instantiate Model Class
    * create model
8. Instantiate Loss Class
    * Cross entropy loss
    * It also has softmax(logistic function) in it.
9. Instantiate Optimizer Class
    * SGD Optimizer
10. Traning the Model
11. Prediction",b8551335,0.8867924528301887
40851,510b8303776bb6,169c28d9,## Processing of test set,18080db8,0.8867924528301887
40854,71c3c1eab0377d,fff8ccd3,Have a look at all the methods of the object 'model',52b4e360,0.8869565217391304
40856,c0ddb77bf32e2b,27225cdd,"# Now, let's take location into consideration.",a0cb45f7,0.8870967741935484
40861,ba4b3bd184acbb,bd808561,Next we can use boolean indexing to select only the apps with ratings of at least 4.5.,0f5de724,0.8872180451127819
40865,06c7ba9203293f,e268b6fe,# Filling out missing 'price',1e1a2b48,0.8873239436619719
40866,631cd434fc3aa2,29b005b8,## Modelling,2b74febb,0.8873239436619719
40869,fdbbd573ba31c2,f0205269,## ExtraTreesRegressor,f7c28d74,0.8875
40870,254cccd5145725,e75aa8d5,# LightGBM,a49b4037,0.8875
40876,b01ee6cb674fa3,90ad983a,"# Arm??e de l'Air --> Arme de lAir

French Air Force

",a8ffd35e,0.8876811594202898
40880,20b372b6e4e276,63b48739,"## 5.5. PCA visualization <a class=""anchor"" id=""5.5""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.8880597014925373
40883,2ada0305b68956,e61dc4e6,### 152. Palette = 'tab10_r',133e26f4,0.8885714285714286
40888,593d1d3d1df05a,5cf339f4,Taking output directly from the img ,bc682ffe,0.8888888888888888
40889,1d73d04c3aaae8,08e2a5d7,"There were over 2000 games where the point spread changed the outcome, i.e. a team beat the spread. How that change the prediction?
",cd43d0aa,0.8888888888888888
40894,d77e6d61ad2e8b,9d8570ca,# Loading the saved model,03fd0e96,0.8888888888888888
40896,bee584f9e4a8f7,e9be50eb,There is still so much of room left to make it even faster!,78dfc031,0.8888888888888888
40904,cf39cde80e66b7,d4aad880,"The residual sum of squares is the top term in the  R2  metric (albeit adjusted by 1 to account for degrees of freedom). It takes the distance between observed and predicted values (the residuals), squares them, and sums them all together. Ordinary least squares regression is designed to minimize exactly this value.

RSS=∑0n−1(yi−y^i)2
 
RSS is not very interpretable on its own, because it is the sum of many (potentially very large) residuals. For this reason it is rarely used as a metric, but because it is so important to regression, it's often included in statistical fit assays.",aed4bc9b,0.8888888888888888
40906,db5a369894fef6,c30ded25,"Awesome!
These visualization tool can help you understand the spread of COVID in different countries. 
## Summary
- pandas: different plots, labels, ticks 
- matplotlib: subplots, function
- plotly: requires specific data frame, melt, awesome interactive features 




",065aaf61,0.8888888888888888
40908,30c8dc87ce52ca,2ac60338,"Model performed really well especially in low False Negative, So only 13 transactions out of 492 Fraud transaction will not be captured by this model.






",805e9d67,0.8888888888888888
40910,0c452d3a0b9339,c437b6f9,Get the Difference between the values of the Nucleotides that got Paired,5d857385,0.8888888888888888
40912,b3e0b7e9ff6849,89814f61,## 6. Final: Calculate CLTV,f6e4bb0d,0.8888888888888888
40914,7341f069d9b2ee,5a4464f9,Better Evaluation Using Cross-Validation,e0a49e62,0.8888888888888888
40916,268a610bbc64b4,bc1e1f1a,"Since we have only 1 month of booking data, we can't see the trend month wise.",8a16f301,0.8888888888888888
40917,c9dc8d00773da4,137ec7a3,# Show image,d9aa2f85,0.8888888888888888
40918,b9328fe3b0cefc,9e87a07b,"we see(可以看到)：
- FGM&FGA: There are similar numbers in FGM with both side, but winning team has less FGA.(命中数上看，决赛中球队的差异最小，未命中上看，决赛中，胜利的一方似乎出手更稳重，也就是投丢了更少的球)
- FGM3&FGA3: The winning (三分球上看，在决赛中，胜利的一方甚至进的三分球要少于失败的一方，这也说明一个现象，通常来说决赛因为防守方式、吹罚方式、球员心态等各种因素，一般球队会采取更保守的得分手段，而较少采用不稳定的三分球)
- FTM&FTA: There is little difference in free throws. The winning team has a higher percentage of free throws.(罚球上看差异不大，胜利方的罚球命中率更高)
- Rebounds: On the offensive rebounds, there are always fewer winning teams, especially in the final. Generally we can said, less offensive rebounds can show that the tactics of the team are conservative, that is, to actively retreat from defense, so as to avoid the opportunity of fast attack caused by grabbing offensive rebounds. On the defensive rebounds, the winning side obviously performed better.(篮板球：灌篮高手里面说，赢得篮板的人赢得比赛，看看是不是这样。可以看到对于进攻篮板，胜利的球队总是更少的，决赛尤其如此，通常来说进攻篮板少可以说明球队的战术偏保守，也就是积极退防，以避免因为抢进攻篮板导致对方出现快攻的机会。而防守篮板上看，胜利方明显表现更好)
- Assists and mistakes: in the final, these two are relatively less, which is because the team usually needs to take more personal attacks due to the problems of defensive strength, penalty scale and so on.(助攻和失误：在决赛，这两个相对都更少，这是因为通常决赛由于防守强度、判罚尺度等问题，球队通常需要采取更多的个人进攻，这也是明星球员的作用，而这种方式通常对应的助攻和失误都会少一些)
- Steals and blocks: These are two different defense methods. Generally, steals have certain risk-taking elements. Because the failure of steals usually means loss the position, so there are fewer steals in the final, and more blocks means more attacks to the paint area, which is also caused by more attack methods in the final.(抢断和盖帽：这是两种不同的防守方式，通常抢断是有一定冒险成分的，因为抢断失败通常意味着失位，因此决赛中抢断更少出现，而盖帽多则表示到油漆区的进攻更多，这也是决赛中更多的进攻方式选择导致的)
- In personal fouls, the difference between the winner and the loser in the final is smaller. Of course, it's not very friendly for some players who like to make fouls.(个人犯规上，决赛中胜利方和失败方相比差异更小，一定程度上说也是因为不想把比赛的结果交给裁判，而是更多的给球员发挥，当然这对一些喜欢造犯规的球员来说就不是很友好)",3a35eb23,0.8888888888888888
40920,df2a7968c08ee4,9d9f43ba,"Again, we are creating a custom dataset and passing it through a dataloader to make predictions on the images.

If anyone with more Pytorch experience could weigh on the efficiency/effectiveness of this method of prediction please let me know!",a2ba0a72,0.8888888888888888
40923,301658c5b2bf29,ddc680fc,EOF,5efbefd4,0.8888888888888888
40924,c349ee5a821411,25a2cf76,"As a citizen of Spain with roots of Poland, is very interesting for me to see how Western Europe occupies the third place while Central and Eastern Europe ocuppies the seventh place. In fact the difference is big, and that explains why Europe has the fourth place in the list of happiest continents in the world.",572b269d,0.8888888888888888
40935,49ac6594c8f5cf,ade74cca,UNDER-SAMPLING,6f19f28a,0.8888888888888888
40938,b0c2805cd5c087,dc5df02e,Image healthcareimagined.net,0446f327,0.8888888888888888
40939,9085cba2265204,b6645a23,## Predicting using test set ,de766eb3,0.8888888888888888
40942,892be0a523578c,4b6b385b,"By extracting the regular exercisers, I find that most of these people spend more than 60 minutes on exercising",b0e8d7c0,0.8888888888888888
40943,6fad63bfd45ef9,4850dc38,# Make predictions,b3c6f1d6,0.8888888888888888
40947,3ac432b2cac29c,81305ee3,Is that MAE good?  There isn't a general rule for what values are good that applies across applications. But you'll see how to use this number in the next step.,a358669e,0.8888888888888888
40950,69ac33d79f5130,b3771df7,#### HeatMap,9d760d2a,0.8888888888888888
40951,d96e03a9e7c030,3aa4fbc2,### Expected Additional Testtakers - Below Poverty Level,d2b72ced,0.8888888888888888
40952,c968dbd8d49ae6,773cf9a6,"As **Random Forest** works better here, let's try changing the weights a bit in favor of the lower side of the scale.",dfb2684d,0.8888888888888888
40953,917957c6c4065f,2b1a3f57,"해당 기간 동안 인기동영상 개수 별 채널의 수입니다. 
인기 동영상을 1개 보유한 채널이 약 50%이고, 3개 이내가 약 75%입니다. ",55b8ed68,0.8888888888888888
40955,c8bf959b9608cf,50403845,"## Train the model
We will use the L-BFGS optimiser to update the generated image. L-BFGS has empirically shown to be much more suitable for this task.

### Steps:
#### 1. Initialize the generated image with content image (rather than initialising it with white noise)
#### 2. Pass the image along with grad and loss values to the optimiser 'fmin_l_bfgs_b()'. The optimiser will return the updated image. 
#### 3. Deprocess the image and save it.
#### 4. Repeat steps 3 and 4 for a fix number of iterations.",155e3672,0.8888888888888888
40965,613bf7bfdcb9e3,e33ecd66,### axis = None => most common values count of all items,32beb65d,0.8888888888888888
40967,b39684e6670dd7,b0e398d5,# to DataFrame ,83de9873,0.8888888888888888
40971,3597174a998d4d,8f823473,"Based on the analysis of variables, I choose original variables and the new constructed variables to model.",276892ed,0.8888888888888888
40973,f4b9042e693b6c,edddefbf,Let's train all 5 folds!,676cacc9,0.8888888888888888
40974,56cc8fb47bef6a,54405bc9,"<p>&nbsp; <span style=""color:#000080""><span style=""font-family:Lucida Sans Unicode,Lucida Grande,sans-serif"">Use the test data to predict , check the confusion matrix and observe the accuracy</span></span></p>",652d6670,0.8888888888888888
40977,cb4ad8ed4cb300,20991f94,"* works quite well
* got only for ""open valve?"" a wrong answer (""How open valve?"" is answered right)",7c0f3236,0.8888888888888888
40980,b6c0ad74f95b8c,005b9e47,The opposite; see the distances increase for neigbours far away,5de5b241,0.8888888888888888
40982,2f0f808765fc67,30e0e1b6,# **H. Optimal Model**,fd1f6494,0.8888888888888888
40983,6a05614abce6d9,d32c62a4,## Submission,c0c9da16,0.8888888888888888
40984,454672c0f11328,34ffbd8f,"### Lv3
**Input**: Predictions from previous two levels.

**Models**: [LogisticRegressor]",bfa2868b,0.8888888888888888
40986,06ecf7a304c309,ef0af92a,이제 오토인코더 모델을 아담 옵티마이저와 Catgegorical Cross Entropy를 손실함수로 사용하여 훈련해봅시다.,714de627,0.8888888888888888
40988,1fcf9c261518d6,e5dccd5d,# V. Final Predictions and Submission,be646fb0,0.8888888888888888
40991,4883314a96dc34,f6b90afa,### Optimize Models by Tuning Hyperparameters (3),50d36836,0.8888888888888888
41002,42e0005bed28aa,22ed4c9b,><h3>Prediction:</h3>,5616d451,0.8888888888888888
41003,f998cece696659,fa805a51,"<a id=""7""></a><br>
# Evaluation Regression Models",7964297e,0.8888888888888888
41006,1883198d6d8c3c,86e47cae," <h3>3. Another method to check the dataset - .info</h3>
",69a1d458,0.8888888888888888
41012,233cb23d9e01b9,63a3a83c,## Age error,ffa56c19,0.8888888888888888
41014,2259c048379b36,7c5f6036,"### Conclusions 
Though the graphics shows that the second time series seem to be irregular (and I would have expected it to be more variable), it is the first time series which is more variable as the standard deviation on percentage change shows. ",43558d11,0.8888888888888888
41017,c65a65d4041018,415775ac,### online platforms,824fb229,0.8897058823529411
41020,1294fb4c86f993,3b46b2b3,Dropping the overshooting values of `guns_per_thousand` (over 20) to get a better scatter diagrames ,4471e513,0.8898305084745762
41023,83df814455f06c,090ceddb,### Visualize decision-trees with graphviz,c9cff71a,0.89
41027,9c26c5dcd46a25,74c9aac3,"#### <font color=""#114b98"" id=""section_3_4"">3.4. Qualité de représentation de la réduction de dimension</font>

Afin d'analyser la performance de notre réduction de dimension via PCA, nous allons regarder le ***COS²*** et la **CTR***.

On peut calculer la **qualité de représentation des variables *(COS²)*** en élevant la corrélation au carré :",1bbbb677,0.8902439024390244
41028,0b01138ad120fc,df04c9f0,"**Everything is OK!  
Reshaping to 3 dimensional**",0b4b72e6,0.8902439024390244
41036,1eb62c5782f2d7,8b14ed16,"### Interpretasi 

kamu dapat melihat bahwa $12.92$ pounds itu diatas mean, $8.12$ pounds itu dibawah mean, dan $9$ pounds itu sama dengan mean.",bb69f147,0.8904109589041096
41037,3c2033cc99c12c,3fbf0ebe,*Design a neural network structure*,dfa22a54,0.8905109489051095
41038,c80939c7c626cf,3bf77e82,# 12 Random Forest,b9ac31e2,0.8905109489051095
41039,0932046e1f485d,8a62e518,Negative reviews WordCloud,218cc7a3,0.890625
41040,c85c94076e9c3a,b0d756a2,## Time Series :,3ea0c443,0.890625
41042,ff3a8ce61fab6a,280d50d3,"<hr>
### Exampel 2 ",9afe1654,0.890625
41046,016abae0483764,cbc90ef8,### Decision Tree Classifier,bc9f289b,0.8909090909090909
41047,5a8c553e21c70f,ab685da4,"## Testing

Each neural network makes makes slightly different predictions for each test sample due to random initialization and high variance of neural networks.",9ebd9d8f,0.8909090909090909
41060,81712ee7510ac5,0274ce04,**list comprehension**,c4685e79,0.8914285714285715
41065,62487bcd70b199,905df325,## <a id='8.4'>8.4. RandomForestClassifier</a>,f6ae50af,0.8916666666666667
41073,63b44c85e32c1f,eb3ad6d3,**intersection( )** function outputs a set which contains all the elements that are in both sets.,fb9b9562,0.8918918918918919
41079,62037c5832129c,9cd3252f,## Diagnosing bias and variance problems with learning curves,61474350,0.8918918918918919
41081,d76896b30cebd3,04d0b1a7,"Relation between Duration(Days) and Pledged Amount(USD)

",1b4e8f34,0.8918918918918919
41082,297cbe4a23c4bf,bd69ff67,# Predicting and Submitting,a843e619,0.8918918918918919
41086,629f2918807a9b,6bd84ccf,"### Which day is suitable for marketing of books on social media ?

### Answer is ""Saturday"" & ""Sunday"" with more than 6000 and close to 6000 books respectively have been sold in both of the days.",be56dc84,0.8921568627450981
41087,52cfd66e9ec908,7b76d38f,"And this is how the Neptune metrics look like:
![](https://i.imgur.com/uspP0q0.png)",c74adcdf,0.8921568627450981
41089,71b75664517244,46fed370,"Both arsenal and Arsene Wenger only manage to do their best around 2000 - 2004, since then they never won any premier league at all",fc905af5,0.8921568627450981
41093,03048e86a6d806,515ca9e5,"### Okay, So Where Can I Be Introduced to These Stuffs?",1285c231,0.8923076923076924
41094,f2f2db16a2f86c,79b005a0,Using the **Root Mean Squared Error** metric,ffc6a115,0.8923076923076924
41098,3cb96bd8eb364b,fa94b608,## Evaluate Model,3157af7e,0.8923076923076924
41099,a8c042af6b7245,b8719ba9,"#### Selecting features with a Random Forest and SelectFromModel

Here we'll base feature selection on the feature importances of a random forest. With Sklearn's SelectFromModel you can then specify how many variables you want to keep. You can set a threshold on the level of feature importance manually. But we'll simply select the top 50% best variables.

> The code in the cell below is borrowed from the GitHub repo of Sebastian Raschka. This repo contains code samples of his book Python Machine Learning, which is an absolute must to read.",2487ac62,0.8923076923076924
41105,726833f92fb87a,d31b365d,Then we further split the train set into a new train and validation set to monitor the accuracy on the validation set and prevent overfitting.,7dc5e1b6,0.8926174496644296
41108,df51d4c54fbb91,34ef06cb,# Submite,4226dd72,0.8928571428571429
41111,f4514ec092a771,93e62f62,"To keep the order of submission, I will read the file id from sample_submission.csv and get the relative transcript from the above dictionary.",3739ab1e,0.8928571428571429
41113,92e9fc3a0ff5c0,e5049f80,## **Make both the Datasets balanced**,d53da425,0.8928571428571429
41114,565ad413cd802f,cd898d21,"We can now save it batck to CSV, and download the file from the sidebar (check the output folder)",397b074e,0.8928571428571429
41123,8dd655515e7d18,75b666ae,"### Agreeableness

A person with a high level of agreeableness in a personality test is usually warm, friendly, and tactful. They generally have an optimistic view of human nature and get along well with others. 

A person who scores low on agreeableness may put their own interests above those of others. They tend to be distant, unfriendly, and uncooperative.",895f41cf,0.8928571428571429
41124,585c280865b46e,4dbd160f,"# Visualization colored by top expressed histone genes, by boolean - expressed or not",4d6056f1,0.8928571428571429
41125,9c33d1955302bf,529bac81,# **Using K=23**,0d9cfc89,0.8928571428571429
41130,b290039151fb39,962df4f9,"## Dropout Scheduling

I create a bunch of different schedulers each of them operating on a different dropout layer. This can obviously be customised however you'd like and can ever be applied to other model hyperparameters.",1836a79c,0.8928571428571429
41133,62ae2b200f6b36,11709566,"The number of registered bikes as can be seen from the plot above isn't measured accurately using the features present in the data-set. Mean absolute error is around 99 and r2 score for the fit line is close to 0.17, which suggests a poorly fitted regresion line.

This suggests that to predict number of registered bikes, we certainly need more features and linear regression model isn't entirely collect for its prediction. ",7da4ea31,0.8928571428571429
41136,7686f42e1f28d2,7bbffcd9,"# Conclusion
There is a 15% gap between train and test accuracy, which suggest that our model is likely overfitting, one can probably achieve better results if the hyperparameters are fine tuned and if the model is regularized properly.
<br>In addtion, if we look at the mean year in `X_test` and `X_train`, we can see that one is around 2003.7 and the other 2008.3, meaning that the test set is on average later than the train set, which may be the reason behind the train/test accuracy gap.",6c128859,0.8928571428571429
41138,99f84fa59cb1da,ebca66f6,"#### Now, you can train and test your different models on `train_folds.csv` and validate your different experiments and model's performance",41e95f63,0.8928571428571429
41139,ee23a565163388,8e81f24d,"As the Logistic Regression model has the decent training and testing score, let's check how it could be further improved. We'll also try to reduce the number of features to reduce the model complexity.",88aacbc4,0.8931297709923665
41141,98a6794067932a,fb96b96e,"La cellule de code suivante permet quant à elle de prendre la liste des clients perdus créée dans la cellule précédente et de ressortir les différentes commandes ayant été effectuées historiquement par ces clients. Dans un premier temps, le code permet d'établir l'index de la base de données comme étant l'identifiant client. Ensuite, le code fait en sorte de comparer les identifiants clients se retrouvant dans le dataframe des clients perdus avec les identifiants clients du dataframe de la base de données complète afin de ne sélectionner seulement ceux qui se retrouvent dans les deux dataframes. Les identifiants clients sélectionnés sont ensuite regroupés dans un nouveau dataframe où il est possible de constater les différentes commandes ayant été effectuées par le passé par ces clients perdus. Cette analyse est donc pertinente afin d'évaluer le comportement historique de ces clients n'ayant pas commandé auprès de l'entreprise depuis un certain temps. De cette manière, il est donc plus facile d'évaluer s'il s'agit de clients habitués qui auraient vécu une expérience peu agréable ou s'il s'agit seulement de clients peu habituels. ",08600fe2,0.8932038834951457
41146,e5dd725b8fa422,a6fdfc4e,# Predicting using SARIMAX model,14675d8b,0.8933333333333333
41148,67b7354e96113a,5eb3253e,**XGB Classifier**,dca94250,0.8933333333333333
41149,b10bd75889dad9,05fdb427,### Predictions with Optimal Cut-off,ee00ceee,0.8933333333333333
41163,5f674175839b32,c12c791b,"Q3)Which are the top 5 platforms on the basis of Gloabal Sales?

Ans:



1.   Ps2 with 1233 million.
2.   X360 with 969 million.
3.   PS3 with 949 million.
4.   Wii with 909 million.
5.   DS with 818 million.



",53a2e343,0.8936170212765957
41169,a2444ab5d5f147,f779702f,### Plotting Top 10 negative sentiment words,10617755,0.8939393939393939
41172,869a39a3d4dea2,1f619338,"2D & 3D histogram <a id=""2dhist""></a>",9020daf8,0.8941176470588236
41177,2ada0305b68956,bcfc39ea,### 153. Palette = 'tab20',133e26f4,0.8942857142857142
41181,f14f6708035916,02e5cb54,A lot of my money is spent Mon-Wed which doesn't make much sense considering I work 9-5.,ca22d04b,0.8947368421052632
41195,840534f2908a9c,41c8afcb,*Average Fare amount has beern increasing over the years.*,8081c3cc,0.8947368421052632
41198,bef2347846e476,56cfe390,"As we can see also in the dataset we have three types of Sentiments.In the grapgh below we can see the polarity and subjectivity of the positive,neutral and nan Sentiment's classification.In the positive Sentiments we observe the less sentiment polarity and much sentiment subjectivity.And in the neutral sentiments the polarity is 0.0 and they are taking the different values at the subjectivity. ",cb93bf51,0.8947368421052632
41199,f35ee6e9fab592,3e4a81c9,"Are video games improving over the years/ or in this case, a year? The following dataset is extremely small to draw comprehensive conclusions, but nonetheless, let's give it a spin",b15f7073,0.8947368421052632
41201,757fa8de4edc4c,75097f42,"**Wooo....
We can see a strong correlation between Scaler Coupling Constant and fc.**

**We can create a training model containing fc**",87211008,0.8947368421052632
41203,29437539745aa5,c45b2139,"<a style=""text-align: font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;"" id=""submit"">5&nbsp;&nbsp;SUBMIT</a>",c17b490a,0.8947368421052632
41204,54004b32784b68,5d8559df,# Random Forest Regressor,27213ca9,0.8947368421052632
41205,c950cff74e51ac,f109c697,Each rooms rented in NYC mostly receives reviews around 0 - 50,d59bf323,0.8947368421052632
41207,1f3295ed0d4e4a,d43196a0,# Dask Array,b0aeb172,0.8947368421052632
41208,52ee792e228d54,259014d1,"### Now our recall is 91% with accuracy of 89%. Even if we had to lower the accuracy, we were able to target more customers who would opt for the Personal Loan.",5096094e,0.8947368421052632
41214,038abade89e59f,14740126,# Rank Prediction & Submit,cb32a3fe,0.8947368421052632
41216,5ce12be6e7b90e,8f8fb537,"### Changing and adding records
We can change the dictionary by simply assigning a new value to a key.",c0ab62dd,0.8947368421052632
41217,826ccb616bd2a8,eac70bac,"# Conclusion

<u><b>Order of Drug Operation:</b></u>
1. If `Na_to_K` greater than `14.829` recommend **Drug Y** (if less, go to step 2)
2. If `Blood Pressure` is `HIGH` then (if not high, go to step 3)
    -  `Age` is `50 or younger` then recommend **Drug A**
    -  `Age` is `Older than 50` then recommend **Drug B**
3. If `Cholesterol` is `HIGH` then **Drug C**, if not the recommend **Drug X**",4d7df2ec,0.8947368421052632
41218,30fdc4a6e3c1db,8c859894,"What we see:
* Almost all weekends have higher sales than the average sales i.e 34k so events do impact sales
* The highest sales is in the weekend before Easter as expected about 44k each day followed by EidAlAdha which are both Religious events (which follows our event type finding of religious event having the highest impact)
* The lowest sales is in the weekend before New Year ",6111ddee,0.8947368421052632
41220,d81d3830152f88,1d67c0d0,"At a Type I error rate of 5%, p-value is larger than the error rate, and therefore we **fail to reject** the Null hypothesis. There is a probability of 50.34% that team will lower or equal 3pt-fg as the opponent team wins the game.",9551eac9,0.8947368421052632
41221,26b93b6f4dc148,211ef651,### Dimensionality Reduction Using PCA,6f667d22,0.8947368421052632
41223,9e27af2600925c,2ee831d9,"**Interpretation**: 
- Different learning rates give different costs and thus different predictions results.
- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). 
- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.
- In deep learning, we usually recommend that you: 
    - Choose the learning rate that better minimizes the cost function.
    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) 
",9b556435,0.8947368421052632
41231,7454fdc444df16,73bca3d3,# Training Model,a7818ef5,0.8952380952380953
41234,55a5e31d03df9f,9880774c,**Optimal way:**,06dce00f,0.8952380952380953
41235,04bac111ffbe9c,8a92050b,## Confusion matrix and accuracy,82576b17,0.8952380952380953
41239,21413205980558,24cd1335,"# After several parameter adjustments, we found that the parameters we selected in the simple logistic regression model : duration, pdays, previous, campaign,balance。if the parameters are too many, it will affect the prediction of the model.
# 经过几次参数调整发现，我们在简单逻辑回归模型中选择的参数有：duration, pdays, previous, campaign,balance，如果参数过多，则反而会影响模型预测。

# In fact, we can see that in the simple logistic regression prediction model, Accuracy rate,Precision rate ，Recall rate are not very ideal. Therefore, we can generally construct a more ideal random forest model.
# 其实可以看到，在简单的逻辑回归预测模型中，准确率，精确率，召回率的百分比都不算很理想。所以我们可以一般而言建构分类模型更理想的随机森林模型。",84197de0,0.8955223880597015
41240,20b372b6e4e276,b8e96b4e,Using my notebook https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert,ec8b0860,0.8955223880597015
41243,a4aa36df07fd53,b86bd511,"Saatnya kita pecah data menjadi 3. Train Data, Validation Data dan Test Data",d2f42b6d,0.8955223880597015
41244,e58e68e4eeefe5,7f87e5e0,# Using Repeated Stratified KFold Method ,a87662ce,0.8955223880597015
41250,5cfb546af2b8ce,62aba670,![title](https://i.imgur.com/QYqqULP.png),e8730ad1,0.8958333333333334
41255,3b5903412fe741,ce93ed6f,"## Assigning data

Going the other way, assigning data to a `DataFrame` is easy. You can assign either a constant value:",ad231969,0.8958333333333334
41262,241cf32abb22d8,1928225e,"For models based on the full features, the Decision Tree model performs significantly better than KNN and Naive Bayes models, at a 5% level of significance. KNN and Naive Bayes models perform at similar levels.",47157066,0.8961038961038961
41263,75adb7945ef9bd,7ac452a8,Default hyperparameters gave the best score.,785c5095,0.8961038961038961
41264,663bbc9eaf267b,e9a8e631,## LightGBM Regressor,32445529,0.8961038961038961
41267,510b8303776bb6,645ca778,### One hot encoding of all purely categorical columns ,18080db8,0.8962264150943396
41273,87e96e14f8f5ce,e3ebca61,# Predictions,f9f7a3a2,0.896551724137931
41277,1cd8be6e679620,28437a56,## Visualize RNA-2D Structure,3ce15a43,0.896551724137931
41282,9535bb04ae042c,1fd9d36b,## Files for this project: https://github.com/eforebrahim/Heart_Health_Project/tree/master,165b6fae,0.896551724137931
41284,6903d3f38c6a66,38cf045f,"Thses are some of the colormap that i use usually but there are amny more.

See All the colormap avalabile on the matplotlib library here : [Matplotlib Documentation](http://https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html)",6067ce5e,0.896551724137931
41287,cd10f3afd970b3,9f6fb1d0,Distribution graphs (histogram/bar graph) of sampled columns:,2db3c8e4,0.896551724137931
41289,20e1ba19eb9b5e,499a59b2,## 3.1 Training data,4569bfc1,0.896551724137931
41291,84127ade6fde87,af130b4a,"Embeddings are an essential tool for when a large number of entries in the vocabulary have to be represented by numeric vectors. But we won’t be using text and text embeddings in this book, so you might wonder why we introduce them here. We believe that how text is represented and processed can also be seen as an example for dealing with categorical data in general. Embeddings are useful wherever one-hot encoding becomes cumbersome. Indeed, in the form described previously, they are an efficient way of representing one-hot encoding immediately followed by multiplication with the matrix containing the embedding vectors as rows.",f55d05b6,0.896551724137931
41299,fb5c6021d127ef,73f19553,"## 8) What do your results tell you?

Given the daily average riders per station over the years, does it make sense that the model is failing?",dd05cbd3,0.896551724137931
41305,45921c50ac56fa,4a9817f8,"We now have the LDA distibution for each topic, let's now visualize it in using pyLDAvis",465973eb,0.896551724137931
41306,fc8e0042411c46,fd14ff54,# VIF,af476c2a,0.896551724137931
41309,f3c6048d1058e3,135b0a17,## LSTM Model,1d9056b0,0.896551724137931
41310,5ea840754577e3,a0edd677,This again goes back to the Sex feature. Mrs and Miss has a higher survival rate over Mr. We will fill the missing values in the age column using the meadian age of each salutations,9cf9b73f,0.896551724137931
41316,156bbcff05dcea,12697172,# Bonus,66ad1fe9,0.8970588235294118
41319,e4c6dd957eb5ce,27b7d055,# Total votes from unique Users,2e383665,0.8970588235294118
41320,7f74a04ae75792,17b5f924,#### Plot By House Ownership,d01e91da,0.8970588235294118
41322,eb0ecd6bebeb15,d70a9042,petal.length'i 5'den küçük ve türü virginica olan gözlemlerin sadece sepal.length ve sepal.width değişkenlerini ve değerlerini yazdıralım.,d7b93a60,0.8970588235294118
41324,9cec5ddf8b6f49,3dbbe0d7,#### Visualize which hyperparameters are affecting the trial duration with hyperparameter importance.,d39fc8e7,0.8970588235294118
41325,02b7e38902069e,519153bc,"#Start with Pretokenized Text

""In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\n) separated sentences, where each sentence is space separated tokens. Just set tokenize_pretokenized as True to bypass the neural tokenizer.""

""The code below shows an example of bypassing the neural tokenizer:""

https://stanfordnlp.github.io/stanza/tokenize.html",726a03a0,0.8970588235294118
41328,c84925c8171900,2164b949,"<a id=""numvar""></a>
<h3>   
      <font color = purple >
            <span style='font-family:Georgia'>
            5.6 Global & Regional Sales Wise Analysis
            </span>   
        </font>    
</h3>",e21ff7ec,0.897196261682243
41330,738bfced935b69,709720b6,## Label Encoding,2d3c592d,0.8972602739726028
41334,163ceeb80d6923,141d9439,## Run on Test Set,4adfbb90,0.8974358974358975
41339,34fff8ce731b03,57f277c3,O Dataframe *output* é escrito no formato CSV para gerar a saída do algoritmo de aprendizado de máquina construído neste notebook.,6f9e5b2e,0.8974358974358975
41346,80ad12f326ab70,e512b11b,"The Engagement is reduced from the 6th month to the 8th month, This is due to the school ends on this period and students are their break, and return to school on september",da404a16,0.8974358974358975
41347,4d91e84c564cbe,9eca2639,"Tuples are often used for functions that have multiple return values.

For example, the ``as_integer_ratio()`` method of float objects returns a numerator and a denominator in the form of a tuple:",355a43e3,0.8974358974358975
41350,ce9ed5e2d601d7,879b4e94,## Prediction visualized,f58a2f43,0.8976377952755905
41351,73d8e56bc709b1,c52ca601,"Predict binary targets (attack vs defend positions) with logistic regression,  
1(attack): CAM, CDM, CF, CM, LAM, LCM,LDM,LF,LM,LS,LW,LWB,RAM,RCM,RDM,RF,RM,RS,RW,ST  
0(defend): CB, GK, LB, LCB, RB,RCB,RWB",78ec3cce,0.8977272727272727
41354,3c2033cc99c12c,d0510cb9,"+ As the task is only a classification problem, so i choos to directly use the linear neural network together with the RELU function, in the last layer, i use a signoid function to map the result into a probabilty.",dfa22a54,0.8978102189781022
41356,f1e162ddd14f11,945ed96e,"to compare the results, we will use displot",cdb2e771,0.8979591836734694
41358,e69a496109e7d8,94b1b23d,This shows people who of age range 40-60 and having node counts from 0-5(most) aren't survived,1c640591,0.8979591836734694
41363,12f4d16fc21645,af79a9d9,<h1 style='color:blue'>Confusion-matrix</h1>,c7752038,0.8979591836734694
41366,ab6da5994949a3,34f99f2b,"# Random Forest Classification Model
## Fitting Random Forest classifier to the Training set",fae6b91d,0.8981481481481481
41367,2f0f808765fc67,dc47dbeb,"Random Forest with 10 fold cross validation was giving the least error and high accuracy across all the models from A-H
Similar apporach within train and validation can be apply to the given test data",fd1f6494,0.8981481481481481
41368,fdc3afd309b850,40b2a8b0,"<a id=""R""></a>
# 10 Results",966bde38,0.8981481481481481
41371,a077820f7ab459,5ecb50b5,### Predicted label and heatmap,05a43104,0.8983050847457628
41373,9169c4e9c33c90,6955c83c,One or more authors made multiple Top 50 appearances in every year.,725bf880,0.8983050847457628
41375,bb0905d33ae417,66acc663,# Generating submission,25fd1965,0.8983050847457628
41376,dac3c8204a2d1b,fff45eba,"If we look at the boxplot above, we see that in general, fiction bestsellers' ratings are higher than those of non-fiction bestsellers.",b0d2d0dc,0.8983050847457628
41378,a44368590e878a,dc0ac01c,## Time,77743ba8,0.8983050847457628
41388,ea4e559a86d613,218e9efc,**Random Vertical flip**,eff47843,0.8985507246376812
41389,548f961125248d,738c166a,### LightGBM,d8c5e8b8,0.8985507246376812
41391,0e2a23fbe41ca9,7a3c186d,There are no new ```card_id```s which are not there in training or test set.,64e4762c,0.8985507246376812
41394,7e275c8d5ff2a0,ead91a47,"We can clearly see the prediction of our model that on '16 August, 2020' there will be a total of ~ 0.7M 
(7 lakh)' recovered  cases in India if the number of recovered cases goes on increasing like this.",b3afcc98,0.8985507246376812
41395,a1a31459abf078,7bf5c5a3,"# Final Predictions<a name=""preds""></a>",66fc0f54,0.8985507246376812
41403,04ff2af52f147b,39e4aa0a,Then we define our model and use the best parameters we are able to find using *GricSearchCV* on our training data.  Change *WANT_HYPERPARAMETERS = TRUE* if the calculation is desired.  *WANT_HYPERPARAMETERS = False* will give best parameters from previous runs for the sake of time.,d5f37be9,0.898876404494382
41404,4c47839b067546,6daea152,"Работа с данными привела к значительному улучшению результата модели (MAPE 87.31%), однако на практике ее применение не принесёт особой пользы. Построим другие модели!",1f517b02,0.898936170212766
41405,dbd96dd275dc60,6af0ab8e,"# Format preds into same format Kaggle has asked
",1ed493a8,0.898989898989899
41407,4ae6a182abac64,c0e9ece3,* **classification_report**,418676c5,0.8991596638655462
41415,36c35f0a9f70f7,eb8b8e4f,## Kernal Navie Bayes,67358bc7,0.9
41416,c18267b203f28a,0a48abb0,### 加载训练完的模型,09ca8efb,0.9
41422,3597174a998d4d,b677dc41,## 3.1 Model with original variables,276892ed,0.9
41423,2730840089c8eb,1027f77e,"We can access a collection of all the keys or all the values with `dict.keys()` and `dict.values()`, respectively.",34d27dac,0.9
41433,396bc36edb95d3,05ce6557,### Model Evaluation - ANN,965e4f8f,0.9
41434,712198370d5521,cfd8ee07,"Unlike campaigns, the deals offered did well. It has best outcome with cluster 0 and cluster 3. 
However, our star customers cluster 1 are not much into the deals. 
Nothing seems to attract cluster 2 overwhelmingly 
",5882e04c,0.9
41441,5ffe6aa38958a1,947af3c3,"# 5.4 Precision and Recall
",11f5412e,0.9
41442,fc3adf9d45953e,9f451de9,"#KAGGLE for KIDS!

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ72gtRa0iHTFUm5eXo1o-uXKjTWGnbVEpvvQ&usqp=CAU)educationalalappstore.com",e3789b90,0.9
41449,ad121e0531afa4,2285d1c3,## [View the Complete Dashboard Here 🎯](https://wandb.ai/anony-mouse-138818/pytorchlightning/runs/1mgcybd2?apiKey=9c1b4ff53762c75e39d283f5434cc5552455b179),a3492905,0.9
41454,10c5a39a87c47e,01566786,"## Step 14: Plotting Sample Prediction (Groundtruth vs Predicted)<a id='step-14'></a>
In this step we will test our model by plotting 12 random prediction in which ground truth is in brackets and predicted value is outside the bracket if both match then the color will be blue whereas incorrect or misclassified instances will show in orange color.",09c7337a,0.9
41456,b547f0f38f7744,b89cb820,### Show Loss,b6ba66b3,0.9
41457,a78d363403fce2,acbe85b9,"# Conclusion
With this setup in place, it should be straight-forward to apply further advanced concepts from the course, e.g. heatmaps.",0f824cb6,0.9
41459,edc19e349fe80a,795cc242,Let's take a peek at the submission csv before submitting,7882221a,0.9
41461,7dd46c750653eb,49529d25,**Name Change over the years**,c2644713,0.9
41467,fdbbd573ba31c2,b22ef8e9,## LGBMRegressor,f7c28d74,0.9
41470,3dd4294f903768,1dd23326,***,0d89d098,0.9
41474,864302b10e7730,7b92d1ce,"# **There are other plots through which we can evaluate the relationships, they are; *Heatmaps*, *Swarmplots* and *Lmplots***",e9dd1d2d,0.9
41476,bfe6c7096b1ad0,821d01bc,"Чтобы отсылать решение сразу на Kaggle, нужно установить библиотеку kaggle и получить токен
![image.png](attachment:image.png)
Токен нужно скинуть в папку .kaggle. На win10 она здесь: C:\Users\Username\",fffd95e0,0.9
41480,8696921d9adc93,82c2c74b,"**CNN**
1.  Convoultional Layer : Here the model learns features from the images, we have to set filters 32 or 64 depending upon the usecase.
2. Pooling : MaxPool 2D is the next layer which picks the maximal value in a region.
3. Dropout : Randomly shutting off some nodes of the network to prevent repetition (here regularization) and thus reduces overfiting (mugging up of results xD).
4. Activation Function : Here we have used ""relu"" [max(0, x)]. It is added to provide non-linearity to the network.
5. Flatten Layer : It is used to convert final feature map into a one single vector.

",b8908b23,0.9
41485,9c044fa3072552,8cdf8638,Looks like the [map](https://www.mappr.co/wp-content/uploads/2018/11/USA-States-Color-Map.jpg) of USA right?,1362842e,0.9
41488,8cd6656a65e6e7,332e5e43,"<h1 id=""analysis"" style=""color:blue; background:white; border:0.5px dotted cyan;""> 
    <center>Analysis
        <a class=""anchor-link"" href=""#analysis"" target=""_self"">¶</a>
    </center>
</h1>",c8e1697a,0.9
41490,6cacdcf8daf400,e01293b9,**Inference**,83939b53,0.9
41492,d07915a6e6992e,703544b0,"**Gradient Boosting**

Gradient boosting is one of the most powerful techniques for building predictive models. Boosting is a method of converting weak learners into strong learners by building an additive model in a forward stage-wise fashion. In boosting, each new tree is a fit on a modified version of the original data set.",2b912140,0.9
41493,9276fa5cc2fef6,57270a4f,## Correlation Matrix,24aa6a52,0.9
41494,2ada0305b68956,c0cabf65,### 154. Palette = 'tab20_r',133e26f4,0.9
41501,38b79494ac749e,a64749db,#### L2,39162a40,0.9
41502,09751c520b0616,feb596df,- Root mean squared error,a4d0c7e9,0.9
41504,193b9c9a4c155f,def046e6,All World,6fb8a8dd,0.9
41507,d6cbd7160961dc,b786dca1,## 6.3.1. Results: First digit,36d74664,0.9
41509,30fdc4a6e3c1db,db95a22b,## Impact of SNAP days,6111ddee,0.9005847953216374
41512,b61ab8f81dc03d,b1a6b6a7,"<a id=""confusion_matrix""></a>
### Confusion Matrix
A confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm.
More information on https://en.wikipedia.org/wiki/Confusion_matrix",64d05394,0.900709219858156
41515,2f47abddfd1928,16946161,"# 5. Model Evaluation

I have already run through many iterations using some of the most common classification algorithm and I have got consistently the best result using XGBoost Classifier.

In this case I will use RepeatedStratifiedKFold to select the hyperparameters of the model.",ae33cc0b,0.9008264462809917
41516,e9b9663777db82,2928132e,#### -LinearRegression with Select From Model,648e8507,0.9008264462809917
41519,4883314a96dc34,869e3fa1,"Random Forest Regressor had the best metrics in previous iterations, therefore let's only tune the hyperparameters for this one model.",50d36836,0.9012345679012346
41523,9bcfa825c8b2e6,3d7a910d,"Görsele bakıldığında gerçekte 0 olan 99 veri varmış bunun 88'i doğru tahmin edilmiş 11'i ise 1 olarak yanlış tahmin edilmiş.
Gerçekte 1 olan 55 veri varken bunların 36'sı doğru tahmin edilmiş 19'u ise 0 olarak yanlış tahmin edilmiş; buradan çıkarılacak sonuç model 0'ları tahmin ederken gösterdiği başarıyı 1'leri tahmin ederken gösteremiyor olur.",220f36e4,0.9014084507042254
41524,3d77c1560bd16e,4fcb4d08,"<a id='8'></a>
# <div style=""background-color:#60cff7; font-size:120%; text-align:center"">Texas choropleth maps</div>",87c141ca,0.9014084507042254
41529,918040fad252ec,ad038d0b,Memanggil dan mengetest gambar,966fcd8f,0.9016393442622951
41530,601e18072783b4,72aff91b,## Top Movie Directors India,36b2b1fa,0.9016393442622951
41531,979f1e99f1b309,73b93a5f,# Predict Testing Data,d1bfebbf,0.9016393442622951
41533,7cfd96218dd933,e2a9e7c0,"#### **ATTENTION**

* HERE IS CHICO
* THE HIGHEST VALUE IN THE WORLD WAS IN THE AMERICA
* THIS SITUATION MAY PROVE THAT THERE IS NO ANOMALY IN THE MEDITERRANEAN LOCAL",7c34d96c,0.9019607843137255
41534,842547b2def18c,14f56229,"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).

The model confidence score is the highest among models evaluated so far.",b8efde6d,0.9019607843137255
41535,523123dad03177,6187443c,This can be a reason as to why Group E scores better at all subjects.- Because they completed the test preparation course.,48a5e4e6,0.9019607843137255
41538,917957c6c4065f,1df5d657,"인기동영상 개수가 1개인 채널의 카테고리 현황입니다.  
People & Blogs ,Entertainment의 비중이 각각 1,2위입니다.  
News & Politics 카테고리는 생각보다 작은 5위이네요.  ",55b8ed68,0.9019607843137255
41541,d0080e3a39bc5c,e6c4ca06,"Now the same thing was done for all the different architectures mentioned above and on both the train sets, followed by Ensembling of the models.

The ensembling was done using **Weighted Averaging** of the probabilities predicted by the different models.

For a detailed analysis on different techniques used for ensembling, here is a great learning resource - 

https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/",2fcde4cf,0.9019607843137255
41542,71b75664517244,12704436,### Josep Guardiola,fc905af5,0.9019607843137255
41545,52cfd66e9ec908,947539b0,"To properly work with Neptune callbacks, replace the `U.` fields in the callback class with your API token, project name etc. Then, add it as a callback to Catalyst and watch the magic happen.",c74adcdf,0.9019607843137255
41547,b01ee6cb674fa3,9ced7683,# US Navy,a8ffd35e,0.9021739130434783
41548,ba4b3bd184acbb,a26d0a83,We can replace the Sentiment category with numerical values.,0f5de724,0.9022556390977443
41549,5169abdc647412,380f7992,### root mean square error,28efc68d,0.9024390243902439
41560,8d70dcae7f40a3,19a3ff6a,### Đánh giá mô hình trên ROC_AUC,472c71ce,0.9024390243902439
41562,fd4017c1514157,c395aa07,"
#### <font color = 'orange'>Spectrogram</font>
* A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. 
* It’s a representation of frequencies changing with respect to time for given music signals.
* .stft() converts data into short term Fourier transform(STFT) so that we can know the amplitude of given frequency at a given time.
* .specshow() is used to display spectogram.",fd8f0896,0.9024390243902439
41563,e19e307b3fd188,87b720b9,#### Train with the best model and best params ,2173955b,0.9024390243902439
41565,ac1abfe1dfe815,181bd20f,## Random Forest,6529dbcb,0.9026548672566371
41566,a5a419dc7245b0,1a054546,##### Accuracy,4279726e,0.9026548672566371
41567,c9b4e282e4e2c1,6de7b716,"Parameter ""Event"" has a lot of NaN values. Let's fill these empty values with the mode of the ""Event"" column.",f44d339f,0.9026548672566371
41570,fdc3afd309b850,0b23d429,"Finally the results! Here we are going to calculate the score on the train and in the test data frame to look for overfitting. Also we will see the predictions as real values, plot the **prediction error** of our model using the **yellow brick** and calculate the metric using **sklearn metrics**. Lastly we are going to save the model using **joblib**.",966bde38,0.9027777777777778
41571,166a62ebb4fc3a,e663c678,And here is the accuracy,db48a079,0.9027777777777778
41574,c01049afb6d307,b1798704,"## BMI categorization
![image.png](attachment:0688feaa-7c0f-4f10-aa0b-5c1f715687c5.png)",d37d3b5d,0.9027777777777778
41584,921fff7d3040db,8c661d0b,# 9. Demo,5f36ced9,0.9032258064516129
41592,0d9a2067267ba1,f7f31047,"### Save data and submit file
",abc194fb,0.9032258064516129
41597,16862cb02d73d5,3ce41ae4,"How to use this?

If the **current timestamp is anomalous** for a use case **drill down** to metrics figure out the set of metrics which have high anomalies in the timestamp to perform RCA on it.

Also a **feedback** from the business user can be updated back in the data which would help in turning this to a supervised/semi supervised learning problem and compare their results.

A enhancement here would be to **combine anomalous behaviour which occur continously** . For eg. big sale days which would result in spike in metrics for few days could be shown as a single behaviour.


",d7ffa1a6,0.9032258064516129
41602,ad26c020235dfc,b1ccca01,Modelling and Prediction:,bf766e48,0.9032258064516129
41609,6cade0b6a41ba2,c5ab757e,## 5.2. Data Modelling on Test Data,e6110293,0.9035087719298246
41612,835a7b4e660d23,884ae61e,### Hierarchical İndexing,53bc7a6e,0.9036144578313253
41613,4daf6153275cbf,9ab64d70,"Below is the map of rating differences. Seems like everyone enjoying Belgian food except Belgians :) One insight after looking for these I got is in Belgium, the food preference is usually French, maybe I should have looked that way.",51db1961,0.9036144578313253
41617,95efc1ad1d3e26,445a991f,"## Latent vectors

$$\alpha l_1 + (1 - \alpha)l_2$$
$$where l_1, l_2 – latent vectors, \alpha \in [0, 1]$$",79de1120,0.9038461538461539
41622,582cb872d19026,07578445,## PieChart of Paid Games by Categories,8d966d69,0.9038461538461539
41623,3d08ca7656dec0,905f4449,# Naive Bayes,bd3f87e3,0.9041095890410958
41624,1eb62c5782f2d7,c99b5392,"### Menemukan Nilai Data Spesifik untuk Probabilitas yang Diberikan
Kita juga dapat menggunakan distribusi normal untuk mencari nilai data tertentu (nilai 𝑥) untuk probabilitas tertentu

### Contoh 4

- Skor untuk ujian Standar dan Pelatihan Perwira Perdamaian California berdistribusi normal, dengan mean 50 dan standard deviasi 10.
- Agensi hanya akan mempekerjakan pelamar dengan skor di 10% teratas.
- Berapa skor terendah yang bisa diperoleh pelamar dan masih memenuhi syarat untuk dipekerjakan oleh agensi?",bb69f147,0.9041095890410958
41631,4c47839b067546,8a636d23,## RandomForestRegressor,1f517b02,0.9042553191489362
41632,71c3c1eab0377d,bb8356ec,"**5 fold Cross Validation Score**


Note: Important Metrics to be seen here are as follows:

     1. Accuracy Metric   : the mean accuracy of the model is 83.8% and the std dev is 2.3%
     2. auc Metric        : Mean AUC is 87% 
     
     
     From these metrics we can say that our model is fairly ok.
     We can further improve it by parameter tuning.",52b4e360,0.9043478260869565
41635,e16860fce156b0,9f704a5c,"#<b><mark style=""background-color: #9B59B6""><font color=""white""> create_report: generate profile reports from a Pandas dataframe.</font></mark></b>

The goal of create_report is to generate profile reports from a pandas DataFrame. create_report utilizes the functionalities and formats the plots from dataprep. It provides information like overview, variables, quantile statistics (minimum value, Q1, median, Q3, maximum, range, interquartile range), descriptive statistics (mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness), text analysis for length, sample and letter, correlations and missing values.

https://towardsdatascience.com/dataprep-eda-accelerate-your-eda-eb845a4088bc",2054f1ce,0.9047619047619048
41642,a76e0e8770b7a0,c4a8fc3c,After 2010?,02863d3b,0.9047619047619048
41646,7454fdc444df16,5ec353ba,### Setting model parameters,a7818ef5,0.9047619047619048
41648,2d40f383473fa4,6bb740d2,## 5.2 Predictions,1da1eff0,0.9047619047619048
41649,f3d5d8917ce5df,9953db91,"# Make a file and submit it <img src =""https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Letterbox_Bourdon.jpg/800px-Letterbox_Bourdon.jpg"" align = ""right"" width= 200>
So, now that we have our predictions, we need to get the submission file in the format that is required by the contest.",e45112f8,0.9047619047619048
41651,b6e698d389d0d3,8e037921,# submission,f02f68b5,0.9047619047619048
41653,1084376bc4897c,c34dd255,### Conclusion: the three methods have comparable accuracy,1b598487,0.9047619047619048
41660,b4ecd6e4277e3c,2ba3fe3e,"### Find final Thresshold

Borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm",94d79d5f,0.9047619047619048
41662,87e94f864d74be,79acf7e6,## Top-20 countries producing most contents:,294bfe9f,0.9047619047619048
41664,c818250dd720eb,97b9a5e6,"# Conclusion


The system scored a QWK of 0.53 on the unseen test data. A score of 1 would be a perfect performance grading every example with the same grade given by expert pathologists. While a score of 0 is a performance no better than chance. The winning entry scored 0.94. Some really interesting ideas utilised by the top solutions included running predictions from several models and chosing the most common predicted value and accessing data from the highest resolution for uncertain regions. I will certainly be gleaning as much as I can from the other entrant's work to improve my own system before taking the knowledge gained with me for other challenges.

This was my first ML competition having started upskilling into the field in March. I thoroughly enjoyed and appreciated the code and ideas that I learnt from and look forward to the next challenge.",68ee40de,0.9047619047619048
41673,898d18d501f68d,319cb695,count of each soil type,d8bdea2d,0.9047619047619048
41677,6a80f915608fc2,31929a65,### Looking at the Test predictions,636938eb,0.9047619047619048
41686,c80939c7c626cf,486eda97,# Score,b9ac31e2,0.9051094890510949
41690,f91f58d488d4af,97a8e052,"#### Creating an Optimizer.

It will handle the SGD process for us.",5df1bbf3,0.9052631578947369
41691,e4525eb0c96f28,36159a19,"Now that we have out Random Forest Regression model, we can plot each of our trees. Note that we will only be plotting the first two levels and the root node for simplicity. If we were to print out the entire tree, it would be too much to fit into a singular plot and we would not be able to see the decisions that go on in each node.",2093a1f1,0.9054054054054054
41693,62037c5832129c,1108d8d6,"* Bias - Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. 

* Varaince - Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.

* Underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data. 

* Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data). If a model suffers from overfitting, we also say that the model has a high variance. 

* Techniques to use to address overfitting:
    - Collect more training data, 
    - Reduce the complexity of the model
    - Increase the regularization parameter,
",61474350,0.9054054054054054
41694,63b44c85e32c1f,b6dc49e7,**difference( )** function ouptuts a set which contains elements that are in set1 and not in set2.,fb9b9562,0.9054054054054054
41697,396bc36edb95d3,5b22816f,#### AOC and ROC for Training data - ANN,965e4f8f,0.9055555555555556
41702,f015d0147e8fbf,d459bb7f,## III. Generating Test Set Predictions,518954fb,0.9056603773584906
41708,2ada0305b68956,615ba125,### 155. Palette = 'tab20b',133e26f4,0.9057142857142857
41713,869a39a3d4dea2,0c9a1158,"similar to the above histogram, 2D histogram also similar however we are not going to use the bin size or hist of 256 becuse the 2D hist a numpy array of 256 by 256 which is 65536 which big and not required, so reducing it to 32 and 8 for 2D and 3D.<br/><br/>

When it comes to multidimensional histogram, we use conjunctive AND operation on the channel **for example Red value of 10 AND Blue value of 30, how many pixels are there**",9020daf8,0.9058823529411765
41716,b86bda7afe3ac3,34326f07,Ruddit -dumb test,16197934,0.905982905982906
41719,726833f92fb87a,70728a04,# XGBoost,7dc5e1b6,0.9060402684563759
41722,c85c94076e9c3a,c357a5bd,### 1- Total_Purchases,3ea0c443,0.90625
41724,3cc097a5859dc1,5f60ad66,# **Applying QDA Model** ,14380d73,0.90625
41732,117fc0956643d0,2088361f,"
## Step 6. Plot Confidence Score for Top 10 Relevant Articles

This section visualizes the confidence score of each question, mean confidence scores of the top 10 articles, and comparison of the confidence score of the 1st relevant articles of each questions. ",68cef9fd,0.90625
41735,9daf8b4a46725e,6707a839,### Predictions are 71% accurate.,7d9cc411,0.90625
41738,49f2274c1dd516,fc67ca74,"# Next Strain
This data set describes the phylogeny of the hCoV-19 / SARS-CoV-2 genomes being sequenced globally. Site numbering and genome structure uses Wuhan-Hu-1/2019 as reference. The phylogeny is rooted relative to early samples from Wuhan. Temporal resolution assumes a nucleotide substitution rate of 8 × 10^-4 subs per site per year.",06b0ffee,0.90625
41739,bd0e173abb7b52,28408e69,**10. Count the average time of work (*hours-per-week*) for those who earn a little and a lot (*salary*) for each country (*native-country*). What will these be for Japan?**,9bce3b0d,0.90625
41741,30fdc4a6e3c1db,ea946b7a,Different states have different SNAP dates so we will have to view them separetly,6111ddee,0.9064327485380117
41742,5ce12be6e7b90e,cd17cd93,"Similarly, we can use this syntax to add new records: ",c0ab62dd,0.9064327485380117
41743,a566b5b7c374e7,f76f4b67,### Deep Sleep versus REM Sleep,b3dc5545,0.9064748201438849
41744,c84925c8171900,2548e9d6,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.6.1 Video Game Rank by Region
            </span>   
        </font>    
</h4>",e21ff7ec,0.9065420560747663
41752,7e1da639035ac5,d78617ae,# <a id='16'>16. KMeans clustering on explored features</a>,120b6c23,0.9066666666666666
41755,9169c4e9c33c90,d2e7fd09,"<a id=""Genre""></a>",725bf880,0.9067796610169492
41760,b2e2c792b886ac,60212742,### Делаем сабмит!!,2a184b39,0.9069767441860465
41763,22bd95f4807a23,3c3cf325,# Conclusion,c05d356f,0.9069767441860465
41765,743ae010f5e875,db8a59bb,# Evaluation Metric,02c54445,0.9069767441860465
41768,063a35f644e3c5,57f7e200,## XGBoost,1c30fb0a,0.9072164948453608
41769,225b4fe5d3894a,9a34fe58,"<a id=""9""></a>
## 9. Evaluate Your System on the Test Set",4b4197b3,0.9072164948453608
41772,ac04ba639d1c93,bda0da6f,# Individual Params Test,748059d5,0.9074074074074074
41776,2f0f808765fc67,9b24b2f4,# **Compute the MSE for the Test Data & Compare with the CV Error**,fd1f6494,0.9074074074074074
41779,fdc3afd309b850,401963e0,"<a id=""scor""></a>
## 10.1 Score Rˆ2",966bde38,0.9074074074074074
41781,c4386b8a01d66e,041785d2,### Random Forest,dc732bf5,0.907563025210084
41783,03048e86a6d806,d902fcd8,So these methods can help you become more familiar with Data Science,1285c231,0.9076923076923077
41789,c115e287523aab,10e401b2,"# Pawpularity Distribution of OOF & Train 
Check **Pawpularity** distribution of `train` and `oof`. ",feb1288b,0.9076923076923077
41795,56785caebaa256,4e380407,"## 7. Comparison with previous forecasts <a class=""anchor"" id=""7""></a>


[Back to Table of Contents](#0.1)",a792961a,0.9078014184397163
41796,957e035ba5b9d5,fccee341,## Predict and save results in csv file,778ab3d3,0.9078014184397163
41798,52ee792e228d54,4b0e610d,### Naive Bayes,5096094e,0.9078947368421053
41805,62487bcd70b199,b10aa7ee,## <a id='8.5'>8.5. lightgbm</a>,f6ae50af,0.9083333333333333
41807,ee23a565163388,b78a9521,This gives out the array of weights given by the logistic regression to each of the features.,88aacbc4,0.9083969465648855
41813,efd44ce2c08541,8ddae20f,# Evaluate,ebc2d00c,0.9090909090909091
41817,a2573183738753,3ef50ba4,# Final Test set ,f6429599,0.9090909090909091
41820,f269d2fbd5f1be,3193ae0d,"**Commentary:**
* The variability in the players' salary in the low category (<75) is the lowest, however this category has the highest number of outliers
* The variability in the players' salary in the middle category (75-89) is high relative to the variability in the players' salary in the low/high (<75 / >=90) category",1264c440,0.9090909090909091
41822,2f964d08c25d93,036bdc47,Distribution graphs (histogram/bar graph) of sampled columns:,1f2e4468,0.9090909090909091
41823,a26032553265a6,d95bcb63,Voted Classifier,f489744e,0.9090909090909091
41824,1466e61d45b718,941564fc,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",b062d92b,0.9090909090909091
41825,be2f4d8a6b73ca,aac58586,**Naive Bayes**,5d8ce40a,0.9090909090909091
41829,bb3d1b4b9f1248,41791170,Average PerCapita analysis 2019 vs 2020 for beer consumption,bf7de324,0.9090909090909091
41840,132fa9714f2046,a46ca642,# Great Job!,3bb1775f,0.9090909090909091
41847,f166950fa915f8,b694d5b4,### Classification Report,a7f6ca5e,0.9090909090909091
41849,b42180a6a5b42f,1262af85,"# Conclusões <a id='conclusoes'></a>
",987cea5f,0.9090909090909091
41856,7b59fc0ff0d10b,0d156bed,"Assuming that Measured NOx = Actual NOx*(1- n* CF),
we can divide the measured NO2 column by a scaling factor of (1-n* CF) (cloud factor) to get an estimate of the actual NOx.",c192b94c,0.9090909090909091
41861,a6eb631926a4d7,58c1bbb3,Cumulative Accuracy Profile / CAP Curve Analysis,0d502aab,0.9090909090909091
41863,d83e5b44d1b80d,851c07bc,***Again Python is the prefered programming Language for all Genders***,62845930,0.9090909090909091
41865,b241b847319d13,e920af3c,# Credits,0fb698f0,0.9090909090909091
41871,6848c86d8a7cb9,869ef05a,"#Acknowledgment:

Yvtsan Levy https://www.kaggle.com/yvtsanlevy/introduction-to-class-activition-map",3bfde6fc,0.9090909090909091
41872,37360278c19104,426b9ce4,"This seems to work pretty well!

Work in progress",21473a41,0.9090909090909091
41877,d1ff7e10ee0102,e9e98226,"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!",2cc71c3c,0.9090909090909091
41884,c13f73168789c2,e45d6354,"## 2. Select columns based on row value<a id='34'></a>
To select columns where rows contain the specified value.

Syntax : `df.loc[:, df.isin([value]).any()]`",16175052,0.9090909090909091
41890,5083d7a61f2426,26fa399e,"Let's see the data in 2020, that was not used in the train or test for the model",541a0fec,0.9090909090909091
41891,90691864eb68c7,4ab3f506,Let's see the parameters for our trained model,3555ef9b,0.9090909090909091
41892,da199f8fb59439,8e3a581c,#### List of Kids Tv-show,baaa665d,0.9090909090909091
41898,83df814455f06c,084a53c6,"Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.


But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. 


We have another tool called `Confusion matrix` that comes to our rescue.",c9cff71a,0.91
41903,312135b445bd23,9c60a820,Clean relevant sentences and add bigrams and trigrams:,8ced381f,0.9101123595505618
41904,d4c5aaa4b36810,c3bffd4e,"### Linear Ridge Regression Evaluation 

The best model was the Linear Ridge Regression model which will now be retrained and evaluated.  ",65441f28,0.9102564102564102
41913,21413205980558,a0256005,# 5.2  Random Forest model（随机森林模型）,84197de0,0.9104477611940298
41929,49ac6594c8f5cf,f1c95a8e,Logistic Regression Model after balancing dataset,6f19f28a,0.9111111111111111
41933,3597174a998d4d,e68e3a6e,"I sort the original variables into qualitative variables and quantitative variables. For quantitative variables, I make the MinMaxScaler.",276892ed,0.9111111111111111
41934,d905cde3391d2b,5772c690,"## Box and whisker plots

**Box and whisker plots** (or box plots) represents five-number summary of the dataset. The five-number values are,
1. Minimum
2. First quartile (25th percentile)
3. Median (50th percentile)
4. Third quartile (75th percentile)
5. Maximum

The following is a representation of box-and-whisker plot

![Box-whisker plot](https://raw.githubusercontent.com/nowke/nowke.github.io/gh-pages/src/pages/stats/images/box-whisker-plot.png)",067dba39,0.9111111111111111
41936,892be0a523578c,633bde1a,"Next, I want to see the sleeping habits of our customers. First I want to see when our customers usually go to bed. To make sure the result is plausible, I only use the the records with total time in bed more than 300 minutes and choose customer with at least 3 records after filtering based on that. **The result shows that most of our customers go to bed between 10 pm ~ 12 pm**.",b0e8d7c0,0.9111111111111111
41938,4fd4b6a80d40e3,40d70cb8,"## Softmax Function Overflow Solution

![image.png](attachment:image.png)",f6913cc3,0.9111111111111111
41942,e0a041e5e2372f,d916a956,"Now I will set level of water contamination based on WQI -

          WQI Range                   Classification

         Less than 25                  Excellent(3)
            26–50                         Good(2)
            51-75                         Poor(1)
        Greater than 75                 Very Poor(0)",7c4357b2,0.9111111111111111
41948,2ada0305b68956,36978f7b,### 156. Palette = 'tab20b_r',133e26f4,0.9114285714285715
41955,99821bc6a45be6,08d6f160,### Analysis:,b9d59346,0.9117647058823529
41958,e170d33ee1da8c,ff2aec5d,"## Submission
Finally we can average the predictions from all models and submit:",253cee3c,0.9117647058823529
41960,3d905ce4828057,3e93358e,"# Comment

**Top 25 and general description of segment A were examined. As a result, based on the mean, std deviation and max values, we saw that the mean and std deviation values of the A segment are very far from each other and that it is not homogeneous within itself. In order to obtain more reliable and homogeneous segments in terms of action, this segment needs to be divided a little more.**",5b006cc3,0.9117647058823529
41961,156bbcff05dcea,1f8f950f,Many times it is required to have the list of distinct values present in a column. How to work that out in PySpark?,66ad1fe9,0.9117647058823529
41965,52cfd66e9ec908,dbf47ee3,## Part 2: Kekas training,c74adcdf,0.9117647058823529
41968,ab657da5329e3f,4f2dd884,# Visual validation,021526f8,0.9117647058823529
41969,395ed8e0b4fd17,189e2aba,### Litecoin(LTC) Volume Traded for last 300 rows,7573ea31,0.9117647058823529
41972,a0a5baa6c7e12a,ec5db9eb,"# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">Benefits</div>

This notebook provides a real-world example of how **AutoViz**, one of the best freeware Rapid EDA tools, can save your time on routinous steps in EDA while allowing you to spend more time on drawing the real insights from your data, using the auto-generated charts producted by **AutoViz**.

The contribution in this notebook expands on the research to prove **AutoViz** to be a good automation tool for Data Analysts/Data Scientists. Previous EDA experiments with **AutoViz** could be reviewed below

- [Using AutoViz to Build a Comprehensive EDA](https://www.kaggle.com/gvyshnya/using-autoviz-to-build-a-comprehensive-eda)
- [Express EDA with AutoViz](https://www.kaggle.com/gvyshnya/mar-21-tpc-express-eda-with-autoviz)
- [EDA and Feature Importance Findings](https://www.kaggle.com/c/lish-moa/discussion/190647#1047649)",551d41de,0.9117647058823529
41973,8f50c9c16db95f,e5ca7438,This larger angle will absorb some extent of energy from the body that makes the energy transmitting to the ball become less.,26cc763a,0.9117647058823529
41974,7f74a04ae75792,6b266d7a,"# <font color=green>Feature Engineering<font>
    
### We have too many Features? Do you think you can make some high level features? 

For example: Instead of three variables for number of ads responded by customer? why not create a single variable which shows ads responded in last three years",d01e91da,0.9117647058823529
41981,c3498779cda661,b8f3505c,"Los grupos están basados principalmente en el año y la cantidad de millas. Es de esperar que estas dos columnas también tengan relación entre sí, ya que en general a mayor cantidad de años más distancia recorrida.",0f531b65,0.9122807017543859
41983,9e27af2600925c,7ece36c6,"## 7 - Test with your own image (optional/ungraded exercise) ##

Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:
    1. Click on ""File"" in the upper bar of this notebook, then click ""Open"" to go on your Coursera Hub.
    2. Add your image to this Jupyter Notebook's directory, in the ""images"" folder
    3. Change your image's name in the following code
    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!",9b556435,0.9122807017543859
41997,fdbbd573ba31c2,8e9898b8,## XGBRegressor,f7c28d74,0.9125
41999,3dd4294f903768,6f453315,# Random Forest,0d89d098,0.9125
42002,98a6794067932a,443943f0,"Le code ci-dessous permet de localiser les différents clients de l'entreprise qui se retrouvent dans le dataframe créé ci-dessus sur une carte des États-Unis. Il s'agit donc des clients n'ayant pas effectué de commandes auprès de l'entreprise depuis un certain temps. Avec l'aide de la fonction Folium, le code sélectionne la latitude et la longitude de chacun des clients présents dans le dataframe ayant été créé dans la cellule ci-dessus et il indique sur une carte du monde avec l'aide d'un marqueur l'emplacement exact de ces clients. Ensuite, le code permet d'afficher la carte qui vient d'être créée. Cette analyse pourra être utilisée par les dirigeants de l'entreprise afin de vérifier l'emplacement des clients n'ayant possiblement pas été satisfaits de leur dernière expérience avec l'entreprise.",08600fe2,0.912621359223301
42005,5f32117bcd5255,8c1378f1,#### CORNER MASS,85882abf,0.912751677852349
42008,77f958b3f41a70,75b5f0ef,# Equity considerations and problems of inequity,2ad9bb69,0.9130434782608695
42010,fe118026267a88,f16c4a34,"## Keep going

Move on to learn **[indexing, selecting and assigning](https://www.kaggle.com/residentmario/indexing-selecting-assigning-reference)**, which are probably the parts of Pandas you will use most frequently.",612efa48,0.9130434782608695
42013,69d50f5e1373f1,360474b7,"# Re-creating the sample submission output

This demonstrates how to loop over the sample submission and make predictions.",ec7545ee,0.9130434782608695
42016,57bad3860b0fa4,4d75f86b,# Building A Submission,05138a5e,0.9130434782608695
42023,b01ee6cb674fa3,136760b0,"# Considerations

With the new features added, some more relevant info can be extracted. 

Lets check the columns we have and Lets start the Space Race

",a8ffd35e,0.9130434782608695
42026,0e2a23fbe41ca9,9fe84c06,"Except null values in ```merchant_id``` column from historical data, every ```merchant_id``` is present in merchant data. Lets explore nulls",64e4762c,0.9130434782608695
42027,33d736abb432d0,9c106fa8,## 第二句分词效果依然不理想，修改用户词典加入有关词汇,d64052a2,0.9130434782608695
42029,f6c1eb62cceb70,5ebf5a9a,Output and submit your predictions,90a1b790,0.9130434782608695
42030,59236ba162ab7b,d066d8b2, 5. Submit,ace5b0ef,0.9130434782608695
42036,9b5de3823ad5ab,6777bada,"### Testing the model
I don't expect much from it, I'm just hoping it's on par with the validation metrics.",33e48774,0.9130434782608695
42046,ce9ed5e2d601d7,285e0722,## New pseudolabels,f58a2f43,0.9133858267716536
42047,44f6a002ecd033,7f5bf9a1,As it appears the making of new columns for log transformations was not very influential for finding a good model.,70bbe106,0.9134615384615384
42053,84127ade6fde87,c031b479,"In non-text applications, we usually do not have the ability to construct the embeddings beforehand, but we will start with the random numbers we eschewed earlier and consider improving them part of our learning problem. This is a standard technique—so much so that embeddings are a prominent alternative to one-hot encodings for any categorical data. On the flip side, even when we deal with text, improving the prelearned embeddings while solving the problem at hand has become a common practice.",f55d05b6,0.9137931034482759
42059,1dd9c6aa74d289,a0bd1edd,"#### List 5 of the most popular crags in USA, France, Spain",5ef9a1be,0.9137931034482759
42062,00001756c60be8,fcb6792d,**Сортировка признаков по важности**,945aea18,0.9137931034482759
42065,e3f3f108cd3869,0c099832,Creating submission file,2b78de2d,0.9137931034482759
42067,0caaec057f7184,a2b87b6a,"Similar trend for fig1 and fig2 (no big difference on item_id or item_sales), since that no sales return isn't that big for each item (max 60)",b875533e,0.9139784946236559
42068,0932046e1f485d,89f4da5f,Distribution of sentiment polarity.,218cc7a3,0.9140625
42069,9c044fa3072552,ee0581fb,"Next, let's use the library folium to create interactive maps with the latitudes and longitudes.",1362842e,0.9142857142857143
42075,675b60eaf415a6,9f6a00b9,"* **Setting compile=False and clearing the session leads to faster loading of the saved model**
* **Withouth the above addiitons, model loading was taking more than a minute!**",68c0b725,0.9142857142857143
42084,3cea0f929a2035,fc033d7f,"Still Russia is the ""winner"". It's nice to see that all the leading countries have decreasing trend in the number of suicides per 100k population. The result of the US is presented here as a comparisan: it's mostly flat but slightly increasing. However, it is still below the numbers in many countries.",04cfbade,0.9142857142857143
42087,0fa9979b5690e9,c0a34ed5,"# Exercício

* Com o conjunto de dados sobre *câncer de mama*, **tente obter o melhor desempenho de acurácia**. 

* Organize e **tenha cuidado** para que seu experimento execute um *protocolo de validação que faça sentido*.

Mais informações sobre esse conjunto de dados poderão ser obtidas em: 
[https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset)",c26eea94,0.9142857142857143
42089,2b36742b49c7bc,490a581a,## Үр дүнгээ илгээх (Submission),c8f8a96d,0.9142857142857143
42095,04bac111ffbe9c,11180387,## Creating the O/P file,82576b17,0.9142857142857143
42097,60da9bbfe39c4b,1b0332ed,"## In July,
* Total Cases=336336
* Total Deaths= 6182
",b0dd8ad6,0.9142857142857143
42099,47a1b1fe51b4ad,694b7c29,"### Submission Dataset

Predicting Survival and Data Preparation",331ded2f,0.9142857142857143
42106,9c26c5dcd46a25,cafe1359,"On remarque ici clairement, avec le cercle et le COS², les corrélations importantes entre l'energie et le caractère ""gras"" des produits. L'axe  $F_1$  va donc parfaitement représenter le facteur ""énergétique"" et l'axe  $F_2$  quant à lui représentera bien les qualités ""sucré / salé"".

Regardons à présent la **contribution des variables aux axes *(CTR)***, elle aussi également basée sur le carré de la corrélation, mais relativisée par l’importance de l’axe :",1bbbb677,0.9146341463414634
42108,4c47839b067546,383d1238,MAPE 14.09%. Результат на kaggle 14.21393.,1f517b02,0.9148936170212766
42111,c7e5f658090347,bbd48249,<a id='Feauture_Importance'></a>,43c78e7d,0.9148936170212766
42113,3f25b363afec54,304e443e,### Hopefully this all will get you started with the competition. Stay tuned with the notebook very soon will upload further code without ```violate competition laws```.,bbdaae25,0.9148936170212766
42115,f6648e47713411,5823b6fb,### 4.2 Save model,f4af4d1c,0.9148936170212766
42116,73893f0467d5e3,9c4dc2ef,# Support Vector Classifier ,279787c6,0.9148936170212766
42117,5f674175839b32,3b9143bc,"Q4)Which are the top 5 publishers on the basis of Global sales?

Ans:

1.   Nintendo with 1784 million.
2.   Electronic Arts with 1093 million.
3.   Activision with 721 million.
4.   Sony Computer Entertainment with 607 million.
5.   Ubisoft with 437 million.

",53a2e343,0.9148936170212766
42118,917957c6c4065f,2b9fad13,"인기동영상 개수가 2~3개인 채널의 카테고리 현황입니다.  
마찬가지로 People & Blogs ,Entertainment의 비중이 각각 1,2위입니다.  
News & Politics 카테고리는 한단계 상승하여 4위이네요.  ",55b8ed68,0.9150326797385621
42119,510b8303776bb6,fca3f301,### Converting ordered categorical fields to numbers,18080db8,0.9150943396226415
42121,f2e5e9fb9eaaf7,5844b696,"<a id=""6.2.6""></a>
### 6.2.6 Prorate of features
Create new `prorate` of each feature that calculate every feature contribution to the sum of all features. Adding `prorate` for each features doesn't improve the model performance. It is decreasing the model from `0.8015` to `0.8014`.",048e0d08,0.9152542372881356
42124,a81661cc35d8d2,bf664a14,"<font size=""3"">Observations:</font>

1. We see that Logistic Regression performs best on dataset with transformed variables. However, even though we have an overall accuracy of 70%, our recall on the predicted class (1) is 48%. Hence, we are only classifying the incidence of heart failure half the time, which is not good performance. This could be due to unbalanced classes, and not enough instances of heart failure for the model to learn from.


2. We get considerable improvement from Random Forest over Logistic Regression, both in terms of accuracy and recall. This could be due to Random Forests being inherently better with unbalanced classes due to drawing samples repeatedly.


3. Applying class weights to the cost function did not really improve upon the results much in terms of accuracy. However, we achieved around 80% recall on the predicted class (1), which could be worth the trade-off on accuracy since we would want to error on the side of caution.",3331f113,0.9152542372881356
42125,1294fb4c86f993,764c829a,"####  'White alone, not Hispanic or Latino, percent, July 1, 2016,  (V2016)' column can be considered to be the most positively correlated to guns per capita",4471e513,0.9152542372881356
42128,b660910fcc2954,87faea32,### Target data,80b74f88,0.9152542372881356
42129,9169c4e9c33c90,54bb80db,"# Genre

[Back to top](#Top)",725bf880,0.9152542372881356
42131,bb0905d33ae417,b76d7391,"As the order of the images loaded into `data` is not necessarily the same order as that in the required submission, we will need to rearrange the predictions on the test set.

The desired order for submission is that of `sample_submission.csv`. while the order of the test set loaded into `data` can be accessed by calling `learn.data.test_ds.items`. We will first create a dictionary assigning each image in the test to its prediction by the model, then call the keys in the order in  `sample_submission.csv`.",25fd1965,0.9152542372881356
42134,a44368590e878a,4251bbf0,"**Columns**

1. **date** YYYY-MM-DD
2. **time** Time (0 = AM 12:00 / 16 = PM 04:00)
3. **test** the accumulated number of tests
4. **negative** the accumulated number of negative results
5. **confirmed** the accumulated number of positive results
6. **released** the accumulated number of releases
7. **deceased** the accumulated number of deceases",77743ba8,0.9152542372881356
42138,d07915a6e6992e,9197a997,"**Support Vector Machines**

SVM builds a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize the error. The idea behind SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.",2b912140,0.9153846153846154
42139,09751c520b0616,586774e7,- Scores,a4d0c7e9,0.9153846153846154
42141,631cd434fc3aa2,9a5c4e0b,"We'll start with XGBoost since gradient boosting dominates many Kaggle competitions and achieves state-of-the-art results on a variety of datasets. Let's see the parameters:

* _colsample bytree_: Percentage of features used per tree. High value can lead to overfitting (default=1).
* _subsample_: Percentage of samples used per tree. Low value can lead to underfitting (default=1).
* _learning rate:_ Step size shrinkage used to prevent overfitting. Range is (0,1).
* _max depth_: Determines how deeply each tree is allowed to grow during any boosting round (default=6).
* _min child weight_: Minimum sum of instance weight (hessian) needed in a child. The larger min child weight: is, the more conservative the algorithm will be (default=1).
* _n estimators_: Number of trees you want to build.
* _reg alpha_: L1 regularization term on weights. Increasing this value will make model more conservative.(default=0).
* _reg lambda_: L2 regularization term on weights. Increasing this value will make model more conservative (default=1).
* _gamma_: Controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits.
* _silent_: 0 means printing running messages, 1 means silent mode (default=0).
* _n_jobs: Number of parallel threads used to run xgboost. (replaces nthread).
* _random state_: Random number seed (replaces seed).

We'll use Cross Validation as model valition technique and Root Mean Square Error (RMSE) as evaluation metric.

**NOTE**: We specify negative MAE since scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere (you can read a bit more of this [here](https://www.kaggle.com/alexisbcook/cross-validation)).",2b74febb,0.9154929577464789
42144,bddd799cdbbae8,3c4ca7b5,****Logistic Regression Classifier****,b44e3c08,0.9154929577464789
42149,f91f58d488d4af,df2cbe2a,"The first thing we do is to replace our linear function with PyTorch's nn.Linear module.

nn.Linear does the same thing as our init_params and Linear together. It contains both weights and bias in a single class.",5df1bbf3,0.9157894736842105
42150,840534f2908a9c,df5c39f8,**4. Prediction**,8081c3cc,0.9157894736842105
42151,169177b6e9edea,1d437b9c,<h3><b>Ensemble,ca42152f,0.9157894736842105
42153,4ae6a182abac64,ac3edc1d,*** ROC Curve**,418676c5,0.9159663865546218
42158,173d2db6d2eddb,31901cbc,"This system recommended the ""Gospa"" to us according to 1 million data.
As someone who loves ""Toy Story"" I will watch ""Gospa"".",0e72ad06,0.9166666666666666
42163,8447633e1d256c,b973baf5,## Metrics,60d593ce,0.9166666666666666
42165,1d5daeca89f48d,c7ba4579,"## Challenges/Limitation in Tf-IDF

==> context 
==> Sequence of word is completley lost

Ex: Movie is fantastically bad",48d478bc,0.9166666666666666
42166,0800c019d227f2,85c9fdf5,"## Discussion
- As described above, I used features as it was. Better score is expected by adequate feature engineering.
- In 'optuna.integration', several models are covered, for example, Keras, sklearn, XGBoost and so on. It's worth tryng them.",9cd3ffa1,0.9166666666666666
42170,7c7a7db391c517,1ad92fa8,# To be continue,f53450dc,0.9166666666666666
42171,593d1d3d1df05a,b56d318a,"> As one can see my trained model is better but only valid for English, while pytesseract works for quite many laguages",bc682ffe,0.9166666666666666
42172,b10bd75889dad9,df54cbf2,## Precision and Recall,ee00ceee,0.9166666666666666
42174,c0e2a467cf23ee,5d7f9907,Scaling coordinates improved the result from **1532958.68** to **1524756.18**.,6d02cd29,0.9166666666666666
42175,dd3721cb49c1fd,f26499aa,"<a id='12'></a>
<div style=""margin: 0px; padding: 10px; background-color: #1de9b6  ;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.2);
            border-radius:2px"">
  <div style=""margin: 0; padding: 0; width: 100%"">
      <h1 style=""color:white;text-align:center""><b>12. Forecasting for the values for the future time periods 📈</b></h1>
  </div>
</div>",1a53fdd9,0.9166666666666666
42177,d8d227c158d883,cabd03c1,### Loop,3391b4a7,0.9166666666666666
42181,396bc36edb95d3,808d6e28,#### AOC and ROC for Testing data - ANN,965e4f8f,0.9166666666666666
42182,b3681fd423741d,5c53fea7,# 7. Apakah tahun pembuatan mobil berpengaruh terhadap total jarak pemakaian? Sertakan argumen yang mendukung jawaban.,1aec06ce,0.9166666666666666
42184,10b5af05d804ff,79572163,.. is 0.0.86011... It is the 13d place!,4a9b1705,0.9166666666666666
42185,e82462cdc998a7,2477d820,"## 6. Prediction & Submission <a class=""anchor"" id=""6""></a>

[Back to Table of Contents](#0.1)",b39bf244,0.9166666666666666
42187,a3ae04b78e45b5,b5fb3760,**COUNT PLOT**,4195da8b,0.9166666666666666
42189,07f5853e4db8f8,40e8835c,# Districts,d13c2c32,0.9166666666666666
42193,5cfb546af2b8ce,88a6b711,Lets have a look at other data ,e8730ad1,0.9166666666666666
42195,6b2776f151ed9c,7904b319,# Ensemble,40406d5c,0.9166666666666666
42196,6b383ec35229a2,a0c75019,"So we try to beat 0.1144 using transfer learning.

With the same parameters (lr=1e-2 and bs=2048), we obtain a better score of 0.1129.
    
Can we make transfer learning work better? Let's try smaller learning rates :

lr=1e-2:     0.1129

lr=5e-3:     0.1090 !!!!!!!!

lr=1e-3:     0.1105

lr=5e-4:     0.1120

We will discuss the results later.


",0c41b61b,0.9166666666666666
42207,71d5f1925c2b4b,feadbcf0,#### All the continuous features are left skewed,63ee03ac,0.9166666666666666
42214,2a377ced98d67a,7ed139ac,### 5.1. Test sample case,262231a8,0.9166666666666666
42217,268a610bbc64b4,57040a1d,# 6. Identify and target users likely to book international flight,8a16f301,0.9166666666666666
42218,d46508f983e086,795684a3,**Testing Model**,454138b8,0.9166666666666666
42222,cf46cd6f7c55c0,e943b2b2,"# Conclusion
In this kernel, we learned what pseudo labeling is, why it works, and how to deploy it. Using it on the data from Instant Gratification competition we observed it increase CV by an impressive 0.005! Pseudo labeling QDA achieved CV 0.970 and LB 0.969. Without pseudo labeling, QDA achieved CV 0.965 and LB 0.965.

When you run your kernel locally, it will only pseudo label the public test data (because that is all that `test.csv` contains). When you submit this solution to Kaggle, your submission will load the full `test.csv` and pseudo label both the public and private test data set. Thus you will approximately double your amount of training data for your submissions!",191b86b8,0.9166666666666666
42225,a69d41047fdd3e,3ab81888,"# Keep going

You've looked at the schema, but you haven't yet done anything exciting with the data itself. Things get more interesting when you get to the data, so keep going to **[write your first SQL query](https://www.kaggle.com/dansbecker/select-from-where).**",b1f28647,0.9166666666666666
42229,999258a81ba32a,63abd559,# Prediction,48cd3d21,0.9166666666666666
42231,0f5085b162bd9f,55802da3,# Mini Batch K-Means,a3d989ee,0.9166666666666666
42233,ab6da5994949a3,ad897e4d,## Random Forest Training Results,fae6b91d,0.9166666666666666
42237,64c5c66fd713b1,613079a5,### First draft 01-16-2020,5814b2b3,0.9166666666666666
42245,7ba63a2d9abb58,84bef78c,<h2>Covid-19 Trend in Top 5 Countries By Mortality Rate</h2>,821a261f,0.9166666666666666
42246,98ea617d18c9cc,0ec6e9f1,# Main function from where all the functions are called,e6316d11,0.9166666666666666
42247,56a583a039b57c,78753c56,# Top 10 highest accounts in terms of mean native balance ,c0526ea5,0.9166666666666666
42248,64a336ac34d95c,95d24082,"# KNN classifier
",be73a990,0.9166666666666666
42251,c349ee5a821411,bcb2cfc0,"In the next days im going to check this Kernel in order to improve it, i would be very happy if someone advises me.

Let's do some Linear Regression, we are gonna use all the features to predict the Happiness Score.",572b269d,0.9166666666666666
42257,2ada0305b68956,dd1a68c4,### 157. Palette = 'tab20c',133e26f4,0.9171428571428571
42259,e9b9663777db82,846bb7c6,### LinearRegression with PCA,648e8507,0.9173553719008265
42262,8ec771f5600a61,add70249,# using neural network,48364c1f,0.9175257731958762
42279,0858e1bb3cbaca,2d71dce1,2. What is the average discount given for each type of products?,78548374,0.9180327868852459
42282,5ce12be6e7b90e,3a5ebcdc,"### Looping over dictionaries

By default, `for` loops over the dictionary keys:",c0ab62dd,0.9181286549707602
42284,ffd1df95ca5289,823e682e,"mean of kfold accuracy is 83% let's check f1 score, precission and recall of dependent variable ",db00c338,0.9183673469387755
42288,0e7ac281a19feb,42117336,## Submission,5b84d10f,0.9183673469387755
42296,d96642860ab3dd,961fc457,"Thus, [ RandomForestClassifier, GradientBoostingClassifier, XGBClassifier ] have the highest accuracy after tunning hyperparameters with RandomizedSearchCV. and the others comes on---",98419d48,0.9186046511627907
42301,ccabe7a86825ce,db6dd98d,**Validate model**,d766cbf9,0.918918918918919
42306,b7b1057764fa02,6f42a419,"Note that most of diagonal elements have values 300, and the non-diagonal elements have values 0, indicating that the model labeled most of our data correctly.

Next I will plot the confusion matrix for the evaluation images. Once again, the matrix is not normalized so it is important to note that for every label we had 30 evaluation images.",5053a192,0.918918918918919
42308,63b44c85e32c1f,34bcfdb5,**symmetric_difference( )** function ouputs a function which contains elements that are in one of the sets.,fb9b9562,0.918918918918919
42312,ac9b48d531bad9,63baceb9,**Understanding the effect of each feature in Random Classifier Model using SHAP**,95965e35,0.918918918918919
42314,c3336e5345f6c5,5f43f453,##5. Validar e implementar modelo preditivo,a5ec5530,0.918918918918919
42316,a6c34cd514e30e,3e43e581,Scatter and density plots:,bf603ddd,0.918918918918919
42323,9ceb7278784462,a376013a,## <a id='24'> 20.Xgboost</a>,3768a567,0.9193548387096774
42328,ee9ddc756b2d4a,3af63bb9,"# Soluzione finale sul test set

### rialleno i modelli sull'intero training set e calcolo le prediction sul test set",e367eab3,0.9195402298850575
42330,d5f78aa381f58d,1939d1e8,## SVM,d60f358f,0.9195402298850575
42331,3c2033cc99c12c,7a3596e4,*Print the model and check the parameter of the network*,dfa22a54,0.9197080291970803
42332,c80939c7c626cf,ccc2940c,# 13 Naive Bayes,b9ac31e2,0.9197080291970803
42335,bbad077c274022,c6eb9474,*Seems like I am very consistent with my Basic steps.*,3c2e3dea,0.92
42337,7e1da639035ac5,70d63d57,### <a id='16.1'>16.1 Elbow method</a>,120b6c23,0.92
42344,cee088a6840708,1fcc474b,# Step 12 -> Learn more about the model,55463e1c,0.92
42348,b10bd75889dad9,4a30a329,"##### Precision
TP / TP + FP",ee00ceee,0.92
42358,50d4ddf1953997,50e91d29,"This was the interactive Black Mirror episode, what about an actual movie?",90bdddd6,0.92
42359,0687cd5c8597db,615c6171,### **Making Predictions on the Unseen Data**,4edec76a,0.92
42362,83df814455f06c,6ab4c6a7,"# **15. Confusion matrix** <a class=""anchor"" id=""15""></a>

[Table of Contents](#0.1)


A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.


Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-


**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.


**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.


**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**



**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**



These four outcomes are summarized in a confusion matrix given below.
",c9cff71a,0.92
42365,639e8aae4e046e,a42335e6,**Submission** ,77deb4cb,0.92
42368,81712ee7510ac5,665f66c7,**functions**,c4685e79,0.92
42369,4c47839b067546,d1851369,## ExtraTreesRegressor,1f517b02,0.9202127659574468
42374,ac1abfe1dfe815,7cd9acb1,## AdaBoost,6529dbcb,0.9203539823008849
42377,d1ff7e10ee0102,d60a8173,"# Last but not the least, dummy variables",2cc71c3c,0.9204545454545454
42380,6d1c11d28bb481,98ef61b5,# Final,62cf76ed,0.9206349206349206
42382,06ecf7a304c309,6ec0099c,이제 입력 시퀀스를 이용해 시퀀스를 예측하는 함수를 만들어봅시다.,714de627,0.9206349206349206
42394,c950cff74e51ac,a43496a1,**Relationship of each numeric variable**,d59bf323,0.9210526315789473
42396,a1dcd92986bc84,23ca255b,"## Final remarks

You can obtain better results by increasing the size of the training sample,
train for more  epochs, explore other base encoders for images and text,
set the base encoders to be trainable, and tune the hyperparameters,
especially the `temperature` for the softmax in the loss computation.",730acaaa,0.9210526315789473
42397,31b564f11ef638,3973c2e1,### submit final predictions,424f9692,0.9210526315789473
42402,04ff2af52f147b,6fbac38d,"Now that the model is initialized and trained with the best parameters we could find, we use cross validation on different validation sets to get an accurate view of the model's performance on our training set.",d5f37be9,0.9213483146067416
42407,4fa553c2b837d4,17572b1f,"The left plot shows the partial dependence between our target, Sales Price, and the Lot Area.

**The partial dependence plot is calculated only after the model has been fit.**",c65a23e9,0.9215686274509803
42409,7cfd96218dd933,8bf3851b,"#### **ATTENTION**

* THE FIRST POINT WITH BRIGHTNESS LATITUDE: 40.042
* THE FIRST POINT WITH BRIGHTNESS LONGITUDE: -121.383
* THE FIRST POINT WITH BRIGHTNESS DATE: 2021-07-20
* THE FIRST POINT WITH BRIGHTNESS TIME: 18.25 FOR TURKEY


* THE SECOND POINT WITH BRIGHTNESS LATITUDE: 40.044
* THE SECOND POINT WITH BRIGHTNESS LONGITUDE: -121.369
* THE SECOND POINT WITH BRIGHTNESS DATE: 2021-07-20
* THE SECOND POINT WITH BRIGHTNESS TIME: 18.25 FOR TURKEY


* AQUA DATA VALUES SAME AS MAIN DATA",7c34d96c,0.9215686274509803
42411,842547b2def18c,d6223bd7,"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).

The model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.",b8efde6d,0.9215686274509803
42416,52cfd66e9ec908,7d5c1812,"Kekas is a more simpler Fastai-esque way to train your model. Apart from having an extremely brilliant name, kekas gives you the simplicity of fastai and allows you to easily train and infer models.",c74adcdf,0.9215686274509803
42417,523123dad03177,e311a655,Group demographics?,48a5e4e6,0.9215686274509803
42419,d0080e3a39bc5c,146fdfbe,Bayesian Optimization was used to find the optimal weights to combine 14 different models trained on either one of the Two Datasets as mentioned above.,2fcde4cf,0.9215686274509803
42420,fc8e0042411c46,39c24830,# VIF,af476c2a,0.9216300940438872
42421,71c3c1eab0377d,a78f28b1,**Check Model Parameters**,52b4e360,0.9217391304347826
42423,ff3a8ce61fab6a,50f148b6,"<hr>
<div>
    🔺 <b>Alert</b><br><br>
    <b>datatype equavilant</b> which we talk about it at first                 <b>alert</b> ☝ has same importance when we use <b>matmul</b> function.
    But ✋ you must know following two case <br> 
    <ol>
        <li>one matrix has compination datatype as float and int</li>
        <li>one matrix has same datatype as float or int</li>
    </ol>
    At first case if this matrix multiplay by other matrix has same           datatype as float or int you will not have error.<br><br>
    This we see it in <b>Exampel 2</b> when first matrix has same float       datatype and second has compination float at 4.0 and int at other numbers.
    <br><br>
    At second case if this matrix multiplay by other matrix has same           datatype and this datatype different from datatype of it you will get     error.
    <br><br>
    You can try this case at <b>Exampel 2</b> by change 4.0 at second         matrix which stored in <b>matrix2</b> constant to 4.  
    
</div>



<p>
    Let's talk about <b>matrix_transpose</b> function  We can use it to     get a value of <b>Matrix transport.</b>
</p>
<h2>Matrix transport [......] <sup>T</sup></h2>
<hr>

### Exampel ",9afe1654,0.921875
42426,b61ab8f81dc03d,68e1e294,"* (Not Survived) True Negatives : **501**, False Positives:  **48**
* (Survived)     False Negatives:  **92**  True Positives : **250**

Considering this results we can calculate the Accuracy hit ratio.

Accuracy is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition. That is, the accuracy is the proportion of correct predictions (both true positives and true negatives) among the total number of cases examined. To make the context clear by the semantics, it is often referred to as the ""Rand accuracy"" or ""Rand index"".It is a parameter of the test. The formula for quantifying binary accuracy is:

Accuracy = (TP + TN)/(TP + TN + FP + FN)
where: TP = True positive; FP = False positive; TN = True negative; FN = False negative

https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification",64d05394,0.9219858156028369
42429,4ae464582bac51,a97e3a7d,# LINEAR REGRESSION BEFORE UNDER-SAMPLING,ca6a52ce,0.922077922077922
42433,663bbc9eaf267b,ee122a58,## CatBoost Regressor,32445529,0.922077922077922
42434,241cf32abb22d8,ba37607b,"For models based on the top 10 features selected by F-Score, the KNN model performs significantly better than the Naive Bayes model. Decision Tree performs similarily comparing with either KNN or Naive Bayes models. It is hard to decide the optimal model at this stage with only AUC score available, but based on the above performance comparison, it is clear that all models with the top 10 features perform better on the test data. Therefore, for further evaluation on accuracy, precision, recall, F1 Score and confusion matrix, I only consider models with the top 10 features.",47157066,0.922077922077922
42437,b0c2805cd5c087,4c7aefbd,Image academyforlife.va,0446f327,0.9222222222222223
42440,d6cbd7160961dc,fd684510,## 6.3.2. Results: Second digit,36d74664,0.9222222222222223
42445,a2176d4653ef60,21e76016,# Testing and Submission,ac908675,0.9223300970873787
42447,6a80f915608fc2,cf000d78,"## <a id=""OutputKaggle"">Output Kaggle Predictions</a>
Back to <a href=""#Index"">Index</a>",636938eb,0.9226190476190477
42448,2ada0305b68956,e7b57301,### 158. Palette = 'tab20c_r',133e26f4,0.9228571428571428
42452,44f6a002ecd033,1e6ea98b,#### Final Model,70bbe106,0.9230769230769231
42461,6f1481148352e9,d565b5a1,**The largest number of fires in summer and autumn ~ 23k. Least number of fires in spring - 9.3k.**,7cfbdb8f,0.9230769230769231
42462,cf08b03b002c13,f428edcf,"In general,any content on Reddit is heavily filtered by moderedotrs. To my surprisde, ML algorithms are more likely to filter mass orinited than NSFW content.",104d416f,0.9230769230769231
42464,020c28a360b0cd,977d95a8,"The hardest part of this project was figuring out how to write the code for being eligible for president. The easiest part of this project was determining that a user is not eligible to be president. The difference between my plan and final project is I did not use == ""no"" for determining if a user needs to be born in the us and instead I used =! so that any other input besides yes would make that statement true. I planned too little because I needed to fix some minor problems when coding. My idea was just right. If I had unlimited time, I would determine if the user was eligible for many random things that are not just related to politics. ",2ba397f0,0.9230769230769231
42465,e8a3483b83a26e,37411c0e,**THE ACCURACY FOR THE MODEL CAME OUT TO BE 97% WHICH IS PRETTY GOOD.**,0c5db5fd,0.9230769230769231
42466,80ad12f326ab70,84df9303,#### Merge datasets,da404a16,0.9230769230769231
42474,71d3e4aee86e3e,1cf56ec9,## Covid 19 State-Wise Tabulated Data,69706f0b,0.9230769230769231
42475,c970849d1f6da2,21dc7d8a,"# Next Steps: 
Explore: 
* Different data cleaning strategies
* Training for more epochs
* Different models eg. CBOW, skipgram
* Start Mining ",056e3955,0.9230769230769231
42479,2facf256353117,b21200a2,"# References
* Larobina, Michele, and Loredana Murino. “Medical image file formats.” Journal of digital imaging vol. 27,2 (2014): 200-6. doi:10.1007/s10278-013-9657-9
* https://www.fda.gov/radiation-emitting-products/medical-imaging/mri-magnetic-resonance-imaging
* https://www.verywellhealth.com/what-is-the-blood-brain-barrier-3980707
* https://www.verywellhealth.com/gadolinium-enhanced-lesion-2440506
* 
",18f579be,0.9230769230769231
42484,dbe40fdf51456d,fd2ca98d,"As can be seen the results are more than satisfactory.

In a forthcoming notebook I shall apply double ML to some '*real-world*' data.

# References, links, and related reading

Double ML
* [Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins ""*Double/debiased machine learning for treatment and structural parameters*"",  The Econometrics Journal, Volume 21, Pages C1-C68 (2018)](https://doi.org/10.1111/ectj.12097)
* [Alexandre Belloni, Victor Chernozhukov, and Christian Hansen ""*Inference on Treatment Effects after Selection among High-Dimensional Controls*"", The Review of Economic Studies, Volume 81, Pages 608-650 (2014)](https://doi.org/10.1093/restud/rdt044)

Causal Forests
* [Susan Athey and Guido W. Imbens ""*Recursive partitioning for heterogeneous causal effects*"", PNAS volume **113** pages 7353-7360 (2016)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf)
* [Susan Athey, Guido W. Imbens, and Stefan Wager ""*Approximate residual balancing: debiased inference of average treatment effects in high dimensions*"",  Journal of the Royal Statistical Society Series B (Statistical Methodology) volume **80** pages 597-623 (2018)](https://doi.org/10.1111/rssb.12268)
* [Stefan Wager and Susan Athey ""*Estimation and Inference of Heterogeneous Treatment Effects using Random Forests*"", Journal of the American Statistical Association
volume **113** pages 1228-1242 (2018)](https://doi.org/10.1080/01621459.2017.1319839)
* [Susan Athey, Stefan Wager ""*Estimating Treatment Effects with Causal Forests: An Application*"", arXiv:1902.07409 (2019)](https://arxiv.org/pdf/1902.07409.pdf)

Packages

* [EconML](https://github.com/Microsoft/EconML) (GitHub)
* [EconML documentation](https://econml.azurewebsites.net/)
* [Causal ML: A Python Package for Uplift Modeling and Causal Inference with ML](https://github.com/uber/causalml) (GitHub)
* [DoubleML - Double Machine Learning in Python](https://github.com/DoubleML/doubleml-for-py) (GitHub). See also: [Philipp Bach, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler ""*DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python*"", arXiv:2104.03220 (2021)](https://arxiv.org/pdf/2104.03220.pdf)

Related reading

* [Judea Peral ""*The Book of Why: The New Science of Cause and Effect*"", Basic Books (2018)](https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097616/)
* [Eleanor Dillon, Jacob LaRiviere, Scott Lundberg, Jonathan Roth, and Vasilis Syrgkanis ""*Be careful when interpreting predictive models in search of causal insights*""](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html)
",f3e8a1e4,0.9230769230769231
42486,5af9bf52e5f17c,05e536ba,"Work to be continued.

1) Fix the way countries with provinces are grouped (done)

2) Figure out how best to reshape the dataframe. By country? Or by date?

3) Plot based on date, or date since a certain day (done)",f98ab90d,0.9230769230769231
42490,8e9d63e1f6319e,bd453905,Picture with coordinate orientation is in this [topic](https://www.kaggle.com/c/google-football/discussion/188220). It's easy to prove with several game steps and correspondings ball positions from the video above.,4743b346,0.9230769230769231
42495,aa46e9376825a5,bbad462f,### Display the observations where fatal flag is “Yes”,57792d96,0.9230769230769231
42500,3f451680b1857b,63bc8974,# Train,56c45a1b,0.9230769230769231
42512,f2f2db16a2f86c,cb64bbad,The RMSE metric squares the error before it is averaged and so higher weight is given to larger errors. The presence of ouliers significantly affects the performance.,ffc6a115,0.9230769230769231
42514,4d91e84c564cbe,c4ebe37c,These multiple return values can be individually assigned as follows:,355a43e3,0.9230769230769231
42515,6e8f3e8ed1c241,e5089dd0,"def _floodfill(self, img):
    back = Back._scharr(img)
    # Binary thresholding.
    back = back > 0.05
    
    # Thin all edges to be 1-pixel wide.
    back = skm.skeletonize(back)

    # Edges are not detected on the borders, make artificial ones.
    back[0, :] = back[-1, :] = True
    back[:, 0] = back[:, -1] = True

    # Label adjacent pixels of the same color.
    labels = label(back, background=-1, connectivity=1)
    
    # Count as background all pixels labeled like one of the corners.
    corners = [(1, 1), (-2, 1), (1, -2), (-2, -2)]
    for l in (labels[i, j] for i, j in corners):
        back[labels == l] = True",c2cfb626,0.9230769230769231
42522,c8c4705cca1ebb,f1389a4f,# 4. Output,6d9d7107,0.9230769230769231
42526,8ac70416723897,b8784806,Submission run using the best model,d32fd8f6,0.9230769230769231
42528,49ee86d074de69,ad2d2fc1,"* if the probality is:
  * below 0.5, it places a 0
  * above 0.5, it places a 1",71ccc6d3,0.9230769230769231
42534,9169c4e9c33c90,baff2a70,Which genre has been more popular in the Top 50 over the years?,725bf880,0.923728813559322
42535,1294fb4c86f993,7c80e0d8,Finding the best fit line using `numpy polyfit` function,4471e513,0.923728813559322
42538,55a5e31d03df9f,6cb02e91,Let's replicate the same for test,06dce00f,0.9238095238095239
42540,7454fdc444df16,7a8fcf73,### Fitting the model ,a7818ef5,0.9238095238095239
42541,b01ee6cb674fa3,a4e2a736,"The year 1957 marked the beginning of the space race, with the USSR launching Sputnik-1 by october 4th. 

November 3rd they launched the second spacecraft, Sputnik-2, whose passenger was the dog Laika

---

The next launch was made by US Navy on december 6th, Vanguard TV3, but was a failure. 2 monthes later, ABMA was responsible for launching the satellite Explorer 1, which became the first US satelite to orbit the earth",a8ffd35e,0.9239130434782609
42542,30fdc4a6e3c1db,2bbc9cd7,"What we see:
* All the states have higher sales on SNAP days. We can see the max increase in WI (~2k more sales on each day) while CA and TX have ~1k more sales on each day of SNAP
* We have seen higher sales during the first 10-15 days of a month.We have also seen that SNAP days fall in the first 15 days. So, there might be a combined effect.",6111ddee,0.9239766081871345
42548,4ae6a182abac64,207127d2,![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576),418676c5,0.9243697478991597
42554,0ad8d416b89b78,c708745e,# Using folds to further improve accuracy,0b0562f0,0.9245283018867925
42555,23df07a474aaae,ab68c5a4,**Visualising the output**,0ea40276,0.9245283018867925
42560,738bfced935b69,0496ecbb,## Data Modeling,2d3c592d,0.9246575342465754
42562,0caaec057f7184,a254bbd8,"#####  About the item with the most return sales
* item_id: 2331
* sales return and sold: 60/542
* category: 20 (Game-PS4)

    * 100 negative-sales items
    * 157 items with record in training data
    * 175 items in total",b875533e,0.9247311827956989
42564,ba4b3bd184acbb,30d9b716,Then we can merge the DataFrames to have all the necessary information in one DataFrame.,0f5de724,0.924812030075188
42566,fdbbd573ba31c2,971d2bb1,## MLP,f7c28d74,0.925
42567,0d58c434c7db1e,4ffaadc6,"Goryachkina, Aleksandra from Russia is youngest GM title holder.",517e01d3,0.925
42574,6e28c4f557f736,3b1ba59a,"# NLP - EDA, Bag of Words, TF IDF, GloVe, BERT: https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert ",021fdf75,0.925
42582,1011899b959f44,9e63d1f1,"Display a bar chart that showcases the ranges of individuals who died in relation to each allegience.

From the graph, it seems that most individuals who died do not have an allegiance to any particular house. However, out of those that do it seems that the Night's Watch has suffered the most deaths and House Arryn has suffered the least.",0b112382,0.925
42588,20b372b6e4e276,51d82f61,There are a number of clear patterns that allow us to hope that we can improve the solution.,ec8b0860,0.9253731343283582
42589,a4aa36df07fd53,c0a1b260,"## Building a model

ada banyak macam tipe model. Kita akan mencoba yang simple vs kompleks. 
Logistic Regresion Vs Gradient Boosted Macine
",d2f42b6d,0.9253731343283582
42594,2f47abddfd1928,204943bd,"The confusion matrix shows 502 classified as dead and 47 classified as dead when they are actualy alive.

In the case of passengers that are actually alive, the model found 266 correctly alive, but 76 that are actualy dead were found as survivors.

The false positives are probably our worst performance.

Let's see now the result in the test set:",ae33cc0b,0.9256198347107438
42598,fdc3afd309b850,7c28188a,"<a id=""pred""></a>
## 10.2 Predictions",966bde38,0.9259259259259259
42599,2b39f4ff896f97,282e9797,Save model using Pickle,3ddfe182,0.9259259259259259
42600,db5a369894fef6,0ce9210c,"## NOT THE END, Just the beginning
# Part 2 NEXT WEEK:
- missingno
- plotly

",065aaf61,0.9259259259259259
42611,4883314a96dc34,a0df510e,### Evaluate Model Performance (3),50d36836,0.9259259259259259
42613,d128317750d689,44caf305,And here is the performance in numbers and accuracy:,d87f7428,0.9259259259259259
42618,b9328fe3b0cefc,27762c6f,"## 数据文件描述(This part only for chinese guy)

数据集中的名称、日期等都做了标准化的处理，对于男女比赛，分别用`M`开头和`W`开头来区分。如果需要引入外部数据的话，可以通过`MTeamSpellings.csv`和`MTeamSpellings.csv`进行名称的映射。

### 数据第一部分 - 基础数据
- 基础信息：
    - 球队ID和球队名。
    - 历史锦标赛种子。
    - 常规赛、竞标赛、NCAA锦标赛的最终分数。
    - 包括日期和地区名称的赛季级的详细信息(关于赛季，比如2020赛季，通常指的是结束于2020年，而不是开始于2020年，因此如果是2019年开始，2020年结束，依然称之为2020赛季)。
- MTeams.csv and WTeams.csv
    - 球队信息，注意不是每个球队在每个赛季都有比赛，因为比赛数据只包含甲级部分。
    - 球队ID：标识一个大学的一只球队，4位数字，1000~1999表示男队，3000~3999表示女队，该ID是不会变的。
    - 球队大学名称的简洁写法。
    - 球队为1级联赛的第一年，最早是1985年，注意这部分只有男队。
    - 球队为1级联赛的最近的一年，如果目前依然是一级，那么该值为2020，同样也只有男队。
- MSeasons.csv and WSeasons.csv
    - 不同赛季的历史数据，包含一些赛季属性。
    - 赛季表示比赛的年份，注意当前是2020赛季。
    - DayZero表示赛季开始的那一天的日期，集合DayNum可以计算当前日期。
    - RegionW,RegionX,RegionY,RegionZ表示四个分组区域，最后四强就是这4个区域中各自的最后冠军。
- MNCAATourneySeeds.csv and WNCAATourneySeeds.csv
    - 锦标赛的种子信息。
    - 赛季年份。
    - 种子：格式为3/4字符组成，前3个如下，例如W01，表示W分区的1号种子，如果有第4位，可能是a/b，进一步区分前3个字符相同的球队。
    - 球队Id。
- MRegularSeasonCompactResults.csv
    - 除NCAA锦标赛外，或者说之前所有比赛信息。
    - 赛季年份。
    - DayNum。
    - 胜利方的Id、分数，失败方的Id、分数。
    - 胜利方是主队、客队，还是在第三方球场。
    - 加时次数，大于等于0.
- MNCAATourneyCompactResults.csv(注意：女子比赛的赛程安排上与男子比赛不同，总共有过四种不同的安排，具体要参考当前赛季对应规则)
    - NCAA锦标赛的历史信息，数据与上面一致，注意WLoc全部为N，表示比赛总是在第三方进行。
    - 通过DayNum判断目前锦标赛处于哪一轮：
        1. DayNum=134或135（星期二/星期三）- 最先四场，确定最终的64支球队。
        2. DayNum=136或137（星期四/星期五）- 第一轮，64进32。
        3. DayNum=138或139（周六/周日）- 第二轮，32进16。
        4. DayNum=143或144（星期四/星期五）- 第3轮，也被称为“甜蜜16强”，16进8。
        5. DayNum=145或146（周六/周日）- 第4轮，也被称为“精英八强”或“区域总决赛”，8进4。
        6. DayNum=152（Sat）- 第5轮，也被称为“最终四强”或“全国半决赛”，4进2。
        7. DayNum=154（周一）- 第6轮，也被称为“全国总决赛”或“全国锦标赛”，决出最终的锦标赛冠军。
- MSampleSubmissionStage1.csv
    - 提交文件，第一阶段是过去5年的NCAA每一场比赛做胜负预测，第二阶段则是对今年的锦标赛做预测。
    - ID由SSSS_XXXX_YYYY组成，XXXX和YYYY为两支球队的ID，低位的在前，SSSS为赛季号，这里也看出赛制是一轮制。
    - Pred为XXXX赢得比赛的概率，如果你认为YYYY赢为60%，那这里就应该是40%。
    
### 数据第二部分 - 球队统计得分数据
- 基本信息：
    - 自02-03赛季以来的各种比赛的罚球、防守篮板、失误等数据。
    - 与第1部分的CompactResult相比，前8列是一致的，从WTeamId到NumOT，但是这里会有其他很多数据，比如二分球个数、三分球个数、发球个数、犯规次数等等。
    - 最终分数可以由以下方式表示：2×FGM + FGM3 + FTM，即2×进球数+三分球数+罚球数=总分。
    - 这里的信息与第1部分中的比赛信息是严格对应的。
- MRegularSeasonDetailedResults.csv
    - 03赛季以来所有非NCAA锦标赛以外的比赛的详细信息。
- MNCAATourneyDetailedResults.csv
    - 03赛季以来所有NCAA锦标赛以外的比赛的详细信息。
- 详细指标
    - WFGM：投篮命中数。
    - WFGA：未命中数。
    - WFGM3：3分命中数，注意这部分数据是包含在WFGM中的。
    - WFGA3：3分未命中数。
    - WFTM：罚球命中数。
    - WFTA：罚球未命中数。
    - WOR：进攻篮板。
    - WDR：防守篮板。
    - WAst：助攻数。
    - WTO：失误数。
    - WStl：抢断数。
    - WBlk：盖帽数。
    - WPF：个人犯规数。
    - 失败方为用L替换W，意思一致。",3a35eb23,0.9259259259259259
42620,24e550b8226932,fe9fb574,# Submission:,0caee953,0.9259259259259259
42621,aa7db7b023d0a2,e602f8a3,"<h1 style=""background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% / 10% 40%"">Prognostic factors in ALS: A critical review</h1>

Citation: Chiò A, Logroscino G, Hardiman O, et al. Prognostic factors in ALS: A critical review. Amyotroph Lateral Scler. 2009;10(5-6):310-323. doi:10.3109/17482960802566824

""The discrepancies about ALS survival found in the published literature are mostly related to differences in study design. However, when considering only studies based on register methodology (more likely to report the full spectrum of the ALS population), the range of median survival is quite narrow (around 30 months from first symptom). Interestingly, these studies are also characterized by an older age of onset (62–67 years) than those based on other designs. However, in 10–20% of cases survival exceeds five years and, in 5–10%, 10 years.""

""Despite the evidence of several publications, it is still impossible to predict with a good approximation the prognosis for an individual patient at the time of his/her diagnosis. However, several prognostic factors are well established.""

""There is a general consensus that older age and bulbar onset are negatively related to ALS outcome, but the complex relationship between age, female gender and bulbar onset remains to be clarified. Also, the time delay from onset to diagnosis and the El Escorial diagnosis of definite ALS at the time of presentation, seem to have prognostic relevance, since they probably reflect a more rapid progression of the disorder.""

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3515205/",ec912af3,0.9259259259259259
42625,ddcdecdd6a3b6d,60896f29,"### GoogLeNet模型
完整模型结构  
![1576155704%281%29.png](attachment:1576155704%281%29.png)",90831448,0.9259259259259259
42627,c1984e64b35234,a6638957,"So this patient seems to have the ApoE-ε3 allele. ![image.png](attachment:image.png)
which does not seem to indicate elevated Alzheimer's risk from just looking at these 2 SNPs.",1811225b,0.9259259259259259
42630,9ad9a97e628bfa,61bab54f,"**기타 Atribute 생성**
(추후 추가예정)",0a7e1136,0.9259259259259259
42634,c6f8ff61a5fa87,2b7ad28a,"# <span style=""color:blue;""><strong>10.Best Model</strong></span>",3eea586b,0.9259259259259259
42637,55ce731a138ca7,b3547e64,# References,4996250b,0.9259259259259259
42648,02b7e38902069e,5854f858,#Exception: spaCy tokenizer is currently only allowed in English pipeline.,726a03a0,0.9264705882352942
42652,e4c6dd957eb5ce,ed380d7b,"Nice!!! Some interesting things to note:<br>
### Note: In this charts, I'm considering only Experts, Masters and Grandmasters. 
First Chart: 
- We can see that the ""Top unique votes"" are SRK with more than 7k votes from unique users. 
- The second and third (Anistropic and DATAI) are very close from each other.
- We can see interestingly that Andrew (top 1) and Pedro Marcelino (The king of titanic) are almost in a draw;

In the top 30 users with more votes from unique users, we have:
- 3 Contributors
- 2 Experts
- 14 master 
- 11 grandmaster

We can note that in this chart the highest ratio total_votes/unique_votes:<Br>
- 1.92 of Bojan Tunguz 
- 1.89 of DATAI 
- 1.76 Chris

So in mean, Bojan received 1.92 votes from each user. 
____________________
Second Chart:<br>
We have 4 guys with more than 8k votes
- DATAI have almost 12k votes in total. 
- SRK few more than 10k 
- Andrew with that is the 1º rank are the third with more votes ~ 9.5k

This time, we can see that the highest values of the ratio of unique users are: Yury Kashnitsky and NowYSM with ~2.5 votes for each user
___________________________

Third Chart:<br>
- We can see that many users have values more than 3 in mean of votes;
- I noted a common pattern, many of them have a high number of votes but a small value of forks.
- In general the ratio of Upvotes / Forks is higher than 2 ~ 3;


So, the top isn't necessary the guy with more unique votes or with highest diversity of votes.

The next object is to get the total kernels and divide by the total of unique user votes... I find that it will give us interesting insights. ",2e383665,0.9264705882352942
42653,eb0ecd6bebeb15,9278cd9d,Hedef değişkenimiz variety'e göre bir gruplama işlemi yapalım değişken değerlerimizin ortalamasını görüntüleyelim.,d7b93a60,0.9264705882352942
42654,9cec5ddf8b6f49,2b60bde8,# 5. Prepare final paramters and predictions,d39fc8e7,0.9264705882352942
42655,c65a65d4041018,5d118325,"It is worth noticing while kagglers in most countries use several platforms, in Russia Coursera is dominating.

By the way, 46 people mentioned that they use mlcourse.ai. This is a recently created course by Russian community ods.ai and it is great :)",824fb229,0.9264705882352942
42658,b10bd75889dad9,c0c8cb02,"##### Recall
TP / TP + FN",ee00ceee,0.9266666666666666
42662,8cefb86a675e5d,211c20e5,**Evaluate Decision Tree Regression Accuracy using Root Mean Square Error**,79f9e69b,0.926829268292683
42663,47b2c9be5e31cb,8ce5d8f0,Scatter and density plots:,7d4afe56,0.926829268292683
42666,fd4017c1514157,44dc2196,"#### <font color = 'orange'>MFCC</font>
* This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. 
* The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope.
* .mfcc() is used to calculate mfccs of a signal.
* By printing the shape of mfccs you get how many mfccs are calculated on how many frames. The first value represents the number of mfccs calculated and another value represents a number of frames available.",fd8f0896,0.926829268292683
42668,8d0aebab1e5914,53775100,"From above you can see that if we take 200-dimensions, approx. 90% of variance is expalined.

Our intention with princpal component analysis is to reduce the high-dimensional input to a low-dimensional input 

Ultimately that low-dimensional input is intended for use in a model, since adding more components increases the cost and the accuracy",084e671f,0.926829268292683
42675,0e09587faffa8f,aa6d10da,"From the above line plot, we can see that the peak for the maximum number of summons in a day occured around **9 AM**",0d563d61,0.926829268292683
42680,5a8c553e21c70f,37433030,"Next, we evaluate **bagging ensemble** which is the average of all models. **y_ensemble** holds the average decisions for all samples in **test_X**. As classification threshold, we use the average of thresholds computed above for each method.",9ebd9d8f,0.9272727272727272
42682,016abae0483764,4f20f88f,Let's compare,bc9f289b,0.9272727272727272
42687,17a24d566ffa59,e7d1c926,"Dimensionality reduction using TruncatedSVD (aka LSA).

This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.

In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in sklearn.feature_extraction.text. In that context, it is known as latent semantic analysis (LSA).

SOURCE: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html",89049e56,0.927536231884058
42689,0e2a23fbe41ca9,fdf503f8,"Except the id cols, we'll not go into other columns. Among ids, only ```merchant_id``` column has 178,159 nulls, which is 0.0061% of the total data. We'll need to handle these when we are creating the final data",64e4762c,0.927536231884058
42692,ea4e559a86d613,cddc4330,**Hair Removal**,eff47843,0.927536231884058
42700,063a35f644e3c5,1117ef3a,R2 score,1c30fb0a,0.9278350515463918
42703,fc8e0042411c46,f884a437,"The NaN, in this case, is interpretted as no correlation between the two variables.",af476c2a,0.9278996865203761
42704,a566b5b7c374e7,2d5ea6ca,### Total Sleep Time by Sleep Stage,b3dc5545,0.9280575539568345
42705,917957c6c4065f,e1c5c0fc,"인기동영상 개수가 4개 이상인 채널의 카테고리 현황입니다.  
이전의 결과들과 다르게 News & Politics 카테고리가 가장 많은 비중을 차지합니다.  
그 다음은 Entertainment가 2위이고, People & Blogs가 3번째로 많은 비중을 차지합니다.",55b8ed68,0.9281045751633987
42711,c54ea4523bd49c,943adcf3,Output predictions file,097ccba2,0.9285714285714286
42712,b8849a04581d32,8fcb480e,### The outcome,b8a568cd,0.9285714285714286
42714,2d40f383473fa4,6e07a425,Make the predicitions with the model with all features and append the results to submission file.,1da1eff0,0.9285714285714286
42717,400bbcc496138f,dc97ed9b,"# Perspectives

I did not found any improvement on QDA (with pseudo labels) models. Feel free to investigate the technique on more models and contribute.  ",191b86b8,0.9285714285714286
42718,2ada0305b68956,ef46abd2,### 159. Palette = 'terrain',133e26f4,0.9285714285714286
42728,a758983a68c014,2de85d25,"That's interesting, we can use W1 or W2 as embeddings, or maybe both. Look at those plots above. In case of W1 we see, that this embedding learnt that Mercedes and Berlin are somehow similar. So as Ford and Boston. That's because they co-occur often with Germany and USA respectively. Food objects highly concentrated, so as beverages. Cola and juice are near sugar. W2 is interesting as well. Ford-USA and Merceds-Germany vectors seems quite similar. So as Boston-USA and Berlin-Germany. Again, fruits are highly conentrated,as they co-occur just with fruit and eat tokens. That's why eat and fruit are near as well.",ab89f181,0.9285714285714286
42730,324c699253abc2,4221b460,Doing PCA we can identify one feature that describe very well all the variables that we have from a country. The Human Freedom Index was reverse engineered with this approach and now we are able to replicate the index with new data.,7e81e44e,0.9285714285714286
42737,801233907c232b,96fd0e1e,====以下为训练后，载入得到的权重做评估（与上面无关）,5f1e891e,0.9285714285714286
42747,8dd655515e7d18,b3baa1b1,"### Neuroticism / Emotional stability

Emotional stability refers to a person's ability to remain stable and balanced. At the other end of the scale, a person who is high in neuroticism has a tendency to easily experience negative emotions. Neuroticism is similar but not identical to being neurotic in the Freudian sense. Some psychologists prefer to call neuroticism by the term emotional stability to differentiate it from the term neurotic in a career test",895f41cf,0.9285714285714286
42750,0d59a3e0130db0,ba87312c,"And now let's take a random example from the Internet, which is not in our dataset",285f04b2,0.9285714285714286
42751,565ad413cd802f,25b02c16,You can now upload this submission file here: https://www.kaggle.com/c/jovian-pytorch-z2g/submit,397b074e,0.9285714285714286
42753,585c280865b46e,b8423c43,# Visualization colored by top expressed histone genes,4d6056f1,0.9285714285714286
42754,953ab4b6631ba8,240f7a63,"Wow! Huge difference! The random forest model, without any tuning of hyperparameters, performs a lot worse than simple logistic regression.</br><br>
<h3><i>
    And this is a reason why we should always start with simple models first. A fan of random forest would begin with it here and will ignore logistic regression model thinking it’s a very simple model that cannot bring any value better than random forest. That kind of person will make a huge mistake. In our implementation of random forest, the folds take a much longer time to complete compared to logistic regression. So, we are not only losing on AUC but also taking much longer to complete the training. Please note that inference is also time-consuming with random forest and it also takes much larger space.</i></h3>",8bf6bfa1,0.9285714285714286
42755,2730840089c8eb,e9df07c6,"The very useful `dict.items()` method lets us iterate over the keys and values of a dictionary simultaneously. (In Python jargon, an **item** refers to a key, value pair)",34d27dac,0.9285714285714286
42760,758e6b0aa40398,76508712,That's it to it.,481b1b30,0.9285714285714286
42763,fcb15f03bd0239,50aec712,"# conclusion at first sight
'

the 'a' is omnipresent in esperanto and lacks factor 3x in voynich

the e should be swapped with c, since they are written very lookalike,thats possible and makes those letters collide better
the h should be swapped with the r, here again this is an excellent improvement
",3b9bee3a,0.9285714285714286
42765,99f84fa59cb1da,cadb4266,"### You should trust your CV more instead of Public Leaderboard. 
### You will less likely overfit the data and will not experience major shakeups in the private leaderboard",41e95f63,0.9285714285714286
42766,67efe818cb2372,e54d32be,"That's it. A submission from this 'only' scores 0.968 accuracy on the 28000 test samples in the MNIST competition. However, it's a very first attempt and there are many modifications and optimisations that can be applied. A decent start I hope?",f28a2a34,0.9285714285714286
42769,0023886d8f785a,671163b0,More to come. Stay Tuned.!,409fba35,0.9285714285714286
42771,1c381451c17150,1c48529a,# Save Text Output,e79b530f,0.9285714285714286
42776,98fd05fcc5c3e3,6da8ca6a,## Visualizing Output,55fe7ece,0.9285714285714286
42778,1084376bc4897c,44c1e2da,## 4.6 Visualize the decision tree,1b598487,0.9285714285714286
42783,38b79494ac749e,0deaa73e,#### Summary,39162a40,0.9285714285714286
42784,c65f7b375af4ef,0c327c0a,"# İKİ VERİTABANI BİRLEŞTİRME
  Veri tabanı birleştirirken 1 veya daha fazla eş öznitelik bulunabilir, en çok kullanılan ise ""id"" barındıran öznitelikler dir . 
 ' yukarıda bir örnği vardı burada ufak bir faklılık yapıcaz '
",871d53ca,0.9285714285714286
42785,1a285e4c830f3f,1e58fb9e,[![smile.jpg](https://i.postimg.cc/0jVh4z64/smile.jpg)](https://postimg.cc/fS0HtTf7),360b50e9,0.9285714285714286
42787,b61ab8f81dc03d,521f01da,"**Formula:**

Accuracy = (TP + TN)/(TP + TN + FP + FN) 

Replacing the values on the formula we have:",64d05394,0.9290780141843972
42789,ce9ed5e2d601d7,d0f437b7,"To submit these predictions to the competition, follow these steps:

1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.
2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.
3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.
4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.

You have now successfully submitted to the competition!

# Next Steps #

If you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.

Be sure to check out [other users' notebooks](https://www.kaggle.com/c/tabular-playground-series-dec-2021/code) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion), where you can share ideas with other Kagglers.

Have fun Kaggling!",f58a2f43,0.9291338582677166
42791,c9b4e282e4e2c1,b3112d7f,1-Which is the event associated with the most number of injuries?,f44d339f,0.9292035398230089
42792,a5a419dc7245b0,51cd7a4a,**Classification Report**,4279726e,0.9292035398230089
42793,dbd96dd275dc60,3abbae46,"## Feature Importance

which diff attributes of the data were most important when it comes to predicting target variables(SalesPrice)",1ed493a8,0.9292929292929293
42794,869a39a3d4dea2,c49f95e5,"From the result above of 2D, shades of blue represent the low pixel count and shaded of red represent large pixel count",9020daf8,0.9294117647058824
42803,0932046e1f485d,88133cb2,Seperating the distribution between the positive and negative reviews.,218cc7a3,0.9296875
42804,5ce12be6e7b90e,7dc9c16c,"### Dictionaries as containers
We can check if a dictionary contains a *key* using the `in` operator:",c0ab62dd,0.9298245614035088
42806,c3498779cda661,c62c8ea4,"# Visualización

Se usa PCA para reducir dimensiones y graficar todas las características.",0f531b65,0.9298245614035088
42807,54004b32784b68,ca812daf,# To CSV,27213ca9,0.9298245614035088
42808,9d27afa9ca3f96,b3b5345a,"- read the paper 
- load pretrain : done
- check the out-bound and not normalized box : done
- fine tune : done
- yolo5x out of memory => yolov5l : done
- 0-10% background image : done
- save model and get the submission from it : done
- large image size, done but not experiment yet
- little bit tune in number of freeze layer 
",2d86a18d,0.9298245614035088
42813,30fdc4a6e3c1db,ab970a11,# 6. Analysis on prices changes,6111ddee,0.9298245614035088
42821,eda49464dd6d1b,e0c30c23,"### The Tensorflow dense neural net yields a very similar AUC to that of the random forest, 0.855 for both.
### XGBoost and Catboost are other programs similar to the random forest, not included in this notebook but available elsewhere for this dataset.",8421f81f,0.9300699300699301
42830,22bd95f4807a23,ab92882b,"* A majority of consumers seem to have votes positively.
* Tastes in clothes change by age. For example trendy cloth wear had a higher median rating by individuals between 30 to 35 years of age where as knit wear were more preferred by individuals who are between the ages of 80 to 90
",c05d356f,0.9302325581395349
42834,1014e6be391084,ea5f1ce1,"Creating Confusion martix and calculating accuracy, precision, recall",46f9168f,0.9305555555555556
42837,166a62ebb4fc3a,2824b290,The prediction accuracy for the test data set using the above Logistic Regression Model is **91.22%**,db48a079,0.9305555555555556
42839,593d1d3d1df05a,f4d8b204,"Now, let us take the output for each character and see, how pytesseract does",bc682ffe,0.9305555555555556
42842,09751c520b0616,e2a34519,- Plot actual and predicted values,a4d0c7e9,0.9307692307692308
42844,d07915a6e6992e,719aa497,Let's plot feature importance by various algorithms,2b912140,0.9307692307692308
42846,84127ade6fde87,3b1ec42a,"When we are interested in co-occurrences of observations, the word embeddings we saw earlier can serve as a blueprint, too. For example, recommender systems—customers who liked our book also bought ...—use the items the customer already interacted with as the context for predicting what else will spark interest. Similarly, processing text is perhaps the most common, well-explored task dealing with sequences; so, for example, when working on tasks with time series, we might look for inspiration in what is done in natural language processing.",f55d05b6,0.9310344827586207
42848,858da4bb312f67,27343f43,Training multiple CycleGANs takes a lot of time. So I'm plannning to implement StarGANv2 to create images across multiple domain (multiple classes).,9cca4391,0.9310344827586207
42849,401338428b2d1c,ac9e49cc,## Visualising the Test set results,e4b768be,0.9310344827586207
42858,656185a18260be,e6d73cdc,"# Training

Time to train! We'll use everything that we defined above. ",0318cab5,0.9310344827586207
42861,b91c9eef23d284,95916c61,"#Text by https://www.russianforfree.com/text-in-russian-advanced-victory-day.php

Э́тот День Побе́ды. This Victory Day. (The 9th of May) Э́то ра́дость. This is happiness.",8fb353fa,0.9310344827586207
42867,20e1ba19eb9b5e,499a6d24,# 4. Prediction and submition,4569bfc1,0.9310344827586207
42871,1cd8be6e679620,6cf7cf06,## Graph Representation of RNA structure,3ce15a43,0.9310344827586207
42875,5fc2f23dfbeeb1,9ad033b7,### Creat Submission File,f37b4110,0.9310344827586207
42876,00d295edcd117e,ce47dbbf,下面让我们来看看模型对于哪一类的识别效果比较好。,f5810f4b,0.9310344827586207
42881,a4f0a3e1316ff9,897dff2e,"# Polynomial Regression Graph

This is purely for fun. I don't believe it is useful for currency all that much.",53bf0160,0.9310344827586207
42883,6903d3f38c6a66,52d9336b,We'll show the range of each colormap. ,6067ce5e,0.9310344827586207
42884,9535bb04ae042c,ff9d61e6,## Working Demo of this project: https://heart-health-predictors.herokuapp.com,165b6fae,0.9310344827586207
42888,6a1d04e8153df3,149f188c,"**Observation(s):**
- More denser the colour means more number of data.
- denser colour means if you visualize in 3-dimension you can able to see the peak of the hill.
- Denser part denote that there are many womens who are suffering from the breast cancer has less number of axillary nodes is just in between 0 to 7.
",38572b05,0.9310344827586207
42894,7cfd96218dd933,be547277,### GENERAL MAPS,7c34d96c,0.9313725490196079
42895,71b75664517244,5adee927,"Last is Josep Guardiola, this spanish manager make a great performance lately.two season in a row 2018 and 2019.",fc905af5,0.9313725490196079
42912,d1ff7e10ee0102,bca4418e,Easy mode.,2cc71c3c,0.9318181818181818
42914,f269d2fbd5f1be,6ce641d2,# Visual View Between Weight & Height by Position,1264c440,0.9318181818181818
42916,90964081c7faab,59ff6ec0,### Hyperparameter Optimization (Too expensive),b423b0c3,0.9318181818181818
42920,d83e5b44d1b80d,ca71d684,# Most Recommended Progamming language,62845930,0.9318181818181818
42923,0a918602a04693,49b68583,"# Selection of ML Model
It was found that the Logistic Regression performs with better accuracy.",c1ef0e95,0.9318181818181818
42928,98a6794067932a,acbbf889,"Comme la carte générée ci-dessus était plutôt difficile à interpréter lorsque plusieurs points de clients se retrouvaient au même endroit, nous avons créé la ""Heat map"" ci-dessous afin de mieux représenter la situation des clients perdus. Encore une fois avec la fonction Folium, le code permet de prendre en compte la latitude et la longitude de chacun des clients présents dans le dataframe des clients perdus et il indique leur emplacement sur une carte du monde en utilisant des zones cette fois-ci. Le code nous permet de sélectionner le rayon désiré dans lequel les différents clients sont regroupés. Après avoir essayé des rayons de 5-10-15-20 et 25, nous avons déterminé que le rayon le plus adéquat était celui de 15. Si la couleur d'une région est chaude donc se rapprochant du rouge, cela signifie qu'il y a une forte densité de clients dans cette région. À l'inverse, si la couleur est plutôt froide donc se rapprochant du vert, cela signifie qu'il y a une faible densité de clients dans cette région. Finalement, le code permet d'afficher la carte qui vient d'être créée. Cette analyse pourra donc être très utile afin d'évaluer la densité des marchés où les clients semblent possiblement être le moins satisfaits. De cette manière, les dirigeants pourront orienter leurs décisions dans le but de viser des marchés de clients précis pour lesquels ils veulent augmenter les efforts afin de conserver la fidélité de ces clients.",08600fe2,0.9320388349514563
42929,a077820f7ab459,58eab402,## Classification Report,05a43104,0.9322033898305084
42931,a81661cc35d8d2,fe48459a,# Conclusion,3331f113,0.9322033898305084
42942,dac3c8204a2d1b,ed668161,"As you can see, there is a strong negative correlation of these two factors, as we expected. That means that when the price goes up, the number of the reviews tend to decrease.",b0d2d0dc,0.9322033898305084
42944,e1fff2f67cbe32,9fe989e6,"## Making predictions.
Code is taken from https://www.kaggle.com/sishihara/riiid-lgbm-5cv-benchmark",c6dfde64,0.9324324324324325
42945,62037c5832129c,8d9371fb,"### Learning Curves, Validataion Curves
* Both curves show the training and validation scores of an estimator on the y-axis.
* A learning curve plots the score over varying numbers of training samples.
    - The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased).
* A validation curve plots the score over a varying hyper parameter.
    - Finds the best hyper parameters
    - Some hyper parameters (number of neurons in a neural network, maximum tree depth in a decision tree, amount of regularization, etc.) control the complexity of a model. We want the model to be complex enough to capture relevant information in the training data but not too complex to avoid overfitting.",61474350,0.9324324324324325
42946,63b44c85e32c1f,f9044b99,"**issubset( ), isdisjoint( ), issuperset( )** is used to check if the set1/set2 is a subset, disjoint or superset of set2/set1 respectively.",fb9b9562,0.9324324324324325
42948,e4525eb0c96f28,3221b9b4,"Showing the upper levels of each tree in the forest gives us a clue of what the most significant decisions are for each tree. Based on the plots above, we can see that whether or not the game was published after 1993 makes a big difference on which half of the tree you go, as evident in the root nodes of all the trees. Other important factors are the interaction terms between year and whether or not the game a Platformer and whether or not the game is a Shooter, since nodes using these features as decisions fall right after the root node. 

Note that by default, if you were to create random forest models again and again, you would get a different forest with different trees and nodes each time. This is because the random forest regressor randomly decides which feature to use for each decision at each node. Of course, Sci-Kit Learn lets us control this by setting a random state. Hence, why we set a random state parameter when instantiating the random forest regressor.

Now, let's try predicting some values to see whether or not our predictions come close. Since this model is not plottable on a scatter plot, there is no way to plot the residuals for it. So instead of doing that, we will conduct a 10-fold cross validation procedure. This procedure begins by spliting the dataset into ten seperate subsets. It will then iterate through each subset, holding out the current subset, and using it as a test set for the model trained by all the other observations. It calculates the total error for each iteration, allowing us to see how far this model is off by.

As comparison, we will conduct the same procedure on our linear regression model with the same interaction terms and compare the errors we get against the errors from the random forest model. We will conduct another two-tailed t-test on the difference of the errors with a significance level of 0.05 since we want most differences to be ""close"" to zero.",2093a1f1,0.9324324324324325
42953,44f6a002ecd033,66798e03,It seems like our basic logistic regression model actually appeared to do the best for this dataset. Before calling it quits a few things will be checked out to see if any slight improvements can be made to our Logistic Regression model.,70bbe106,0.9326923076923077
42958,20b372b6e4e276,d8be7533,"## 5.6. Clustering <a class=""anchor"" id=""5.6""></a>

[Back to Table of Contents](#0.1)",ec8b0860,0.9328358208955224
42964,061d6757dfbce0,5890e30e,"### Correlations

Now that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.

The correlation coefficient is not the greatest method to represent ""relevance"" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) are:


* .00-.19 “very weak”
*  .20-.39 “weak”
*  .40-.59 “moderate”
*  .60-.79 “strong”
* .80-1.0 “very strong”
",c0c2915a,0.9333333333333333
42965,892be0a523578c,434848f5,"We can also know that:
* Total asleep time of most customers is between 7-8 hours
* Total inbed time of most customers is between 8-9 hours
* The asleep time percentage of most customers is between 80%-90%",b0e8d7c0,0.9333333333333333
42975,6e472c6c591c7d,a04a64a6,"---
**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**





*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*",65532a3d,0.9333333333333333
42978,04bac111ffbe9c,1276242e,## Comparison,82576b17,0.9333333333333333
42979,49ac6594c8f5cf,81041025,SVC model ,6f19f28a,0.9333333333333333
42980,3597174a998d4d,24f50f6b,It can be found that the best model is RandomForest model. Its accuracy is 0.84.,276892ed,0.9333333333333333
42985,f7436bc492474c,c88c8d43,"And finally, create the submission file.",328fd235,0.9333333333333333
43001,c91c137284976f,68ef93a8,# Extra: testing on test data,c6888c0a,0.9333333333333333
43004,d5dcd3f3253568,16c918aa,## Seems like I am flogging a dead horse.,7c1555a3,0.9333333333333333
43006,712198370d5521,b80bad85,"<a id=""8""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">PROFILING</p>

Now that we have formed the clusters and looked at their purchasing habits. 
Let us see who all are there in these clusters. For that, we will be profiling the clusters formed and come to a conclusion about who is our star customer and who needs more attention from the retail store's marketing team.

To decide that I will be plotting some of the features that are indicative of the customer's personal traits in light of the cluster they are in. 
On the basis of the outcomes, I will be arriving at the conclusions. ",5882e04c,0.9333333333333333
43010,e78f177ca86768,6cb64c06,# Top 40,120e25c1,0.9333333333333333
43014,bc058fe14d3d1b,1a21a0f5,### 2.2 Random Forest,d0273670,0.9333333333333333
43017,67b7354e96113a,58da914b,## Ensembling,dca94250,0.9333333333333333
43018,864302b10e7730,0a184d3a,# I think you've got a basic idea about how to begin with data visualization. I would suggest you to visit the documentations of these plots in seaborn website to further improve upon your skills. ,e9dd1d2d,0.9333333333333333
43020,be616f0785c32d,e24f8ffe,"<div id='PartDlim'></div>

#### D.3 Limitations

There are numerous existing software for epitope prediction, including whether the epitope is on the surface, their docking score and MHC classes. However, studies using homology should be taken carefully, as the overall sequence similarity (based on our other studies) between COVID-19 and SARS-COV (the 2003 version ~82), Ebola (~40%) and Human Coronavirus (65-70% depending on the exact strain) is very limited. It should be taken critically that an exactly conserved epitope (~10 amino acids) indicates same effectiveness across these species. 



",b78e18aa,0.9333333333333333
43023,9ab194500e4ff9,12b94659,"Precision =  The ratio of how much of the predicted is correct.
Recall   =   The ratio of how many of the actual labels were predicted.",41a10eba,0.9333333333333333
43025,cb570c7b7f0501,62a1d93a,"that's really amazin ..
let's check the ratio of non-show for the second appiontment",a200a0ec,0.9333333333333333
43027,b10bd75889dad9,5ddbb56e,### Precision and recall tradeoff,ee00ceee,0.9333333333333333
43031,4bada947d597ac,1cfc0500,# Code to generate submission file,eab5094a,0.9333333333333333
43032,b547f0f38f7744,b5664928,## Save Model,b6ba66b3,0.9333333333333333
43040,c65a65d4041018,f74e38f6,### tools for interpretation,824fb229,0.9338235294117647
43042,e9b9663777db82,13777458,#### Gradient Boosting Regressor,648e8507,0.9338842975206612
43048,52ee792e228d54,b83e3602,### K Nearest Neighbours,5096094e,0.9342105263157895
43049,2ada0305b68956,04550fa5,### 160. Palette = 'terrain_r',133e26f4,0.9342857142857143
43051,c80939c7c626cf,a5821085,# Score,b9ac31e2,0.9343065693430657
43055,601e18072783b4,1890644d,- An interesting observation above is that none of the top directors are present in the directors list which have atleast 5 movies on Netflix!,36b2b1fa,0.9344262295081968
43059,c84925c8171900,82e5488a,"<center><img src = ""https://i.ibb.co/WtxX3ZZ/Wii-Sports.gif""></center>",e21ff7ec,0.9345794392523364
43060,917957c6c4065f,5f925bff,"인기동영상 개수별 채널 현황 및 카테고리를 살펴보니 인기동영상을 3개 이내로 보유한 채널이 전체의 75%였고,  
4번 이상 보유한 채널들의 동영상들을 살펴봤을 때, News & Politics 카테고리가 가장 많은 비중을 차지했습니다.
 
Entertainment 카테고리의 동영상을 다루는 채널들은 모든 범위에서 인기동영상에 많이 선정된다고 보이고,  
News & Politics 카테고리의 동영상을 다루는 채널들이 다른 카테고리 채널들에 비해, 인기동영상에 많이 선정된다고 보입니다.",55b8ed68,0.934640522875817
43062,72d528df923403,ab85913b,Lest's obseve to top five items sold,d51c8e8e,0.9347826086956522
43063,0e2a23fbe41ca9,cd10d4e7,# New Historical Data,64e4762c,0.9347826086956522
43071,e19e307b3fd188,62c4baa0,#### Predictions,2173955b,0.9349593495934959
43074,75adb7945ef9bd,64a92d51,"Next, we inspect the tweets that predicted probability differs the most from target outcome",785c5095,0.935064935064935
43079,c13f73168789c2,16ec6757,"## 3. Subsetting using filter method<a id='35'></a>
Subsets can be created using the filter method like below.

Method 1 : `df.filter(items=['column_name1', 'column_name2'])`

Method 2 : `df.filter(like='row_index/label', axis=0)`

Method 3 : `df.filter(regex='[^column_letter]')`

Method 4 : `df[(df['column_name'] > value) & (df['column_name'] > value)]`",16175052,0.935064935064935
43081,ab6da5994949a3,16a35318,## Random Forest Test Results,fae6b91d,0.9351851851851852
43082,fdc3afd309b850,764fb3de,In general we cannot expect to get exactly correct results from a regression model. What we hope for is that your predictions are overall close to the real values.,966bde38,0.9351851851851852
43086,535da6591a3246,a730f548,The model that showed the best performance was logistic regression.,9ef8d0c7,0.9354838709677419
43093,16862cb02d73d5,c4392978,"Special thanks to my manager **Shrinivas Ron**, **Goldee** and my teammates **Sudarson, Adam**    in Myntra for their help in anomaly detection.",d7ffa1a6,0.9354838709677419
43096,57070ad5e0f94f,0592bd6a,# **Now We are Making Predictions On Test Set And Format Them As Wanted**,d97edc41,0.9354838709677419
43097,0925f172b5eb74,7c556520,# Training,ec34cd72,0.9354838709677419
43100,b59b5aaeedb1fb,6b41edfd,#### features with most correlation with rings,1ad63faf,0.9354838709677419
43109,f15eac23fbcc9d,33405d0c,"The following code is for user that want to download the prediction csv. If you only want to submit, then it's not needed. You can do it on the Kernels page under 'Output' tab.",ea46d8af,0.9354838709677419
43111,30fdc4a6e3c1db,edf46bb5,Sales data of each item at a weekly level,6111ddee,0.935672514619883
43119,c7e5f658090347,b4d491ec,"## Comparison of the Feature Importances for the Different Models
Although nearly all features are from PCA (so little insight possible from analysing the feature importances), it could be interesting to see how similarily each model weigths each of the features. It is of course always important to remember the caveat that feature importance does not mean feature impact.

Below are the calculated [permutation importances](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html) for each model based on the validation dataset. 

Clearly features ""V14"" and ""V4"" were found to be the most important features for distinguishing between both classes. Further, there are some model specific differences in what features were determined to be useful.",43c78e7d,0.9361702127659575
43121,5f674175839b32,7ab783d1,"Q5)IN which Year maximum number of releases is seen?

Ans:IN year 2009 with 1431 new game releases.",53a2e343,0.9361702127659575
43122,f6648e47713411,9d18ee3f,# 5. Make prediction,f4af4d1c,0.9361702127659575
43126,3f25b363afec54,e9c03a59,Creating the predictor variable i.e ```The outcome variable```,bbdaae25,0.9361702127659575
43127,4c47839b067546,b9840dc6,MAPE 21.55%. Результат на kaggle 13.00797.,1f517b02,0.9361702127659575
43136,f91f58d488d4af,cd8738d3,"Now, our training loop can be simplified.

and using fastai's SGD class as an optimizer.",5df1bbf3,0.9368421052631579
43139,ce9ed5e2d601d7,7bbae878,"## Op-level graph

Start TensorBoard and wait a few seconds for the UI to load. Select the Graphs dashboard by tapping “Graphs” at the top. ",f58a2f43,0.937007874015748
43140,eda49464dd6d1b,25a39d38,"# Training of TF Model for Final Submission
### train_target has to be made 2-dimensional, just like y_train before.  All training data will be used, not split as it was for the foregoing model.",8421f81f,0.9370629370629371
43144,c85c94076e9c3a,bafda666,### 2-Total_Spent,3ea0c443,0.9375
43145,ffc9490c4f6c38,9d0af336,'useless' columns are revealed?,ae7bbbb3,0.9375
43146,5ffe6aa38958a1,436cc27a,This suggests that the threshold for determining survival in our model should be around 0.45,11f5412e,0.9375
43152,2ca509e51a6e4b,b9eb5d41,"The baseline score was 0.7467, while we get a slight improvement to 0.7472 with the SVD encodings.",e63b0ba6,0.9375
43157,49f2274c1dd516,1ca06b47,"# Open Table
OpenTable is publishing changes in restaurant reservations across several regions in 2020 as a year-over-year percentage change compared to 2019",06b0ffee,0.9375
43160,fbe822854174fe,3b6ed445,"# Upcoming

* **Random Forest**
* **Decision Tree**
* **K Neighbors**",1f8e0bdb,0.9375
43161,8c7e00ca3dc5a7,19f5061e,"With the few submission trials , the ElasticNet gives the best score. So submitting with that. Other algorithms are also giving similar performance.",c83346e4,0.9375
43163,3cc097a5859dc1,a89fad4e,# **Applying MultinomialNB Model**,14380d73,0.9375
43164,9daf8b4a46725e,47302d77,### Visualizing the Decision Tree,7d9cc411,0.9375
43166,13c7672da1b571,e20336a9,"The last thing which remains would be optimize the XGBRegressor parameters, e.g. using GridSearchCV.",002d3ec0,0.9375
43168,47e3a1925754c2,5252c1d6,This notebook will be updated soon.,ecc615b7,0.9375
43170,ff029d7b52ae1d,3da3a491,"## HackerEarth Challenge Rank ~160 out of 1600 participants (top 10% rank)
",c987f868,0.9375
43172,3dd4294f903768,3d74b3fa,***,0d89d098,0.9375
43174,5e02999ca74e7e,bd8254ba,# **That's it! thanks for watching! hope you guys like it! don't forget to give me feedback and upvote if you like it!**,b69da28e,0.9375
43178,fdbbd573ba31c2,684e8485,## Mix Model,f7c28d74,0.9375
43180,68cceffe5bb8ec,161994b0,# Kaggle,dcbfcd6e,0.9375
43185,db0d65597e00d1,d4ee18cd,"**Can we get every element in the csv file into a serie like we can pick the ages with data[""age""] but I want to pick every element as a serie
It will look like this 
data[""age""][25] output = 71
data[*all][25] output = 71 1 76	0	2	140	197	0	2	116	0	1.1	1	0	2	1 ,I want to get this
**",a235ec9a,0.9375
43187,e7ab5703594800,eff22def,"## 2. A reddit ask can have multiple sub questions. An ask having multiple questions affect the score?

### The variables are negatively correlated to each other. As the number of questions increases, the score decreases. 

### We can also see from the graph that the score is high for the questions that have 1 or 2 sub questions. ",78546b1b,0.9375
43189,3b5903412fe741,68fbc3c8,Or with an iterable of values:,ad231969,0.9375
43193,28a1ff0f223da9,4b44bcea,"According to this dataset there are 485 Pd.D. degree holder professors in Pakistan.
According to [HEC data](http://hec.gov.pk/english/universities/Pages/AJK/University-wise-enrollment-of-yearwise.aspx) there are around 1.2 million students who got enrolled in universities all around Pakistan in different degrees in 2014-2015. This [HEC data](http://hec.gov.pk/english/universities/Pages/AJK/PhD-Produced-by-Pakistani-Universities.aspx) shows that there are total 10,691 PhD degree awarded by both public and private sector universities from 2010 to 2014.",c945b27d,0.9375
43194,2a724fb7835cdc,2537ac96,**Making submission...**,c38ac61d,0.9375
43197,51a46d0a7597f5,70e37c84,"<br/>
<A name=""section1.5"">5. Players with largest release clause (Top 11)?</A>",e9e25b17,0.9375
43204,ac1abfe1dfe815,7f9401d6,## Extra Trees,6529dbcb,0.9380530973451328
43205,8ec771f5600a61,51274b6c,## prediction of neural network,48364c1f,0.9381443298969072
43213,b01ee6cb674fa3,44c5e164,"With the history of the laumches above, it can be splitted into 3 different zones:
- 57 to 79 - space race
- 80 to 16 - post space race
- 17 to present days - new space race?

The first zone we clearly see an enormous rise in space launches, reaching a mark of 100 launches per year and a continuation of these. This marks the space race, with events such as the race to the moon by the 60's and the 70's were marked by a shift of objectives, such as space stations 

",a8ffd35e,0.9384057971014492
43215,c115e287523aab,8ee75e12,"# Submission
This notebook can't be used for submission as this noteboook uses **TPU** which requires `internet access`.",feb1288b,0.9384615384615385
43224,f2f2db16a2f86c,4b4b0dc6,### **Loading model to compare results**,ffc6a115,0.9384615384615385
43225,03048e86a6d806,93463f80,"Data folks relied on Coursera, Kaggle Learn Courses, and Udemy as their online courses to enrich themselves in Data Science.",1285c231,0.9384615384615385
43227,3fb15e6e48aec2,c3c1c255,# Voting Classification,9d1f4358,0.9385964912280702
43229,f1e162ddd14f11,8628e1ee,"since the graph shows the normal distribution, it means the model is giving great results.",cdb2e771,0.9387755102040817
43230,f35bf4df70d310,a0068391,## Conclusion,10bb859a,0.9387755102040817
43234,12f4d16fc21645,b0fe1b49,<h1 style='color:blue'>Classification Metrices</h1>,c7752038,0.9387755102040817
43239,396bc36edb95d3,2bfd92b1,### Conclusion for Artificial Neural Network (ANN),965e4f8f,0.9388888888888889
43240,ee23a565163388,55379fd5,Let's try with reduced number of features to avoid the model getting more complex. We'll try different combinations and conclude the best one.,88aacbc4,0.9389312977099237
43241,9c26c5dcd46a25,7056c838,"Afin de **tester une dernière modélisation à partir des variables synthétiques** $F_1$ à $F_4$, nous allons les ajouter à notre dataset initial :",1bbbb677,0.9390243902439024
43244,71c3c1eab0377d,4478d6bb,**Predict the Target variable for the Test Set**,52b4e360,0.9391304347826087
43246,70193f0c034b98,597adb54,# Running model,f8cacd26,0.9393939393939394
43249,f166950fa915f8,4d070fff,### Accuracy Score,a7f6ca5e,0.9393939393939394
43252,b42180a6a5b42f,058d51c3,"Conforme era de se esperar, a diferença entre os dados referentes à data da notificação do óbito e seu efetivo acontecimento, nos revela um novo modelo de distribuição semelhante àquele obtido no estudo realizado pela Singapore University of Technology em meados de abril, a saber, Distribuição Normal Padrão.

Por fim, deixo uma nota crítica aos trabalhos realizados por estatísticos da União e das diversas unidades federativas do Brasil, que por imprecisão e imperícia vêem legitimando, mediante suas análises, a tomada medidas bastante sensíveis pelos gestores estaduais, a ponto de afetar não apenas a economia do país mas diversos setores da sociedade.

Em vídeo recente (https://www.youtube.com/watch?v=OaCbcqLwg0c), Dra. Margaret Harris, Porta-Voz da OMS, disse:

    ""Nunca dissemos façam lockdown — dissemos identifiquem, façam o rastreamento 
    e acompanhem o desenvolvimento dos casos, isolem e tratem os doentes""
(https://www.smh.com.au/politics/federal/who-backs-australian-move-to-ease-restrictions-20200426-p54na9.html)

Fica mais que evidente que a mudança de narrativa, vez por outra, por partes das autoridades internacionais e nacionais, demonstram um total desconhecimento (para não dizer que há uma conspiração coordenada) dos padrões analisados por diversos estudos que comprovam que a quarentena não produz efeitos eficazes a fim de ""achatar a curva"" de gauss.

Lado outro, já sabíamos, bem como estamos presenciando os efeitos nefastos de uma quarentena prolongada em todo o mundo.

Em um próximo estudo gostaria de analisar os estudos recentes da Universidade de Stanford que demonstra um índice de letalidade demasiadamente menor do que aquele que vem sendo propagado, o qual concluiu que a letalidade do Covid-19 chega a ser de aproximadamente 4 vezes menor do que a da influenza.

Mas por ora é isto.",987cea5f,0.9393939393939394
43253,7c89a32e3562ca,c30b2034,# Model comparison,32dd8913,0.9393939393939394
43256,4b64dc653fb7eb,1cc458ab,"Saving data in csv format to use it in different notebook, or you can continue working in the same notebook.",57675cc2,0.9393939393939394
43258,b241b847319d13,753c9bd7,Copied from the below mentioned notebook and modified according to the given data for this competition,0fb698f0,0.9393939393939394
43259,9ceb7278784462,b816ba0c,## Model Tuning,3768a567,0.9395161290322581
43263,4daf6153275cbf,24369ae5,#### 8. Relationships within Average Values,51db1961,0.9397590361445783
43266,ba4b3bd184acbb,56a1ac08,"We can now find the mean of the Sentiment grouped by App.

The results will be sorted by average Sentiment score and then by Rating.",0f5de724,0.9398496240601504
43270,2ada0305b68956,ce30748c,### 161. Palette = 'twilight',133e26f4,0.94
43271,7dd46c750653eb,4ac96713,"**Inference**
* Over the years the number of Name Changes are gradually Increasing.",c2644713,0.94
43272,4cd25e50c7e007,05c8d270,## R-squared:	0.80,ceb0c525,0.94
43277,83df814455f06c,5e2c6c6f,"# **16. Classification Report** <a class=""anchor"" id=""16""></a>

[Table of Contents](#0.1)


**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. I have described these terms in later.

We can print a classification report as follows:-",c9cff71a,0.94
43279,49ee86d074de69,a636297c,"<a id = ""17""></a><br>
## Save The Model",71ccc6d3,0.9401709401709402
43284,20b372b6e4e276,f03e725d,Using my notebook https://www.kaggle.com/vbmokin/covid-19-week5-global-forecasting-eda-extratr,ec8b0860,0.9402985074626866
43289,565ad413cd802f,68320d42,## Save to Jovian,397b074e,0.9404761904761905
43290,87e94f864d74be,9dd3c696,## World Cloud-Movie Genre,294bfe9f,0.9404761904761905
43292,1294fb4c86f993,0f93e4cd,"<a id='Q7'></a>
### Research Question 7: What is the distribution of the total registerations of guns?
",4471e513,0.940677966101695
43293,c3dfa835621ac4,3a86bcaa,"P.S. Here is another rule set figured out for task train/2281f1f4, which is somewhat lengthy (18 rules). Each rule has two conterparts for horizontal and vertical directions respectively, and some of them are dealing with corner cases.

```
------ mark grey blocks on top row as green to move down
. # .
. e ., 0 -> g
. k .
------ extend green blocks down
. g .
. k . -> g
. . .
----- mark grey blocks on right column as blue to move left
. . .
. e #, 0 -> b
. . .
----- extend blue block left
. . .
. k b -> b
. . .
----- mark cross block red
. g .
. . b -> r
. . .
----- deal with horizontal consecutive red block marking
. g .
. . r -> r
. . .
----- deal with vertical consecutive red block marking
. r .
. . b -> r
. . .
----- deal with rectangle consecutive red block marking
. r .
. . r -> r
. . .
----- generate green blocks down from red one if stops
. r .
. k ., 0 -> g
. . .
----- generate blue blocks left from red one if stops
. . .
. k r, 0 -> b
. . .
----- change blue blocks near a red block back to grey (corner case)
. . .
r b # -> e, 1
. . .
----- change blue blocks near a red block into yellow for removal
     . . .
[FH] . b r -> y
     . . .
----- change green blocks near a red block into yellow for removal
     . r .
[FV] . g . -> y
     . . .
----- extend yellow blocks and turn back to grey if reach the border
. # .
. g . -> e, 1
. y .
-----
. . .
y b # -> e, 1
. . .
----- extend yellow blocks
     . y .
[FV] . g . -> y
     . . .
-----
     . . .
[FH] y b . -> y
     . . .
----- remove yellow blocks and memorize it
. . .
. y . -> k, 5
. . .
```",0126bdad,0.9411764705882353
43295,4fa553c2b837d4,8f57e6ef,"# Cross Validation

The diagram below shows an example of the training subsets and evaluation subsets generated in k-fold cross-validation. Here, we have total 25 instances. In first iteration we use the first 20 percent of data for evaluation, and the remaining 80 percent for training([1-5] testing and [5-25] training) while in the second iteration we use the second subset of 20 percent for evaluation, and the remaining three subsets of the data for training([5-10] testing and [1-5 and 10-25] training), and so on.

![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/crossValidation.jpg)

**Unlike train_test_splits it does not divide the training and testing dataset into a given percentage.**",c65a23e9,0.9411764705882353
43300,917957c6c4065f,74c49343,## 3. 조회수와 연관이 있는 항목들은 무엇인가  ,55b8ed68,0.9411764705882353
43303,c4386b8a01d66e,734d8144,### XG Boost,dc732bf5,0.9411764705882353
43305,52cfd66e9ec908,d5cf2cac,Keker is the equivalent of a Catalyst Runner.,c74adcdf,0.9411764705882353
43306,a0a5baa6c7e12a,e73c2196,"# <div style=""color:white;background-color:#1d1545;padding:3%;border-radius:50px 50px;font-size:1em;text-align:center"">References</div>

- https://www.kaggle.com/damagejun/tps-dec-2021-eda
- https://towardsdatascience.com/heres-what-i-ve-learnt-about-sklearn-resample-ab735ae1abc4
- https://towardsdatascience.com/how-to-handle-imbalance-data-and-small-training-sets-in-ml-989f8053531d
",551d41de,0.9411764705882353
43312,7f74a04ae75792,a91bfc67,"# <font color=green>Feature Selection<font>

### Check the correlation among the variables",d01e91da,0.9411764705882353
43314,71b75664517244,d2c6aa11,#### Comparison With Team,fc905af5,0.9411764705882353
43316,842547b2def18c,a217fb39,"### Model evaluation

We can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. ",b8efde6d,0.9411764705882353
43318,36d0d4cb9c7993,7b68b61d,Both single layer perceptron and logistic regression are used to separate linearly separable Datasets,34b93e27,0.9411764705882353
43320,b779c3ce7b671a,50d41a60,![movie](./movies/multi_planets.gif),ca778770,0.9411764705882353
43322,1d1598b6fa2aa7,9c40262e,"### Conclusions

Plotly is very powerfull instrument for visualising data which more flexiable and interactive.",e066accf,0.9411764705882353
43323,dc0b0e1cb46c6f,c3311dca,"<a id='5'></a>
# <p style=""background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;"">5. Covid World Dataset</p>",47b17a7b,0.9411764705882353
43329,156bbcff05dcea,013c38b6,Another possible way :-,66ad1fe9,0.9411764705882353
43331,55c34673c1f760,af5f2f9a,## Testing Predictions,2663c47f,0.9411764705882353
43336,6cac6d4743088f,cd104cbd,"# Dicas

- Exemplo de acesso aos CSVs disponíveis no dataset
> df = pd.read_csv('../input/anv.csv', delimiter=',')",55fb02fa,0.9411764705882353
43340,d0080e3a39bc5c,6bc63977,"The final ensemble attained a cross-validation F-1 score of 0.985

The final CSV file generated is - 

https://drive.google.com/open?id=1ztaz0vK_8lGZUjKR2zwJWe3WqnyoGaLd",2fcde4cf,0.9411764705882353
43341,629f2918807a9b,eca2adbb,"### Which book is being returned/ cancelled mostly ?
### Answer is ""انٹرنیٹ سے پیسہ کمائیں"" has the most returned/cancelled rate with count = 485.",be56dc84,0.9411764705882353
43346,e4c6dd957eb5ce,37f75de2,"# TOP users with more Kernels
- Let's explore the users with more kernels",2e383665,0.9411764705882353
43347,8f50c9c16db95f,034de6f5,"# Conclusion and further research <a id=""conclusion""></a>

* This notebook suggests taking the kick length as a valid metric for evaluating performance of the kicking team in a kickoff special game play. It also reveals from data, that two potential drivers to a long kick length. Those are 
    * foot speed
    * kicking direction
* Adding new measures of the tracking could benefit the research on a more granuarly level. To be more specific, the current competition don't provide data on movement of kickers' toe, knee, hip, support leg, shoulder and foot.",26cc763a,0.9411764705882353
43348,4c47839b067546,d5d3b375,## XGBoostRegressor,1f517b02,0.9414893617021277
43352,3c2033cc99c12c,d560684c,*Transform the numpy array into the torch tensor*,dfa22a54,0.9416058394160584
43354,62487bcd70b199,aeb8061f,## <a id='8.6'>8.6. xgboost</a>,f6ae50af,0.9416666666666667
43360,598b6228760590,a843e88f,"
<div style=""color: #fff7f7;
           display:fill;
           border-radius:10px;
           border-style: solid;
           border-color:#424949;
           text-align:center;
           background-color:#8ac6d1 ; 
           font-size:20px;
           letter-spacing:0.5px;
           padding: 0.7em;
           text-align:left"">  
<center>Thinking about the effect of the model:  </center> 
 <hr>
 <ul>
Well, although the recall rate is lower than the single model, we also see that f1_score is higher than the single model. <br>It shows that the effect of model stacking has been improved a little. At the same time, we have also seen that the score of a single model is always below 80%, indicating that our feature engineering is not very good, or the distribution of training set and test set is not good. Same, leading to poor model fitting effect.
 </ul>
 <hr>
</div>",be30ab66,0.9420289855072463
43365,7e89d387feb9f5,45da5a82,# Подготовка и сохранение конечного результата,989e3a1b,0.9420289855072463
43373,2f47abddfd1928,d418f46d,To finalize let's check each feature importance to see which ones were more determinant.,ae33cc0b,0.9421487603305785
43375,99bf357eaf61f1,8941c0c2,# Output predictions,9d92fafe,0.9423076923076923
43382,582cb872d19026,32839b79,## Conclusion,8d966d69,0.9423076923076923
43395,171494b45650a2,e594bedb,## ***6. Data Correlation***,9c8cc578,0.9428571428571428
43401,7454fdc444df16,291e1f44,### Model Evaluation,a7818ef5,0.9428571428571428
43402,60da9bbfe39c4b,cecc08dd,"## In August,
* Total Recovered=347158",b0dd8ad6,0.9428571428571428
43405,55a5e31d03df9f,0de97365,"## Prefetching and caching

We will prefetching and shuffle. This will help us with computing time as the memory wouldn't not be sequential batching and before a batch is done the other one will get prepared.",06dce00f,0.9428571428571428
43416,867a9f977fa945,7d858c29,> ****Simplified Lesk****,2740fcca,0.9428571428571428
43426,b61ab8f81dc03d,21626729,"<a id=""roc-auc_score""></a>
### ROC-AUC Score
There is a complete information about ROC (Receiver Operating Characteristic) and AUC (Area Under Curve) on https://en.wikipedia.org/wiki/Receiver_operating_characteristic",64d05394,0.9432624113475178
43428,957e035ba5b9d5,53bb6f19,## Multi-label classification with a Multi-Output Model,778ab3d3,0.9432624113475178
43429,b10bd75889dad9,bc444ac5,#### Looking at the above graph we see that the optimal cutoff would be 0.35,ee00ceee,0.9433333333333334
43432,43e60eb1362f5c,abd335f8,# Model fitting and results,87934234,0.9433962264150944
43433,0ad8d416b89b78,75dbdddb,"Upon research for model implementation a hyperparameter training methodology was identified and implemented, the code is drawn from [Andrew Lukyanenko's](https://www.kaggle.com/artgor/bayesian-optimization-for-robots) implementation of this technique. the use of folds effectively combines a number of successful models together ultimately working together to produce a improved single classification model.",0b0562f0,0.9433962264150944
43434,a070fd03ae8ed2,4697847d,# 8. Сводная таблица по метрикам моделей,c0ec4138,0.9433962264150944
43439,510b8303776bb6,7a15230d,## Predicting over test set,18080db8,0.9433962264150944
43442,3d77c1560bd16e,b190d9fd,"
<div align=""left"">
    <h2>Texas Vaccine Rollout Timeline</h2>
<span>
To date, Texas has opened up vaccine eligibility according to the following timeline:
<ul>
    <li> May 12, 2021: Everyone 12 years old and older
    <li> March 29, 2021: Everyone 16 years old and older
    <li> March 15, 2021: Phase 1C (people 50 to 64 years of age)
    <li> March 3, 2021: Schools and licensed child care personnel
    <li> December 29, 2020: Phase 1B (people 65+ or people 16+ with a health condition that increases risk of severe COVID‑19 illness)
    <li> December 14, 2020: Phase 1A (front-line healthcare workers and residents at long-term care facilities)
</ul>
Texas continues to receive doses of the Pfizer, Moderna, and Johnson & Johnson COVID-19 vaccines, and is distributing statewide to hospitals, pharmacies, local health departments, freestanding ERs, and other clinics</span><br>
<div>
Source : <a href=""https://www.dshs.state.tx.us/coronavirus/immunize/vaccine.aspx#eligible"">Texas Department of State Health Services</a>
    </div>
 </div>",87c141ca,0.9436619718309859
43449,312135b445bd23,3a85921a,Prepare our corpus for LDA model:,8ced381f,0.9438202247191011
43450,04ff2af52f147b,2b3dba04,"**Feature Importance:**

Now that the model is trained and cross-validated, we check to see the feature importances.",d5f37be9,0.9438202247191011
43453,c84925c8171900,1d4b4b5b,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.6.2 Year Wise Regional Sales
            </span>   
        </font>    
</h4>",e21ff7ec,0.9439252336448598
43456,69ac33d79f5130,4775a357,## Ask and Answer Question,9d760d2a,0.9444444444444444
43458,4d1d6dbab10b20,14247881, let me know if you guys have any suggestion that can be done this dataset.i'm still exploring and experimenting ciao.,fe366345,0.9444444444444444
43460,95d896e75f9a50,360c5948,"This also makes sense, it's clear that some features such as 73 split the data into plus/minus almost perfectly, while others are a lot more subtle.",2721b6f5,0.9444444444444444
43472,df2a7968c08ee4,e5503db5,"The following cell goes through all the images in the submission loader, and takes the class with the highest predicted probability. These are appended to a list, and converted to a pandas dataframe to be scored!",a2ba0a72,0.9444444444444444
43476,9085cba2265204,3ff718e9,## Saving predictions ,de766eb3,0.9444444444444444
43482,e2a94f078e1161,025e3313,Result :- The hypothesis was Correct for all activities those with absenses over 7 participated less than those who did'nt. However the Absenses Data was far too vague,5fc53059,0.9444444444444444
43486,fdc3afd309b850,b0f95b21,"That's seems a very good model, by the way.🤩",966bde38,0.9444444444444444
43487,d6cbd7160961dc,4fff2c7f,## 6.3.3. Results: Third digit,36d74664,0.9444444444444444
43488,36efe086c3f23c,b0175f59,"**DUE FROM HERE**


'''Combined statistical representations with Distplots ""Distribution plots""
The same plot will be used for comparing the prices in different parts
of our nation.
But for now let us look at data from the City that is chosen on Histogram,
Distribution and Rug plots of the data on a monthly resolution.
Namely Basic Distplots will reveal whatever we want to know'''
",d0676cd0,0.9444444444444444
43489,fbb1f9d3818830,b106a750,"# Conclusion

This exercise is quite useful for developers devising new models or techniques in order to see wether their model is capturing the right items or not. In this notebook the K-means was done using a single image inference but keen developers can try by doing batch inference with slight tweak to the function & also increase the number of clusters since the variance in feature maps will shoot up in case of a batch inference ( theoretically it should not be too different for single image inference). Knowing these details can help a lot in the project you are trying to develop or these details can be used to understand why your model works !

**Improvements to my code, any mistakes found or any other suggestions are always welcome.**

Thank You for your time.",c7027f86,0.9444444444444444
43490,b0c2805cd5c087,63b8770f,Image conference-board.org,0446f327,0.9444444444444444
43492,ac04ba639d1c93,b4213cf4,"# Conclusion

As we can see in prints bellow, if you separate the parameters for each type you can get a great improvement in your score.",748059d5,0.9444444444444444
43494,2975a21f5ce2ae,087fa259,"## References

* Based on SRK's kernel: https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings
* Vladimir Demidov's 2DCNN textClassifier: https://www.kaggle.com/yekenot/2dcnn-textclassifier
* Shujian's https://www.kaggle.com/shujian/fork-of-mix-of-nn-models",4d6a6aa0,0.9444444444444444
43499,cb4ad8ed4cb300,80ab8bc2,"**Possible Improvements**

* Provide more possible questions for one answer, should increase accuracy of providing the right answer.
* Possibility of answering more questions at once (e.g. ""Is there a valve and is it related to Pipe1?"")
* May add some threshold of cosine similarity. If question is below that threshold provid a answer like ""Sorry I don't understand your question, maybe you mean (most similar question)"".
* Possiblility to say something like ""That was not my question"", if one got a false answer. Then the machine should answer something like ""Oh sorry for that, maybe you meant (second most similar question)"". If than one answers ""Yes"" the machine should provide the answer to the second most similar question. If one answers ""No"" one can start the game from begining.
* Possibility to provide two answers if the question is quite similar to 2 or more questions at once.",7c0f3236,0.9444444444444444
43500,166a62ebb4fc3a,42c8c074,Lets Apply Random Forest model,db48a079,0.9444444444444444
43502,20e523830aab51,47d6e554,"# Data Every Day  

This notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  

***

Check it out!  
https://youtu.be/NeebXvQBa_g",810b8785,0.9444444444444444
43503,396bc36edb95d3,f87ce112,"<b>Train Data:</b>  
    AUC: 81%        
    Accuracy: 78%             
    Precision: 65%        
    f1-Score: 61%       
            
<b>Test Data:</b>      
    AUC: 83%      
    Accuracy: 77%         
    Precision: 68%       
    f1-Score: 61%     
  
The ANN model seems to over-fit.   ",965e4f8f,0.9444444444444444
43505,cf39cde80e66b7,15ff773c,"Refer: 
- [Metrics and Python](https://towardsdatascience.com/metrics-and-python-850b60710e0c)
- [Understanding Regression Error Metrics](https://www.dataquest.io/blog/understanding-regression-error-metrics/)
- [The Absolute Best Way to Measure Forecast Accuracy](https://www.axsiumgroup.com/the-absolute-best-way-to-measure-forecast-accuracy-2/)
",aed4bc9b,0.9444444444444444
43509,3ac432b2cac29c,2ca0011f,"# Keep Going

Now that you can measure model performance, you are ready to run some experiments comparing different models. The key is to understand **[Underfitting and Overfitting](https://www.kaggle.com/dansbecker/underfitting-and-overfitting)**. It's an especially fun part of machine learning. 

---
**[Course Home Page](https://www.kaggle.com/learn/machine-learning)**


",a358669e,0.9444444444444444
43511,9289395e9c480f,42e3220f,"@inproceedings{Population studies that related to COVID-19, author = {Hang, Wu}, title = {Population studies that related to COVID-19}, year = {2020}, month = {June}, url = {\url{https://www.kaggle.com/nike0good/population-studies-that-related-to-covid-19} } }",55d03d67,0.9444444444444444
43512,10b5af05d804ff,afcb58f6,**Waw!!!! Our magic mirror can get us +12 improvement on the leaderboard in Dota2!!! It's really magic!!! **,4a9b1705,0.9444444444444444
43513,b3e0b7e9ff6849,152685ef,"<p>Together, we get the most valuable five customers for a year! </p>
<p>This model is capable of producing valuable information to be used for several purposes by various departments such as marketing, sales, and product development. By using it, we can simply create segments according to the CLTV. We can also extract the least valuable customers and send them to customer relationship management to increase customer engagement with the company.",f6e4bb0d,0.9444444444444444
43515,a4a494c667c673,63d5f570,"# Data Every Day  

This notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  

***

Check it out!  
https://youtu.be/9-T48384oM0",397d12f8,0.9444444444444444
43522,5f4ae633cfd090,93b36c22,**77% is the accuracy after feature engineering**,a30a16e2,0.945054945054945
43523,91473a39b85068,38f33cd5,### Metric values of macro and micro f1 score are significantly increased when compare to previous model.,6e3d91c2,0.9452054794520548
43526,3d08ca7656dec0,9f4874da,# Compare model score,bd3f87e3,0.9452054794520548
43527,1eb62c5782f2d7,f673b239,"### Interpretasi

Skor terendah yang bisa diperoleh pelamar dan masih memenuhi syarat untuk dipekerjakan oleh agensi adalah sekitar 63.",bb69f147,0.9452054794520548
43529,0932046e1f485d,371adbbf,"There are more negative sentiment reviews that are closer to zero then there are positive ones. Also, the extream value of 1 is more frequent on the positive reviews.",218cc7a3,0.9453125
43531,f0fab078f8533b,1d1c9643,## g. From the below bar chart we can see that most of the content is added towards the last qarter of a year,bdb5ea32,0.9454545454545454
43535,2ada0305b68956,d0cc1f21,### 162. Palette = 'twilight_r',133e26f4,0.9457142857142857
43539,d76896b30cebd3,3f46cd2d,"Success Measure

I have made another column called ""success_measure"" by dividing the Pledged Amount by the Goal of the Campaign which gives an idea of how many times the goal, the pledged amount was. But the issue was a lot of campaigns had weird goals like 10 or amounts such as 200. To consider only serious campaigns I eliminated any campaign whose goal was below the median goal of all the campaigns and after that I got a list of the 10 most successful campaigns in Kickstarter",1b4e8f34,0.9459459459459459
43555,09751c520b0616,782b1d40,### Fitting model to whole data,a4d0c7e9,0.9461538461538461
43559,726833f92fb87a,92d36334,# Results Summary,7dc5e1b6,0.9463087248322147
43564,6a80f915608fc2,325361de,"### Make use of the classification to adjust the target values

In v18 and earlier, the rows identified as MoA=0 had their target values
reduced (divided by 2 to limit the penalty for false positives.)

In v19+ we will increase the target predictions for the MoA>0 detected rows,
and reduce the target predictions for the other rows.
",636938eb,0.9464285714285714
43569,d6ddbe57f59cf7,620806fe,# Heatmap,504a3cda,0.9466666666666667
43570,b10bd75889dad9,613b1e41,### Making predictions on the test set,ee00ceee,0.9466666666666667
43571,e5dd725b8fa422,7040d035,# [Creating a dataframe for Submission](http://),14675d8b,0.9466666666666667
43581,f6648e47713411,43b923e3,## 5.2 Create pridictor ,f4af4d1c,0.9468085106382979
43587,5ce12be6e7b90e,06b1eb05,"# Exercise : Dictionary
Create a dictionary with 5 records.<br>
Each record should be a song name as the key and the artist who preforms it as the value.",c0ab62dd,0.9473684210526315
43588,caa0ce2715bf34,f477fbb0,"## Observations
1. Cluster 0 : It is highest amounts for Tenure, medium for Purchases, whereas lowest for Payments.
2. Cluster 1 : It is lowest for Purchases,whereas medium for Tenure & Payments.
3. Cluster 2 : It is highest for Purchases & Payments, whereas lowest for Tenure.",78a5dc51,0.9473684210526315
43593,9e27af2600925c,7381a49d,"<font color='blue'>
**What to remember from this assignment:**
1. Preprocessing the dataset is important.
2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().
3. Tuning the learning rate (which is an example of a ""hyperparameter"") can make a big difference to the algorithm. You will see more examples of this later in this course!",9b556435,0.9473684210526315
43595,b94d32f6733801,5b465465,All analysis is this and thanks for everything that read or review,749022f9,0.9473684210526315
43596,1f3295ed0d4e4a,3f836317,"Here X and Y are Dask arrays 

> Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.",b0aeb172,0.9473684210526315
43610,5c3b8925bb1e43,e4337dda,So the result shows Random Forest always works better than Decision trees since its an ensemble technique. THe overfitting of Data that is faced in Decision trees is immuned by using Random Forest,d606a719,0.9473684210526315
43613,a6873c03336728,28db7a25,"# Real Gold Price History

![image.png](attachment:d26856e8-a949-4f6c-95e3-d48ff18a67ce.png)![image.png](attachment:3cd4b6c5-0018-43ba-921b-1c18509df11f.png)

https://goldprice.org/ko/gold-price-charts/all-data-gold-price-history-in-hong-kong-dollars-per-kilogram",99b35239,0.9473684210526315
43619,30fdc4a6e3c1db,537a3f31,Distribution of median prices of items over the years,6111ddee,0.9473684210526315
43621,c2a9f2fb3e1594,ce63dcd3,"<a id=""ch12""></a>
# Step 7: Optimize and Strategize
## Conclusion
Iteration one of the Data Science Framework, seems to converge on 0.77990 submission accuracy. Using the same dataset and different implementation of a decision tree (adaboost, random forest, gradient boost, xgboost, etc.) with tuning does not exceed the 0.77990 submission accuracy. Interesting for this dataset, the simple decision tree algorithm had the best default submission score and with tuning achieved the same best accuracy score.

While no general conclusions can be made from testing a handful of algorithms on a single dataset, there are several observations on the mentioned dataset. 
1. The train dataset has a different distribution than the test/validation dataset and population. This created wide margins between the cross validation (CV) accuracy score and Kaggle submission accuracy score.
2. Given the same dataset, decision tree based algorithms, seemed to converge on the same accuracy score after proper tuning.
3. Despite tuning, no machine learning algorithm, exceeded the homemade algorithm. The author will theorize, that for small datasets, a manmade algorithm is the bar to beat. 

With that in mind, for iteration two, I would spend more time on preprocessing and feature engineering. In order to better align the CV score and Kaggle score and improve the overall accuracy.

",53411c04,0.9473684210526315
43622,9f3710be6aea65,b72c4b9f,## Submiting Predictions,ae9bda88,0.9473684210526315
43639,90691864eb68c7,226e6fea,Model Evaluation using Confusion Matrix,3555ef9b,0.948051948051948
43640,663bbc9eaf267b,03f61672,## XGBoost Regressor,32445529,0.948051948051948
43641,2cb457b60dd246,3db88658,### Generate the Classification Report,339367df,0.948051948051948
43646,a1ba5ffd30dbde,b10dc76e,- 97% accurate in predicting Disease,48e57546,0.9482758620689655
43652,00001756c60be8,915643b3,**Создание датафрейма с предсказаниями**,945aea18,0.9482758620689655
43658,84127ade6fde87,f68a17d4,Conclusion,f55d05b6,0.9482758620689655
43660,063a35f644e3c5,2b007251,## GradientBoost,1c30fb0a,0.9484536082474226
43668,4d91e84c564cbe,d67a6ad3,Finally we have some insight into the classic Stupid Python Trick™ for swapping two variables!,355a43e3,0.9487179487179487
43669,0a1fcda859252c,3edf25f1,"Now we save our model architecture in ""model_2.json"" and model weight in ""model_2.h5"" that can be used for implementation",13a38774,0.9487179487179487
43670,d4c5aaa4b36810,2bb02141,Most of these predictions look reasonable however so are not even possible. For instance Afghanistan has a predicted happiness of -37 out 10. Lets round these up and down accordingly and use the leave one out method to asses the accuracy.,65441f28,0.9487179487179487
43681,9eed0fae1c7958,7515c0d4,# test on 3 external images,3fb1438e,0.9487179487179487
43687,c80939c7c626cf,ef307c0a,# 14 SVM,b9ac31e2,0.948905109489051
43689,fdc3afd309b850,f81c5813,"<a id=""pep""></a>
## 10.3 Prediction Error Plot",966bde38,0.9490740740740741
43697,f2e5e9fb9eaaf7,7c7cf39e,"<a id=""6.2.7""></a>
### 6.2.7 Exponential of features
It seems converting all the features into a exponential decrease the OOF AUC from `0.8015` to `0.8011`.",048e0d08,0.9491525423728814
43698,a81661cc35d8d2,2d7551ab,"In a real world use case, we would ideally want to go with the Logistic Regression model, as even though the overall accuracy may be less, the high recall on predicted class means that there will be fewer misclassifications of patients who are actually at a risk of heart failure. This will have a positive effect on patient care outcomes.  ",3331f113,0.9491525423728814
43699,b660910fcc2954,42cd352c,## Features correlation,80b74f88,0.9491525423728814
43701,1294fb4c86f993,b293f124,We can explore the distribution of total guns registeration for every month.,4471e513,0.9491525423728814
43702,9169c4e9c33c90,e7673a82,The non fiction genre accounts for 56.4% of the accumulated Top 50 charts.,725bf880,0.9491525423728814
43706,0e2a23fbe41ca9,b18fbeec,"## Sanity Check

- All ```card_id```s in training and testing set
- All ```merchant_id```s in merchant file
- Nulls
",64e4762c,0.9492753623188406
43711,4ae6a182abac64,b9f33ad3,### Submission,418676c5,0.9495798319327731
43718,ad121e0531afa4,b54188fd,![](https://imgur.com/XyvhYFZ.gif),a3492905,0.95
43724,c18267b203f28a,0645bbdf,"# Creating a submission file
Now that we've trained a model and made predictions we're ready to submit to the competition! You can run the following code below to get your submission file.",09ca8efb,0.95
43727,3dd4294f903768,41afb61f,# Gardient Boost,0d89d098,0.95
43729,4cd25e50c7e007,fb2dd984,## Model Evaluation,ceb0c525,0.95
43734,312d2a3c7547f1,1e376118,Evaluating the model,8fd1efcd,0.95
43738,09bac0c221388e,8bac3d2a,"As we expected, the **first output shows verbose and content-heavy sentences**, while the **second one is much more concise**. The second summary withal inclines to focus more on what is going well with the patient, omitting consequential information about what the doctor is worried about.",bea4aa2e,0.95
43741,b95c657bd26c57,fb01d1f6,We get better results because of better feature scaling. To improve the precsion and recall futher hyper parameter tuning is required,eb13c3f8,0.95
43743,f652abd9973afa,48a4202c,"

Please upvote this kernel so that it reaches the top of the chart and is easily locatable by new users. Your comments on how we can improve this kernel is welcome. Thanks.
",ab2f9cc1,0.95
43744,396bc36edb95d3,cf27d341,"#### 2.4 Final Model - Compare all models on the basis of the performance metrics in a structured tabular manner (2.5 pts). Describe on which model is best/optimized (1.5 pts ). A table containing all the values of accuracies, precision, recall, auc_roc_score, f1 score. Comparison between the different models(final) on the basis of above table values. After comparison which model suits the best for the problem in hand on the basis of different measures. Comment on the final model.",965e4f8f,0.95
43753,5626e84c4e6bf8,ee979658,Maybe this is not the most optimized map but you can always fork this kernel to play with parameters and fully optimize this implementation.,e2ecb669,0.95
43754,2bace980aeb34c,15b46e01,"# Submit your results

Once you have successfully completed Step 2, you're ready to submit your results to the leaderboard!  If you choose to do so, make sure that you have already joined the competition by clicking on the **Join Competition** button at [this link](https://www.kaggle.com/c/home-data-for-ml-course).  
1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  
2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.
3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.
4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.

You have now successfully submitted to the competition!

If you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.


# Keep going

Move on to learn about [**cross-validation**](https://www.kaggle.com/alexisbcook/cross-validation), a technique you can use to obtain more accurate estimates of model performance!",dc05ef6c,0.95
43756,e3c0b55ed519e2,519ad9e0,"# *  York PA tops the list with lowest ICU beds for infected covid19 patients.
# While,East Long Island NY tops list with lowest hospital beds for infected covid19 patients.",9f51352e,0.95
43758,f0faf9e7ac5abd,7605a4cf,Das War's Kaggle Notebook Runner: Marília Prata  @mpwolke,794edb83,0.95
43761,edc19e349fe80a,3815d98e,### 6. Save our submission file,7882221a,0.95
43766,e9b9663777db82,9a732893,#### Random Forest Regressor,648e8507,0.9504132231404959
43775,0858e1bb3cbaca,dbf63b06,3. Which type of product have the highest average price? How about the lowest?,78548374,0.9508196721311475
43784,8d0aebab1e5914,c961f187,"# PCA is a method that brings together:
* A measure of how each variable is associated with one another. (Covariance matrix.)

* The directions in which our data are dispersed. (Eigenvectors.)

* The relative importance of these different directions. (Eigenvalues.)

* PCA combines our predictors and allows us to drop the Eigenvectors that are relatively unimportant.

",084e671f,0.9512195121951219
43788,e19e307b3fd188,d36102cc,#### Evaluate,2173955b,0.9512195121951219
43789,8d70dcae7f40a3,8cf877d3,"#### Biểu đồ trên có AUC = 0.72 có ý nghĩa là mô hình có thể phần loại được Positive và Negative nhưng vẫn còn lỗi (FP, FN).

### **Các mức ý nghĩa của AUC:**
#### + *AUC = 1*: Mô hình phân loại rạch ròi giữa 2 lớp. Không có lỗi (FN, FP).
#### + *AUC = 0.5*: Trường hợp tệ nhất, mô hình không phân biệt được 2 lớp.
#### + *AUC ~ 0*: Mô hình phân loại ngược.

### **Kết luận**
#### + Dựa vào biểu đồ ta có thể chọn ngưỡng phù hợp cho bài toán.(TPR, FPR)
#### + AUC giúp ta xác điệu hiệu suất mô hình.",472c71ce,0.9512195121951219
43791,0e09587faffa8f,aa518e10,![](https://i.ytimg.com/vi/0QI4eG8D0Ic/maxresdefault.jpg),0d563d61,0.9512195121951219
43792,786475feda0190,0cb6eea4,"**These here are somewhat clean and has the important part which can help in classification.**

- These file should be stored and used for modelling.",e4663d97,0.9512195121951219
43793,514d8de15cb7ef,eed83edd,>  ## Result Analysis ,cfe111b2,0.9512195121951219
43796,fd4017c1514157,c4a1e456,"#### <font color = 'orange'>Chromagram</font>
* Chromagram closely relates to the twelve different pitch classes. 
* Chroma-based features, which are also referred to as “pitch class profiles”.
* One main property of chroma features is that they capture harmonic and melodic characteristics of music, while being    robust to changes in timbre and instrumentation.",fd8f0896,0.9512195121951219
43798,2ada0305b68956,8932265d,### 163. Palette = 'twilight_shifted',133e26f4,0.9514285714285714
43801,98a6794067932a,4f6fc950,L'analyse effectuée dans la cellule ci-dessous permet de déterminer le nombre d'expéditions par type de livraison pour lesquelles les clients perdus n'ont possiblement pas été satisfaits. Le code permet donc de créer une liste indiquant le nombre d'expéditions regroupées par le type de transport à partir du dataframe des expéditions historiques des clients n'ayant pas commandé auprès de l'entreprise depuis un certain temps. Cette analyse est pertinente afin de déterminer le type de transport qui a été utilisé par les clients ayant possiblement été insatisfaits. Les dirigeants peuvent ensuite évaluer si ce type de transport répond aux attentes des clients ou si ceux-ci sont mal informés par rapport à la performance de ce type de livraison.,08600fe2,0.9514563106796117
43807,4daf6153275cbf,4a8df5a0,"Our data is based on countries, so every point on these plots represent a country. There seems to be inverse relation with average price with average rating and average price with average number of reviews. Here, we can conclude that pricing has effect on the rating, higher the price, lower the rating. Also, higher the price, lower the number of reviews, that is maybe due to preference.",51db1961,0.9518072289156626
43808,835a7b4e660d23,ac93575f,### Groupby,53bc7a6e,0.9518072289156626
43813,fdc9f4863744b1,e83588d0,## Split the Data Set,b4529365,0.952054794520548
43814,4c47839b067546,77599663,MAPE 21.08%. Результат на kaggle 13.91838.,1f517b02,0.9521276595744681
43818,06ecf7a304c309,46a40cb3,"이제 예측을 만들어 봅시다.
",714de627,0.9523809523809523
43822,31268b33de97b5,8146e1e1,# Log Transformation,1e6f7d14,0.9523809523809523
43829,60d500d196eb42,93f502b8,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",2ad55f3f,0.9523809523809523
43834,fda19edaf5c621,d5c5c3b4,"From the graph right above (Time taken to finish the Survey for each group) what we can say that the average time taken by the oldest in age (and the most experienced with life) Kagglers is nearest to the time taken by the youngest Kagglers. And time taken represents how focused and quick these Kagglers were and hence we can say - Kagglers get young as they grow old.

Let's explore some takeaways from this small analysis:
* The high time taken group is 55-69 yrs and they also had a vast variation as well. We can say that they have better things to do than Kaggling (Just kidding)
* The 50-54 group really stands out in respect to time taken. They break the increasing trend. This Gen X stands out. Kudos 50-54 group, you guys are the Outliers, you will be treated Specially ;)
* The 22-44 yrs group, the biggest chunk of the Kaggler community, are the most predictable group. They follow a trend simple enough to be predicted by a simple linear regression. I personally like it, since I belong in this group :)
* 18-21 years group - really proud of this upcoming Gen Z Kagglers !!",5cfff76e,0.9523809523809523
43836,1084376bc4897c,af0b3079,#### By default Xgboost combine 100 trees to make the final decision. The figure belows shows the 30'th tree in the forest. We can see that the features near the root are the ones deemed significant by our exploratory analysis. This is one big advantage of decision tree over logistic regression: it is more interpretable.,1b598487,0.9523809523809523
43837,8985a124d4b657,bc675462,"Heatmap is a very useful visualization tool to know how much each feature is correlated. 
vmax = max value of the heatmap
fmt = number of decimal places upto which the value is shown
square = do you want the heatmap to be square shaped
linewidth = width of the lines in the heatmap
annot = should the boxes be labelled with the value",586d1846,0.9523809523809523
43847,a758983a68c014,b3edfbbe,"[This](https://ronxin.github.io/wevi/) is fantastic interactive tool, which helps to understand how these to models work for those who love good visualization. I suggest you to have a look.",ab89f181,0.9523809523809523
43853,066c5ee1ef39e6,3f3d3587,## Submission,0f394e1b,0.9523809523809523
43854,e16860fce156b0,3781812f,"LinAlgError: singular matrix

#""A person who never made a mistake never tried anything new.""  Albert Einstein",2054f1ce,0.9523809523809523
43855,15eb884262ba09,3cd2d00a,Inspired by [K-Means Clustering : World Map by Mutyala Yogita](https://www.kaggle.com/yogitamutyala/k-means-clustering-world-map),d703bdab,0.9523809523809523
43856,9456df44ab9308,70e1c6db,## *Download .csv & .mp4 for sport analysis*,a8e94298,0.9523809523809523
43857,659f5f3ef8aa0e,6d83f219,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",3654c2d0,0.9523809523809523
43858,b74076b2f8ba1d,2e89704c,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",9ace22d4,0.9523809523809523
43861,6471597c5d2f66,e8a9b658,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",a41b4abe,0.9523809523809523
43862,adb8441ad28019,c4c65cb8,"<div class=""alert alert-success"">
    <h1 align='center'>Comparing Performance of the Models on the Test Set</h1>
</div>",d89de993,0.9523809523809523
43863,04bac111ffbe9c,df9a73b8,## XGBoost,82576b17,0.9523809523809523
43864,6f4795cfdc96c7,dea844da,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",1f3ab82f,0.9523809523809523
43871,869a39a3d4dea2,ad8ea20b,"Histogram Equalization<a id=""equalization""></a>",9020daf8,0.9529411764705882
43873,fc8e0042411c46,f874859d,# Plotting the ROC Curve,af476c2a,0.9529780564263323
43876,ff3a8ce61fab6a,b3ed5d25,"<hr>

# 7. Graph

<p>
    We can consider <b>graph</b> as a history that contain all operations
    we do it through our code.
</p>",9afe1654,0.953125
43892,22bd95f4807a23,e8f9cd19,# Further Analysis,c05d356f,0.9534883720930233
43898,ab6da5994949a3,fea17a61,## Visualising the Random Forest Training set results,fae6b91d,0.9537037037037037
43901,a8c042af6b7245,a54bf8cf,"#### Feature scaling
As mentioned before, we can apply standard scaling to the training data. Some classifiers perform better when this is done.",2487ac62,0.9538461538461539
43911,d07915a6e6992e,55bf27ec,"**Voting Classifier**

Voting is one of the simplest method of combining the predictions from multiple machine learning models. It is not an actual classifier but a wrapper for set of different ones that are trained and valuated in parallel in order to exploit the different peculiarities of each algorithm. Here we are combining the predictions from  models that we built and predict based on votes.",2b912140,0.9538461538461539
43915,14defffcd250f3,4f64294e,# Evaluation,3a683b94,0.9540229885057471
43918,917957c6c4065f,f67170f4,"조회수에 영향을 주는 항목들이 무엇인지 알아보기 위한 산점도입니다.  
소수의 조회수 상위 항목들에 의해 그래프의 분포가 영향을 받는 것 같습니다.  
조회수 상위 데이터(조회수 백만 이상 데이터 570개)를 제외하고 그래프를 그려보겠습니다.",55b8ed68,0.954248366013072
43922,b066ab2167199c,f96fc955,- From graph understood that there is **no missing values** in this dataset.,18a1753d,0.9545454545454546
43924,be2f4d8a6b73ca,4ab26e9f,**Support Vector Machine**,5d8ce40a,0.9545454545454546
43926,da199f8fb59439,3f8f0e63,**Countries Movies/Tv-show available on netflix**,baaa665d,0.9545454545454546
43930,c2be02442e8cfd,d7d877dc,"# Observation: 

1. 75% of people survived with 3 or less axillary nodes detected
2. people who are not survived having more axillary nodes.",2d364acc,0.9545454545454546
43932,ae058c3f1439c3,94dd9b76,"> After looking at the above cloud of words which states the most demanded Preferred Additional Requirements, We can Conclude that Some of the most Important Preferred Requirements Include Technical Experience, Certifications, Master Degree, Computer Science, etc.",965da99d,0.9545454545454546
43934,dd02a9b545f742,27aee96c,"# Bibliography

[0] Design

Larman, G. (2002). Applying UML and Patterns: An introduction to
    Objected-Oriented Analysis and Design and the Unified Process (2nd ed.).
    New Jersey, The United States of America: Prentice Hall.

Soegaard, M. (2018). The Basics of User Experience Design: A UX Design Book by
    the Interaction Design Foundation (1st ed.). Denmark: Interaction Design
    Foundation.


[1] Programming
Guttag, J. (2013). Introduction to Computation and Programming Using Python.
    Massachusetts, The United States of America: The MIT Press.

Lott, S. (2019). Mastering Object-Oriented Python | Build powerful applications
    with reusable code using OOP design patterns and Python 3.7 (2nd ed.).
    Birmingham, United Kingdom: Packt Publishing Ltd.

Smith, Taylor G., et al. pmdarima: ARIMA estimators for Python, 2017-,
    http://www.alkaline-ml.com/pmdarima [Online; accessed 2020-12-18].


[2] Computer Science

Raschka, S., & Mirjalili, V. (2019). Python Machine Learning: Machine Learning
    and Deep Learning with Python, scikit-learn, and TensorFlow 2 (3rd ed.).
    Birmingham, United Kingdom: Packt Publishing Ltd.


[3] Knowledge field

Baekert, G., & Hodrick, R. (2012). International Financial Management (2nd ed.).
    New York, The United States of America: Pearson Education, Inc.

Yan, Y. (2017). Python for Finance | Apply powerful finance models and
    quantitative analysis with Python (2nd ed.). Birmingham, United Kingdom:
        Packt Publishing Ltd.

Lagplot. (2016). Available at https://www.statisticshowto.com/lag-plot/
        

[4] Technical Notes

Issue: NumPy 1.19.4 to NumPy 1.19.3 because of RuntimeError
Note: https://tinyurl.com/y3dm3h86 (see (1) hessler.evan
03 Nov a 03: 28; (2) Kevin Sheppard 03 Nov a 08: 04)

Issue: What everyone is looking for: API Workarounds 
Note: https://www.kaggle.com/c/jane-street-market-prediction/discussion/200024",7116cd2d,0.9545454545454546
43938,69e2428808c415,a329e45d,Now we can see there are no null values and we completed are EDA and Data pre-processing.,ae798c16,0.9545454545454546
43939,0cb9adc158b705,5bb067f1,"Fastai is extremely flexible and powerful at the same time. This is just the baseline notebook. You can easily build on top of it. Here are some things that you can experiment with:

- Preprocess and Feature Engineering
- Data Augmentation and External Datasets
- Different Model Architectures
- Training Schedule, Optimizer, etc
- Postprocess

You can find the **[inference notebook here](https://www.kaggle.com/ankursingh12/fastai-plant2021-starter-inference)**.

Hope you had fun reading the notebook. Kindly consider **upvoting**.",3abf056e,0.9545454545454546
43942,930cd79ca51204,66cb3cdb,This is looking more and more like Hans Rosling's plot! 🥳,5506779a,0.9545454545454546
43945,e323e594ef918f,ef051781,"We now see a stronger activation in the bottom left corner which I think makes sense. 

# End. ",6e829ab6,0.9545454545454546
43956,90964081c7faab,91a16b3e,"rf_parameters = {'n_estimators':range(10,300,10),'criterion':('gini', 'entropy'),'max_features':('auto','sqrt','log2')}
gs_rf = GridSearchCV(rf,rf_parameters,scoring='roc_auc',cv=10)
gs_rf.fit(X_train,y_train)

print(gs_rf.best_score_)
print(gs_rf.best_estimator_)",b423b0c3,0.9545454545454546
43960,d1ff7e10ee0102,1917ca6a,# Conclusion,2cc71c3c,0.9545454545454546
43961,ba4b3bd184acbb,de0d673e,"Another way we could find the mean of the Sentiment grouped by App is to use the `pivot_table` method.

Using App as the index, we are selecting that as the group by variable and the default aggregation function is mean so we do not have to specify it.

Using the `eq` and `all` methods together, we can check if every cell is the same for both DataFrames. ",0f5de724,0.9548872180451128
43966,1a222fee3089d2,7c522815,# Submission,59ab8894,0.9552238805970149
43974,49ac6594c8f5cf,47b55dfe,KNN model,6f19f28a,0.9555555555555556
43979,892be0a523578c,7ca8ac2c,"Finally, I want to see wether the weight and BMI of our customers change during wearing the devices, based on the hypothesis that the smart devices can prompt them to lead a more healthier style. However, it seems that the index doesn't vary a lot. But this result is limited to the number of customers who have enough data for analysis and the tracking length. ",b0e8d7c0,0.9555555555555556
43982,396bc36edb95d3,4289b2b2,#### Comparison of the performance metrics from the 3 models,965e4f8f,0.9555555555555556
43983,3597174a998d4d,279585f6,## 3.2 Model with new variables,276892ed,0.9555555555555556
43984,188731d7fa0604,878f7ab2,## 결과 체점 (수험자는 알 수 없는 부분임),7cc543d3,0.9555555555555556
43985,d905cde3391d2b,d6ede43b,"There's one problem with this graph. We have outliers to the right of the graphs. Instead of showing **min** and **max** as the two ends of the whisker, we calculate the following.

* Lower fence = $Q_1 - 1.5 \times IQR$,
* Upper fence = $Q_3 + 1.5 \times IQR$

Note the `whis` parameter in the above code set to `range`. By default `whis` is set to `1.5`.",067dba39,0.9555555555555556
43986,b0c2805cd5c087,e19cf5d9,"* Artificial Intelligence Index Report 2019
* Raymond Perrault, Yoav Shoham, Erik Brynjolfsson, Jack Clark, John Etchemendy, Barbara Grosz, Terah Lyons, James Manyika, Saurabh
Mishra, and Juan Carlos Niebles, “The AI Index 2019 Annual Report”, AI Index Steering Committee, Human-Centered AI Institute,
Stanford University, Stanford, CA, December 2019.
* (c) 2019 by Stanford University, “The AI Index 2019 Annual Report” is made available under a Creative Commons AttributionNoDerivatives 4.0 License (International)
https://creativecommons.org/licenses/by-nd/4.0/legalcode
* The AI Index is as an independent initiative at Stanford University’s Human-Centered Artificial Intelligence Institute (HAI). 
",0446f327,0.9555555555555556
43987,d96e03a9e7c030,dbc78ffc,"## Step 6: Potential Areas for Additional Exploration and Limitations of the above analysis
First of all - thanks for reading this far into this notebook! Below are some observations as I was completing this analysis, along with a few areas that I would have liked to explore further:
1. There were no interaction effects taken into any of the above. Potential insights could be yielded by looking when looking at (as an example) crimes reported at a school along with statewide performance metrics. There are countless numbers of possible interactions, however, so it would take a good deal of time and effort,
2. The web scraper functions from the Department of Education is just scratching the surface. There is a ***ton*** of data available in a standardized format on the Dept of Education site for each school (example for [Stuyvesant High School here](http://tools.nycenet.edu/guide/2017/#dbn=02M475&report_type=HS), all going a few years back
3. Related to #2, all the data scraped is in regards to the most recent year currently available, 2017. There are no time series information of data taken from the past 2, 3, 4 years that could yield additional information
4. The 'additional expected testtakers' for black, hispanic students and students below poverty level are taken as a direct percentage of the school's Grade 8 enrollment. In other words, it assume that, within a school, the distribution of students that take the SHSAT and the distribution of students who are part of specialized schools' underrepresented group are the same. This may or may not be the case
5. Schools whose *actual* SHSAT percentage was much different than *predicted* (i.e., outliers) could be incredibly useful in uncovering some of the dynamics involved in why students decide to take (or not take) the SHSAT. Below is the list of such schools. In the cases of these particular schools, the regression model described above was *not* a good representation of the relationship between school characteristics and SHSAT. Which begs the question - what is going on in these schools that either encourage or discourage students from taking the test?

Given the areas of additional exploration above, however, much of the analysis is in line with prior research, notably the Research Alliance for NYC Schools' [Pathway to an Elite Education](http://steinhardt.nyu.edu/scmsAdmin/media/users/sg158/PDFs/Pathways_to_elite_education/WorkingPaper_PathwaystoAnEliteEducation.pdf) report from 2015. This lends credence to many of the conclusions that can be reached from the much of what is found in the rest of this analysis.",d2b72ced,0.9555555555555556
43993,ac1abfe1dfe815,81a3047e,----,6529dbcb,0.9557522123893806
43998,3d905ce4828057,221b399d,"# Comments
When we look at the scaled clv in the D segment, we can say that the mean and median values are very close to each other and this segment is homogeneously distributed.

# Action Comment
When we compare the A and D segments, since the value added by the D segment to the company is very low, a feasibility study can be organized in order to increase the number of sales and to promote other products with high prices. We can determine the products to be promoted according to the needs of the customers by conducting a survey. Thus, we can gradually move those in this segment to the next segment.",5b006cc3,0.9558823529411765
44001,7f74a04ae75792,b5b2f896,### Create a Heatmap of correlation between the variables,d01e91da,0.9558823529411765
44002,eb0ecd6bebeb15,31ea0e03,Hedef değişkenimiz variety'e göre gruplama işlemi yaparak sadece petal.length değişkenimizin standart sapma değerlerini yazdıralım. ,d7b93a60,0.9558823529411765
44004,02b7e38902069e,db784695,"#After the snippet above I couldn't make anything more with Stanza since Hindi is not supported yet.

Exception: spaCy tokenizer is currently only allowed in English pipeline. ",726a03a0,0.9558823529411765
44013,fdbbd573ba31c2,52819212,# Submission,f7c28d74,0.95625
44017,2409b2d74a9871,7c9d5bb6,"**Conclusion**: Logistic Regression and SVM seems to work well here without much feature engineering. Here during PCA features have been reduced from 233 to 15. Algorithms behave differently with different number of features. More features can be tested for the effectiveness of price prediction like longitude, latitude, name (separating adjectives like cozy, clean, spacious etc.).",b0a6c313,0.9565217391304348
44019,fe118026267a88,97649aaf,"---
**[Pandas Micro-Course Home Page](https://www.kaggle.com/learn/pandas)**

",612efa48,0.9565217391304348
44036,71c3c1eab0377d,fa51e846,**Convert the H2O dataframe to pandas DataFrame**,52b4e360,0.9565217391304348
44043,3319c5c562f607,ae37cfbf,"# Conclusion 
* More Update will be done Soon
* This is the 2nd step of text cleaning
* 3rd part will come soon
* For the 1st part clivk here is the link 
* https://www.kaggle.com/jurk06/sentiment-analysis-part-i/#Stop-Words",f298250a,0.9565217391304348
44047,90ead00a8ee283,43df0a0b,"## Keep going

Move on to the **[indexing, selecting and assigning workbook](https://www.kaggle.com/residentmario/indexing-selecting-assigning-workbook)**

___
This is part of the [Learn Pandas](https://www.kaggle.com/learn/pandas) series.",612efa48,0.9565217391304348
44050,598b6228760590,59a5e970,"- **Fare has a missing value, the problem is not big, we can temporarily fill it.**",be30ab66,0.9565217391304348
44052,a566b5b7c374e7,fe6ef8dd,## Word Clouds,b3dc5545,0.9568345323741008
44053,f3c6048d1058e3,f00d4767,"# CNN Model
https://github.com/mrunal46/Text-Classification-using-LSTM-and-CNN/blob/master/LSTM%20and%20CNN%20on%20imdb.ipynb",1d9056b0,0.9568965517241379
44055,38b79494ac749e,d0ec7f1c,### Excercise 3 - Scaling (2 points),39162a40,0.9571428571428572
44059,2730840089c8eb,5b3d4b5e,"To read a full inventory of dictionaries' methods, click the ""output"" button below to read the full help page, or check out the [official online documentation](https://docs.python.org/3/library/stdtypes.html#dict).",34d27dac,0.9571428571428572
44060,2ada0305b68956,952b5c32,### 164. Palette = 'twilight_shifted_r',133e26f4,0.9571428571428572
44070,3f25b363afec54,e660c445,"## Creating the outcome variable.

--- In progress",bbdaae25,0.9574468085106383
44071,73893f0467d5e3,52277fa3,## Accuracy Check for test and train,279787c6,0.9574468085106383
44075,4c47839b067546,6ea2aaf1,## StackingRegressor,1f517b02,0.9574468085106383
44076,5f674175839b32,11ef3f3a,"Q6)IN Ps2 platform which is the best selling game?

Ans: pro evolution soccer 2011 is the best selling game on PS2 platform released in 2011,with 20 million sales.",53a2e343,0.9574468085106383
44080,631cd434fc3aa2,4b665d95,"## Final training and submission
We've almost reached the end of this kernel.
The lasts steps we have to perform is to apply our pipeline are: 
* retrain our model using the whole train set
* predict value of _SalePrice_ using the test set
* create the submission file.",2b74febb,0.9577464788732394
44087,f91f58d488d4af,5f7f7928,#### Model Training.,5df1bbf3,0.9578947368421052
44094,1d5daeca89f48d,09bf57a0,"# Conclussion

  To overcome the context problem, we use differnt technigues in the upcoming session",48d478bc,0.9583333333333334
44101,2a377ced98d67a,8f395a62,## 7. Interactive application for admission likelihood estimation,262231a8,0.9583333333333334
44106,28a1ff0f223da9,4fb40a7c,"According ti this [HEC data](http://hec.gov.pk/english/universities/Pages/AJK/University-wise-Full-time-faculty.aspx) there are about 10,133 full PhD faculty members in Pakistan in all Pakistani universities.",c945b27d,0.9583333333333334
44108,a69d41047fdd3e,ff2d18c8,"---
**[SQL Home Page](https://www.kaggle.com/learn/intro-to-sql)**





*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*",b1f28647,0.9583333333333334
44111,02773bdc5d3c7a,a9bf3720,"# Concluding Remarks

Checking the above plots, we come to know that for this particular problem, Undersampling seems to underperform as compared to other methods. Although, undersampling is giving quite good recall scores, it is doing so at the cost of the precision scores.
Even, SMOTE gives average performance for the given data and I would place my bets on Oversampling technique in combination with a decision tree model to achieve decent values of precision, recall, accuracy and even F1 score.

Last but not the least, the above techniques are just the ones which I have implemented and there definitely would be room for improvement. 

Thanks for going through this kernel. Do let me know in the comments if you have any suggestions or doubts.",86245f35,0.9583333333333334
44113,a3ae04b78e45b5,361f8ddf,> **COUNT PLOT THROUGH BAR PLOT**,4195da8b,0.9583333333333334
44122,923e97b05be00b,596c2cde,"If you liked this presentation, welcome to follow me on twitter at <a href=""https://twitter.com/AdlebergJason?ref_src=twsrc%5Etfw"" class=""twitter-follow-button"" data-show-count=""false"">@AdlebergJason</a><script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script>!",3a4a22dd,0.9583333333333334
44127,c01049afb6d307,a393e465,* overweight and normal people outnumber,d37d3b5d,0.9583333333333334
44130,69ac33d79f5130,d8ebef3a,"1. Are there more accident on coastal areas ? 
Yes, according to my finding the coastal areas are more accident prone as compared to others.
Reason is High population

2. Does visibilty are the main reason for these accients ?
Yes, In my findings its clerly shown that in winters accident rate are high.

3. Why New york is not in the top list instead having more population ?
Data are not given for the NewYork
4. Which states have more accident prone cities in top 100 ?
  
5. which days, months have more accidents ?
In weekdays accidents are more and winter months have more accidents.

6. What time in a day have most accident ?
More accidents happened on office timing.


",9d760d2a,0.9583333333333334
44135,2f47abddfd1928,296076e4,"The most important classes are quite expected:

- We saw that having ""Mr"" as title had very low chances to survive.
- Being of 3rd class was bad for your survival chances.
- Woman in a family with dead children has shown to be very useful, although it was kind of expected just looking to the graph.
- The rest of created features have been important enough to be worth to use them.
- Alone has not been used, so probable we could keep just the family size feature and get the same result. Probably if we were using other algorithm this ""duplicated"" feature may have helped further.",ae33cc0b,0.9586776859504132
44144,1eb62c5782f2d7,bd07416e,"### Menemukan Nilai Data Spesifik untuk Probabilitas yang Diberikan
### Contoh 5

- Dalam sampel wanita usia 20-34 yang dipilih secara acak, mean total tingkat kolesterol adalah 181 miligram per desiliter dengan standard deviasi 37,6 miligram per desiliter.
- Asumsikan kadar kolesterol total berdistribusi normal.
- Temukan kadar kolesterol total tertinggi yang dapat dimiliki oleh seorang wanita dalam kelompok usia 20-34 tahun ini dan masih berada di 1% terbawah.",bb69f147,0.958904109589041
44145,91473a39b85068,e7588d6e,"### Why Logistic Regression
Our One vs Rest classifier can take any model and give results. Here I have limited myself only to Logistic Regression and not used other models like Support Vector Machines, Random Forest, Gradient Boosting Decision Trees etc is because of Multi-Fold. i.e, we have a very high dimensional data with 500 models to build and less time complexity. In this case the mentioned other models like SVM, Random Forest and GBDT cannot work well when compare to Logistic Regression. I have still used Linear SVM with SGD Classifier considering loss value as hinge to get good results but failed! Logistic Regression is much more faster than any other model.",6e3d91c2,0.958904109589041
44147,30fdc4a6e3c1db,efe36bb1,Let's use a threshold of 3.42 to as the median to bucket each item as Cheap or Costly,6111ddee,0.9590643274853801
44148,5ce12be6e7b90e,6af9e0f6,"# Functions

We _define_ functions with the __def__ command.
The general syntax is:

```py
def function_name(input1, input2, input3,...):
    # some processes
    .
    .
    .
    return output1, output2, ...
```

For example:",c0ab62dd,0.9590643274853801
44149,ffd1df95ca5289,f1c02683,f1 score of 1 has also been increased from 81% to 82% so as the accuracy from 81% to 83%,db00c338,0.9591836734693877
44155,f35bf4df70d310,615b3840,"From our experiment, the settings which produce the highest score is 
> **PCA while maintaining 90% of data variance**


This approach has reduce 

* 35.42% of the training time
* 69.9% of the components

while maintaining the **same accuracy score**",10bb859a,0.9591836734693877
44157,e69a496109e7d8,ee6e3fe3,# Conclusions,1c640591,0.9591836734693877
44161,e4525eb0c96f28,93eab34d,"Sadly it appears that our random forest regression did not make too much of an improvement to our linear regression. Just as our t-test tells us, we do not have enough evidence to reject the null hypothesis that the mean difference between the errors of the regressors is not significantly high nor low.",2093a1f1,0.9594594594594594
44164,62037c5832129c,26eb0cb8,## Addressing over- and underfitting with validation curves,61474350,0.9594594594594594
44165,63b44c85e32c1f,90b14740,**pop( )** is used to remove an arbitrary element in the set,fb9b9562,0.9594594594594594
44181,519e936017c30a,051728c5,"Como podemos observar, los ingresos obtenidos por ""XBOX 360"" son superiores a los de ""PS3"", registrando casi un valor de 600 millones de dólares en su totalidad. Casi la mitad de dichos ingresos corresponde a las ventas obtenidas en Norte América y en cierta medida por las ventas de Europa. A diferencia de su competidor, esta plataforma no ha tenido un buen nivel de ventas en la región de Japón, siendo las recaudaciones de ""PS3"" aproximadamente de 100 millones de dólares. Cabe destacar que ambas plataformas tienen un recaudación similar en los países Europeos.",dc34915d,0.96
44182,9395559895004f,eaa334a4,## Thank You!!,b5a0494b,0.96
44186,7e1da639035ac5,7408cd94,### <a id='16.2'>16.2 KMeans clustering implementation and visualization</a>,120b6c23,0.96
44187,caaa6793391520,a051a199,"# Next Steps #

Now we need to find a dataset that is known to have a gender or race bias, and demonstrate, that with this technique we can avoid amplifying bias, and maybe even decrease the bias by applying a scaling factor",1e79f342,0.96
44188,21c1e34efd71b8,78f5c3b7,"Hmm the Recall score for Decision tree Classifier is gone Up.. good news.. I'll take it. But wait, did you observe the DTC learining curve dipping down as the CV test size increases..
So, what do you'll say, which model is a good model??
Comments?",23b2cdd6,0.96
44189,6338f6b0178d13,68322590,# The jaccard score comes out to be 0.65 which means that our model has prediction score of 65% ,ae81b18b,0.96
44190,10c5a39a87c47e,f557d100,## Conclusion & Future Improvements <a id='conc-fut'></a>,09c7337a,0.96
44194,274b32da3b19a8,5f5c8d5d,"### [Train notebook Here](https://www.kaggle.com/durbin164/chaii-baseline-training) 
### If like Please UP Vote.",408f7268,0.96
44196,bbad077c274022,0551d44d,"**Conclusions:-**
* I walk more in mornings and late evenings. (Hope this helps me stay fit yo!)
* I am consistent with my Basic walking pattern rather than the Outdoor ones.
* I walk more outside than at home (Sundays are exceptions. I love experimenting in the kitcken that day).
* Thursdays are the days to save energy for Friday nights. I walk very less on Thursdays.",3c2e3dea,0.96
44200,5cb7f999fd1ecb,15f41008,"# Thank You

If you have any suggestion or advice or feedback, I will be very appreciated to hear them.
### Also there are other kernels
* [FIFA 19 Player Data Analysis and Visualization EDA](https://www.kaggle.com/ismailsefa/f-fa-19-player-data-analysis-and-visualization-eda)
* [Crimes Data Analysis and Visualzation (EDA)](https://www.kaggle.com/ismailsefa/crimes-data-analysis-and-visualzation-eda)
* [Google Play Store Apps Data Analysis (EDA)](https://www.kaggle.com/ismailsefa/google-play-store-apps-data-analysis-eda)
* [World Happiness Data Analysis and Visualization](https://www.kaggle.com/ismailsefa/world-happiness-data-analysis-and-visualization)
* [Used Cars Data Analysis and Visualization (EDA)](https://www.kaggle.com/ismailsefa/used-cars-data-analysis-and-visualization-eda)
* [Gender Recognition by Voice Machine Learning SVM](https://www.kaggle.com/ismailsefa/gender-recognition-by-voice-machine-learning-svm)
* [Iris Species Classify Machine Learning KNN](https://www.kaggle.com/ismailsefa/iris-species-classify-machine-learning-knn)
* [Breast Cancer Diagnostic Machine Learning R-Forest](https://www.kaggle.com/ismailsefa/breast-cancer-diagnostic-machine-learning-r-forest)
* [Heart Disease Predic Machine Learning Naive Bayes](https://www.kaggle.com/ismailsefa/heart-disease-predic-machine-learning-naive-bayes)
* [Mushroom Classify Machine Learning Decision Tree](https://www.kaggle.com/ismailsefa/mushroom-classify-machine-learning-decision-tree)",88b54f70,0.96
44202,83df814455f06c,616a3478,"# **17. Results and conclusion** <a class=""anchor"" id=""17""></a>

[Table of Contents](#0.1)


1.	In this project, I build a Decision-Tree Classifier model to predict the safety of the car. I build two models, one with criterion `gini index` and another one with criterion `entropy`. The model yields a very good performance as indicated by the model accuracy in both the cases which was found to be 0.8021.
2.	In the model with criterion `gini index`, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting.
3.	Similarly, in the model with criterion `entropy`, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021.We get the same values as in the case with criterion `gini`. So, there is no sign of overfitting.
4.	In both the cases, the training-set and test-set accuracy score is the same. It may happen because of small dataset.
5.	The confusion matrix and classification report yields very good model performance.",c9cff71a,0.96
44207,52ee792e228d54,74cd964e,### SVM,5096094e,0.9605263157894737
44210,523123dad03177,325a7285,Nah...,48a5e4e6,0.9607843137254902
44211,d0080e3a39bc5c,0b3c1206,"**Now that we have reached the end of the kernel, I am assuming you liked the kernel, since you didnt close it mid-way.**

**If you did like it, please UPVOTE the kernel. That keeps me going !**

**Any suggestions and criticism are welcome.**

**Cheers !**",2fcde4cf,0.9607843137254902
44214,52cfd66e9ec908,34ddd936,Similar to fastai - unfreeze model and then freeze to.,c74adcdf,0.9607843137254902
44216,7cfd96218dd933,328f0a9b,"#### **ATTENTION**
* AT LOCAL BASIS, TURKEY HAS THE HIGHEST VALUE IN THE MEDITERRANEAN REGION.
* IT MAY BE AN INDICATOR THAT THERE IS NOT A REGIONAL ANOMALY.",7c34d96c,0.9607843137254902
44223,0932046e1f485d,95b93ee1,Games than to have the most extream values on the box plot. The conclusion I draw from it is that they tend to have more passionate positive and negative reviews that either go to +1 or -1.,218cc7a3,0.9609375
44224,722cd844dfbe8f,832c6a1e,The submission.csv file will be used for the competition *(evaluation with AUC under ROC curve)*. We can also look at the **distribution of the predicted probabilities** :,0cedb385,0.961038961038961
44225,241cf32abb22d8,07725f57,"Suppose the school wants to predict students who are likely to fail the Mathematics course in order to give these students more support in advance. Then the recall of ""0"" (fail) is an important metric to consider in this case. The Decision Tree model is the most optimal one.
However, if the school wants to correctly predict students who are likely to pass in order to select good students to attend Mathematics competitions, the recall of ""1"" (pass) should be emphasized. Therefore, the KNN model can be the most optimal one.
In general, the Decision Tree model can be regarded as the best model in terms of the F1-score as this score is a weighted harmonic mean of precision and recall.",47157066,0.961038961038961
44229,75adb7945ef9bd,5872d48a,"Among the top 50 mispredicted tweets, only 4 are false positive",785c5095,0.961038961038961
44237,6f1481148352e9,4a15ff88,"**The largest number of fires occurred in July, and there were also many fires in October, August and November.**",7cfbdb8f,0.9615384615384616
44239,d0f6276d5b628c,548ccbe8,"These whole project ends here declaring these movies which are the most effective towards subscribers and if contents are produced similar to this ones, thus creating profit .
",c64f5ce5,0.9615384615384616
44244,a915263bc207da,4c052aa0,"Results look good intuitively. However tests might be performed to check if the user will actually like or not like the movies suggested. On of the ways to do this would be to use a train test method and match the predictions vs user ratings. To be more clear, the rating data could be split using train test and this model could be prepred using the training data. Afterwards using this model we could try predicting the ratings of the test users and match it with the actual ratings. But such test can't completely evaluate a recommender engine because users may like what they have never seen before. If the recommender gives the user a suggestion to watch movie 'X' which he hasn't watched and therefore did not rate also. But just because he didn't rate it doesn't mean that thehe won't enjoy the movie.",b17ebcda,0.9615384615384616
44247,d78988cb5a1b02,58b2b326,**Thus we can say that the roc curve is well structured.**,233f3a92,0.9615384615384616
44249,582cb872d19026,a3175830,"** There are two main types of apps in google paly store, free and paid. Such applications can be used for games, 
  movies, education and video, etc. There are other categories of applications, such as All of these apps are 
  available on the Google Play Store.

** The categories used by the applications of these games are Word, Trivia, Simulation, Sports, Strategy, Racing, 
  Role_Playing, Puzzle, Music, Educational, Card, Casino, Casual, Board, Action, Adventure and Arcade.

** While the Game Casino category is in the first place in the growth for 30 days, the Game Board category appears 
  in the first place in the growth for 60 days. Accordingly, it will be a more accurate method to examine the growth 
  kinematics of games that can be examined in long-term prediction.",8d966d69,0.9615384615384616
44257,aae204e78a48d1,1e872a07,"It seems like the attrition base have a higher number of months inactive versus existing customers, the data is less distributed than existing customers. 

**Hypothesis 5: partially proven**
",53ab6133,0.9615384615384616
44262,cf08b03b002c13,93344906,"NSFW content takes up a very small part of Reddit. Nevertheless, NSFW content definitely attracts a dicent portion of attention from the community with both number of comments and score 4x times higher than average for mass orinted content. ",104d416f,0.9615384615384616
44268,09751c520b0616,6222cd8b,### Making prediction for test dataset,a4d0c7e9,0.9615384615384616
44272,ee23a565163388,1135445a,The model performs decently in both training and testing datasets with an accuracy of ~84%. This tells us that the model is neither underfitted nor overfitted.,88aacbc4,0.9618320610687023
44276,7a058705183598,09b1aa85,Need to collect class_ids to check model accuracy with y_test,b0ead917,0.9619047619047619
44282,43e60eb1362f5c,285089ff,# Model Analysis,87934234,0.9622641509433962
44287,23df07a474aaae,81010610,# Conclusion,0ea40276,0.9622641509433962
44288,f3c8651cb08234,d07dabdf,# Really appreciate your time,37f86e36,0.9622641509433962
44290,fc8e0042411c46,fd5ced78,# Precision and Recall,af476c2a,0.9623824451410659
44292,254cccd5145725,15318414,# Submission,a49b4037,0.9625
44293,5ffe6aa38958a1,ca7a8620,"# 5.5 Submitting our prediction 

Finally, we use the model to submit our prediction. One final thing we have used here is to choose a threshold value that is different from default. Based on precision/recall curves, we choose a prediction probability of 0.3",11f5412e,0.9625
44296,c84925c8171900,6069591f,"<h4>   
      <font color = darkgreen >
            <span style='font-family:Georgia'>
            5.6.3 Global Vs Regional Sales
            </span>   
        </font>    
</h4>",e21ff7ec,0.9626168224299065
44299,2ada0305b68956,c9ce0669,### 165. Palette = 'viridis',133e26f4,0.9628571428571429
44300,b9328fe3b0cefc,83602e6c,## The end.,3a35eb23,0.9629629629629629
44305,efbcfe95cd7fde,ac372a72,"Luego de hacer una analisis a la visual de la correlación que existe entre si sufrir una enfermedad cardiaca y sus sintomas se evidencia que existe una correlacion positiva en el cp(tipo de dolor en pecho),restecg(resultados de electrocardiograma),thalach(el ritmo cardiaco),slope(the slope of the peak exercise ST segment), ",54be281a,0.9629629629629629
44307,4883314a96dc34,d66e8455,# Save final model,50d36836,0.9629629629629629
44310,c1984e64b35234,ae170507,"For further reference about Apo-e4 genotypes: 

APOE-ε4 carriers may have their risk of developing Alzheimer's disease modified by SNPs elsewhere in their genomes. For example:

rs2373115, a SNP in the GAB2 gene
Inheritance of the rs1799724(T) allele appears to synergistically increase the risk of Alzheimer's in ApoE-ε4 carriers and is associated with altered CSF Abeta42 levels [PMID 15895461]
A haplotype of 3 SNPs in the POLD1 gene; the combined presence of this POLD1 I-G-T haplotype and the ApoE-ε4 allele almost doubles the risk of AD (odds ratio: 10.09, CI: 3.88-26.25, =<0.0001) compared to ApoE-ε4 carriers alone.[PMID 17498878]",1811225b,0.9629629629629629
44311,1883198d6d8c3c,388520aa,It also shows us the whole data frame has 205 rows and 26 columns in total,69a1d458,0.9629629629629629
44312,fdc3afd309b850,fbd6ca59,"<a id=""mr""></a>
## 10.4 Metrics Results",966bde38,0.9629629629629629
44318,fce6f1b02867e3,fda6fd6a,## Replace the values,3fb572c2,0.9629629629629629
44320,4392956f62c040,f8026c3e,"## WHAT NEXT  

1. This was not such a conclusive experiment. There was no huge improvements/difference in the outputs
2. Perhaps, if we tune hyper parameters differently for the two languages, there might be some impact.",c3ed519d,0.9629629629629629
44326,55ce731a138ca7,0f77d292,"https://github.com/lukemelas/EfficientNet-PyTorch <br/>
https://github.com/albumentations-team/albumentations <br/>
https://www.kaggle.com/khyeh0719/pytorch-efficientnet-baseline-train-amp-aug <br/>
https://github.com/abhishekkrthakur/tez <br/>
https://www.kaggle.com/mlubbilgee/baseline",4996250b,0.9629629629629629
44328,f4b9042e693b6c,e3f8d9be,"#### Now, **WE ARE DONE!**

If you enjoyed this kernel, please give it an upvote. If you have any questions or suggestions, please leave a comment!

Feel free to check out my [EDA notebook](https://www.kaggle.com/tanlikesmath/seti-simple-eda-to-help-you-get-started) to learn more about the SETI E.T. Signal competition.

Also, check out my [related notebook](https://www.kaggle.com/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r) with more detailed information on PyTorch XLA/TPU training.",676cacc9,0.9629629629629629
44329,6d29650083cbde,6eddd78e,"**6. CONCLUSION**

Thank you for taking the time to read through my first random forest! I look forward to your thoughts, advice and suggestions.",e65fd993,0.9629629629629629
44331,9ad9a97e628bfa,d8e41660,Modeling,0a7e1136,0.9629629629629629
44332,1667a100fc8b42,446e644d,"That's all about PCA.
THANK YOU",6c8cd6b6,0.9629629629629629
44334,db5a369894fef6,c94a5a1e,"### Talk to me! 
- e-mail: eddie_toth@hotmail.com
- Add me on: https://www.linkedin.com/in/edward-toth/ 
- Join the community: https://www.meetup.com/Get-Singapore-Meetup-Group/",065aaf61,0.9629629629629629
44336,c968dbd8d49ae6,8cf1aa12,"As we can see, if we change the weights, we can get better results on the false negatives.
At this point, it would depend on the companys demands. Wether they want to avoid false negatives at the cost of expending more resources on false positives, or if they want a more balanced result at the cost of losing more clients...",dfb2684d,0.9629629629629629
44343,c65a65d4041018,a6ab10ec,In Russia people tend examine feature importance and use partial dependency plots more than in other countries. In India plotting decision boundaries in popular. And in USA sensitivity analysis is relatively popular.,824fb229,0.9632352941176471
44351,c80939c7c626cf,828ba143,"# Testing 
Testing our Model is the most importnt step because it tests the accuracy/prescision of the model",b9ac31e2,0.9635036496350365
44354,5a8c553e21c70f,a80f5082,"Averaging predicted probabilities decreases the variance.

Precision-recall curve of the ensemble is depicted below",9ebd9d8f,0.9636363636363636
44358,0e2a23fbe41ca9,82090155,No extra ```card_id``` in the new merchants file.,64e4762c,0.9637681159420289
44364,99f84fa59cb1da,cdbff1ba,"### Best luck for the competition
### Please show your love by a upvote 👍
### Peace ✌",41e95f63,0.9642857142857143
44366,87e94f864d74be,be269927,## World Cloud-TV Show Genre,294bfe9f,0.9642857142857143
44368,0dd3ac2d55efd7,45c05df7,**Do upvote if you find this helpful in any way. It will help me stay motivated to share many such notebooks and resources. Cheers!!**,e9aa2cc2,0.9642857142857143
44369,d1f92a87a0a1a5,c6a30608,"# Conclusion
### It is difficult to predict when a huge earthquake that occurs rarely will occur. The above model has predicted that there will be no huge earthquakes in the next 60 years. But, it is doubtful. Another approach is required.",2cd610b2,0.9642857142857143
44375,585c280865b46e,0802ddfe,# Visuzalization colored by histone genes (normalized expression sum),4d6056f1,0.9642857142857143
44377,1a285e4c830f3f,c93d9fb5,"- Versuchen Sie, die erhaltenen Resultate zu beschreiben. Was wurde hier mit den Parameterkurven bzw. dem GridSearch erreicht? Hat sich der Aufwand gelohnt? Welche Leistung würden Sie einem Abnehmer Ihres Prototypen versprechen?",360b50e9,0.9642857142857143
44381,8dd655515e7d18,be5176ed,### Word Count,895f41cf,0.9642857142857143
44382,f4514ec092a771,0230993c,"## Conclusion
This kernel is only a reference if you want to run the challenge by Kaldi. I do search keyword Kaldi on Kaggle but it seems not very popular, so I decided to make this as a tutorial. However please give me some advises if you see something wrong.  
The final result I got is 75% on Public LB and 77% on Private LB.",3739ab1e,0.9642857142857143
44394,b8849a04581d32,36beba1c,"### The XGBClassifier model shows the best result and achieves an accuracy of 71.97%.  
### In the future, it is planned to supplement the data from player tracking. The combination of data on the position of the ball and the players on the field after a punther strike allow receive is the best quality of the model and its application in real play on the field.",b8a568cd,0.9642857142857143
44397,b61ab8f81dc03d,7aefa756,It is an excellent score!,64d05394,0.9645390070921985
44398,a5a419dc7245b0,33b5a44f,**Loss-Train-Test**,4279726e,0.9646017699115044
44401,869a39a3d4dea2,eaf8dbbf,"Equalization is the process of improving contrast of an image, by streching its distribution. Assume an image having a peak at its center by applying the equalization would strech the distribution as well as improvide the contrast.Mostly used in enhancing the medical or satellite images",9020daf8,0.9647058823529412
44405,9e27af2600925c,135ce10d,"Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:
    - Play with the learning rate and the number of iterations
    - Try different initialization methods and compare the results
    - Test other preprocessings (center the data, or divide each row by its standard deviation)",9b556435,0.9649122807017544
44414,c2a9f2fb3e1594,8f9de364,"<a id=""ch90""></a>
## Change Log:
11/22/17 Please note, this kernel is currently in progress, but open to feedback. Thanks!  
11/23/17 Cleaned up published notebook and updated through step 3.  
11/25/17 Added enhancements to published notebook and started step 4.  
11/26/17 Skipped ahead to data model, since this is a published notebook. Accuracy with (very) simple data cleaning and logistic regression is **~82%**. Continue to up vote and I will continue to develop this notebook. Thanks!  
12/2/17 Updated section 4 with exploratory analysis and section 5 with more classifiers. Improved model to **~85%** accuracy.  
12/3/17 Update section 4 with improved graphical statistics.  
12/7/17 Updated section 5 with Data Science 101 Lesson.  
12/8/17 Reorganized section 3 & 4 with cleaner code.  
12/9/17 Updated section 5 with model optimization how-tos. Initial competition submission with Decision Tree; will update with better algorithm later.  
12/10/17 Updated section 3 & 4 with cleaner code and better datasets.  
12/11/17 Updated section 5 with better how-tos.  
12/12/17 Cleaned section 5 to prep for hyper-parameter tuning.  
12/13/17 Updated section 5 to focus on learning data modeling via decision tree.  
12/20/17 Updated section 4 - Thanks @Daniel M. for suggestion to split up visualization code. Started working on section 6 for ""super"" model.  
12/23/17 Edited section 1-5 for clarity and more concise code.  
12/24/17 Updated section 5 with random_state and score for more consistent results.  
12/31/17 Completed data science framework iteration 1 and added section 7 with conclusion.  ",53411c04,0.9649122807017544
44416,6cade0b6a41ba2,ad6b9790,## 5.3. Model Parameters,e6110293,0.9649122807017544
44417,3fb15e6e48aec2,e4a27280,# Submission,9d1f4358,0.9649122807017544
44419,c09fac3c943d51,f6f43a6b,# Ensembling dragons,678d076d,0.9651162790697675
44420,d96642860ab3dd,68bae819,### make sample_submission file,98419d48,0.9651162790697675
44422,eb800c50fcfbb2,b1e672db,"# References
- https://www.paperswithcode.com/paper/siamese-neural-networks-for-one-shot-image
- https://keras.io/examples/vision/siamese_network/
- https://sorenbouma.github.io/blog/oneshot/",e7173f4d,0.9655172413793104
44424,858da4bb312f67,dd13ef8a,"## Next Step
Generated images shown above are not so good at this point. So the next step is tuning models and hyper parameters to generate better images. Then train classifier with newly generated images (fake images) and compare its performance with the original classifier.

Thank you for reading!",9cca4391,0.9655172413793104
44429,fb5c6021d127ef,c09ae294,"# 9) Next steps

Given what you've learned, what improvements do you think you could make to your model? Share your ideas on the [Kaggle Learn Forums](https://www.kaggle.com/learn-forum)! (I'll pick a couple of my favorite ideas & send the folks who shared them a Kaggle t-shirt. :)",dd05cbd3,0.9655172413793104
44430,84127ade6fde87,de46c7ce,"We’ve covered a lot of ground in this chapter. We learned to load the most common types of data and shape them for consumption by a neural network. Of course, there are more data formats in the wild than we could hope to describe in a single volume. Some, like medical histories, are too complex to cover here. Others, like audio and video, were deemed less crucial for the path of this book. If you’re interested, however, we provide short examples of audio and video tensor creation in bonus Jupyter Notebooks provided on the book’s website (www.manning.com/books/deep-learning-with-pytorch) and in our code repository (https://github.com/deep-learning-with-pytorch/dlwpt-code/tree/master/p1ch4).",f55d05b6,0.9655172413793104
44432,45921c50ac56fa,c6acf8fd,"This is one of my first notebooks on Kaggle. I hope you gained some insight about how to implement LDA   
and increased your curiosity about Topic Modelling.

I have not included spell-check in the pre-processing step, but do check out my other notebook,  
where I have implemented spell-check functions to handle all types of spelling mistakes.  
[https://www.kaggle.com/amarananth/spellcheck-python](http://)    

If you want to learn more and the math behind LDA, one of my references for the textual information in the beginning.
[https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158](http://)

",465973eb,0.9655172413793104
44437,6a1d04e8153df3,8c7a98b6,"# CONCLUSION

- After analysing brielf we conclude that many women who are in the range of 45 to 60 year old are suffering from breast cancer.
- Women's who had breast cancer and the count of axillary nodes are less then 10 then the chances of survival are very less.
- The survival rate and death rate according to their age is same just all depends in how much is the count of axillary nodes.
- long survival is more from age range 47–60 and axillary nodes from 0–3.

**This Notebook is the beginner notebook made by me.I am new to this field.**
**IF YOU READ UNTIL THIS ,PLEASE GIVE UPVOTE IT WILL MOTIVATE ME**
- Analysing done !!


# THANK YOU,HAVE A GREAT DAY AHEAD !!!

",38572b05,0.9655172413793104
44439,9535bb04ae042c,c5f381f7,"## Create something new:
## Now, that you've script needed for deployment you can use any kaggle dataset to deploy your project.",165b6fae,0.9655172413793104
44444,bb8f5d7807718b,cdcc0499,![](https://parulpandeycom.files.wordpress.com/2020/08/punk1-1.png),181ec286,0.9655172413793104
44450,4b7039cb44a54c,74d28b10,"**If you find this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels. 😊**",24e806af,0.9655172413793104
44451,6b54e39f86bdb5,ec3fd1db,"## References

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, november 1998.",198084bc,0.9655172413793104
44454,cd10f3afd970b3,35b5c783,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",2db3c8e4,0.9655172413793104
44459,18a96bb5711ed9,7e5db414,"# End

That's all for now. I will add up more analysis and possibly create a NLP project with Refugee news artcles. 

## <center> Thank you! </center>

",e79768db,0.9655172413793104
44461,5ea840754577e3,6da7e1bd,"## Conclusions

1. The sex of the passenger has quite a good impact on the survivability of the passenger.
2. People on the higher end of the socio-economic status were given more priority
3. Young people has a lesser chance of surviving.
4. Passengers who paid a higher fare survived better.
5. People with 0 parent / children and siblings/spouses survived less.
6. The salutations matches with the age group where specific salutations survived.",9cf9b73f,0.9655172413793104
44468,fdc9f4863744b1,63d7681c,We split our data set into two sections; Train and Test data sets.,b4529365,0.9657534246575342
44471,49ee86d074de69,ba71f99c,### Save The Scale,71ccc6d3,0.9658119658119658
44472,0a918602a04693,c963fbae,# Train/ Test Execution Information,c1ef0e95,0.9659090909090909
44473,73d8e56bc709b1,5c0913ee,"Accuracy score is 0.927, which could be improved when we pick out some important features.",78ec3cce,0.9659090909090909
44474,d1ff7e10ee0102,d98ae103,"That's it! We reached the end of our exercise.

Throughout this kernel we put in practice many of the strategies proposed by [Hair et al. (2013)](https://amzn.to/2uC3j9p). We philosophied about the variables, we analysed 'SalePrice' alone and with the most correlated variables, we dealt with missing data and outliers, we tested some of the fundamental statistical assumptions and we even transformed categorial variables into dummy variables. That's a lot of work that Python helped us make easier.

But the quest is not over. Remember that our story stopped in the Facebook research. Now it's time to give a call to 'SalePrice' and invite her to dinner. Try to predict her behaviour. Do you think she's a girl that enjoys regularized linear regression approaches? Or do you think she prefers ensemble methods? Or maybe something else?

It's up to you to find out.",2cc71c3c,0.9659090909090909
44479,bb0905d33ae417,c9122a23,"# Future work

1. [DONE] Generate sample submission to ensure functional code. (0.6007)
2. [DONE] Implement skeleton model for baseline (0.9106).
3. [DONE] Prepare AUC metric (0.9233).
4. [DONE] Deploy reasoned data augmentation (0.9241).
5. [DONE] Deploy test-time augmentation (0.9364).
6. Test other architectures (which are found [here in the Pytorch docs](https://pytorch.org/docs/stable/torchvision/models.html))
    1. [DONE] ResNet-34 (0.9362)
    2. [DONE] DenseNet-169 (0.9370)
7. [DONE] Implement weight decay. (0.9368)
8. [DONE] Retrain model with higher resolution images. (0.9573)",25fd1965,0.9661016949152542
44480,1294fb4c86f993,bd870e75,"It is clear that in the majority of months the registeration did not exceed 100,000.  ",4471e513,0.9661016949152542
44487,a81661cc35d8d2,f5b38d49,<b><font size='4'>Next Steps</font></b>,3331f113,0.9661016949152542
44490,04ff2af52f147b,923e8320,"**Submission:**

Finally, we calculate and submit our final predictions.",d5f37be9,0.9662921348314607
44497,726833f92fb87a,dca2b298,# Feature Importance,7dc5e1b6,0.9664429530201343
44504,5b92c712910a11,4a159a76,"# Conclusion 
* More Update awill be done Soon
* This is the firdt step of text cleaning
* 2nd part will come soon 
* Till then keep liking this ",e1d17100,0.9666666666666667
44505,cf4d1c1ad1476c,acf29422,"## Upcoming
* More tuned Models 
* Creating Folds to train 
* More pre processing ",768c1a59,0.9666666666666667
44507,b547f0f38f7744,1a32bd98,"## Let's Inference

See [TorchVision Faster R-CNN Inference](https://www.kaggle.com/gocoding/torchvision-faster-r-cnn-inference)",b6ba66b3,0.9666666666666667
44512,d6cbd7160961dc,94f38c0f,"#### Graphs (Observed versus Benford's Law)

* The leftmost graph below shows the aggregated results for the first digit.

* The center graph shows the aggregated results for the second digit.

* The rightmost graph shows the aggregated results for the third digit.

Note thow fibonacci's sequence adheres perfectly to Benford's Law. This is a secret from nature that I can only admire, but never explain.",36d74664,0.9666666666666667
44516,f6488772605bb5,9e17a65a,# ***THE END***,068d4697,0.9666666666666667
44517,541d0fa0e26b80,7db93c94,"Group E students , children of Master's Degree parents and those who completed test preparation course are more likely to score 
[O or A] Grade in Examination.",a29e0f29,0.9666666666666667
44518,62487bcd70b199,eee850cc,"## <a id='9.'>9. Ensemble Performances </a>
## <a id='9.1'>9.1. Ensemble Performance Metrics</a>
",f6ae50af,0.9666666666666667
44519,712198370d5521,7c78c34f,"**Points to be noted:**

The following information can be deduced about the customers in different clusters.

<img src=""https://github.com/KarnikaKapoor/Files/blob/main/Colorful%20Handwritten%20About%20Me%20Blank%20Education%20Presentation%20(3).png?raw=true"">
  ",5882e04c,0.9666666666666667
44520,396bc36edb95d3,7aa7444d,#### ROC Curve for the 3 models on the Training data,965e4f8f,0.9666666666666667
44521,3597174a998d4d,7ed990cb,"It can be found that models with new variable have a greater performance than the above ones. 

**For RandomForest model, it increase accuracy from 0.84 to 0.89.**

Good!",276892ed,0.9666666666666667
44522,864302b10e7730,f010293b,# THANK YOU !,e9dd1d2d,0.9666666666666667
44524,bc058fe14d3d1b,c8dbc081,### 2.3 Gradient Boosting Regression Tree,d0273670,0.9666666666666667
44527,be616f0785c32d,1d6c4e6a,"
[Go Top](#top)


## Acknowledgement

I thank Kaggle organizing team for trouble shooting, my research assistant Jiantao Guo for sharing the feature extraction code and my husband Wei Dong for useful comments. 

This work is supported by NIH/NIGMS R35-GM133346 'Machine Learning for Drug Response Prediction'. 
",b78e18aa,0.9666666666666667
44529,9276fa5cc2fef6,33fa1fbd,"## Feedback And More...
* Highly appreciate your feedback. 
* Do share your feature engineering ideas and I shall see them implement in upcoming versions of this kernel


### Todo 
* Show tutorial to use meta-features in XGBoost And LSTM Models
* More Features And Engineering...",24aa6a52,0.9666666666666667
44531,e9b9663777db82,577a4f76,# Model Evaluation & Conclusion,648e8507,0.9669421487603306
44532,2f47abddfd1928,6d1b1095,As last step let's compare with the correct values to see the accuracy of the model.'PassengerId'df_result_check,ae33cc0b,0.9669421487603306
44538,918040fad252ec,bc38b21f,Memberikan akurasi dan loss ke data model alias,966fcd8f,0.9672131147540983
44540,917957c6c4065f,97865eca,특별한 패턴을 보이는 항목은 없어보입니다.,55b8ed68,0.9673202614379085
44541,b01ee6cb674fa3,f84779d4,"US launches, USSR launches (1957 - 1991) and the comparison between US and USSR/Russia.

The comparison can be done in two stances, USSR and Russia

## US - USSR
The starking contrast is seen that USSR had way more launches than US. It started small and increased with the years, kept a value near 75 and by 79, it dropped to less than 50, and than USSR collapsed (stangelly there was a peak of launches by the last  year.
US seems to have had a big start, then it dropped and raised again by mid 60's
- how many launches each country had by the time?

It is seen that US had initially more launches than USSR, 
asdfasd
asd

",a8ffd35e,0.967391304347826
44544,45568f3ca94aca,81915fbf,"Overall, the model was able to accurately predict a malignant cancer 100% of the time when the cancer was actually malignant (by looking at recall score using malignant cancer as a positive label). Since a malignant cancer is more dangerous than a benign cancer, this metric is more important. Being able to reduce the number of false negatives is more important in this setting in order to better identify which patients actually have a malignant cancer.",2c6ea8e4,0.967741935483871
44546,16862cb02d73d5,76e6b071,"Next I will write more on **forecasting time series and identifying anomalies**,methods of doing it along with their pros and cons. 

**Thank you.Your suggestions and comments are welcome.**

**Please do share the kernal if you find it useful and interesting.**",d7ffa1a6,0.967741935483871
44550,535da6591a3246,0c91bc50,"# Thanks for checking my notebook out, I'm still a beginner so any comment or suggestion is welcome, thanks!",9ef8d0c7,0.967741935483871
44560,56e58d53ac9c57,f2bca66a,"just by looking at the wordcloud and rating distributions, it is easy to see that almost all ratings and reviews are quite positive. In wordcloud, among most used words, it is even hard to find a negative word.",90e2ab8e,0.967741935483871
44561,098fedfcd07456,27f758bd,"# Thanks for completing ,If you like please UpVote.
1. I feel i am still early learner all your comments are wellcome.
1. Please click on name to view my profile
* Notebook created by <a href = ""https://www.linkedin.com/in/narendrasharma/"">Narendra Sharma</a>
<hr>
",052ece26,0.967741935483871
44562,0cb456a5456cf9,1d5edad7,"# **Hooray!!!!! The accuracy increase from 0.71 to 0.86, the random forest model is much better than logistic regression one.**<br>模型的精确度从0.71增加到0.86，表现有明显的提升，可见针对这个数据，随机森林模型比logistic 回归更加适用",5701729c,0.967741935483871
44567,9ceb7278784462,f73aa711, ## <a id='25'> 21.Model Comparison </a>,3768a567,0.967741935483871
44570,f6648e47713411,4acc1a4f,### 5.3 Make submission,f4af4d1c,0.9680851063829787
44571,4c47839b067546,64b7c072,MAPE 16.72%.Результат на kaggle 14.28970.,1f517b02,0.9680851063829787
44582,2ada0305b68956,f16572ce,### 166. Palette = 'viridis_r',133e26f4,0.9685714285714285
44585,3cc097a5859dc1,e7e21f8d,# **Applying DecisionTreeClassifier Model**,14380d73,0.96875
44586,117fc0956643d0,aab7c444,"## Conclusion

We have tackled the COVID19 challenge competition by utilizing LDA for extracting the COVID19-related articles and applying a fined-tuned ALBERT model to extract answers from the top 10 articles with the highest confidence score. The visualization of our result above shows that each question gets satisfactory results with the corresponding reliable confidence score, mostly higher than 6. Since there is no additional tuning required for each question, our approach can be extended to answer other types of questions in the biomedical domain. 

Our work is a continuation of the work in https://www.kaggle.com/mahparsa/sbert-bert-for-cord-19-data ",68cef9fd,0.96875
44588,96c4c0e36b8ec0,a90b2bf1,"What the graphs tells us:
* Fare goes against the trend between class and survival and shows no real trend at all relative to survival
* There is very little difference between the fare of second and third class and a larger proportion of first class

",4dd6de8c,0.96875
44595,c85c94076e9c3a,d77fd240,### 3-Customer_Age_When_Enrolled,3ea0c443,0.96875
44597,5e02999ca74e7e,4b79c6b7,"## **Here's my another notebook that i made:**

**Data Analysis and Visualization:**

- [Amazon Top Selling Book](https://www.kaggle.com/knightbearr/amazon-book-eda-top-5-knightbearr)
- [Apple Stock Price Analysis](https://www.kaggle.com/knightbearr/apple-stock-price-analysis-knightbearr)
- [World Covid Vaccination](https://www.kaggle.com/knightbearr/data-visualization-world-vaccination-knightbearr)
- [Netflix Time Series Visualization](https://www.kaggle.com/knightbearr/netflix-visualization-time-series-knightbearr)
- [Taiwan Weight Stock Analysist](https://www.kaggle.com/knightbearr/taiwan-weight-stock-index-analysis-knightbearr)

**Regression and Classification:**

- [Pizza Price Prediction](https://www.kaggle.com/knightbearr/pizza-price-prediction-xgb-knightbearr)
- [S&P 500 Companies](https://www.kaggle.com/knightbearr/pricesales-eda-rfr-knightbearr)
- [Credit Card Fraud Detection](https://www.kaggle.com/knightbearr/credit-card-fraud-detection-knightbearr)
- [Car Price V3](https://www.kaggle.com/knightbearr/car-price-v3-xgbregressor-knightbearr)
- [House Price Iran](https://www.kaggle.com/knightbearr/house-price-iran-knightbearr)
- [Loan Prediction](https://www.kaggle.com/knightbearr/loan-prediction-eda-knightbearr)

**Deep Learning:**

- [Rock Paper Scissors](https://www.kaggle.com/knightbearr/rock-paper-scissors-knightbearr)

**Some Python Code:**

- [Python Cheat Sheet](https://www.kaggle.com/knightbearr/python-cheat-sheet-knightbearr)
- [22 Python Progam](https://www.kaggle.com/knightbearr/22-simple-python-program-knightbearr)",b69da28e,0.96875
44598,49f2274c1dd516,a0f99ac1,"# Our World in Data
Our World In Data aims to aggregate existing research, bring together the relevant data and allow their readers to make sense of the published data and early research on the coronavirus outbreak. They have provided data on coronavirus cases and testing.",06b0ffee,0.96875
44602,0932046e1f485d,27ccaafc,"The last thing we are going to do is check if there is any substantial difference in the length of the positive, negative and neutral reviews.",218cc7a3,0.96875
44606,225b4fe5d3894a,393d5ab2,### Thats the final Test Score,4b4197b3,0.9690721649484536
44607,063a35f644e3c5,be0d15bd,R2 score,1c30fb0a,0.9690721649484536
44609,03048e86a6d806,b442fa6a,"Kaggle, YouTube and Blogs are the most preferred media sources for data science topic for any job title, while Journal Publications are also often referred by Research Scientist.",1285c231,0.9692307692307692
44610,c115e287523aab,fe0ff487,# Remove Files,feb1288b,0.9692307692307692
44616,3cb96bd8eb364b,0e84959e,## References:,3157af7e,0.9692307692307692
44618,f2f2db16a2f86c,4706dc81,### **Saving model to disk**,ffc6a115,0.9692307692307692
44621,ee23a565163388,d37d8ffa,# **Further Steps**,88aacbc4,0.9694656488549618
44624,2f964d08c25d93,cfad42f6,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",1f2e4468,0.9696969696969697
44627,b42180a6a5b42f,05928546,"# Referências <a id='referencias'></a>

[Singapore University of Technology] (https://www.flasog.org/static/COVID-19/COVID19PredictionPaper20200426.pdf).

[Ministério da Saúde] (https://data.brasil.io/dataset/covid19/_meta/list.html), (https://covid.saude.gov.br/)
",987cea5f,0.9696969696969697
44628,b241b847319d13,732a8b31,"https://www.kaggle.com/mistag/data-create-tfrecords-of-vinbigdata-chest-x-rays

Thankyou so much sir.",0fb698f0,0.9696969696969697
44632,f166950fa915f8,98531999,### Save model,a7f6ca5e,0.9696969696969697
44635,ba4b3bd184acbb,4729d2f2,We can write the completed analysis back to the HDF file.,0f5de724,0.9699248120300752
44637,4cd25e50c7e007,f92e30f1,"# EQUATION OF BEST FIT LINE
count = 0.3038 + 0.2327 *year + 0.3937 *temperature +(-0.1525) *windspeed +(-0.1460) *spring +(-0.0727) *july +0.0531 *sept +(-0.2748) *Light Snow + (-0.0804) *Mist

Overall we have a decent model, but we also acknowledge that we could do better.",ceb0c525,0.97
44638,83df814455f06c,1e6306cd,"# **18. References** <a class=""anchor"" id=""18""></a>

[Table of Contents](#0.1)


The work done in this project is inspired from following books and websites:-

1. Hands on Machine Learning with Scikit-Learn and Tensorflow by Aurélién Géron

2. Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido

3. https://en.wikipedia.org/wiki/Decision_tree

4. https://en.wikipedia.org/wiki/Information_gain_in_decision_trees

5. https://en.wikipedia.org/wiki/Entropy_(information_theory)

6. https://www.datacamp.com/community/tutorials/decision-tree-classification-python

7. https://stackabuse.com/decision-trees-in-python-with-scikit-learn/

8. https://acadgild.com/blog/decision-tree-python-code
",c9cff71a,0.97
44642,20b372b6e4e276,f37661c9,There are a number of clear clusters that allow us to hope that we can improve the solution.,ec8b0860,0.9701492537313433
44643,21413205980558,76630f4a,"# It can be clearly seen that the accuracy rate of the random forest prediction model is 81.59%, the accuracy rate is 79.86%, and the recall rate is 81.46%. All indicators are far better than the simple logistic regression prediction model, so we choose the random forest model in the banking business forecast.
# 可以很明显的看出，随机森林预测模型的构建的准确率为81.59%，精准率为79.86%，召回率为81.46%，各项指标都远好于简单的逻辑回归预测模型，因此在银行业务预测中选择随机森林模型。",84197de0,0.9701492537313433
44646,156bbcff05dcea,e7b51d4e,"Reference:-
* https://stackoverflow.com/questions/64896418/how-can-i-plot-a-seaborn-percentage-bar-graph-using-a-dictionary
* https://stackoverflow.com/questions/58404845/confusion-matrix-to-get-precsion-recall-f1score
* https://stackoverflow.com/questions/39383557/show-distinct-column-values-in-pyspark-dataframe
* https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.MulticlassMetrics.html
* https://github.com/elsyifa/Classification-Pyspark/blob/master/Classification_Using%20_Pyspark.py
* https://spark.apache.org/docs/latest//api/python/reference/api/pyspark.ml.functions.vector_to_array.html
* https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/",66ad1fe9,0.9705882352941176
44647,8f50c9c16db95f,3cc2c881,"# Reference <a id=""reference""></a>

1. [The definition of special teams](https://en.wikipedia.org/wiki/American_football_positions#Special_teams)
2. [The definition of kickoff play?](https://operations.nfl.com/learn-the-game/nfl-basics/terms-glossary/)
3. [Another tweak to the kickoff rule promotes more touchbacks](https://profootballtalk.nbcsports.com/2018/07/06/another-tweak-to-the-kickoff-rule-promotes-more-touchbacks/)
4. [The formula of kinetic energy](https://en.wikipedia.org/wiki/Kinetic_energy)
5. [Biomechanical considerations of distance kicking in Australian Rules football](https://acephysed.files.wordpress.com/2015/01/atricle-1-reference-list.pdf)
6. [5 Scientific Facts About Left-Footedness](https://www.psychologytoday.com/us/blog/the-asymmetric-brain/202002/5-scientific-facts-about-left-footedness)
7. [The biomechanics of kicking in soccer: A review](https://ftvs.cuni.cz/FTVS-2332-version1-the_biomechanics_of_kicking_in_soccer_a_review.pdf)",26cc763a,0.9705882352941176
44648,c65a65d4041018,7c6fab94,### tools for reproducability,824fb229,0.9705882352941176
44653,7f74a04ae75792,48423442,### Summarize your findings - what have you learned from your insights?,d01e91da,0.9705882352941176
44656,395ed8e0b4fd17,4e611427,"# <center>If you find this notebook useful, support with an upvote👍</center>",7573ea31,0.9705882352941176
44658,e4c6dd957eb5ce,2e429234,"WTF! Kevin have a lot of Kernels... But we have a problem here. When I go to her profile, I can see Kernels(412) where are the other kernels of him? Maybe it's hidden... Someone knows about that?
<br>
We can note that many beasts of DS have more than 100 kernels; 
- ",2e383665,0.9705882352941176
44659,21bce4ec54b3fa,06380420,"# Conclusion

As you can see using permutation importances didn't seem to prove improve accuracy. It reduced the number of columns down to 21, which seems a little low. It could be due to correlated inputs, for more info see here: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py.
Anyway, it was interesting to implement new Scikit Learn methods, although for practicality I would use in-built importances for dimensionality reduction and then use permutation importances or LIME / SHAP for model interpretation. Additionally, it would be worthwile to play around with automated multicollinearity reduction techniques in the future.",35546e30,0.9705882352941176
44660,9cec5ddf8b6f49,2f99da25,# 6.. Prepare final predictions and Submit,d39fc8e7,0.9705882352941176
44661,3d905ce4828057,cbf3d3f4,# Submitting records to database,5b006cc3,0.9705882352941176
44663,02b7e38902069e,81ed606e,"Sources:
    
 Stanza Package: A Python NLP Package for Many Human Languages
 https://stanfordnlp.github.io/stanza/sentiment.html   
 
 
 Text Processing for Indian Languages using Python
 https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/",726a03a0,0.9705882352941176
44666,7cfd96218dd933,6093de92,"#### **ATTENTION**

* TURKEY'S VALUES ARE HIGH IN TERRA DATA.",7c34d96c,0.9705882352941176
44669,1d1598b6fa2aa7,fe454fa9,"### Demo assignment and useful resources

- **[Plotly Python Open Source Graphing Library Basic Charts](https://plotly.com/python/basic-charts/)**.

- **[Fundamentals Plotly Examples](https://plotly.com/python/#fundamentals)**

- **[Plotly Examples for AI and ML](https://plotly.com/python/#ai_ml)**

- **[Dash is the most downloaded, trusted framework for building machine learning web apps in Python.](https://plotly.com/building-machine-learning-web-apps-in-python/)**

- **[Plotly tutorial for beginners](https://www.kaggle.com/kanncaa1/plotly-tutorial-for-beginners)**

- **[Earthquake Animation with Plotly](https://www.kaggle.com/kanncaa1/earthquake-animation-with-plotly)**


",e066accf,0.9705882352941176
44671,30fdc4a6e3c1db,24425042,Let's look at the changes in prices over the weeks for each Category and Price Bucket level,6111ddee,0.9707602339181286
44673,3c2033cc99c12c,ad5afa9a,"**Findings:** *From the experiment above, we can find that there seems no difference between the accuracy of deep learning method and traditional statistical learning method, I think that the reasons are mmainly due to the amount of data. Although there are more than 200000 rows of data, there only exist about 400 fraud credit card record, so if I put the whole dataset into the neural network, it will set all the parameters 0 so that it can achieve an accuracy of 99.8%. As such, I also implement the method of undersampling in the dataset, the average testing accuray in the test dataset is between 88% - 94%, which is similar to the statistical method. If possible, I would also use the method of Data Augmentation to manually combat the limit of data.*",dfa22a54,0.9708029197080292
44677,98a6794067932a,26f72b0e,"# 3. Analyse des résultats obtenus

La troisième partie de ce rapport permettra d'explorer les résultats obtenus avec l'aide des analyses effectuées lors de la partie précédente de ce rapport. Nous allons donc expliquer les résultats obtenus afin de démontrer le type de décisions qui pourraient découler de nos analyses. Ces explications seront donc présentées sous forme de recommandations stratégiques aux dirigeants de l'entreprise afin de démontrer l'utilité de notre outil d'analyse descriptif. Nos recommandations seront divisées en cinq thèmes principaux soient l'analyse des commandes clients, l'analyse des ventes par région, l'analyse des expéditions par région, l'analyse des régions desservies et finalement, l'analyse des clients perdus.

**3.1 Analyse des commandes clients**

Tout d'abord, de manière globale, il est possible de constater qu'un peu moins de 10 000 commandes ont été effectuées auprès de l'entreprise en quatre ans. La moyenne du montant de ces commandes est de 230,12 dollars alors que le plus petit montant pour une commande a été de 0,44 dollar et le plus grand montant a été de 22 638,48 dollars. Plus précisément, pour ce qui est des trois segments de consommateurs, il est possible de constater que les particuliers ont des commandes moyennes de 225,02 dollars avec un montant maximal de 13 999,96 dollars et un montant minimal de 0,44 dollar, les entreprises ont des commandes moyennes de 231,42 dollars avec un montant maximal de 17 499,95 dollars et un montant minimal de 0,56 dollar et les bureaux à la maison ont des commandes moyennes de 242,80 dollars avec un montant maximal de 22 638,48 dollars et un montant minimal de 0,99 dollar. 

Ensuite, pour les expéditions, il est possible de constater que 52% des expéditions de l'entreprise sont pour des particuliers, 30% pour des entreprises et 18% pour des bureaux à la maison. 

Pour les différents types de transports, il est possible de constater que la répartition des expéditions est pratiquement la même pour les trois segments de clients. En effet, peu importe le segment de client, le type de livraison standard est utilisé à approximativement 60%, le type deuxième classe à 20%, le type première classe à 15% et finalement le type journée même à 5%. 

Au niveau de la popularité des sous-catégories de produits en fonction du segment de client, il est possible de constater que la répartition est très semblable dans les trois cas. Effectivement, pour les trois segments de clients, les produits les plus populaires sont les cartables, le papier et les fournitures alors que les deux moins populaires sont les machines et les copieurs.

Sur le plan stratégique, les résultats obtenus en ce qui concerne les commandes clients nous permettent d'effectuer plusieurs recommandations. Premièrement, il est possible de constater qu'actuellement plus de la majorité des expéditions effectuées par l'entreprise sont pour des particuliers. Il est donc évident qu'il pourrait être intéressant pour les dirigeants de l'entreprise de favoriser leurs opérations en fonction de ce segment client, surtout en prenant en compte l'explosion du commerce électronique au cours des dernières années. De plus, il est possible de constater que le montant moyen des commandes est pratiquement le même pour les trois segments de clients, donc les revenus de l'entreprise ne seraient pas réellement affectés négativement si celle-ci décidait de se concentrer majoritairement sur le segment des particuliers. Deuxièmement, pour ce qui est des types de livraison préconisés par les différents segments de clients, il est possible de constater que leur comportement ne diffère pas d'un segment de client à l'autre contrairement à ce que nous aurions pu croire. Il est également possible de constater que les clients de l'entreprise ne semblent pas réellement voir d'avantage à opter pour la livraison de type journée même. Il pourrait donc être intéressant de réviser le prix de ce type de livraison à la baisse afin que les clients soient plus portés à le choisir. Surtout si les dirigeants optent afin de favoriser le commerce en ligne pour les particuliers, ce type de livraison offert à un prix adéquat pourrait devenir un très grand avantage compétitif. Finalement, notre troisième recommandation serait axée sur les sous-catégories de produits. Encore une fois, si l'entreprise décide de se concentrer davantage sur le commerce en ligne, il pourrait être pertinent de laisser tomber la vente de produits comme les copieurs et les machines. Effectivement, il s'agit de produits volumineux qui pourraient générer des coûts très élevés en transport et nos analyses ont démontré qu'il s'agissait de produits très peu populaires auprès des différents segments de clients. L'entreprise pourrait donc plutôt se concentrer sur des produits à plus faible volume tels que les cartables, le papier et les fournitures qui représentaient tous des produits très populaires auprès des trois segments de clients. 


**3.2 Analyse des ventes selon les territoires**

Tout d'abord, grâce à la carte interactive représentant les ventes totales en dollars pour les différents états, il est possible de constater que les cinq principaux états où l'entreprise a effectué les plus grands totaux en ventes sont la Californie avec 446k dollars,New York avec 306k dollars, le Texas avec 169k dollars, Washington avec 135k dollars et finalement la Pennsylvanie avec 116k dollars. 

Sur le plan stratégique, les résultats obtenus en ce qui concerne l'analyse des ventes en dollars selon les territoires nous permettent d'effectuer une recommandation principale aux dirigeants de l'entreprise. Bien évidemment, cette recommandation serait de favoriser un service client de la plus grande qualité possible auprès des clients étant situés dans les cinq états principaux soient la Californie, New York, le Texas, Washington et la Pennsylvanie. Il pourrait donc être intéressant pour les dirigeants de l'entreprise de favoriser leurs efforts marketing dans ces régions étant donné qu'il s'agit de leurs marchés principaux. Pour ce qui est de la décision concernant l'emplacement des centres de distribution de l'entreprise, il est évident que ceux-ci devront être situés à proximité des ces cinq états afin d'offrir un service rapide aux clients s'y trouvant, toutefois c'est plutôt l'aspect du nombre d'expéditions par état qui sera favorisé afin de prendre une décision par rapport à ce sujet étant donné que les ventes en dollars peuvent être influencées par des produits ayant un prix de vente élevé. En effet, d'un point de vue logistique, il est plus pertinent d'évaluer l'emplacement possible d'un centre de distribution en tentant de minimiser le nombre de déplacements et maximisant le nombre de clients auxquels l'entreprise a accès, ce qui permet de limiter les coûts en transport. 

**3.3 Analyse des expéditions selon les territoires**

Cette fois-ci, en observant plutôt le nombre de commandes expédiées vers les différents états, il est possible de constater que les cinq mêmes états composent toujours le top 5 des états principaux pour l'entreprise. L'ordre d'importance a toutefois changé. Le top trois reste inchangé avec la Californie en première place avec 1946 expéditions, New York en deuxième plage avec 1097 expéditions et le Texas en troisième place avec 973 expéditions. La Pennsylvanie est maintenant en quatrième position avec 582 expéditions et Washington occupe le cinquième rang avec 504 expéditions. En regardant plus attentivement la carte, il est possible de repérer que les expéditions de l'entreprise sont majoritairement concentrées au Nord-Est avec la région de l'état de New York, au Sud avec l'état du Texas et celui de la Floride et finalement à l'Ouest avec les états de la Californie et de Washington. 

Ensuite, la préférence des sous-catégories de produits a été évaluée en fonction des différentes régions desservies par l'entreprise soit le Centre, l'Est, l'Ouest et le Sud. De manière globale, il a été possible de constater que la répartition du pourcentage des expéditions entre les différentes sous-catégories de produits était plutôt semblable pour les quatre régions. En effet, les produits les plus populaires étaient toujours les classeurs, le papier, l'entreposage, les téléphones et les fournitures alors que les produits les moins populaires étaient les machines et les copieurs. 

Au niveau des villes les plus importantes en termes de nombre d'expéditions effectuées pour des clients se retrouvant dans ces villes, il est possible de constater que New York City occupe la première place avec 891 expéditions, suivi de Los Angeles avec 728 expéditions, de Philadelphie avec 532 expéditions, de San Francisco avec 500 expéditions et finalement de Seattle avec 426 expéditions. 

En s'attardant sur les trois états principalement desservis par l'entreprise soit la Californie, New York et le Texas, il est possible de ressortir trois villes principales pour chacun d'eux. Pour l'état de la Californie, les trois principales villes où l'entreprise effectue des expéditions sont Los Angeles avec 728, San Francisco avec 500 et San Diego avec 170. Pour l'état de New York, les trois principales villes sont New York City avec 891, Rochester avec 36 et Long Beach avec 32. Finalement, pour l'état du Texas, les trois principales villes sont Houston avec 374, Dallas avec 156 et San Antonio avec 59.

Sur le plan stratégique, les résultats obtenus en ce qui concerne les expéditions par territoire nous permettent d'effectuer plusieurs recommandations. Premièrement, il est possible de constater qu'il pourrait être intéressant pour l'entreprise de séparer ses expéditions selon trois centres de distribution distincts. Ce premier centre de distribution pourrait être situé dans la partie supérieure de l'état de la Californie afin de pouvoir couvrir les expéditions de la région ouest du pays. Ce centre de distribution pourrait donc être situé dans la ville de San Francisco qui représente une des cinq villes les plus importantes au niveau des expéditions. Le deuxième centre de distribution pourrait être situé dans la partie supérieure de l'état du Texas afin de pouvoir couvrir les expéditions de la région sud du pays. Ce centre de distribution pourrait donc être situé dans la ville de Dallas qui représente une des principales villes situées dans la partie supérieure de cet état. Malgré le fait qu'elle n'ait pas été sélectionnée parmi les cinq principales villes desservies par l'entreprise, cette ville semble être un emplacement stratégique afin de couvrir la région Sud du pays. Le troisième centre de distribution pourrait être situé à la frontière entre les états de New York et de la Pennsylvanie afin de pouvoir couvrir les expéditions de la région Nord-Est du pays. Ce centre de distribution pourrait donc être situé dans la ville de Scranton qui se trouve dans la partie supérieure à l'est de l'état de la Pennsylvanie. Bien que cette ville n'ait pas été sélectionnée parmi les cinq villes les plus populaires de l'entreprise, elle semble être un bon compromis entre les villes de New York City et Philadelphie, qui elles représentent deux de ces cinq villes. La deuxième recommandation que nous pourrions faire aux dirigeants de l'entreprise serait de ne pas opter pour des centres de distribution spécialisés. En effet, nos analyses ont permis de constater que la répartition des expéditions au niveau des sous-catégories de produits était pratiquement identique pour les quatre régions desservies par l'entreprise. Les trois centres de distribution pourraient donc être conçus et opérés tous de la même manière, ce qui faciliterait grandement les décisions et la gestion des dirigeants de l'entreprise.

**3.4 Analyse des territoires desservis**

Tout d'abord, en regardant de manière précise l'emplacement de chacun des clients ayant effectué une commande auprès de l'entreprise au cours des quatre années pour lesquelles nous avons des données, il est possible de constater que l'entreprise est en mesure de couvrir tout le territoire des États-Unis, bien que les commandes soient majoritairement concentrées dans les régions mentionnées dans la section précédente. Il est même possible de constater que l'entreprise a effectué quelques expéditions pour des clients au Canada, pour un client en Équateur et finalement un client en Angleterre. 

Sur le plan stratégique, les résultats obtenus en ce qui concerne les territoires desservis par l'entreprise nous permettent d'effectuer plusieurs recommandations. Pour notre première recommandation, il pourrait être intéressant que les dirigeants de l'entreprise évaluent s'il est réellement pertinent de couvrir tout le territoire des États-Unis comme c'est le cas actuellement. En effet, il est possible de constater qu'une grande portion de territoire se trouvant au centre du pays n'a pratiquement aucun client. Les commandes expédiées dans cette région doivent donc coûter énormément cher au niveau des coûts de transport. Si les dirigeants de l'entreprise veulent absolument couvrir tout le territoire américain, il pourrait donc être intéressant d'instaurer un coût minimal de commande pour ces régions où il y a très peu de clients afin de rentabiliser leurs expéditions. Autrement, ces régions ne devraient plus être desservies par l'entreprise d'un point de vue logistique étant donné que les profits tirés des ces clients doivent être minimes voir même inexistants. Notre deuxième recommandation serait axée au niveau des expéditions internationales effectuées par l'entreprise. Dans le même sens que les expéditions effectuées dans les régions où se trouvent peu de clients, l'entreprise devra évaluer de plus près l'option d'expédier des commandes à l'international. Comme l'entreprise possède très peu de clients à l'extérieur des États-Unis actuellement et que le transport international demande des qualifications supplémentaires, il serait préférable de concentrer les activités de l'entreprise à l'intérieur des États-Unis à moins d'augmenter de manière considérable leur base de clients internationaux et d'en faire une spécialisation.

**3.5 Analyse des clients perdus**

Tout d'abord, comme mentionné dans la méthodologie, cette analyse a été créée avec l'aide de paramètres modifiables afin que les dirigeants puissent évaluer plusieurs scénarios différents. Nous avons donc analysé le cas où les dirigeants voudraient évaluer la situation des clients n'ayant pas effectué une commande au cours de la dernière année. En observant la carte représentant les expéditions effectuées par le passé à ces clients n'ayant pas commandé auprès de l'entreprise dans la dernière année, il est possible de constater qu'ils sont distribués également un peu partout au travers des États-Unis. Toutefois, en portant attention à la ""Heat map"", il est possible de constater que ces clients ayant possiblement été insatisfaits de leur achat auprès de l'entreprise sont majoritairement situés au Nord-Est du pays. Par contre, historiquement plus de commandes sont effectuées dans cette région, ce qui pourrait également expliquer que la concentration des clients perdus de cette région soit plus élevée. Il ne semble donc pas y avoir d'explication précise étant reliée à l'emplacement pour les clients perdus étant donné que ceux-ci représentent la distribution normale des clients de l'entreprise.

Ensuite, avec l'aide de la liste des commandes effectuées par ces clients perdus au cours de la dernière année, il est possible de constater que 528 de ces expéditions avaient été réalisées par le type de transport standard, 183 avec le type de transport deuxième classe, 124 avec le type de transport première classe et 33 avec le type de transport journée même. 

Sur le plan stratégique, les résultats obtenus en ce qui concerne les clients perdus nous permettent d'effectuer plusieurs recommandations. Pour notre première recommandation, il est possible de constater qu'il ne semble pas y avoir de problèmes majeurs d'insatisfaction par rapport aux différentes régions desservies par l'entreprise. Les dirigeants devraient donc continuer la gestion des territoires de la manière qu'elle est effectuée actuellement. Par contre, notre deuxième recommandation serait au niveau des types de transport ayant été utilisés par les clients perdus. Nos analyses ont dégagé le fait que la grande majorité des clients perdus avaient opté pour le type de livraisons standard lors de leurs dernières expéditions. Il pourrait donc être pertinent d'évaluer si les attentes des clients envers ce type de livraison sont réalistes ou si ceux-ci s'attendaient à un meilleur service en choisissant ce type de livraison. Afin de remédier à cette situation et regagner la confiance de ces clients, l'entreprise pourrait diminuer le tarif du type de livraison standard ou offrir une promotion à ces clients en leur offrant la livraison de type deuxième classe au prix de la livraison de type standard lors de leur prochaine commande.
",08600fe2,0.970873786407767
44678,a1a31459abf078,f75ae710,"### References 
I got to learn about Rapids from this great notebook . Please take a look if you want to learn more about Rapids. - https://www.kaggle.com/andradaolteanu/answer-correctness-rapids-crazy-fast",66fc0f54,0.9710144927536232
44688,17a24d566ffa59,c85b9f4b,# LDA (Latent dirichlet allocation),89049e56,0.9710144927536232
44693,a566b5b7c374e7,50e3cbf7,### Dream Notes,b3dc5545,0.9712230215827338
44695,60da9bbfe39c4b,02a5da06,## If you like this work then please upvoted!!,b0dd8ad6,0.9714285714285714
44696,9c044fa3072552,fb624b1c,Have fun interracting with the heatmap!,1362842e,0.9714285714285714
44698,38b79494ac749e,ea0e6ca4,"As a general rule, it is recommended to scale input features before fitting a regularized model so that the features/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline  second step (`StandardScaler`) is responsible for. 

Why is scaling important? What are the underlying reasons?",39162a40,0.9714285714285714
44711,675b60eaf415a6,9349721a,* **Summary of the model gives us the list of all the layers in the network along with other useful details**,68c0b725,0.9714285714285714
44714,6b65d81a5743dd,9ae8a8c3,So XGBoost has the best Accuracy of 72.4%,4080a2d2,0.9714285714285714
44715,3cea0f929a2035,61dd5ef8,"Thanks a lot for stopping and reading it out. :)
 
If you have some comments to improve the kernel, you are always welcome.
 
If you liked it, and found it useful, **UPVOTEs** are highly appreciated;)
",04cfbade,0.9714285714285714
44716,f50dc95483c98f,405cd50c,"# If you really like the work then please Upvote
![Upvote](https://media.giphy.com/media/K2kRXR8j5Yykg/giphy.gif)

# And Do Comment for the suggestion, it will be highly obliged and consider for improvement",cd9e9621,0.9714285714285714
44717,fe6750354fb64f,bd548e0b,# Actual vs Prediction,271741f0,0.9714285714285714
44722,f4b603905215b7,1b7d4f68,"1. https://www.svds.com/learning-imbalanced-classes/
2. https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work
3. https://stackoverflow.com/questions/34831676/how-to-perform-undersampling-the-right-way-with-python-scikit-learn
4. https://www.xenonstack.com/wp-content/uploads/xenonstack-credit-card-fraud-detection.png",efe1d587,0.9714285714285714
44728,b61ab8f81dc03d,b4941bf5,"<a id=""submit_results""></a>
# Submit the results",64d05394,0.9716312056737588
44736,bddd799cdbbae8,fdf8ea08, # <a id='9'> 9. Conclusion</a>,b44e3c08,0.971830985915493
44742,1d73d04c3aaae8,1e23565f,"The point spread predictions are better than 50% at 52.4%. That is probably not enough to beat the line in Las Vegas, due to the house advantage of 10%. You might make some money in your office pool!",cd43d0aa,0.9722222222222222
44745,10b5af05d804ff,354898a8,Do not give thanks ... Although..,4a9b1705,0.9722222222222222
44750,cf39cde80e66b7,d124cee8,"[Back to Table of Contents](#top)

<a class=""anchor"" id=""theend""></a>
# Final",aed4bc9b,0.9722222222222222
44751,69ac33d79f5130,20099302,## Summary and Conclusion,9d760d2a,0.9722222222222222
44755,268a610bbc64b4,1ac32647,"We don't have any international flight booking data. So we can't see historiacl trend of those bookings across different metrics.

However, one general notion is that since international flights are expensive, users book their fights well in advance. So one way to make cohorts of users is by creating bands of 'months from booking' and higher bands would most likely be booking of international flights.

To identify users who are likely to book international flights is by seeing their past booking patterns. For ex:

1) How many times in past 1 or 2 years they have booked an international flight

It will help us to  make an informed decision how frequent are they in booking international flights. More frequency would translate to higher probability of our prediction.

2) Have they booked earlier international flights on vacation/festival period?

If yes, they are likely to book again in upcoming festivals.

3) Modeling the user behavior

We can model the user behavior through techniques like decision tree classifier or gradient boosting. It will help us to target a month in which a user is most likely to book an international flight. Out of 12 months, we can take top 2-3 most probable months in which a user is most likely to book.

Approach:
a) See the historical trends and create features to capture those. For ex: is booking month a festival/vacation month, months booked in advance, if offer period, macro scenarios like industry wide teep discounts on flights, if both way flight then duration of stay etc

b) It would be a classification problem where we have to calculate proabability of each customer booking a flight. Some great classification techniques are Decision tree classifier. Since the international flight data would be less we might need to do oversampling/undersampling to make our model robust against class imbalance. We can control parameters like height of the tree to adjust overfitting.

c) Once we get the probabilities, we will make bands of users based on probability outcome. For users in mid to higher bands to need to target them specifically.

Targetting those customers:

The customer identification is purely based on his past international bookings so once we get the users probable month of booking we can check the user's history whether he is more likely to book if there was an offer going on. If so, we can offer him discounts. It will increase the probability of his booking the flight.
Apart from direct flight discounts we can offer users to book through our app and get benefits from our partner brands.",8a16f301,0.9722222222222222
44756,1014e6be391084,a7e3bf04,Generating AUC ROC curve,46f9168f,0.9722222222222222
44757,593d1d3d1df05a,230b951d,"It is still a bit faulty, compared to my model",bc682ffe,0.9722222222222222
44758,ab6da5994949a3,d07dcc83,## Visualising the Random Forest Test set results,fae6b91d,0.9722222222222222
44760,b3e0b7e9ff6849,e7add81e,"Thank you for making it to the very end of this notebook!

Please feel free to leave a comment to improve it together. ",f6e4bb0d,0.9722222222222222
44762,18a864b56ac3b8,52afdf42,"# Final Thoughts:

The model didn't exactly hit my goal. Managed to get about a 96% accuracy, which isn't bad, but not ahead of similiar models.

I did not expect to get so small a boost in performance when I finally used the almost full sized images. This is my first attempt at progressive resizing so I am still experimenting.

There is also one major flaw with this model, because I recreated the dataset for each iteration (twice for the images shrunk down to 64 by 64, once for the images at 224 by 224, and a fourth time with the images at size 480 by 480), each time I recreated the dataset the validation and training data was reshuffled. This means that the model is being tested on data that was likely already labeled for it in a previous iteration. The way around this would be to create a training and validation folder and feed those to the databunch object (I ran a little tight on time with this one so I didn't get around to that).

This year I've challenged myself to complete one task on Kaggle per week, in order to develop a larger Data Science portfolio. If you found this notebook useful or interesting please give it an upvote. I'm always open to constructive feedback. If you have any questions, comments, concerns, or if you would like to collaborate on a future task of the week feel free to leave a comment here or message me directly.",f3ca0a7c,0.9722222222222222
44769,c349ee5a821411,628198ac,With this R^2 the model explains all the variability of the response data around its mean so its a pretty good way to make our predictions .,572b269d,0.9722222222222222
44772,91473a39b85068,a5aaf86e,Micro and Macro f1 values are 9 and 32 then we improved to 13 and 36 using one vs rest it's not giving impressive results but time and data permit i consider out 6 million data points I took only 300k points if we took more points we can improve performance metrics values.,6e3d91c2,0.9726027397260274
44782,a6c34cd514e30e,8a4a8338,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",bf603ddd,0.972972972972973
44784,b7b1057764fa02,fc8e16de,"This is a much more interesting confusion matrix, with a lot of information. Our model performed best with images of P and F. We also see that the model got none of the images for A correct, and that it mistakenly labeled a signficant number of Vs as Ks. Studying this confusion matrix gives us a detailed and interesting view into the workings of our model on the evaluation images.",5053a192,0.972972972972973
44788,63b44c85e32c1f,3427c216,**remove( )** function deletes the specified element from the set.,fb9b9562,0.972972972972973
44789,e4525eb0c96f28,a76102f7,"## Conclusion
The task of predicting video game sales is actually harder then what one might suspect. It is especially hard to do within a single tutorial. However, we still tried to do it by looking closely at how the game's ESRB rating, country of origin, genre, and year of production might play into how good the game might do in sales. 

Through our exploratory data analysis we discovered  that the best performing ESRB ratings over time were M and E10, Sports games grossed the highest sales in total, Japanese and American games do the best out of all games, and that the bestselling platforms of all time seem to be of older geneneration. We also saw a persistent pattern in the majority of our scatter plots, where certain games appear high above the rest of the observations in certain years. This became more appearent as we tried to use two different machine learning regressors to try and predict sales using interaction terms between the variables mentioned above. As expected, we ended up with residuals that were heavily skewed right and came nowhere close to accurate.

The insights we achieved from this tutorial sheds light on the possibility that there are perhaps other variables that play a role into how well a game sells. Therefore, more data needs to be collected and more experimentation needs to be done so we can get closer to finding our what exactly makes a game sell. There are still questions that have not been answered by this tutorial whose answers have yet to be discovered. Does the availibility of a game translate to more sales? If so, how can we measure availibility? Perhaps we could have looked into how the critic scores of each title correlates with the game's sales. We might also wonder what it is about a game that makes the critic scores go up if it happens to be strongly correlated with total sales. As you can see, we can already go on and on without end about the possible questions we might ask about how well a product does in the video game industry.",2093a1f1,0.972972972972973
44790,2dda7facf3c1e0,e2796fcf,"## Submitting to the *Spin the Model* Chartbusters challenge

To submit your composition(s) and model to the *Spin the model* chartbusters challenge you will first need to create a public repository on [GitHub](https://github.com/). Then download your notebook, checkpoint files, and compositions from SageMaker, and upload them to your public repository. Use the link from your public repository to make your submission to the Chartbusters challenge! ",45552d2b,0.972972972972973
44793,bdf23d2d396916,4384bbf7," ## Sonuç:

**- Random Forest eğitim sürecinde feature seçimini rastgele yapar ve herhangi bir feature 'a  büyük ölçüde bağlı değildir. Bu nedenle, Random Forest, verileri daha iyi genelleyebilir.**

**- Random Forest, datasetimizin orta veya büyük ölçekli olduğu durumlar için daha uygundur. Ancak birden fazla Decision Tree'yi birleştirdiğinden, yorumlanması daha zor hale gelir. Burada datasetimiz küçük ölçekli olduğu için SVM modelimiz daha iyi bir sonuç çıkarmıştır.**",b0e45a49,0.972972972972973
44801,67b7354e96113a,f7bfed7d,Thanks **Manav Sehgal** for your kernel,dca94250,0.9733333333333334
44803,cb570c7b7f0501,5165bd61,"No significant change :(

But anyway, the question worth the analysis",a200a0ec,0.9733333333333334
44806,4c47839b067546,79e15a46,"# 6. Выводы по моделям:

Лучший результат метрики MAPE в лидерборде показала модель RandomForestRegressor(14.09%) и Stacking ExtraTreeRegressor с XGBoostRegressor c мета моделью LinearRegression (16.72%). ExtraTreeRegressor и XGBoostRegressor показали менее успешные результаты по отдельности, 21.55% и 21.8% соответственно. Предположительно, высокий показатель метрики у RandomForestRegressor стал результатом переобучения модели. Поэтому в качестве модели для final submission был выбран стэкинг.",1f517b02,0.973404255319149
44808,ac1abfe1dfe815,6ca139b0,"As we you can see from the confusion matrix for predicted and actual values.  
Decision Tree classified all of the tweets as positive and negative only, Extra trees classified all of them as negative, so its accuracy is equal to the acuracy baseline, and we can see it clearly in the plot.

The best model is `SVC`, it gives the best score, precision, recall compared to the other models.
",6529dbcb,0.9734513274336283
44810,31b564f11ef638,5cad71e3,# I would appreciate it if you upvote my notebook. Thank you!,424f9692,0.9736842105263158
44816,caa0ce2715bf34,7254032d,**** End ****,78a5dc51,0.9736842105263158
44817,a1dcd92986bc84,73264841,**Credit:** All I did here is I took the tutorial from https://keras.io/examples/nlp/nl_image_search/ and then I made a bunch of small modifications in order to get it to run inside of a GPU-enabled Kaggle Notebook.  The natural language image search is a neat concept!  It was fun to explore.,730acaaa,0.9736842105263158
44820,d369f200a84c2a,47f3391d,"## *Referencies*

* [Cezanne Camacho blog](https://cezannec.github.io/Capsule_Networks/)
* [Kendrick](https://kndrck.co/posts/capsule_networks_explained/#fn:1)
* [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf)",8fef4d48,0.9736842105263158
44821,f35ee6e9fab592,d9d047d4,A very weak improvement trend is noticeble but as mentioned earlier; no far-reaching inference must be drawn,b15f7073,0.9736842105263158
44822,c950cff74e51ac,72f2d547,"From correlation above, there are no (positive/negative) high correlation between the columns.",d59bf323,0.9736842105263158
44826,71c3c1eab0377d,b98f06bd,**Create the Submission file for the Kaggle.**,52b4e360,0.9739130434782609
44830,241cf32abb22d8,35185362,"# Summary <a class=""anchor"" id=""5""></a> 

The Decision Tree model based on the top 10 features selected by F-Score is the best model for prediction in this analysis, under a limited hyperparameter tuning and feature selection approach. Although there is not enough statiscal evidence to show that this Descision Tree model performs better than the other KNN and Naive Bayes models, at a 5% level of significance, it yields the highest AUC score (0.916) and F1 score (0.9). In general, all models with only the top 10 features selected by F-Score perform significantly better than those with the full features, at a 5% level of significance.",47157066,0.974025974025974
44836,2ada0305b68956,c936aea4,### 167. Palette = 'vlag',133e26f4,0.9742857142857143
44838,4bbe953f82d29b,35a92ebc,"### Бонус для тех, кто дочитал до конца
Полезные ссылки

https://towardsdatascience.com/feature-engineering-on-time-series-data-transforming-signal-data-of-a-smartphone-accelerometer-for-72cbe34b8a60  
https://otus.ru/nest/post/1024/  
https://tsfresh.readthedocs.io/en/latest/text/forecasting.htmlhttps://tsfresh.readthedocs.io/en/latest/text/forecasting.html",772301f2,0.9743589743589743
44839,e424c111c44669,f314c3f6,"##Reference:

https://www.kaggle.com/gregoiredc/arrhythmia-on-ecg-classification-using-cnn

https://www.kaggle.com/coni57/model-from-arxiv-1805-00794",d9fccfba,0.9743589743589743
44846,80ad12f326ab70,c34cbf8a,Rural Areas are highly engaged than the others. it could be the a case since most schools are outside rural areas and this could be the better means to education.,da404a16,0.9743589743589743
44847,0a1fcda859252c,3702b2bc,"Nice!!! So, our model has a 98% recall. In such problems, a good recall value is expected. But if you notice, the precision is only 80%. This is one thing to notice. Precision and Recall follows a trade-off, and you need to find a point where your recall, as well as your precision, is more than good but both can't increase simultaneously. 

That's it folks!! I hope you enjoyed this kernel. Happy Kaggling!!",13a38774,0.9743589743589743
44848,4d91e84c564cbe,515fae0b,"# Your Turn

You learn best by writing code, not just reading it. So try **[the coding challenge](https://www.kaggle.com/kernels/fork/1275173)** now.",355a43e3,0.9743589743589743
44851,9b42412e75d640,05872cfc,"Looking at accuracy, roc-auc and precision/recall we can agree that the Logarithmic regression and Naive Bayes give the best results. Very close are KNN and SVC while decision trees and regression fortest performed worse in this case. ",b616570a,0.9743589743589743
44852,34fff8ce731b03,c2e506ee,"## Considerações Finais

Agora é com **você**! Ainda existe muito espaço para melhoria na acurácia do modelo que desenvolvemos até agora. Utilize o material complementar abaixo para modificar este notebook e construir um algoritmo melhor.

- [Curso de Aprendizado de Máquina de Stanford com Andrew Ng](https://www.coursera.org/learn/machine-learning)
- [Mãos à Obra: Aprendizado de Máquina com Scikit-Learn & TensorFlow](https://www.amazon.com.br/M%C3%A3os-Obra-Aprendizado-Scikit-Learn-TensorFlow/dp/8550803812)
- [Introduction to Machine Learning with Python](https://www.amazon.com.br/Introduction-Machine-Learning-Andreas-Mueller/dp/1449369413)
- [Data Science do Zero](https://www.amazon.com.br/Data-Science-zero-Joel-Grus/dp/857608998X)
- [Customer Churn Classification Using Predictive Machine Learning Models](https://towardsdatascience.com/customer-churn-classification-using-predictive-machine-learning-models-ab7ba165bf56)",6f9e5b2e,0.9743589743589743
44855,7705fc44251194,c4184c9b,"REFERENCES
>https://codelabs.developers.google.com/codelabs/keras-flowers-tpu/#0
>https://www.kaggle.com/mgornergoogle/five-flowers-with-keras-and-xception-on-tpu",bd2cebe8,0.9743589743589743
44857,9169c4e9c33c90,0545c1a6,"In general, books with the most reviews have lower prices, and are non fiction.",725bf880,0.9745762711864406
44862,c4386b8a01d66e,4404abaa,### SVM tuned,dc732bf5,0.9747899159663865
44863,fc8e0042411c46,7bf3e418,Making predictions on the test set,af476c2a,0.9749216300940439
44864,1011899b959f44,085f24a8,"# The End
The world of data has much to offer and countless possibilities in allowing us to predict, analyze, and visualize. Hope you enjoyed my tutorial and learned something new from it!",0b112382,0.975
44870,e3c0b55ed519e2,c94ffdd1,"# Hence, We could conclude these two areas could face major threat in future. Please upvote if you liked the visuals. Thank you. ",9f51352e,0.975
44871,5626e84c4e6bf8,654ac215,"**Go to the end of the comments to see the interpretation of the first map**

**Kindly upvote, comment, share and fork this kernel if you like my work. I am open to suggestions.**",e2ecb669,0.975
44872,6e28c4f557f736,fdd4a883,"To do: 
- Use crawled tweets outside of the dataset",021fdf75,0.975
44875,0d58c434c7db1e,23fd0680,"Gaprindashvili, Nona from Georgia is the oldest GM title holder.",517e01d3,0.975
44877,9f0ccf5b9e8f03,626b9f55,Kaggle Notebook Runner: Marília Prata  @mpwolke,66691203,0.975
44880,3dd4294f903768,c74631a5,***,0d89d098,0.975
44887,8d0aebab1e5914,171a6e7f,"----
----

# My Other Notebook:
* [Simple Linear Regression](https://www.kaggle.com/mukeshmanral/linear-regression-basic)
* [Multiple Linear Regression](https://www.kaggle.com/mukeshmanral/multiple-linear-regression-basic)
* [Polynomial Regression](https://www.kaggle.com/mukeshmanral/polynomial-regression-basic)
* [Advanced Linear Regression](https://www.kaggle.com/mukeshmanral/advance-linear-regression-basic-gridsearchcv-hpt)

----

* [Feature Engineering 1](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-1)
* [Feature Engineering 2](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-2)
* [Feature Engineering 3](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-3)
* [Feature Engineering 4](https://www.kaggle.com/mukeshmanral/feature-engineering-diff-dataset-4)

----

* [How KNN-Algorith(1) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-1-basic)
* [How KNN-Algorith(2) Works (Basic)](https://www.kaggle.com/mukeshmanral/k-nn-algorithm-2-basic)

____

* [Ensemble-Bagging-Random Forest-Extra Tree Basic](https://www.kaggle.com/mukeshmanral/bagging-ensemble-concept-rf-extree-basic) 

____
____",084e671f,0.975609756097561
44888,786475feda0190,0060fbf4,"### References:

1. [Beginner's Guide to Audio Data](https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data)

2. [Deep Learning for Audio Classification](https://youtu.be/Z7YM-HAz-IY)

3. [Practical Cryptography](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)

4. [source of cover image](https://www.freepik.com/premium-vector/sound-wave-display-equalizer-rainbow-color-illustration-about-dynamic-audio-from-electronic-equipment_5840116.htm)

5. [source of Mel filter bank image](https://www.researchgate.net/figure/Mel-filter-banks-basis-functions-using-20-Mel-filters-in-the-filter-bank_fig1_288632263)

6. [Python_specch_features Documentation](https://python-speech-features.readthedocs.io/en/latest/)

7. [librosa Github](https://github.com/librosa/librosa)",e4663d97,0.975609756097561
44893,74a03887600114,7b4bf3f6,"- It can be concluded that we need to recommend ""Angela (1995)"" movie to people who watched ""American President, The (1995)""
- It can be concluded that we need to recommend ""Gospa (1995)"" movie to people who watched ""Toy Story (1995)""
- It can be concluded that we need to recommend ""Jupiter's Wife (1994)"" movie to people who watched ""Taxi Driver (1976)""",c0ffb2f0,0.975609756097561
44894,e19e307b3fd188,3304fe77,"Although the model predicted higher values in the range of 2.000,00, it seems to do a good job of forecasting the other values.",2173955b,0.975609756097561
44895,fd4017c1514157,4add8a46,"### <font color = 'green'>**Refrences**</font>

* https://www.kaggle.com/andradaolteanu/birdcall-recognition-eda-and-audio-fe
* https://www.kaggle.com/stefankahl/birdclef2021-exploring-the-data",fd8f0896,0.975609756097561
44896,514d8de15cb7ef,e8668438,"<p> The output of this analysis is that when we are predicting quality the best out of all is SVM Linear Approach with more than 65% accuracy.  KNN with 7 Neighbors is closer to it and can be our second bet. And then we have th 3 Neighbours KNN with accuracy of 62%. But The others including SVM Gaussian and Linear Naive Bayes have lesser accuracy of around 55-58%.
    So the preferred approach will be SVM Linear Approach</p>
   
  
  <p>The output of the 2nd  analysis is that when we are predicting price the best out of all is Naive Bayes Multinomial with accuracy going to 40%. And then there is SVM Linear Approach around 39% accuracy and third choice among these will be KNN with  7 Neighbors. But others have the accuracy of around30-32%. But the problem here is the accuracy for the best approach is also 40% around which is way too less to be reliable enough and we can't support data on these methods. So there can be various other techniques also that can be applied to it for better results like Neural Networks. and its various Forms  But amongst these for price  prediction our best bet will be Naive Bayes Multinomial Apporach</p>",cfe111b2,0.975609756097561
44898,5169abdc647412,143702e7,"## questiuon:
### is root mean square error is good for this kind of desicion tree problem
",28efc68d,0.975609756097561
44899,0e09587faffa8f,c52cc9a9,"This brings us to the end of the notebook

In this Kaggle notebook, we read 5 Million rows from the NYC Parking dataset, performed data wrangling and later analysed to deliver insights

Since I'm a beginner, I would love to have your valuable feedback and suggestions so that I can keep on improving

Also, if you liked my work, please consider upvoting this notebook, would mean a lot to me!

Thank you😄",0d563d61,0.975609756097561
44900,8d70dcae7f40a3,ecdb8ad6,"### **<span style=""color:green""> When to Use ROC vs. Precision-Recall Curves? </span>**

#### + ROC curves nên dùng khi số lượng nhãn giữa các lớp gần như bằng nhau.
#### + Precision-Recall curves nên dùng khi số lượng nhãn giữa các lớp lệch từ trung bình đến lệch cao.",472c71ce,0.975609756097561
44902,47b2c9be5e31cb,c9606e4c,"## Conclusion
This concludes your starter analysis! To go forward from here, click the blue ""Edit Notebook"" button at the top of the kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!",7d4afe56,0.975609756097561
44906,0d59a3e0130db0,0f2083b2,"Well, looks good!
Thank You for watching :)",285f04b2,0.9761904761904762
44909,898d18d501f68d,48c84e2a,here cover type 5 and 7 can be found in every soil type,d8bdea2d,0.9761904761904762
44917,a76e0e8770b7a0,60127aa5,World is caotic!!!!!!,02863d3b,0.9761904761904762
44923,27778055896e17,75625679,"# Conclusion

Fraud detection is a complex issue that requires a substantial amount of planning before throwing machine learning algorithms at it. Nonetheless, it is also an application of data science and machine learning for the good, which makes sure that the customer’s money is safe and not easily tampered with.

As always, if you have any questions or found mistakes, please do not hesitate to reach out to me.",1dbe0165,0.9761904761904762
44925,a758983a68c014,685f2159,"### Summary

Skip-Gram model is a good Word2Vec mechanism which sometimes track semantic similarity and context of words. But why is that not the best solution in case of this competition. First and most obvious, too few data. That's simply not enough to train good Word2Vec model. Second, cross entropy error has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events. Third, evaluating the normalization factor of the softmax for each term is costly and training model even on our competition tweets' corpus takes much time. Fourth, Skip-Gram is sensitive to noise and poorly vectorize rare words from corpus. Nevertheless, understanding the mechanism of Skip-Gram is important for further diving to more complex Word2Vec models. Hope thin notebook is useful. If so, please upvote.",ab89f181,0.9761904761904762
44926,2d40f383473fa4,7fd59cd6,"# 6. Conclusion

The automation tools to speedup the procees worked well, making steps like EDA more smooth and allowing more time to make FE.
About the AutoML, was possible to achieve a good result with little steps, allowing more time to research other solutions.
Thanks for your reading!",1da1eff0,0.9761904761904762
44928,e93a41c03638fe,5e564066,# 5. Please upvote this notebook if you find it helpful.,7363527b,0.9764705882352941
44935,806ce45c8fa303,edf85d79,"## Conclusion

In this analysis, we have found that Support Vector Machine classifier is able to classify the data upto **93%** without encoding and decoding. However, the effect of autoencoder comes when the data gets transformed from non-linear to linearly separable then linear classifier like **Logistic Regression** could perform in a better way.

The accuracy score of Logistic Regression can go upto **97%**, this is something not happens too often in logistic algorithm. ",3e5c34dc,0.9767441860465116
44936,22bd95f4807a23,ed0bbd5a,"* The data can be further analysed by 
    * Creating a wordcloud of the review text for each product type.
    * Analysing the most important words derived from the review text for each product.
    * Segmentation of consumers using clustering algorithms
",c05d356f,0.9767441860465116
44940,743ae010f5e875,820227c5,"|   | CV | LB |
| --- | --- | --- |
| s57 adnl_govt_labels |   | 0.573 |
| 42 w/ adnl_govt_labels |   | 0.557 |
| adnl_govt_labels_26897 | 0.136 |   |
| adnl_govt_labels |   | 0.556 |
| adnl_govt_labels KEN |   |   |
| 42 w/ adnl_govt_labels KEN |   | 0.557 |
| **RKM adnl_govt_labels w/ 42** | **0.514** | **0.574** |
| RKM adnl_govt_labels BS_CLEANING w/ 42 |   | 0.568 |
| RKM adnl_govt_labels_26897 |   | 0.244 |
| RKM adnl_govt_labels w/ 42_48 |   | 0.574 |
| **RKM adnl_govt_labels w/ 42_36** |   | **0.574** |
| RKM adnl_govt_labels w/ 42_60 |   | 0.574 |
| RKM adnl_govt_labels w/ 42_24 |   | 0.573 |
| RKM adnl_govt_labels w/ 42_60 |   |   |",02c54445,0.9767441860465116
44943,1660daf8867980,12501c8f,"# References
1. Reinforcement Learning: An Introduction  
   Richard S. Sutton and Andrew G. Barto  
   1st Edition  
   MIT Press, march 1998
2. RL Course by David Silver: Lecture playlist  
   https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ",42d7cffc,0.9767441860465116
44946,fdc3afd309b850,28c197e4,"<a id=""sm""></a>
## 10.5 Saving the model",966bde38,0.9768518518518519
44947,09751c520b0616,7e688862,## 5.Final Submission,a4d0c7e9,0.9769230769230769
44949,d07915a6e6992e,eec228fd,"# Conclusion
![Conclusion.png](attachment:Conclusion.png)",2b912140,0.9769230769230769
44955,ee23a565163388,a393ed7a,"The baseline model was first built and then it was fine tuned. The more complex model may lead to overfitting whereas the least complex model may lead to underfitting. Hence it is very important to make sure that the model learnt the underlying patterns in the data. The model can be further improved by,

- Hyperparamater tuning 
    - By performing GridSearchCV or RandomStratifiedCV
- Averaging techniques
    - By using either hard or soft Voting Classifier
- Bagging techniques
    - Bagging Classifier
- Boosting techniques
    - AdaBoost Classifier
    - Gradient Boosting Classifier
    - Extreme Gradient Boosting",88aacbc4,0.9770992366412213
44957,81712ee7510ac5,43897aec,**Documantation String**,c4685e79,0.9771428571428571
44962,73d8e56bc709b1,ce73263c,Thanks for reading! Welcome any discussion (*^▽^*),78ec3cce,0.9772727272727273
44963,f269d2fbd5f1be,5c0fec8a,"**Commentary:**
* It seems that the average Guard player is 'smaller' in physical size as compared to other players playing in other position",1264c440,0.9772727272727273
44966,5083d7a61f2426,93b6528e,"The model predicts that the average temperature would be 48 ° C and the correct answer is 26 ° C. We need to improve the model by using more layers and testing other parameters


This kernel was a simple example to show only the basic steps, I hope this can help someone.",541a0fec,0.9772727272727273
44967,a0b321057e7402,d93c2a0f,"# **Conclusion**

SARIMAX is a great and interesting method to predict univariate time-series data. It is a great way to refresh your knowledge on time-series data and strengthen the number of prediction models you know (because there is no free lunch in ML). I had fun making it and I hope you had fun reading it.

Thank you for your time.",5f73fb91,0.9772727272727273
44968,5e1d001f8764e0,de9507d2," ##### ACCURACY SCORE AFTER ADDING DROPOUT LAYER= 91.05%


 ##### OUR ACCURACY DECREASED A LITTLE IN THIS CASE AFTER ADDING DROPOUT LAYER",62be464c,0.9772727272727273
44970,90964081c7faab,d4f4b8d4,"gb_parameters = {#'nthread':[3,4], #when use hyperthread, xgboost may become slower
               ""criterion"": [""friedman_mse"",  ""mae""],
              ""loss"":[""deviance"",""exponential""],
              ""max_features"":[""log2"",""sqrt""],
              'learning_rate': [0.01,0.05,0.1,1,0.5], #so called `eta` value
              'max_depth': [3,4,5],
              'min_samples_leaf': [4,5,6],

              'subsample': [0.6,0.7,0.8],
              'n_estimators': [5,10,15,20],#number of trees, change it to 1000 for better results
              'scoring':'roc_auc'
              }

gs_gb = GridSearchCV(rf,rf_parameters,scoring='roc_auc',cv=10)
gs_gb.fit(X_train,y_train)

print(gs_gb.best_score_)
print(gs_gb.best_estimator_)",b423b0c3,0.9772727272727273
44971,d1ff7e10ee0102,629630f1,"# <b>References</b>
* [My blog](http://pmarcelino.com)
* [My other kernels](https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python)
* [Hair et al., 2013, Multivariate Data Analysis, 7th Edition](https://amzn.to/2JuDmvo)",2cc71c3c,0.9772727272727273
44980,20b372b6e4e276,9ac359bf,I hope you find this kernel useful and enjoyable.,ec8b0860,0.9776119402985075
44982,49ac6594c8f5cf,f4e0fbe9,Confusion matrix for best model (Logistic Regression),6f19f28a,0.9777777777777777
44988,7341f069d9b2ee,5f89ad1c,"Fine-Tune Your Model

Grid Search
-----------
for few combinations

from sklearn.model_selection import GridSearchCV

Random Search
-----------
Used when hyperparameter search space is large
",e0a49e62,0.9777777777777777
44994,b0c2805cd5c087,2bb030be,Image hai.stanford.edu,0446f327,0.9777777777777777
44995,396bc36edb95d3,13e0f3e8,#### ROC Curve for the 3 models on the Testing data,965e4f8f,0.9777777777777777
45001,7f74a04ae75792,ccd19645,"Ad_Res_Total is heavily correlated with Ad_Res_Ind_Total. This might mean that the advertisements that reached out to the users, were then spread further via the consumer themselves by mouth to mouth, social media and referrals",d01e91da,0.9779411764705882
45003,5f4ae633cfd090,79fbd2d3,**77.9% which is almost 78% accuracy**,a30a16e2,0.978021978021978
45006,3c2033cc99c12c,f8fcb0db,## Summary ,dfa22a54,0.9781021897810219
45009,b49bb7b41806a7,5c552671,Image file type was rendered 45818 times.,d034d34d,0.9782608695652174
45011,7e2644d6b415bc,b93f8391,"# If you like, Please upvote",52de7ef0,0.9782608695652174
45012,0e2a23fbe41ca9,62989132,New merchants file also has nulls in the ```merchant_id```. Other than that every ```merchant_id``` is present in the merchants file.,64e4762c,0.9782608695652174
45013,72d528df923403,cde87b8c,"To do:
- Price analysis.
- Revenue analysis.
- Seasonality and stacionarity analysis.

If you find it useful, please upvote. ",d51c8e8e,0.9782608695652174
45015,9b5de3823ad5ab,5fab70a8,"## Conclusion

Given both validation and test results, I'm not going to bother submitting mine, as it would be just a waste of computer cycles. I'll just leave here what I would have done if I had the time to start over:

1. I would have done a better job with image preprocessing (I'd resize instead of just crop with Keras) and data augmentation (I'd increase the number of training instances 3 or 4 fold.
2. I'd have used EfficientNet-B2, at the very least, but I would try using B3 or higher with a smaller batch size (128, at most).
3. Of course, I would train the model for more epochs and using SGD with Nesterov, to compare with Adam.

With this 3 steps, I think I could have done a better job, without the need to go overboard with ensemble, for instance (remember, this is an undergrad assingment). I'm not satisfied with this results, but it is what I could do with the time I had.",33e48774,0.9782608695652174
45018,0caaec057f7184,01bc6996,"In the 100 items that are sold with return
- The sales sold ranges from a few to 6000 items
- The maximum of sales return is around 60 items
- Most of the returns are smaller than 10 items
- The most return items isn't the best seller in this category",b875533e,0.978494623655914
45023,5f674175839b32,f60f6264,"# Summary

This project gave me a basic idea of gaming industry,I learned a lot of new things while doing this project.",53a2e343,0.9787234042553191
45026,4c47839b067546,7e70e794,# 7. Submission,1f517b02,0.9787234042553191
45027,3f25b363afec54,0661177c,"If you check leaderboard my current ranking is 14, So instead of giving you a complete notebook i gave you this version because

- Let's learn through experiementation.
- Learn together and earn together

Now i want to know from you what do you think, how this problem should approached further let me know in the comment section.

<center>................**Waiting for your response**...................</center>
<br>

<center><img src='https://clientinsight.ca/wp-content/uploads/2018/03/waiting-for-phone.jpg' height=400 width=500/></center>


If you have any doubts till here let me know in the comment section too...

### Come to leaderboard i m waiting for you.",bbdaae25,0.9787234042553191
45028,56785caebaa256,6088043e,I hope you find this notebook useful and enjoyable.,a792961a,0.9787234042553191
45029,f91f58d488d4af,da1b83e6,"### Fastai provides learner.fit which can be used instead of train_model. To create a Learner we first need to create a DataLoaders, by passing in our training and validation DataLoaders:",5df1bbf3,0.9789473684210527
45032,eda49464dd6d1b,cd0df3d9,"### This set of predictions will be evaluated on Kaggle after submission, and a ROC will be calculated and compared to that of other models.",8421f81f,0.9790209790209791
45033,28a1ff0f223da9,912da87a,"**So, in year 2014 the ratio of PhD professors to students in Pakistan is 1:118.**",c945b27d,0.9791666666666666
45039,eb0854a6601407,3f74b84a,There are definitely some clusters of highly correlated features that can be later analyzed together.,6d107747,0.9791666666666666
45041,e82462cdc998a7,7f18700a,[Go to Top](#0),b39bf244,0.9791666666666666
45042,8c7e00ca3dc5a7,5c1f6a9a,"**References:**

Took some ideas from the https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard",c83346e4,0.9791666666666666
45043,3b5903412fe741,987ad0ad,We will see much more `DataFrame` assignment going on in later sections of this tutorial.,ad231969,0.9791666666666666
45045,8ec771f5600a61,194e8739,# CONCLUSION :,48364c1f,0.979381443298969
45046,225b4fe5d3894a,049d8f0d,"<a id=""10""></a>
## 10. References:

Link to the book I followed: [Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://www.amazon.in/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/ref=sr_1_1?dchild=1&keywords=handson+sklearn&qid=1599399632&sr=8-1) - *Aurélien Géron*
    
Top 5 Conceptual Books you might wanna see:
https://www.kaggle.com/getting-started/171809
    ",4b4197b3,0.979381443298969
45047,2a123b4e8f9433,fdce0f39,print target vector,0a082218,0.979381443298969
45050,fdc9f4863744b1,58bc8254,"By our above exploration, we know we will be using single and multi linear regression.
",b4529365,0.9794520547945206
45052,eb33e05704d647,bcd7f01a,"Part 1: https://www.kaggle.com/kishor1210/eda-and-data-processing

Part 2: https://www.kaggle.com/kishor1210/train-faster-rcnn-using-keras

Part 3: comming soon....",cd80436d,0.9795918367346939
45054,f1e162ddd14f11,d71e21f5,linear line shows that our results are good,cdb2e771,0.9795918367346939
45057,ffd1df95ca5289,b0d51a99,### So we will prefer Random Forest rather than Logistics regression,db00c338,0.9795918367346939
45058,e69a496109e7d8,0921a428,"Since more datapoints are overlapping, high accuracy of classification isn't possible. Yet we can consider **No.of.AxillaryNodes** for univariate and **age,No.of.AxillaryNodes** for bivariate and multi variate analysis to give better insights and better classification than others. The accuracy can be high if we had more features.",1c640591,0.9795918367346939
45059,2343dc02ffb96a,82440dde,# This is model is a good fit for this data.,29aa95a4,0.9795918367346939
45060,f35bf4df70d310,2040e169,Analysis by : Jonathan Kristanto &copy; 20 Mei 2021,10bb859a,0.9795918367346939
45062,dbd96dd275dc60,e85db418,"def plot_features(columns, importances, n=20):
    df = (pd.DataFrame({""features"": columns,
                        ""feature_importances"": importances})
          .sort_values(""feature_importances"", ascending=False)
          .reset_index(drop=True))
    
    # Plot the dataframe
    fig, ax = plt.subplots()
    ax.barh(df[""features""][:n], df[""feature_importances""][:20])
    ax.set_ylabel(""Features"")
    ax.set_xlabel(""Feature importance"")
    ax.invert_yaxis()",1ed493a8,0.9797979797979798
45066,83df814455f06c,93c4d4c7,"So, now we will come to the end of this kernel.

I hope you find this kernel useful and enjoyable.
	
Your comments and feedback are most welcome.

Thank you
",c9cff71a,0.98
45067,10c5a39a87c47e,25f6c412,"- As we have seen our model is giving approx 96% recall and auc of around 96.36 which is good overall.
- Further, we can also do data augmentation to increase the dataset and improve the accuracy
- The only misclassified instance shown in our random sample plotting has seems to have a small mark but in real its not which our model errorneously understand as a mark and therefore predicted as infected sample.
- I have deployed this model as a web app where we can upload the blood sample image and the model will predict status of sample whether it is infected or normal.


## If Like please hit UPVOTE !!!!",09c7337a,0.98
45068,0687cd5c8597db,5f3dce2d,"# <center style=""color:red""> 🎉🎊 Upvote the notebook if it is useful and Informative. 🎉 🎊 </center>",4edec76a,0.98
45072,2ada0305b68956,c1e75dff,### 168. Palette = 'vlag_r',133e26f4,0.98
45074,4cd25e50c7e007,ff982d24,"The variables that help us in understanding the bike rental counts are as below:

1. Temperature  
2. Light Snow 
3. Year",ceb0c525,0.98
45076,7dd46c750653eb,c01aef06,"# Conclusion

* In Moscow 2016 has the most number of Births.

* Month of July has the most of Number of Births.

* In Moscow 2010 has the most number of Deaths.

* Month of January has the most number of Deaths.

* In Moscow most of the most couple marry in months of June to September.

* In Moscow couple generally divorce in Month of March and December

* In Moscow Adoptions are decreasing whereas Name changes are Increasing over the years. Further studies need to be done to find out the reason.",c2644713,0.98
45077,64169805aacf17,f62f0087,# ~ fin ~,1f12ded0,0.9803921568627451
45078,52cfd66e9ec908,b711bb1e,And now we `kek` (train) our model!,c74adcdf,0.9803921568627451
45080,71b75664517244,ad5b3d50,This performance show Josep Guardiola make Manchester City win more premier league title in last 4 year.,fc905af5,0.9803921568627451
45081,907f08f9a2c6cf,fdba6144,"### Conclusion
Overall, this is a very good result. The final accuracy attained on the test set by the model is likely even higher than a human could have performed, considering the pixelated view of the characters. With the model that we exported, we can build a software capable of solving CAPTCHA images. To do that, we will now move onto the fourth and final notebook. Thank you for reading, and if you've made it this far please leave an upvote on this notebook, I really appreciate it!",aa84c325,0.9803921568627451
45082,1a0bd2f72bbe36,ed7808ea,"# IF YOU LIKE IT PLEASE UPVOTE::
## And please let me know the things that i should have add in this notebook:
",2fa311dc,0.9803921568627451
45083,d0080e3a39bc5c,62d45d25,"

**Edit 1** - 
**Discriminative Learning Rate - **

This is a really important concept, especially when training models using **pretrained models**.

* As we know, the deeper we go into the layers of a model, i.e, the initial layers learn small nuance features like **edges, corners** etc. and the more we go towards the final layers, the more train set specific features are learnt, like faces, big shapes etc.

  Now, nuance features like corners, edges etc are expected to be **present in every object**, irrespective of what the object is.

* So, the initial layers are still **highly relevant** irrespective of the particular type of objects we are looking at and the farther we go from initial layers, we need the model to learn more **object specific features** which fits our train set better.

* Therefore, logically, in case of pre-trained models, we **need not change the weights by much in the initial layers** as much as we need to for the later layers.Thus, essentially what we are looking for here is to have **different learning rates for different layers**, instead of having it the conventional way - same unifrom learning rate throughout the layers.

* Thus, we are able to **preserve the near optimal weights** in the initial layers and change them by very minute amounts and change the later layers more to suit our training set in a much better way.

This technique of having different Learning rates for different layers is known as **Discriminative Learning Rates******.

",2fcde4cf,0.9803921568627451
45084,523123dad03177,112a0e94,"More to follow. Next we can have some statistical tests done as to find out what contributes to the success of Group E and other things.
If you liked this notebook, then do upvote it.",48a5e4e6,0.9803921568627451
45085,629f2918807a9b,aee6a92c,### Answer is: انٹرنیٹ سے پیسہ کمائیں is the best seller in top 2 cities. i.e. Karachi and lahore,be56dc84,0.9803921568627451
45086,917957c6c4065f,3827243f,"조회수 상위 데이터를 제외한 상관관계입니다.  
likes, dislikes, comment_count가 각각 0.45, 0.57, 0.24의 상관관계를 가진다고 나타났습니다.  
다만 좋아요와 싫어요를 누르거나 댓글을 남기려면 먼저 동영상을 조회해야하기 때문에,  
조회수가 likes, dislikes, comment_count에 영향을 준다고 해석해야 합니다.

tag_count, title_length, description_length가 views와 상관관계가 있지 않을까 생각했었는데, 관계가 없는 것으로 나타났습니다.",55b8ed68,0.9803921568627451
45087,4fa553c2b837d4,470120bd,"So you can see that how the Mean Accuracy Error decresed gradually by applying different techniques.
Try this on other dataset and see the results.

Comment if you have any doubts or you can give any suggestions to improve this notebook.
> **If this notebook helped you in anyways, then Please Upvote.**",c65a23e9,0.9803921568627451
45088,842547b2def18c,545c67fd,"Our submission to the competition site Kaggle results in scoring 3,883 of 6,082 competition entries. This result is indicative while the competition is running. This result only accounts for part of the submission dataset. Not bad for our first attempt. Any suggestions to improve our score are most welcome.",b8efde6d,0.9803921568627451
45090,7cfd96218dd933,f5040cfe,"#### **ATTENTION**
* TURKEY'S VALUES ARE HIGH IN AQUA DATA",7c34d96c,0.9803921568627451
45091,98a6794067932a,a75f1ab3,"# 4. Conclusion : Limites, pistes d'amélioration et enjeux éthiques 

Afin de conclure ce rapport, trois thèmes principaux seront abordés dans le but d’évaluer l’outil d’analyse descriptif que nous avons créé dans le cadre de ce projet. Premièrement, les limites que nous avons pu constater lors de l’élaboration de nos analyses seront présentées. Ensuite, dans un deuxième temps nous effectuerons des suggestions par rapport à des pistes d’amélioration qui pourraient être intéressantes afin d’amener notre outil d’analyse à un niveau supérieur. Finalement, le dernier thème de cette section abordera les enjeux éthiques qui pourraient être mis en cause avec l’utilisation de notre outil d’analyse. 

**Limites :** 

Tout d’abord, la première limite que nous avons rencontrée lors de l’élaboration de cet outil d’analyse a été au niveau de l’accessibilité des informations que nous avions par rapport à l’entreprise. En effet, comme il s’agissait d’une base de données ayant été rendue publique de manière anonyme, nous ne pouvions avoir accès à des informations sur l’entreprise. Il est évident que si nous avions eu accès à de telles informations, nous aurions pu pousser l’analyse de nos résultats à un autre niveau en connaissant la culture et la réputation de l’entreprise. Aussi, le fait d’avoir accès à l’entreprise aurait pu nous permettre de valider certaines informations supplémentaires par rapport aux informations de la base de données.
	
Ensuite, la deuxième limite que nous avons rencontrée a été par rapport au manque de données concernant certaines commandes. Effectivement, lorsque nous avons voulu représenter l’emplacement d’expédition de certaines commandes, nous avons constaté que certaines d’entre elles n’avaient pas de code postal. Comme nous n’avions pas l’information par rapport à ces commandes, nous avons dû les éliminer de la base de données. De ce fait, il est donc possible de que nos analyses pour certaines régions aient été faussées étant donné que des commandes ont été enlevées de la base de données. 

Finalement, la dernière limite que nous avons rencontrée a été le fait que notre base de données datait des années 2015 à 2018 alors que notre outil était utilisé afin d’effectuer des recommandations sur le plan stratégique pour l’entreprise. Il est évident qu’afin d’effectuer de telles recommandations, il serait primordial d’avoir accès à des données le plus récentes possible. Toutefois, lorsque l’entreprise nous donnerait accès à ces données plus récentes, il serait facile de simplement mettre à jour la base de données utilisée par notre outil d’analyse afin de générer de nouveaux résultats plus représentatifs de la situation actuelle de l’entreprise.


**Pistes d’amélioration :** 

Tout d’abord, la première piste d’amélioration que nous pouvons suggérer serait de procéder à l’élaboration d’un tableau de bord permettant de visualiser rapidement et de manière concise les différents graphiques et analyses que nous avons réalisés lors de la création de cet outil. Actuellement, nous sommes conscients qu’il peut être fastidieux de devoir naviguer dans l’outil afin d’avoir accès à l’information désirée. En ayant accès à un tableau de bord, les utilisateurs pourraient rapidement avoir accès aux diverses informations nécessaires afin d’effectuer des recommandations stratégiques lors des réunions de direction. 

Ensuite, une deuxième piste d’amélioration qui pourrait être intéressante à élaborer serait d’identifier les saisonnalités au niveau de la demande. Effectivement, nos analyses n’ont pas couvert cet aspect, mais il pourrait être pertinent dans le but de développer nos recommandations stratégiques d’évaluer les périodes pour lesquelles la demande de certains produits est grandement supérieure à la normale. En ayant accès à cette analyse, il serait plus facile pour l’entreprise de prévoir les niveaux de stocks nécessaires afin de subvenir à la demande des clients et du même coup il serait plus facile de prévoir la capacité nécessaire pour les centres de distribution. Cette analyse pourrait également être utile afin de déterminer les besoins en main-d’œuvre de chacun des centres de distribution en fonction des niveaux de demande des différentes périodes de l’année. 

Finalement, la dernière piste d’amélioration que nous avons soulevée serait d’améliorer la rapidité d’exécution de notre outil d’analyse. Comme nous utilisons une version gratuite du module nous permettant de représenter l’emplacement de nos clients sur une carte, un long lapse de temps est nécessaire afin de générer la carte. Comme notre outil est utilisé afin d’effectuer des recommandations stratégiques pour lesquelles une réponse rapide n’est pas cruciale, il ne s’agit pas d’un enjeu majeur pour notre outil. Toutefois, il serait tout de même intéressant que l’entreprise opte pour la version payante afin d’optimiser la rapidité de l’outil.

**Enjeux éthiques :**
À travers ce travail et en nous basant sur la déclaration de Montréal pour un développement responsable de l'intelligence artificielle nous avons identifié trois enjeux éthiques.
Le premier principe dont nous devons tenir compte est celui de protection de l’intimité et de la vie privée. La déclaration de Montréal avance deux sous enjeux à ce principe qu’il nous semble intéressant de discuter. Les deux principes sont les suivants : (1) « Les SAAD doivent garantir la confidentialité des données et l’anonymisation des profils personnels. » (2) « Toute personne doit pouvoir garder un contrôle étendu sur ses données personnelles, en particulier par rapport à leur collecte, usage et dissémination. L’utilisation par des particuliers de SIA et de services numériques ne peut être conditionnée à l’abandon de la propriété de ses données personnelles. »
À partir du fichier, nous disposons de beaucoup d’information sur nos clients notamment leur nom et leur adresse qui peuvent être des données privées. Il est donc important dans le but de conserver l’anonymat de nos clients de ne pas les identifier avec leur nom, mais de privilégier l’utilisation de leur numéro client. De plus, il sera important de contrôler les accès à la base de données et ajoutant des codes d’accès distribuer uniquement aux personnes en ayant besoin. On pourrait avoir plusieurs des bases de données distinctes pour ne pas conserver le nom des clients et leur adresse dans la même base de données.  

Le second principe identifié est celui de respect de l’autonomie. Deux sous-enjeux de cette catégorie nous semblent intéressants à intégrer dans notre discussion. Il s’agit de (1) « Les SIA doivent permettre aux individus de réaliser leurs propres objectifs moraux et leur conception de la vie digne d’être vécue. » et (2) « Les SIA ne doivent pas être développés ni utilisés pour prescrire aux individus un mode de vie particulier, soit directement, soit indirectement en mettant en œuvre des mécanismes de surveillance, d’évaluation ou d’incitation contraignants. ». En effet, nous avons utilisé le fichier pour être en mesure d’identifier des tendances de comportement chez nos clients et cibler les produits les plus demandés pour chaque type de client afin d’avoir un marketing assez ciblé. Il faudra donc veiller à toujours laisser le choix à nos clients de recevoir ou non nos campagnes marketing ciblées et de leur laisser le libre choix de participer ou non à ces campagnes.
De plus, l’analyse réalisée sur la stratégie de réacquisition de nos clients perdus peut s’avérer problématique. En effet, on essaye de comprendre pourquoi nos clients nous ont quittés dans le but de leur faire des offres ciblées et de les inciter à consommer ce qui peut nuire à leur autonomie et leur libre-choix.

Le dernier enjeu éthique qu’il nous semble pertinent d’intégrer de discuter est le principe de prudence.  « Le développement des SIA doit prévenir les risques d’une utilisation néfaste des données d’utilisateurs et protéger l’intégrité et la confidentialité des données personnelles. » (Déclaration de Montréal pour un développement responsable de l'intelligence artificielle,2018). Nous offrons avec ce travail une description détaillée du comportement de leurs clients à la compagnie. Comme mentionné dans le précédent enjeu, avec ces données, on peut exposer les clients à un certain harcèlement commercial de la compagnie. Afin d'éviter cette situation, nous pourrions partager à la compagnie uniquement les analyses de la consommation par ville et par État, qui sont déjà assez révélatrices de certaines tendances de consommation. Ainsi, la compagnie bénéficierait d’information juste et intéressante pour prendre des décisions stratégiques sur son réseau de distribution tout en garantissant l’anonymat des clients. ",08600fe2,0.9805825242718447
45097,d0f6276d5b628c,3ba31200,"## HURRAH!
We've completed a recommendation system projects.

If you have like this projct go through my other projects on other topics on [kaggle](https://kaggle.com/sagnik1511/notebooks) or in [github](https://github.com/sagnik1511/repositories).

# THANK YOU :)",c64f5ce5,0.9807692307692307
45098,6f1481148352e9,f49c5a3f,"# Conclusion

**Having examined these data, I want to note that before starting this analysis, I had never before conducted an analysis on this topic, with any statistics, figures and facts I got acquainted only from the news, social media, networks. Accordingly, the results I received came as a shock to me. Since I did not expect to see so many fires. Main results of work:**
* State with the most fires - Mato Grosso;
* The largest number of fires was in 2003, 2015 and 2016, during these years there were different top 3 states by the number of fires;
* The largest number of fires occurred in August 2006;
* Almost 2/3 of the fires occurred in the second half of the year, the top 5 states, depending on the half of the year, are different;
* The greatest number of fires was in summer and autumn;
* The highest number of fires was in July.",7cfbdb8f,0.9807692307692307
45101,582cb872d19026,427c3909,### THANKS!,8d966d69,0.9807692307692307
45102,71d3e4aee86e3e,c5a1cbc5,"# Thank You!

> ***Do drop your feedback below for further improvements in my project!***",69706f0b,0.9807692307692307
45106,7454fdc444df16,4ba9e28c,"# Checkpoint 
## Saving Outputs
Now that we have stored our images in array, let's make a checkpoint by saving the array and dataframe. The reason we want to do that, is so that we don't rerun the whole notebook when we want to continue our analysis. This way we can directly start from here. Furthermore, we will be deleting all the  variables that we created in the process to free up some more memory.

We will do this by pickling the file, in other words serializing our data so that we can read it later. Below are 2 Functions that I created to make the process of saving and loading our variables easier.",a7818ef5,0.9809523809523809
45108,4dd47072617594,3f2dcf76,**Thank you for taking the time.**,44ff1d11,0.9811320754716981
45112,510b8303776bb6,d171c209,## Exporting output to csv,18080db8,0.9811320754716981
45114,f3c8651cb08234,8d6ece90,"We can see that XGBoost perform better.

Many thanks for your valuable time and checking my notebook.

If you have any **Question** feel free to ask in the **comments** I will be aswering them all ",37f86e36,0.9811320754716981
45115,23df07a474aaae,48be7501,"So, this is the Streaming prediction based on different features like artist followers, genre and other features using 4 different regression models. Among this models, it seems Random Forest Regression had the prediction more closer to the actual streams.

Thank you for visiting my notebook and Any suggestions will be Appreciated. ",0ea40276,0.9811320754716981
45117,43e60eb1362f5c,81e2a0c3,# Conclusion,87934234,0.9811320754716981
45120,c84925c8171900,87d333f7,"<p style=""font-size:20px;"">
    <span style='font-family:Georgia'>
        Thank you for reading my notebook. This is my first time using Plotly. Please feel free to give your honest feedbacks. If you like the notebook, please upvote
    </span>
</p>",e21ff7ec,0.9813084112149533
45126,2f0f808765fc67,98e1edb9,RANDOM FORETS REGRESSOR GIVES THE LEAST RMSLE. HENCE WE USE IT TO MAKE PREDICTIONS,fd1f6494,0.9814814814814815
45128,ac04ba639d1c93,12d2599d,"<a id=""ref""></a> <br> 
# **8. References** 

[1] OOF Model: https://www.kaggle.com/adarshchavakula/out-of-fold-oof-model-cross-validation<br>
[2] Using Meta Features: https://www.kaggle.com/artgor/using-meta-features-to-improve-model<br>
[3] Lot of Features: https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b <br>
[4] Angle Feature: https://www.kaggle.com/kmat2019/effective-feature <br>
[5] Recovering bonds from structure: https://www.kaggle.com/aekoch95/bonds-from-structure-data <br>
[6] The Tip: https://www.kaggle.com/c/champs-scalar-coupling/discussion/97705#latest-568340

<h3 style=""color:red"">If this Kernel Helps You! Please UP VOTE! 😁</h3>",748059d5,0.9814814814814815
45131,6191c1f476437a,0516b09d,"This is remarkable, how from Nothing to something meaningful within 10 Iterations. With 100 Epochs, we can get almost 99% similar image as of original Celeb Face Dataset.

I would like if someone can run this for 100 Epochs and result obtained after that, In case anyone able to run for 100, Kindly share link of image transformation.

That's It for this Notebook, I will come up with more advanced version of GAN with next notebook, for now Wessertein GAN, worked excellently.

Thanks and Consider Upvoting !! Keep learning !!",9dba3159,0.9818181818181818
45132,016abae0483764,eb01a0af,"Which is almost similarly high, as we can see...",bc9f289b,0.9818181818181818
45138,f13534449a3750,d63d92ee,"We will continue the challange in another notebook to have a better reading and organization. In this first notebook we faced the problem and analyze the data avalibale. In particulat we detected an high unbalanced classification problem (pixel level). We shown the masks and the images and the distributions of images with and without ships. In the next notebooks we will explore the classification of images into two classes, one with ship and another without ships. ",8b7f3332,0.9821428571428571
45143,a5a419dc7245b0,1a02c6e8,**Accuracy-Train-Test**,4279726e,0.9823008849557522
45145,ac1abfe1dfe815,754bdab2,# Conclusion,6529dbcb,0.9823008849557522
45146,30fdc4a6e3c1db,8a7c9bc0,"What we see:
* The prices for products in the cheaper products have increased very less over the years for all the 3 Categories
* In the costlier bucket, we could see increase in avg.prices specially in HOBBIES in 2012 and 2013. FOODS also show some increase in prices 
* The increase in prices in Costly Hobbies product didnt impact the sales in 2012 and 2013, it actually grew quite well in those years.What we actually see is a dip in sales of Cheaper Hobbies products between 2012 and 2013
* The gap between the selling price of cheaper products and costlier products of Food products is lowest, but the same gap is the highest when compared in terms of sales (i..e Betwee Cheaper Food and and Costlier Food).
* The gap between the selling price of cheaper products and costlier products of Hobbies products is highest, but gap is quite low when compared in terms of sales (i..e Betwee Cheaper Hobbies products and and Costlier Hobbies products).",6111ddee,0.9824561403508771
45150,9e27af2600925c,9e544561,"Bibliography:
- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c",9b556435,0.9824561403508771
45152,5ce12be6e7b90e,0865ae17,"## Exercise: Functions (HARD)
Write a function which given a secret and a code, decodes the secret and prints it out.
",c0ab62dd,0.9824561403508771
45154,65619ae436d36d,a7c8df59,referred to the following notebook for data processing https://www.kaggle.com/prateek0x/multiclass-image-classification-using-keras,34343d4d,0.9824561403508771
45157,c2a9f2fb3e1594,c6a93d64,"<a id=""ch91""></a>
# Credits
Programming is all about ""borrowing"" code, because knife sharpens knife. Nonetheless, I want to give credit, where credit is due. 

* [Introduction to Machine Learning with Python: A Guide for Data Scientists by Andreas Müller and Sarah Guido](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) - Machine Learning 101 written by a core developer of sklearn
* [Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics by Nathan Yau](https://www.amazon.com/gp/product/0470944889/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=0470944889&linkId=f797da48813ed5cfc762ce5df8ef957f) - Learn the art and science of data visualization
* [Machine Learning for Dummies by John Mueller and Luca Massaron ](https://www.amazon.com/gp/product/1119245516/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1119245516&linkId=5b4ac9a6fd1da198d82f9ca841d1af9f) - Easy to understand for a beginner book, but detailed to actually learn the fundamentals of the topic
",53411c04,0.9824561403508771
45158,e03eb63c1f725d,8db50914,"# **Friends, if you like my notebook. Please upvote this notebook.**",e204b7e3,0.9824561403508771
45164,1750367e54f407,ff0c3e0d,"### Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. It is always greatly appreciated!",a8e655b2,0.9827586206896551
45168,20e1ba19eb9b5e,bf423751,"# 5. Refrences
* [Comprehensive Data Explorartion with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by **by Pedro Marcellino**: Great basic data analysis
* [Stacked Regression to predict House Prices](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by **by Sergine**: Best structured kernel on modelling I have found so far (for basic learning of machine learning)
* [House Price Prediction EDA/XGBoost - kernel prepared by tooezy](https://www.kaggle.com/tooezy/house-price-prediction-eda-xgboost)",4569bfc1,0.9827586206896551
45170,1dd9c6aa74d289,cbd388c5,"## Summary Q3:
The top 3 countries where the most rope climbing routes had been sent are in Spain (22%), USA (10%), and France (9%). Same ranking for bouldering, are USA (30%), France (11%), and Spain (7%). The numbers only indicate which countries have most climbing logs have been recorded in the data. Some countries may have many crags, so there accumulate many climbs. Or it is just that most climbers in this community come from these countries and they climb nearby. So the crags I list below are the most popular within these countries, while it can not guarantee also be the most popular in the world. Moreover, the exact numbers change time by time when we have more climbs.

Nevertheless, when you happen to be in the US or in the East Europe, consider the following places to go:

- Rope climbing: Rodellar in Spain, Red River Gorge in the US, Céüse in France, and Kalymnos in Greece.
- Bouldering: Bishop in the US, Fontainbleau in France, and Albarracín in Spain.",5ef9a1be,0.9827586206896551
45171,a1ba5ffd30dbde,8b84c68c,### Conclusion : Random Forest and XG-Boost both are 97% accurate in predicting disease.,48e57546,0.9827586206896551
45174,84127ade6fde87,09da1e1b,"Now that we’re familiar with tensors and how to store data in them, we can move on to the next step towards the goal of the book: teaching you to train deep neural networks! The next chapter covers the mechanics of learning for simple linear models.",f55d05b6,0.9827586206896551
45178,49ee86d074de69,9dbdff3d,### See The Files,71ccc6d3,0.9829059829059829
45181,ed8009f482b380,952f6bb7,## Thank You,e99941fa,0.9830508474576272
45183,a44368590e878a,a9e52b15,# To be continued...,77743ba8,0.9830508474576272
45184,1294fb4c86f993,cae5b31c,"This clearly show that more than 70% of data has guns registeration more than 100,000 guns ",4471e513,0.9830508474576272
45185,9169c4e9c33c90,4ea9ab95,"Thanks for reading! Again, feedback and criticism are appreciated. Feel free to leave a thought in the comments.",725bf880,0.9830508474576272
45187,2a56d6b0e153f2,4bc0687c,"after tuning the hyperparameters we can see that, random forest shows the 86% accuracy. 
**Random Forest** model fits the best for this dataset",8dc315e6,0.9830508474576272
45188,bb0905d33ae417,4da30b33,"# Acknowledgements

* [qitvision](https://www.kaggle.com/qitvision/) for his [extremely well-explained kernel](https://www.kaggle.com/qitvision/a-complete-ml-pipeline-fast-ai) on the same competition and for answering my questions on data loading.
* [Gunther](https://www.kaggle.com/guntherthepenguin) for providing the implementation of the AUC metric in [his kernel in the same competition.](https://www.kaggle.com/guntherthepenguin/fastai-v1-densenet169)
* The [fast.ai team](https://www.fast.ai/about/) for creating the [library](https://docs.fast.ai/index.html) and [the v3 course](https://course.fast.ai/index.html) for teaching deep learning in a very accessible manner.",25fd1965,0.9830508474576272
45189,a81661cc35d8d2,468a97ec,"1. Get more data points


2. Hyperparameter tuning


3. Oversampling the imbalanced class with techniques (for e.g. SMOTE)",3331f113,0.9830508474576272
45190,f2e5e9fb9eaaf7,836d304c,"Thank you for reading. If you have any critics or find anything wrong, please let me know. I hope you enjoy it.",048e0d08,0.9830508474576272
45193,dac3c8204a2d1b,57c90cf9,"# Conclusion


In this notebook, I analysed Amazon's top 50 bestselling books of each year from 2009 to 2019. As a result of this analysis, I found out that the number of Amazon bestselling non-fiction books is higher than that of fiction books, and even though non-fiction bestselling books were higher most of the times, there is an exception for the year 2014. Lastly, in contrast to the fact that there were more non-fiction bestsellers, I saw that fiction books usually had higher ratings than non-fiction books did.",b0d2d0dc,0.9830508474576272
45194,4ae6a182abac64,549998cf,# Useful resources,418676c5,0.9831932773109243
45196,541d0fa0e26b80,1bfdb11b,"Kindly Upvote if you like my work.
                                       Thank You!",a29e0f29,0.9833333333333333
45197,62487bcd70b199,bbdcbc6d,## <a id ='9.2'>9.2. Compare Ensemble Metrics</a>,f6ae50af,0.9833333333333333
45198,f6488772605bb5,f408ac2d,"If interested to save all the hyperparams, and some others metrics values foloows this notebook
[Notebook](https://jovian.ml/aakashns/05-cifar10-cnn) ",068d4697,0.9833333333333333
45203,c18267b203f28a,ee5edce0,"Be aware that because this is a code competition with a hidden test set, internet and TPUs cannot be enabled on your submission notebook. Therefore TPUs will only be available for training models. For a walk-through on how to train on TPUs and run inference/submit on GPUs, see our [TPU Docs](https://www.kaggle.com/docs/tpu#tpu6).",09ca8efb,0.9833333333333333
45206,712198370d5521,37da3524,"<a id=""9""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">CONCLUSION</p>

In this project, I performed unsupervised clustering. 
I did use dimensionality reduction followed by agglomerative clustering. 
I came up with 4 clusters and further used them in profiling customers in clusters according to their family structures and income/spending. 
This can be used in planning better marketing strategies. 

**<span style=""color:#682F2F;""> If you liked this Notebook, please do upvote.</span>**

**<span style=""color:#682F2F;"">If you have any questions, feel free to comment!</span>**

**<span style=""color:#682F2F;""> Best Wishes!</span>**

<a id=""10""></a>
# <p style=""background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;"">END</p>",5882e04c,0.9833333333333333
45208,b547f0f38f7744,c5ca0e2d,"## References

- [TorchVision Instance Segmentation Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)
- [Kaggle: Global Wheat Detection](https://www.kaggle.com/c/global-wheat-detection)
  - [Pytorch Starter - FasterRCNN Train](https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train)
  - [Global Wheat Detection: Starter EDA](https://www.kaggle.com/kaushal2896/global-wheat-detection-starter-eda)",b6ba66b3,0.9833333333333333
45209,cf4d1c1ad1476c,82917f1e,"References
https://www.kaggle.com/xhlulu/contradictory-watson-concise-keras-xlm-r-on-tpu",768c1a59,0.9833333333333333
45210,e9b9663777db82,bab2a563,"So, after visualizing , we have found that ,

Model performance :
 Random Forest > Gradient Boosting > PCA > SelectKbest > Mutual_info_regression > Linear Regression > RFE >SelectFromModel
        ",648e8507,0.9834710743801653
45218,e19e307b3fd188,a109c8b8,# Save model,2173955b,0.983739837398374
45220,9ceb7278784462,cf75143f,"## <a id='26'>22.Conclusion </a>

* As you can see our models are not good.<br>
* We have little data. <br>
* Learning is less. <br>

**Models**

![ ](https://media.giphy.com/media/l22ysLe54hZP0wubek/giphy.gif) 
           ",3768a567,0.9838709677419355
45223,c0ddb77bf32e2b,ce2fe16b,"To be continued.

For more recent information about PM2.5 in Tawian please check [台灣即時 PM2.5](https://www.taiwanstat.com/realtime/pm2.5/). This issue should be 
get more attention.  And when summer ends, the situation getting worse.
![monthly](https://imgur.com/2SaVuAT.jpg)

![wic](https://imgur.com/QXkcSXD.jpg)

To do list
- [ ] Adjust LSTM for better performance. The dependency of prior step's output  is too much, make the prediction varies too little, became a steady line compared to RF. Maybe using Attetion mechanism.(I just believe it can performance better)
- [ ] Optimize run time. Padding zero for deleted rows and processing with larger batch size. Try coupling the input and forget gates (CIFG) or removing peephole connections (NP)
- [ ] Take location into consideration. Assume the time now is $t=0$ , make features such as 'Dayuan-1' repesenting PM2.5 of Dayuan at $t=-1$, 'Dayuan-2' repesenting PM2.5 of Dayuan at $t=-2$, this maybe help predicting  $t=1$ at Banqiao.

   t=-2(2 hours ago)           t=-1(1 hour ago)            t=0(now)            t=1(1 hour latter)

- [ ] Predict further future. 3 hours later, 12 hours, 1 day?
- [ ] Tranlate some Chinese news into English.",a0cb45f7,0.9838709677419355
45225,06ecf7a304c309,2c7d5eed,"### Excellent References
- https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/
- https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798
- https://blog.keras.io/building-autoencoders-in-keras.html
- https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html
- https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/


마지막으로 이렇게 좋은 커널을 만들어주신 @ShivamBansal님에게 다시 한번 감사의 인사를 드립니다.

Finally, thanks to @ShivamBansal for making such a good kernel.",714de627,0.9841269841269841
45227,8985a124d4b657,a7065115,**Don't forget to upvote if you learnt and enjoyed this notebook as much as I did and share this kernel with your friends!!! Bye.**,586d1846,0.9841269841269841
45232,0932046e1f485d,cb8c85d1,## <a id=done>Finish line</a>,218cc7a3,0.984375
45235,ff3a8ce61fab6a,7f2b292a,"<hr>

<div>
    🎁 <b>Congratulations</b> 😊<br><br>
    Now we finish <b>Tensorflow basics</b> and we will know more about <b>Tensorflow Framework</b> in other kernels.<br>
    Good luck 👍
</div>

<br>

<h1>Please If you find this kernel useful, upvote it to help others see it 😊</h1>",9afe1654,0.984375
45237,d07915a6e6992e,a98cfd32,"

Title, Sex_Female, Fare & PClass seems to be common features preferred for classification.

While Title & Age feature represents the Age category of passengers the features like Fare, PClass, Cabin etc. represents the economic status. Based on findings we can conclude that Age, Gender & features representing social/economic status were primary factors affecting the survival of passenger.
",2b912140,0.9846153846153847
45240,3cb96bd8eb364b,0352bf54,"
 - [DIY Rain Prediction Using Arduino, Python and Keras](https://create.arduino.cc/projecthub/danionescu/diy-rain-prediction-using-arduino-python-and-keras-abdec6)
 - [Keras Timeseries forecasting for weather prediction](https://keras.io/examples/timeseries/timeseries_weather_forecasting/)

 - [TensorFlow Keras Tutorial](https://www.tensorflow.org/tutorials/keras/classification)",3157af7e,0.9846153846153847
45241,a8c042af6b7245,8289a18b,"#### Conclusion

Hopefully this notebook helped you with some tips on how to start with this competition. Feel free to vote for it. And if you have questions, post a comment.",2487ac62,0.9846153846153847
45243,03048e86a6d806,5b4f34b2,==============================================================================,1285c231,0.9846153846153847
45246,1645979263c148,c21ef936,"## Conclusion
I will choose a **naive bayes algorithm** for this dataset.

**naive bayes score**

1. **f1_score: 1.0**
2. **auc: 1.0**",fa11663e,0.9846153846153847
45248,ee23a565163388,e39fff70,# **Conclusion**,88aacbc4,0.9847328244274809
45250,a2444ab5d5f147,b8f2ac9e,### Similarly we can analyze the data even more to understand it even further,10617755,0.9848484848484849
45251,ba4b3bd184acbb,52a54d8a,***,0f5de724,0.9849624060150376
45253,a4aa36df07fd53,94c41e2b,Train set error mengecil tapi kenapa valid set error membesar? Ada apa?,d2f42b6d,0.9850746268656716
45254,20b372b6e4e276,caab8006,Your comments and feedback are most welcome.,ec8b0860,0.9850746268656716
45257,21413205980558,92776d78,# Section Ⅵ Summary,84197de0,0.9850746268656716
45259,7f74a04ae75792,623f215e,Pur_Total has a positive correlation with Ad_Res showing that promotion done by the company does indeed led to the increase in sales. We may conclude that the advertising campaign was a success but further research may be required.,d01e91da,0.9852941176470589
45260,eb0ecd6bebeb15,d5b8ce47,"Emeğiniz, ayırdığınız vakit ve ilginiz için teşekkürler.",d7b93a60,0.9852941176470589
45263,02b7e38902069e,34e096c7,#Journeys of a thousand miles begin with a single step!  BioBERT wait for me with Stanza!,726a03a0,0.9852941176470589
45264,156bbcff05dcea,2df2a821,## Thanks!,66ad1fe9,0.9852941176470589
45267,e4c6dd957eb5ce,8f4ce2aa,"# I'm working at this Kernel, so upvote the kernel and stay tuned
- Please, DON'T FORGET THE VOTE, IT'S IMPORTANT TO ME GET THE GRANDMASTER TIER!!!! ",2e383665,0.9852941176470589
45268,3c2033cc99c12c,a5f88465,**Note:** *In this part i will do a brief summary of the data analysis of the prediction of the credit card fraud detection*,dfa22a54,0.9854014598540146
45270,a1a31459abf078,9eaa751e,"### Next Steps - 
* Using higher volume of training data
* Continue working on Feature Engineering
* Experiment with model ensembling and Hyperparameter Optimization for XGBoost
* Create a running lookup table of user performance as we keep processing batches of user data on test dataset and use it to make predictions",66fc0f54,0.9855072463768116
45279,7e275c8d5ff2a0,41a6394c,"We can clearly see the prediction of our model that on '16 August, 2020' there will be a total of ~ 36k  Deaths cases in India if the number of Deaths cases goes on increasing like this.",b3afcc98,0.9855072463768116
45280,17a24d566ffa59,67c59661,"The normal distribution is a probability distribution over all the real numbers. It is described by a mean and a variance. The mean is the expected value of this distribution, and the variance tells us how much we can expect samples to deviate from the mean. If the variance is very high, then you’re going to see values that are both much smaller than the mean and much larger than the mean. If the variance is small, then the samples will be very close to the mean. If the variance goes close to zero, all samples will be almost exactly at the mean.

The dirichlet distribution is a probability distribution as well - but it is not sampling from the space of real numbers. Instead it is sampling over a probability simplex.

And what is a probability simplex? It’s a bunch of numbers that add up to 1. For example:

- (0.6, 0.4)
- (0.1, 0.1, 0.8)
- (0.05, 0.2, 0.15, 0.1, 0.3, 0.2)

These numbers represent probabilities over K distinct categories. In the above examples, K is 2, 3, and 6 respectively. That’s why they are also called categorical distributions.

When we are dealing with categorical distributions and we have some uncertainty over what that distribution is, simplest way to represent that uncertainty as a probability distribution is the Dirichlet.

SOURCE: 
- [What is an intuitive explanation of the Dirichlet distribution?](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)
- [VIDEO: Digging into the Dirichlet Distribution](https://www.hakkalabs.co/articles/the-dirichlet-distribution)
- [Introduction to Latent Dirichlet Allocation:](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)
- [Gensim LDA: Tips and Tricks](https://miningthedetails.com/blog/python/lda/GensimLDA/)",89049e56,0.9855072463768116
45282,a566b5b7c374e7,5a851709,### Sleep Notes,b3dc5545,0.9856115107913669
45285,2730840089c8eb,0611ada4,"# Your Turn

Try the [hands-on exercise](https://www.kaggle.com/kernels/fork/962743) with strings and dictionaries
",34d27dac,0.9857142857142858
45286,9c044fa3072552,459021a2,"## Summary and Insights
- No data from New York(which is the most poppulated city in the US)
- Less the 5% of cities have more than 1000 yearly registered accidents.
- Accidents are more frequent from 6am to 10am and 3pm to 6 pm during the weekdays.",1362842e,0.9857142857142858
45290,2ada0305b68956,dbbdce6a,### 169. Palette = 'winter',133e26f4,0.9857142857142858
45294,56785caebaa256,a3d1cfd7,Your comments and feedback are most welcome.,a792961a,0.9858156028368794
45295,3d77c1560bd16e,b066cee5,> Thanks for going through the notebook. I welcome any comments/feedback to improve further. ,87c141ca,0.9859154929577465
45300,bddd799cdbbae8,15a76201,"The support vector machine is the best machine learning model for predicting cyberbullying tweets. The accuracy obtained using the SVM model is about 83%, with a weighted average precision percentage of 82%. Even so, the sensitivity obtained is 83%. It means that about 17% of positive cases missed the model's predictions.",b44e3c08,0.9859154929577465
45301,eda49464dd6d1b,e450cd41,"<font size=""+1"" color='blue'><b> I hope you enjoyed this kernel , Please don't forget to appreciate me with an Upvote.</b></font>",8421f81f,0.986013986013986
45303,166a62ebb4fc3a,c1e0474b,The prediction accuracy for the test data set using the above Random Forest is **92.71%**,db48a079,0.9861111111111112
45304,c01049afb6d307,2f353ea0,* Average of Absenteeism time in hours for each BMI category.,d37d3b5d,0.9861111111111112
45306,593d1d3d1df05a,5c849e77,### ****My model can be improved drastically by increasing the amount of training model. Currently the alphanumeric dataset only has roughly 1000 files.****,bc682ffe,0.9861111111111112
45307,69ac33d79f5130,a1bb1d9e,"### Insight
 #### New york data is not available
 #### Number of accident per cities decreases exponentelly.
 #### Less than 5% cities are high accident cities. To be precies it's 704 cities.
 #### Over 1200 cities have 1 accidents.",9d760d2a,0.9861111111111112
45311,738bfced935b69,a3de474d,## Conclusion,2d3c592d,0.9863013698630136
45312,91473a39b85068,c82d0074,"### ACKNOWLEDGMENT
Special Thanks **Joseph Stalin Peter** for continuous support and guidence.",6e3d91c2,0.9863013698630136
45315,e4525eb0c96f28,c6631a92,"## Related Links
If you are interested in doing more Data Science on the topic of video game sales, you might find the following links useful.

**Video Game Data**
- Video Game Data from Kaggle: https://www.kaggle.com/ashaheedq/video-games-sales-2019
- Video Game Region Data from Kaggle: https://www.kaggle.com/andreshg/videogamescompaniesregions

**Statistical Analysis**
- Ways of Data Imputation: https://www.theanalysisfactor.com/seven-ways-to-make-up-data-common-methods-to-imputing-missing-data/
- Seaborn Violin Plot: https://seaborn.pydata.org/generated/seaborn.violinplot.html
- Matplotlib Violin Plot: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.violinplot.html

**Machine Learning**
- Linear Regression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
- Dummy Variables and Interactions in Linear Regressions: https://towardsdatascience.com/on-the-role-of-dummy-variables-and-interactions-in-linear-regression-558d9644fc67
- Random Forest Regression: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html

**Other Video Game Information**
- Guide to ESRB Ratings: https://www.esrb.org/ratings-guide/
- Guide to Video Game Genres: https://www.idtech.com/blog/different-types-of-video-game-genres",2093a1f1,0.9864864864864865
45317,63b44c85e32c1f,c3ff1f24,**clear( )** is used to clear all the elements and make that set an empty set.,fb9b9562,0.9864864864864865
45318,62037c5832129c,73031dcc,...,61474350,0.9864864864864865
45319,e1fff2f67cbe32,112f37d0,"## Reference Link
[https://www.kaggle.com/vikassingh1996/riiid-eda-xgb-and-feature-importance](http://)",c6dfde64,0.9864864864864865
45320,726833f92fb87a,29f7c47c,"- The most important features to determine if a new customer will accept the deposit is poutcome (>30%):<br>
    - **This means that the deposit satisfied the customers who accepted it previously, however, currently, the company is probably aiming at the 'wrong' customers** 
 <br>
- Moreover, accoring to XGBoost, other important features to predict the success of a deposit are:
    - Contact: by looking at the data it looks like the great majority of customers with unknown contact did not accept the deposit: it should be investigated why these contacts are missing.
    - Housing: customers without housing loans seems to accept the deposit
    ",7dc5e1b6,0.9865771812080537
45322,cb570c7b7f0501,8fa69333,"
<a id='conclusions'></a>
## Conclusions

After analysing the whole data.

I highly recommend that the main reason of no-show cases is that people didn't get an appiontment in the same date of booking.
people may lose their urge, forget the date or even become busy with another unpredicted events.
as we get a ratio of no-show =0.04 . For 34277 cases. when they booked an appiontment in the same day.

limitations :
* we have people booking more than one appiontments with in a really short time like 1 mins or few seconds.
* patients with 0 age are they just new born babies !?
* the distance from the hospital to the neighbourhood is not provided.
",a200a0ec,0.9866666666666667
45323,7e1da639035ac5,1b0758c1,#### This is certainly not the end. More awesome things(unsupervised learning) are yet to come. Please upvote and comment so I can make this even better.,120b6c23,0.9866666666666667
45324,67b7354e96113a,0aaf9407,**Don't Forget to upvote :)**,dca94250,0.9866666666666667
45331,52ee792e228d54,b2357f61,"### With recall of 100% and accuracy of 99.5%, SVM is definitely the best model to choose for the campaign.",5096094e,0.9868421052631579
45332,917957c6c4065f,1b9f05aa,## 4. 결론,55b8ed68,0.9869281045751634
45333,663bbc9eaf267b,8b0f2681,"* We can see the default version of CatBoostRegressor model performs slightly better than the other estimators. However, by tuning hyperparameters, we may end up with a different ranking.",32445529,0.987012987012987
45335,2cb457b60dd246,ffad26fc,"**Recall** = Given a class, will the classifier be able to detect it?<br>
**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>
**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.",339367df,0.987012987012987
45336,241cf32abb22d8,0e6d8f2a,"# References <a class=""anchor"" id=""6""></a> 
* P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. ",47157066,0.987012987012987
45339,75adb7945ef9bd,c8ebd08b,"Reading the tweet texts, it is quite strange that they are labelled as disaster tweets indeed.

That's it for now. Stay tuned for more analysis!",785c5095,0.987012987012987
45340,722cd844dfbe8f,4d9570f2,"# <span style=""color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold"" id=""section_5"">Try another approach: Transfer Learning</span>

In order to complete these models, we will try another approach using **Transfer Learning methods**. We will use a pre-trained deep model to detect the features *(like EfficientNet ...)* and an LSTM layer for the final classification on the matrices obtained. This approach is available in the Notebook :

<span style=""font-size:18px"">[🧠Brain Tumor - Transfert Learning MRI - All MRI](https://www.kaggle.com/michaelfumery/brain-tumor-transfert-learning-mri-all-mri/)</span>

<span style=""color:red; font-size:18px"">Don't forget to **upvote** if this Notebook helped you!</span>",0cedb385,0.987012987012987
45341,d4c5aaa4b36810,855c2be3,"Some of these features have surprisingly high coefficients. Such as, *Average precipitation in depth (mm per year)* one might expect a country where it rains a lot to be unhappy however if there is a lot of rain it is easier to create a food surplus. It is interesting to note that *Fixed broadband subscriptions (per 100 people)* has a strong positive effect on happiness but *Mobile cellular subscriptions (per 100 people)* has the smallest effect of any feature. The feature with the strongest negative effect is unsurprisingly *Refugee Rate* as high *Refugee Rate* indicates a war in the country. Unsurprisingly *GDP per capita* has the largest positive effect on happiness. Very surprisingly *Life expectancy at birth, total (years)* has a very small negative effect on happiness.  ",65441f28,0.9871794871794872
45343,4d91e84c564cbe,7f1928d9,"---




*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161283) to chat with other Learners.*",355a43e3,0.9871794871794872
45345,80ad12f326ab70,83d1e1e7,"### More analysis to perform:
* Upload the external datasets- COVID-19 US State Policy database and KFF
* Merge and perform analysis on the external, products, engagement and districts data
* Uncover more insights impacting Covid-19
* Draw conclusions",da404a16,0.9871794871794872
45353,4883314a96dc34,6ef96691,"**Final Notes**
* The best model was Random Forest Regressor. 
* We dummified the Route, normalized the features, tuned parameters and hyperparamters.
* The RMSE of the model's performance in predicting the % successf rate of reaching the peak of Mt Rainier was only improved from 0.439 to 0.436. The dataset is too small (1889 total rows) for room for extensive further optimization and tuning and improvements. 
* In the future, we could try with a much larger dataset. ",50d36836,0.9876543209876543
45354,faa8e6c8ab9246,20b8c288," **If you enjoy this notebook, please share and give an upvote. Any suggestions or comments are appreciated. Happy Learning :)**",2bea1419,0.9876543209876543
45356,fd4017c1514157,4a26b103,"## If you find this notebook useful, do give me an upvote 👍.

## This notebook will be updated frequently so keep checking for further developments.

## In case of any doubts reach out to me on [LinkedIn](https://www.linkedin.com/in/rajendra-sarpal-rs465/).",fd8f0896,0.9878048780487805
45357,9c26c5dcd46a25,61b73646,"On remarque ici une très grosse perte de précision si l'on base la modélisation sur les variables synthétiques de la PCA. **La réduction dimensionnelle, à ce stade de préparation des données n'est pas pertinante pour notre application**. En effet, le nettoyage initial a déjà permis de réduire considérablement le nombre de variables du dataset.",1bbbb677,0.9878048780487805
45358,9ceb7278784462,c9caaf66,"## <a id='27'><font color=""LIGHTSEAGREEN"" size=+2.5><b>23.End Note</b></font> </a>

I hope you enjoyed my kernel.If you like this notebook, an <font color=""DARKCYAN""><b>Upvote</b></font> would be great ! :)

I am new with data science. Please <font color=""GREEN""><b>comments</b></font> me your <font color=""GREEN""><b>feedbacks</b></font> to help me improve myself. 
    
Thanks for your time",3768a567,0.9879032258064516
45361,87e94f864d74be,d049f08f," <span style=""color:crimson;font-family:serif; font-size:20px;"">  Please upvote if you liked the kernel! 😀
    <p style=""color:royalblue;font-family:serif; font-size:20px;"">KEEP KAGGLING!</p> 
</span>",294bfe9f,0.9880952380952381
45362,565ad413cd802f,75ca0943,"You can also use the ""Save Version"" button on Kaggle itself, to save a copy on your Kaggle profile.",397b074e,0.9880952380952381
45364,e93a41c03638fe,36b21540,"References: 
https://www.tensorflow.org/text/tutorials/classify_text_with_bert",7363527b,0.9882352941176471
45366,513ce405d7f6a3,3cb605ba,# I hope you find this notebook useful please upvote the notebook and give your comment.,8461e086,0.9882352941176471
45367,30fdc4a6e3c1db,95b6a8d5,"Highly Influenced by :
https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda",6111ddee,0.9883040935672515
45369,d96642860ab3dd,7b335632,### End_ For_ Now,98419d48,0.9883720930232558
45371,d5f78aa381f58d,c892d33b,___,d60f358f,0.9885057471264368
45377,d1ff7e10ee0102,e3209d8b,"# Acknowledgements

Thanks to [João Rico](https://www.linkedin.com/in/joaomiguelrico/) for reading drafts of this.",2cc71c3c,0.9886363636363636
45379,73d8e56bc709b1,921a28df,"# References
[1] https://www.kaggle.com/laowingkin/fifa-18-find-the-best-squad  
[2] https://www.kaggle.com/ap1495/fifa-19-classification-regression  
[3] https://www.kaggle.com/dczerniawko/fifa19-analysis  ",78ec3cce,0.9886363636363636
45381,312135b445bd23,3e255197,"# Conclusion
We hope this notebook will help researches in the medical domain to better gain information about the new COVID-19 virus.
Please feel free to to use this notebook for your own needs.

Any comments and upvotes will be much appreciated.",8ced381f,0.9887640449438202
45382,04ff2af52f147b,588dd6ed,Thank you to [Gunes Etivan's Titanic - Advanced Feature Engineering Tutorial](http://https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial/notebook) and [Manav Sehgal's Titanic Data Science Solutions](http://https://www.kaggle.com/startupsci/titanic-data-science-solutions/notebook) for their helpful ideas and overall contribution to this competition.,d5f37be9,0.9887640449438202
45384,d6cbd7160961dc,87665e6a,# THANK YOU FOR READING!,36d74664,0.9888888888888889
45386,396bc36edb95d3,1ab9107e,"Out of the 3 models, Random Forest has slightly better performance than the CART and ANN model. 

Overall all the 3 models are reasonably stable enough to be used for making any future predictions. From CART and Random Forest Model, the variable Agency_Code is found to be the most useful feature amongst all other features for predicting if a customer has claimed the insurance or not. ",965e4f8f,0.9888888888888889
45388,892be0a523578c,f7b648b8,"<div>
    <h1 id=""Share (PPT included)"" style=""color:#4e79a7"">
        Share (PPT included)
    </h1>
 </div>
 
So far,we have found many interesting things from the data, but not all the points are valuable to the target, which is giving recommendations for marketing strategies. For me, I think the following points may be useful.</font>
* Our customers can be separated into 2 major groups, customers who are regular exercisers or not fond of exercising. The ratio between the two groups is 50% to 50%. So I recommend the advertisement content should be group specific, since the customers who don't like sports may be less sensitive to the content relating to sports.
* Most of our customers go to bed after 10 pm, so the advertisement resources should be arranged before this time   

These 2 points might be a bit shallow, any other ideas are very welcome!
**Here is my [PPT](https://docs.google.com/presentation/d/e/2PACX-1vSNPw4xQ_0z-O-lyAKmQif1iG15xwANrkMP-WIxrAUYufNbGu-sxn-QgreirteOneuBz9pgPq4rSnhE/pub?start=true&loop=false&delayms=3000&slide=id.g10f94bb5a10_2_112) for presentation.** 

That's the end! Thank you!",b0e8d7c0,0.9888888888888889
45389,b0c2805cd5c087,eda49bdc,"* Kaggle Notebook Runner: Marília Prata @mpwolke
",0446f327,0.9888888888888889
45390,5f4ae633cfd090,49b66984,"So, after careful feature engineering and data visualization, the accuracy of our model increased by almost 30%.

Note : It should be considered that the model used as a baseline was a DummyClassifier and not a proper model.
",a30a16e2,0.989010989010989
45392,0caaec057f7184,05fdf622,"# Conclusion

Several analysis on training data are implemented. 

The types of test data (item, shop) have 3 kinds of condition
- having history data of the item in the shop
- not having data of the item in shop but in other shops
- not having any history data. 

Depending on the way we view the kind of data, the way we design the model varies. Note that there are 41% of data that are not having data of the item in shop but in other shops, and how to deal with them can also play an essential roles on predicting. 

In addition, special cases like negative price and sales are discussed. 

More analysis techniques and topics can be implemented in the future.",b875533e,0.989247311827957
45399,225b4fe5d3894a,c9eb3e70,### Do Upvote if you like :),4b4197b3,0.9896907216494846
45401,063a35f644e3c5,6abe631b,### So from above Two model we can see that XGBoost performs better...,1c30fb0a,0.9896907216494846
45402,8ec771f5600a61,84ed3254,## ALL OTHER CLASSIFIRES OVERFIT RESULTS ACCEPT SO WE ARE USING NEURAL NETWORK FOR OUR FINAL RESULTS,48364c1f,0.9896907216494846
45404,dbd96dd275dc60,37e787fc,"**Question to finish: Why knowing the feature importances of a trained machine learning model is helpful?**

Final challenge/extension: What other machine learning models could you try on our dataset? Hint: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html check out the regression section of this map, or try to look at something like CatBoost.ai or XGBooost.ai.",1ed493a8,0.98989898989899
45405,83df814455f06c,9762ae2b,[Go to Top](#0),c9cff71a,0.99
45407,4cd25e50c7e007,59234583,**Please do leave your comments /suggestions**,ceb0c525,0.99
45408,7cfd96218dd933,425dec51,"#### **ATTENTION**

PLEASE REVIEW THE RESEARCH DONE BEFORE THIS RESEARCH.

* www.kaggle.com/brsdincer/turkey-recent-wildfire-analysis-investigation",7c34d96c,0.9901960784313726
45409,842547b2def18c,cf60982c,"## References

This notebook has been created based on great work done solving the Titanic competition and other sources.

- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)
- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)
- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)",b8efde6d,0.9901960784313726
45410,71b75664517244,47a5dc60,"## End

I have put everything i could come up with in this dataest, hope you guys enjoy exploring this dataset, and i would really appreciate it if you would help me by upvoted it. Thank you for your time.

Have a good day kagglers",fc905af5,0.9901960784313726
45412,629f2918807a9b,e40c443c,"### Thats all for EDA of Gufhtugu Data set, Hope you have enjoyed whole notebook. I will cover prediction of number of orders or book name in my new notebook very soon Insha-ALLAH. Till then upvote this notebook if you liked it. Thankyou.*******",be56dc84,0.9901960784313726
45415,98a6794067932a,d9f79774,"# 5. Contribution des membres de l'équipe

Dans le cadre de ce travail, nous avons veillé à une répartition équitable des tâches. Dès les premières rencontres, lors du choix du sujet et de la définition de l’objectif du projet, chacun a partagé son expérience en Python et son niveau d’aisance avec le code. Nous souhaitions que chacun participe au travail, mais que les analyses demandées soient réalistes quant aux habilités de code de chacun. 

Dans notre équipe, deux membres étaient plus à l’aise avec le code que deux autres. Ainsi, nous avons confié le soin à ces deux coéquipiers de s’occuper du nettoyage de la base de données et de s’assurer que le fichier que nous allions utiliser pour poursuivre les analyses était « propre ». Afin d’équilibrer la charge de travail, nous avons convenu que les deux autres membres de l’équipe s’occuperaient de rédiger une plus grande partie du rapport. Une fois la base de données nettoyée et utilisable, nous avons convenu des analyses que nous souhaitions faire.  Nous avons tous les quatre réalisé quelques analyses communes pour nous assurer que nous avions les bons résultats et qu’il n’y avait pas d’incohérence dans les données.  Nous nous sommes ensuite réparti une ou deux sous-parties de l’analyse pour être en mesure d’identifier des tendances dans les commandes selon le type de client, les territoires desservis et les catégories de produits. 

Afin de ne pas nous gêner ou de casser les codes des uns et des autres, nous travaillions chacun sur des notebooks Kaggle distincts, mais nous nous étions partagé les accès afin de suivre les avancées de chacun et établir des liens entre nos analyses. Depuis la mi-session, nous nous rencontrions également chaque semaine pour mettre en commun notre travail, valider les analyses réalisées et nous partager le travail restant. Ces réunions ont été une force dans notre travail car elles permettaient que chacun présente ses lignes de code et en cas d’erreur ou de problèmes, nous pouvions nous entraider. Ceux plus à l’aise en code pouvaient aussi aider ceux moins à l’aise. De plus, comme nous faisions chacun nos analyses de notre côté, lors de la mise en commun nous pouvions nous partager des techniques et des astuces apprises en code ! 

Finalement pour la rédaction du rapport final, nous avons veillé à laisser des commentaires sur le code ou sous nos lignes de codes pour que tout le monde soit en mesure de comprendre le code et d’expliquer chaque partie du rapport. Chacun a ainsi rédigé une section du rapport les deux personnes n’ayant pas participé au nettoyage des données ayant pris les plus grosses parties du rapport.",08600fe2,0.9902912621359223
45417,99bf357eaf61f1,4349fd42,I am simply used random forest regressor and linear regressor..If you liked this notebook upvote it....Thanks for viewing!!!,9d92fafe,0.9903846153846154
45418,44f6a002ecd033,de10393a,From this we are predicting that the test dataset will have 305 accepted loans and 62 denied loans. Thinking back to the amount of loans that were approved and denied in the train dataset there were about 68% of applications that were accepted. With our model there are about 83% of applications that are accepted. This could be in part to either the fact that we are 80% accurate or the fact that there are simply more applicants in the test dataset that would qualify for a loan than in the train dataset.,70bbe106,0.9903846153846154
45419,7454fdc444df16,124ee886,"-
--

-


-

-


-

-

-

-

--

-

-

-

-

-


-

-

-

-

-

-

-

-

-

-

-


-

-


-

-

-

-

-

-

-

-

-

-


-",a7818ef5,0.9904761904761905
45425,43e60eb1362f5c,15a245c8,"We can see that maximum arrival Delays are dependent on the Departure Delays of the Airport. The first part is dealt with cleaning and exploration of the data set to get more insights and the second part dealt with the sitting of the model to predict the delays. For exploratory Analysis I used visualization tools like tableau and seaborn as well as matplotlib. The Second part dealt with the model fitting and predicting the Arrival delays of the airlines.

We can see that departure delay is the main problem which is creating Delay in the aviation industry. Departure Delays can be caused due Security Delay, Airline System Delays, Airlines Delay etc. The Delays affect the revenue of the company to a great extent so the delays has to be reduced as much as possible so as to increase the profitability in the Airline Industry. Customer Satisfaction will also be greatly enhanced if the delays can be brought down as low as possible.",87934234,0.9905660377358491
45427,c84925c8171900,1c466c1d,![](https://www.tinyprints.com/inspiration/wp-content/uploads/2019/02/thank-you-quotes-1-800x534.jpg),e21ff7ec,0.9906542056074766
45429,ab6da5994949a3,6e58b30d,"# Results :
| Classifier | Logistic Reg| SVC | K-NN | Naive Bayes | Decision Tree | Random Forest |
| --- | --- | --- | --- | --- | --- | --- |
| Train accuracy score | 0.9057 | 0.9289 | 0.9430 | 0.8980 | 1.0000 | 0.9991 |
| Average accuracy score | 0.9057 | 0.9281 | 0.9314 | 0.8982 | 0.8920 | 0.9288 |
| SD | 0.0097 | 0.0112 | 0.0097 | 0.0114 | 0.0128 | 0.0106 |
| Test accuary score | 0.9028 | 0.9258 | 0.9307 | 0.8966 | 0.9016 | 0.9295 |",fae6b91d,0.9907407407407407
45430,fdc3afd309b850,aebbe6dc,"<a id=""CON""></a>
# 11 Conclusion",966bde38,0.9907407407407407
45432,ac1abfe1dfe815,6b116eb2,"We was able to apply 7 diffrient models to to classify sentiment of tweets corresponding to various airlines. And the best model was `SVC` with acuracy =76. 

There're ,much more one can do, for example, process the location, better extract the words from the tweets. Dealing with tweets is not like any othersource of text. It's full of texting abbreviations and misspelled words. Some words like cool, would be writen in twitter as `COooooo1` and those words even if they belong to one class, it's not easy dealing with it. But the `SVC` performance was good.",6529dbcb,0.9911504424778761
45433,c9b4e282e4e2c1,55de3832,The injuries are related with ball_snap event.,f44d339f,0.9911504424778761
45436,fe7360cddc13e5,9f91d880,"<font color='red'>*Created with* ❤ *by Mustafa Batuhan Ermiş.*<font>
    
<font color='blue'>Resources:

2005 / Peter Fader

http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf",8979e423,0.9912280701754386
45440,2ada0305b68956,9fe8559b,### 170. Palette = 'winter_r',133e26f4,0.9914285714285714
45444,9169c4e9c33c90,dde1bd28,[Back to top](#Top),725bf880,0.9915254237288136
45445,1294fb4c86f993,696611fe,"<a id='conclusions'></a>
## Conclusions
<img src=""Images/conclusion.png"" align=""right"" width=""500"" height=""500"" /><br>
<b> From the above discussion we can conclude the followings:</b>
* Trend of `gun registration` shows some positive correlation with some `life threatening events` in state Louisiana and Texas (2005-2006 during `hurricane Katrina` and 2012 `The Sandy Hook school shooting`<br> <a href=""#Q5"">as discussed in Q5</a> and also seen in 9/11 event. 
* There is a positive correlation between the percent of `White population percent` in a state and the `registered guns per capita` in this state based upon statistics 2016 <br> <a href=""#Q6"">as discussed in Q6</a><br>
* In the majority of months, the registeration did not exceed 100,000. <a href=""#Q7"">as discussed in Q7</a><br>


<b> Notes:</b><span style=""color:red"">(Resubmission Note)</span><br>
The above conclusions were limited by the followings:<br>
1. Guns registration data file:
  * We can't be certain that the number of guns registered is the same as the ones sold as per Harvard study ___[here](https://github.com/BuzzFeedNews/nics-firearm-background-checks/blob/master/README.md)__ .
> A forthcoming study conducted by Harvard researchers found that roughly <b>40 percent</b> of respondents had acquired their most recent firearm <b><u>without</u></b> going through a background check.
  * The Data file contained information about 55 states(with non-official 5 states) . I omitted those data to match `census` dataframe.



2. Census Data:
  * <b><u>Only year 2016</u></b> was analysed. This is because it was the year that only carried some complete data about the race's percentage. Further investigation of other years and going through some other important factors like non-employment, income per capita, persons per square feet could make the conclusions more precise.
  * All footnotes were eliminated to get numerical data.
  * Data file was not consistent about expressing the percentage, sometimes with the symbol `%` and sometimes with decimal point. This was overcomed by data manipulation but it can affect the accuracy of the data.
 
",4471e513,0.9915254237288136
45446,4ae6a182abac64,5c25d24c,"- [How to Build a Machine Learning Model](https://towardsdatascience.com/how-to-build-a-machine-learning-model-439ab8fb3fb1)

- [How to find optimal parameters using GridSearchCV?](https://www.dezyre.com/recipes/find-optimal-parameters-using-gridsearchcv)",418676c5,0.9915966386554622
45450,e9b9663777db82,5b1c4a92,"After model evaluation we can conclude that, high dimensional data can not be fit well in low dimensional models and can give abrupt conclusions that may led to lower accuracy.

The Random Forest uses leafs that can reduce the dimensional complexity and generalize the model in a better approach. Thus it presents the best accuracy over the data.",648e8507,0.9917355371900827
45454,9ceb7278784462,3a61fed0,"## <a id='17'> <font size=""+2"" color=""LIGHTSEAGREEN""><b>Reference</b></font><br>
* https://seaborn.pydata.org/api.html
* https://plotly.com/python/
* https://stackoverflow.com 
* https://www.theanalysisfactor.com/missing-data-mechanism/ <br>
* https://www.statisticssolutions.com/missing-values-in-data/ <br>
* https://www.displayr.com/missing-data-handle/ <br>
* https://pandas.pydata.org/pandas-docs/stable/reference/index.html <br>
* https://www.displayr.com/what-is-a-correlation-matrix/ <br>
    
* https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15 <br>
* https://r4ds.had.co.nz/exploratory-data-analysis.html#introduction-3  <br>    
    
    ",3768a567,0.9919354838709677
45456,0932046e1f485d,eb08bb70,"There are of course a lot more things that can be done with the data and this EDA only covers a small segment of it.

Thank you for your time. Please let me know if it helped you in any way with your analysis and constructive feedback is always welcome.",218cc7a3,0.9921875
45458,d07915a6e6992e,f45d7bb7,"**If you liked this notebook and found this notebook helpful, Please upvote and leave a comment**
![Good%20Bye.png](attachment:Good%20Bye.png)",2b912140,0.9923076923076923
45460,a4f8ad33c823c5,fc58884e,"# References

This is my first time participating in a kaggle competition. There were some good resources I referenced to and have provided the link to them below. 

* https://www.kaggle.com/scarecrow2020/wids-whole-pipeline-eda-clean-fe-grids-xgboost 
* https://www.kaggle.com/iamleonie/wids-datathon-2021-diabetes-detection

# Future work 

I hope to explore the below questions in detail further as I continue to work on this dataset.
* Are there certain age groups more vulnerable to diabetics?
* People with certain bmi vulnerable to diabetics? categorise them
* Intepret blood pressure results categorise them ",fcd48307,0.9923076923076923
45461,ee23a565163388,e62da3d9,"I'll update this notebook in regular intervals with more interesting insights and sophisticated models. The points mentioned in the further steps can be carried out by yourself, which can help you gain more knowledge in predictive modelling.

Hope you enjoyed reading this notebook. Please upvote and leave your comments if you like my work. Thanks!",88aacbc4,0.9923664122137404
45462,ba4b3bd184acbb,08c5e7a7,"# Conclusion

Congrats! You know enough now to perform your own analysis using Pandas!

Here are some ideas for furthur analysis of this dataset to get your feet wet:
- Find the highest rated app for each content rating
- Which app has the largest revenue (approximately)?
- Make a generator that outputs a random review for a given app

Working with these ideas will get you into the mindset of using Pandas, but I think you are ready to take it to the next level!

The following resources will be useful as you continue using Pandas:
- [Datasets from Kaggle](https://www.kaggle.com/datasets)
- [Documentation and more Tutorials](https://pandas.pydata.org/docs/)
- [Troubleshooting with Stackoverflow](https://stackoverflow.com/questions/tagged/pandas)

Good luck and happy coding!",0f5de724,0.9924812030075187
45463,20b372b6e4e276,321c4468,[Go to Top](#0),ec8b0860,0.9925373134328358
45466,c80939c7c626cf,cf840141,# This is the accuracy and the data we have obtained from the Dirty Data ,b9ac31e2,0.9927007299270073
45467,3c2033cc99c12c,375f2dae,"<b>*The process of EDA and Fraud Detection on the dataset is enjoyble and meaningful, during which I get some insight in data processing and analysis, in the following part I will summarize the points and finish my analysis.*<b> 
+ **Data Exploration:** 
> In this part, i have done the Null_value detection, Duplicate_value dropping, Outlier_detection, Min-Max scaler, Balancing data combating. As the most significant part before the machine learning algorithm, the data cleaning and preprocessing process make the data distribution more smooth and achieve the purpose of removing duplicate information, correcting existing errors, and providing data consistency.  
>>**Outlier detection with imbalanced data:** From previous experiemnt, I found that in most case the Fraud data has relatively larger variance and distribution range, so I choose to not apply the outlier dectection function(IQR standard) to the Fraud part of the data. If I do this, the amount of data will be cut to only 240 rows, one point I should admit is that the outlier feature of data are more likely to contribute more to the Fraud probability.  
>>**Under Sampling:** The UnderSampling method is a way to cambat the data imbalance, but the cost is that the Under Sampling method would loss a large amount of Normal data, so for those deep learning method, the UnderSampling is a disaster to information loss. In comparison, the SMOTE method could be better.
+ **Data Visualization:**
> In this part, I visualize the distribution of all the features and seek the correlation between the features and Class, besides, i also summarize the statistical values of some features.
>> **Distribution difference between classes:** In general, I found that the fraud data has ralatively larger variance and the range of the min and max in most cases larger than the normal part, that is mainly due to that there exits some unnormal feature values causing the variance larger.  
>> **The connection between distribution and correlation:** From the experiment, I found that those features having similar distribution usually not so significant in the correlation.   
>> **Time data distribution:** From the experiment, we could find that fraud class data are more likely to lie in the range from 2 to 9, which is the dawn time in day, so maybe this time period makes it more easy for credit card fraud.

+ **Dimension Reduction:**
> In this part, I used three famous dimension reduction algorithm to test the performace--PCA, SVD ,T-SNE
>>**T-SNE's best performance:**Among nearly all the dimension reduction algorithms, the T-SNE methods generally has the best performance, with randomly sampling from stochastic t-distribution, the running time of T-SNE is usually much longer than PCA, so this algorithm is mainly used in the visualization, which could better help us get the insight of potential correlated features.

+ **Prediction with machine learning methods:** 
> In this part, I make use of some machine learning algorithm to predict the reuslt of credit cards, the Logistic Regression method and Support Vector Machine method both have an accuracy of 93.3%.
>>**Validation Curve and Training Performance:** From the above experiment we could find that the validation set initially sperate from training score and gradually approaching it, which means after iteration, the validation set could become increasingly accurate in representing the performance.  
>>**The difference between validation and testing:** From the experiement we could find that in general the result of validation set is usually better than the result of testing set, I think it mainly lies on that the distribution of validation set is similar to the training set, which may cause a liitle overfitting problem, but even in this case, the validation set plays an important role in modifying the model and choose those with better performance without bias.  
    >>**Comparison between two algorithms:** Although the performance of two statistical method in this case is similar, I am also interested in the different applying fields of these two. The principle of SVM is to only consider support vectors, that is, the few points most relevant to classification, to learn the classifier. Logistic regression greatly reduces the weight of points far away from the classification plane through nonlinear mapping, and relatively increases the weight of the data points most relevant to the classification.  

+ **Choice of deep learning and statistical learning**
>In this part, I design a nerual network to test its adaptility to imbalanced data, however in this case the neural network did not perform perfectly as I expected, in this case, If we put all the data into the neural network, it will set all the parameters 0 so that it could satisfy the requirement of most data, however, when encountered with fraud data, it could not change the weight too much. So I can only use undersampling method to combat with imbalance. The deep learning method finally get a result of 92%, a little lower than statistical learning method. 
>> **In what situation DL could defeat traditional algorithms:** Deep learning could perform better when the dataset has large amount of data and different classes of data are balanced. Besides, deep learning could better simulate the humanbeing behaviour like images recognition and self-driving, where traditional algorithm could not be applied.  
>>**The disadvantages of DL:** The training of Deep Learning method is much more difficult than traditional algorithms, it usually requires high-performance GPU to do the multi-thread calculation, also, it is also time-consuming in tunning the hyperparameters like learning rate, momentum or batch-features. ",dfa22a54,0.9927007299270073
45470,0e2a23fbe41ca9,43d3718a,"Following the same pattern as the historical data, we will only discuss id columns here. Col ```merchant_id``` has 26,216 nulls which is 0.013% of the data.",64e4762c,0.9927536231884058
45472,56785caebaa256,8f2c77e7,## [Go to Top](#0),a792961a,0.9929078014184397
45473,957e035ba5b9d5,3729c222,"# Conclusion

* For sure we want to use pre-trained networks where we can
* Being able to use train_test_split seems to me a best option even so we are dumping everything into memory.
* In regards to the images the *drawings* and *engraving* are close together in style and therefore have some overlap in results.",778ab3d3,0.9929078014184397
45474,b61ab8f81dc03d,77a47dac,"<a id=""references_credits""></a>
# References and Credits
This notebook has been created based on great work done solving the Titanic competition and other sources.



[Complete Machine Learning and Data Science: Zero to Mastery](https://www.udemy.com/course/complete-machine-learning-and-data-science-zero-to-mastery/) by [Daniel Bourke](https://www.mrdbourke.com/)

[Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic)

[Predicting the Survival of Titanic Passengers](https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8)

[Titanic - Hyperparameter tuning with GridSearchCV](https://www.kaggle.com/ihelon/titanic-hyperparameter-tuning-with-gridsearchcv)

[Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions)

[Titanic Wikipedia](https://en.wikipedia.org/wiki/Titanic)",64d05394,0.9929078014184397
45475,eda49464dd6d1b,1a067281,"<img src=""https://i.pinimg.com/originals/e2/d7/c7/e2d7c71b09ae9041c310cb6b2e2918da.gif"">",8421f81f,0.993006993006993
45476,738bfced935b69,f90f5cd2,"As the years progress, we notice that the prices of cars increase and are directly proportional to the sizes of the engines, as the greater the size of the engine, the higher the price of the car and inversely proportional to the number of miles, as the more it increases, the price of the car decreases. All of this also depends on the type of transmission if manual or automatic or semi-auto or other.",2d3c592d,0.9931506849315068
45477,fdc9f4863744b1,2d88e42c,"Conclusion:

<ul>
    <li>Dependent Variable is 'SALE PRICE'</li>
    <li>Predictive variables are 'LAND SQUARE FEET', GROSS SQUARE FEET', 'BOROUGH'</li>
    <li>Predictive Models to be used - Single Linear or Multi Linear Regression</li>
    ",b4529365,0.9931506849315068
45479,726833f92fb87a,53ac9ad1,**Thanks for reading my notebook ! Let me know if you have questions or if you want me to check out your works !!**,7dc5e1b6,0.9932885906040269
45484,917957c6c4065f,6497818d,"지금까지 인기 동영상들의 전반적인 상황을 살펴봤습니다.  
분석의 목적인 ""채널의 방향을 어떻게 잡아야 인기 동영상에 갈 수 있나""를 고려했을 때, 시사점은 다음과 같습니다.  

시사점
1. 인기동영상에 게시된 동영상들은 조회수 20만 이하의 영상이 대부분이고, 조회수가 가장 적은 영상은 2,050이다.   
2. 3일 이내에 인기동영상이 되지 못하면, 인기 동영상이 되기 힘들다고 보인다.
3. 인기 동영상의 70%는 Entertainment, News & Politics, People & Blogs의 카테고리에 속한다.
4. Entertainment 카테고리 동영상을 다루는 채널들은, 채널 별 인기동영상의 개수를 기준으로 보면, 모든 범위에서 많은 비중을 차지한다.
4. 참고: 좋아요와 댓글수를 늘릴 때 상품이 큰 영향을 준다.  

전략
1. 채널은 Entertainment, News & Politics, People & Blogs의 3개 카테고리의 동영상을 다뤄야하며, 그 중에서 Entertainment 카테고리가 가장 유리함.   
2. 태그 개수는 조회수와 크게 상관이 없는 것으로 보이나, 대부분의 인기동영상은 10개 정도 사용함.  
3. 제목의 길이는 조회수와 크게 상관이 없는 것으로 보이나, 대부분의 인기동영상은 10~60자 정도의 제목을 사용함.  
4. 설명의 길이는 조회수와 크게 상관이 없는 것으로 보이나, 대부분의 인기동영상은 500자 이내의 설명을 사용함.
5. 3일 이내로 조회수 최소 2,000을 넘기지 못했다면 다음 동영상을 준비해야함. 
6. 돈이 된다면 광고를 하더라도 인기 동영상에 선정 될 수 있음. ",55b8ed68,0.9934640522875817
45485,fc8e0042411c46,fa004c20,# Final Observation:,af476c2a,0.9937304075235109
45487,6a80f915608fc2,74c9e832,"## <a id=""TheEnd"">The End</a>
Back to <a href=""#Index"">Index</a>",636938eb,0.9940476190476191
45488,30fdc4a6e3c1db,ce3d0f08,**Please Upvote my notebook if you like it** 🙏,6111ddee,0.9941520467836257
45489,5ce12be6e7b90e,199bd186,"# Colophon
This notebook was written by [Yoav Ram](http://python.yoavram.com) and is part of the [_Scientific Computing with Python_](https://scicompy.yoavram.com/) course at IDC Herzliya.

The notebook was written using [Python](http://python.org/) 3.6.5.
Dependencies listed in [environment.yml](../environment.yml).

This work is licensed under a CC BY-NC-SA 4.0 International License.

![Python logo](https://www.python.org/static/community_logos/python-logo.png)",c0ab62dd,0.9941520467836257
45492,396bc36edb95d3,23986ca1,-----HAPPY LEARNING-----,965e4f8f,0.9944444444444445
45494,fdc3afd309b850,dd8b3b47,"The main idea of this notebook was accomplished.  We deal with a relatively small data set extracted with Web Scraping and with hard work in the feature engineering, were able to build a very good predictor of prices for apartments in Brasília. 
That was my first regression project and I really wanna know what you guys think about it.

<h3>If you like the notebook, please upvote and leave your feedback in the comments' section !</h3>

<br>
That's it for now.

<br>
<br>

Cheers 🍀

<br>
<br>
<br>
<br>







",966bde38,0.9953703703703703
45495,9ceb7278784462,58e3d062,"<font size=""+2"" color=""LIGHTSEAGREEN""><b>My Other Kernels</b></font><br>

<a href=""https://www.kaggle.com/drfrank/loan-data-visualisation-eda-machine-learning"" class=""btn btn-primary"" style=""color:white;"">Loan Data Visualisation & EDA & Machine Learning</a>

<a href=""https://www.kaggle.com/drfrank/lego-transfer-cnn-classification"" class=""btn btn-primary"" style=""color:white;"">Lego Transfer-CNN Classification</a>

<a href=""https://www.kaggle.com/drfrank/face-image-classification"" class=""btn btn-primary"" style=""color:white;"">Face Image Classification</a>

<a href=""https://www.kaggle.com/drfrank/book-review-ratings-data-analysis-visualization"" class=""btn btn-primary"" style=""color:white;"">Book Review Ratings Analysis & Visualization</a>

<a href=""https://www.kaggle.com/drfrank/insurance-prediction-lgbm-gbm-xgboost-eda"" class=""btn btn-primary"" style=""color:white;"">Insurance Prediction- LGBM,GBM,XGBoost EDA</a>

<a href=""https://www.kaggle.com/drfrank/fish-market-data-visualisation-machine-learning"" class=""btn btn-primary"" style=""color:white;"">Fish Market Data Visualisation & Machine Learning</a>

<a href=""https://www.kaggle.com/drfrank/seabron-plotly-for-beginners"" class=""btn btn-primary"" style=""color:white;"">Seabron & Plotly For Beginners</a>

<a href=""https://www.kaggle.com/drfrank/basketball-players-stats-data-visualisation"" class=""btn btn-primary"" style=""color:white;"">Basketball Players Stats Data Visualisation</a>

<a href=""https://www.kaggle.com/drfrank/women-s-football-results-visualization"" class=""btn btn-primary"" style=""color:white;"">Women's Football Results Visualization</a>

<a href=""https://www.kaggle.com/drfrank/us-police-shootings-data-visualisation"" class=""btn btn-primary"" style=""color:white;"">Us Police Shootings Data Visualisation</a>",3768a567,0.9959677419354839
45497,b10bd75889dad9,4e0dd7ad,"#### The Top 8 predictor variables are :

+ total_ic_mou_8     ->         -3.3163
+ last_day_rch_amt_8 ->         -0.4209
+ total_og_mou_8     ->         -0.7451
+ av_rech_amt_data_8 ->         -0.3218
+ loc_ic_t2m_mou_8   ->          1.3164
+ arpu_8             ->          0.2832
+ loc_og_mou_8       ->         -0.1834
+ fb_user_8_1.0      ->         -1.7700",ee00ceee,0.9966666666666667
45499,2ada0305b68956,8835b75f,# Thank You :),133e26f4,0.9971428571428571
